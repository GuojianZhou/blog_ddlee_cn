{"meta":{"title":"萧爽楼","subtitle":"李家丞","description":"我叫李家丞，现在在同济大学念书，学的是数学。博客的内容主要包括个人的生活、读书感想，也包括数学、互联网、编程语言、人工智能等方面的学习侧记。","author":"ddlee","url":"http://blog.ddlee.cn"},"pages":[{"title":"About","date":"2016-07-30T13:19:00.000Z","updated":"2017-08-10T14:04:17.948Z","comments":true,"path":"About/index.html","permalink":"http://blog.ddlee.cn/About/index.html","excerpt":"","text":"呦呦鹿鸣，食野之苹。我有嘉宾，鼓瑟吹笙。 每个人都有隐藏的一隅。 一个人，安静下来，想一些没那么重要又如此重要的一些人，做一些喜欢又没那么喜欢的事儿，在烦恼与忙碌终于远去的时光的罅隙里。 有人说，照片是凝固了的时光。所以很多人着迷于摄影，用镜头记录想要回味和分享的瞬间。 有人说，凝固了的时光不好，流动的时光才有趣。所以每天拍一段几秒的视频，过个三年五载，再将它们串联起来，像串珍珠那样。 而我觉得，文字记录的时光，却是每时每刻都与现在流淌着的日子融合了的。 每次重读一行行文字，便是一趟联接着当下、过去，还有未来的心灵旅程。 文字太模糊了，不足以让你记起全部的细节，你只好亲自走回去，回到青葱的岁月，像看故事里的人物那样看看那时的自己； 文字又太准确了，这些陌生又熟悉的字眼，这些亲切又早已忘却的句子，就像当年的自己把故事亲口哼唱给你听，此时此刻，恰如彼时彼刻。 文字脆弱，却又力量无穷。 关于博客博客叫萧爽楼，从《浮生六记》中借来的名字。身边，很难寻找古人春游、结社、集会之乐趣。只愿文字在这个小小的空间里仍然能够发挥古老的力量。 博客的内容主要包括个人的生活、读书感想，也包括数学、互联网、编程语言、科学计算等方面的学习侧记。 关于我我现在在同济念书，学的是数学。对统计学习、数据分析、传播学、个人管理等感兴趣。摄影控，喜欢游泳和长跑。 我非常崇敬古代文人的这一人生理想： 为天地立心 为生民立命 为往圣续绝学 为万世开太平 在这里(右击复制RSS源)订阅我的博客。 更多关于我的情况请访问我的主页。 我也在写一个生活化的博客丞喃旧事，关于我这个生于96，长于00和10年代的男生的微不足道的故事，您有兴趣也可以看看。"},{"title":"tags","date":"2017-07-18T12:50:20.581Z","updated":"2017-07-01T16:59:42.000Z","comments":true,"path":"tags/index.html","permalink":"http://blog.ddlee.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"[论文笔记]Going deeper with convolutions","slug":"论文笔记-Going-deeper-with-convolutions","date":"2018-01-06T12:39:58.000Z","updated":"2018-01-06T12:39:58.055Z","comments":true,"path":"2018/01/06/论文笔记-Going-deeper-with-convolutions/","link":"","permalink":"http://blog.ddlee.cn/2018/01/06/论文笔记-Going-deeper-with-convolutions/","excerpt":"","text":"Overview本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的GoogLeNet拿下了ILSVRC14分类任务的头名。文章也探讨了网络在不断加深的情况下如何更好地利用计算资源，这一理念也是Inception系列网络的核心。 MotivationInception单元的启发主要来自Network in Network结构和Arora等人在神经科学方面的工作。 提高深度模型的一个简单想法是增加深度，但这样带来过拟合的风险和巨大的计算资源消耗，对数据量和计算力的要求可能会超过网络加深带来的收益。 解决这些问题的基本思路是使用稀疏连接的网络，而这也跟Arora等人工作中的Hebbian principle吻合：共同激活的神经元常常集聚在一起。换句话说，某一层激活的神经元只向下一层中特定的几个神经元传递激活信号，而向其他神经元几乎不传递信息，即仅有少部分连接是真正有效的，这也是稀疏的含义。 然而另一方面，现代计算架构对稀疏的计算非常低效，更适合的是密集的计算，这样便产生了矛盾。而Inception单元的提出就是为了用密集的结构来近似稀疏结构，在建模稀疏连接的同时又能利用密集计算的优势。 很多文章认为inception结构的意义在于将不同大小核的卷积并行连接，然后让网络自行决定采用哪种卷积来提取特征，有些无监督的意味，然后将1×1的卷积解释为降维操作。这种想法有待验证，是否在5×5卷积有较强激活的时候，3×3卷积大部分没有激活，还是两者能够同时有较强的激活？不同的处理阶段这两种卷积核的选择有没有规律？ 在此提出一个个人的理解，欢迎讨论。 首先是channel的意义。我们知道，卷积之所以有效，是因为它建模了张量数据在空间上的局部相关性，加之Pooling操作，将这些相关性赋予平移不变性（即泛化能力）。而channel则是第三维，它实际上是卷积结构中的隐藏单元，是中间神经元的个数。卷积层在事实上是全连接的：每个Input channel都会和output channel互动，互动的信息只不过从全连接层的weight和bias变成了卷积核的weight。 这种全连接是冗余的，本质上应是一个稀疏的结构。Inception单元便在channel这个维度上做文章，采用的是类似矩阵分块的思想。 根据Hebbian principle，跨channel的这些神经元，应是高度相关的，于是有信息压缩的空间，因而使用跨channel的1×1的卷积将它们嵌入到低维的空间里（比如，Inception4a单元的输入channel是512，不同分支的1×1卷积输出channel则是192,96,16和64，见下面GoogLeNet结构表），在这个低维空间里，用密集的全连接建模（即3×5和5×5卷积），它们的输出channel相加也再恢复到原来的输入channel维度（Inception4a分别是192+208+48+64），最后的连接由Concat操作完成（分块矩阵的合并），这样就完成了分块密集矩阵对稀疏矩阵的近似。 这样来看，3×3和5×5大小的选择并不是本质的，本质的是分块低维嵌入和concat的分治思路。而在ResNeXt的工作中，这里的分块被认为是新的维度（称为cardinality），采用相同的拓扑结构。 Stacked Inception Module在GoogLeNet中，借鉴了AlexNet和VGG的stack(repeat)策略，将Inception单元重复串联起来，构成基本的特征提取结构。 Dimension Reduction朴素版本的Inception单元会带来Channel维数的不断增长，加入的1×1卷积则起到低维嵌入的作用，使Inception单元前后channel数保持稳定。 Auxililary Classifier这里是本文的另一个贡献，将监督信息传入中间的feature map，构成一个整合loss，作者认为这样有助于浅层特征的学习。 Architecture of GoogLeNet下面的表显示了GoogLeNet的整体架构，可以留意到Inception单元的堆叠和Channel数在子路径中的变化。NetScope可视化可参见GoogLeNet Vis。源文件位于NN_Structures/caffe_vis/。 Conclusion文章是对NiN思想的继承和推进，不同于AlexNet和VGG，网络的模块化更加凸显，多路径的结构也成为新的网络设计范本，启发了众多后续网络结构的设计。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"Neural Network","slug":"Neural-Network","permalink":"http://blog.ddlee.cn/tags/Neural-Network/"}]},{"title":"[论文笔记]Speed/accuracy trade-offs for modern convolutional object detectors","slug":"论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors","date":"2017-12-24T13:55:22.000Z","updated":"2017-12-27T14:02:35.092Z","comments":true,"path":"2017/12/24/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/","link":"","permalink":"http://blog.ddlee.cn/2017/12/24/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/","excerpt":"","text":"https://arxiv.org/abs/1611.10012 Introduction这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。 文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了Proposal个数、输入图片尺寸，生成Feature map的大小等作为超参，并行实验，探索精度和速度方面的trade-off。 文章也将源码公开，作为Tensorflow的Object Detection API。 下图是三种元结构的图示： Experiments 信息量非常大的一张图。 横纵两个维度分别代表速度和准确度，横轴越靠左说明用时越少，纵轴越靠上说明mAP表现越好，因而，sweet spot应分布在左上角 两个超维是元结构和特征提取网络，元结构由形状代表，特征提取网络由颜色代表 虚线代表理想中的trade-off边界 分析： 准确度最高的由Faster-RCNN元结构、Inception-ResNet提取网络，高分图片，使用较大的feature map达到，如图右上角 较快的网络中准确度表现最好的由使用Inception和Mobilenet的SSD达到 sweet spot区特征提取网络由ResNet统治，较少Proposal的Faster-RCNN可以跟R-FCN相当 特征提取网络方面，Inception V2和MobileNet在高速度区，Incep-ResNet和ResNet在sweet spot和高精度区，Inception V3和VGG则远离理想边界（虚线） 上图是特征提取网络对三种元结构的影响，横轴是特征提取网络的分类准确率，纵轴是检测任务上的mAP表现，可以看到，SSD在纵轴方向上方差最小，而Faster-RCNN和R-FCN对特征提取网络更为敏感。 上图的横轴是不同的特征提取网络，组内是三种元结构的对比，纵轴是不同尺寸物体的mAP。 可以看到，在大物体的检测上，使用较小的网络时，SSD的效果跟两阶段方法相当，更深的特征提取网络则对两阶段方法的中型和小型物体的检测提升较大（ResNet101和Incep-ResNet都显现了两阶段方法在小物体上的提升） 上图显示了输入图片尺寸对mAP的影响。高分的图片对小物体检测帮助明显，因而拥有更高的精度，但相对运行速度会变慢。 上图探究了两阶段方法中Proposal个数的影响，左边是Faster-RCNN，右边是R-FCN，实线是mAP，虚线是推断时间。分析： 相比R-FCN，Faster-RCNN推断时间对Proposal个数相当敏感（因为有per ROI的计算） 减少Proposal的个数，并不会给精度带来致命的下降 上面两图是对FLOPS的记录，相对GPU时间更为中立，在图8中，GPU部分显现了ResNet跟Inception的分野（关于45度线，此时FLOPS跟GPU时间相当），文章认为分解操作(Factorization)减少了FLOPs，但增加了内存的IO时间，或者是GPU指令集更适合密集的卷积计算。 上两图是对内存占用的分析，总体来说，特征提取网络越精简、feature map尺寸越小，占用内存越少，运行时间也越短。 最后，文章描述了他们ensemble的思路，在一系列不同stride、loss和配置的Faster-RCNN中（ResNet和Incep-ResNet为特征提取网络），贪心地选择验证集上AP较高的，并且去除类AP相似的模型。选择的5个用于ensemble的模型如下： Conclusion这篇文章是不错的实验结果报告，测试了足够多的模型，也得出了合理的和有启发的结论。几点想法： RFCN并没有很好的解决定位跟分类的矛盾，per ROI的子网络最好还是要有，但要限制Proposal的个数（实际大部分都是负样本）来减少冗余 小物体的检测仍然是最大的难点，增大分辨率和更深的网络确有帮助，但不是实质的。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]Light-Head R-CNN: In Defense of Two-Stage Object Detector","slug":"论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector","date":"2017-12-22T13:55:36.000Z","updated":"2017-12-27T14:00:34.351Z","comments":true,"path":"2017/12/22/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/","link":"","permalink":"http://blog.ddlee.cn/2017/12/22/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/","excerpt":"","text":"https://arxiv.org/abs/1711.07264 Introduction文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。 针对这两种元结构(Faster-RCNN和RFCN)，文章提出了“头”轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。 Light-Head R-CNN 如上图，虚线框出的部分是三种结构的RCNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为sperable convolution，产生的Score map也减少至10×p×p（相比原先的#class×p×p）。 一个可能的解释是，“瘦”（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较“厚”的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。 进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。 light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。 粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。 Experiments实验部分，文章验证了较“瘦”的Score map不会对精度产生太大损害，也展现了ROI Align, Multiscale train等技巧对基线的提升过程。 文章的主要结果如下面两图（第一个为高精度，第二个为高速度）： 只能说这样的对比比较诡异。 第一张图中三个light-head结果并不能跟上面的其他结构构成多少有效的对照组，要么scale不同，要么FPN, multi-scale, ROI Align不同。唯一的有效对照是跟Mask-RCNN。 在高精度方面，基础网络不同，采用的scale也不同，没有有效的对照组。 Conclusion我并不觉得这是对两阶段方法的Defense。文章对两阶段方法在精度和速度方面的分析比较有见地，但实验的结果并不能可靠地支撑light-head的有效性。相比之下Google的那篇trade-off可能更有参考价值。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]You Only Look Once: Unified, Real Time Object Detection","slug":"论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection","date":"2017-12-20T13:38:31.000Z","updated":"2017-12-27T14:03:40.021Z","comments":true,"path":"2017/12/20/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/","link":"","permalink":"http://blog.ddlee.cn/2017/12/20/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/","excerpt":"","text":"https://arxiv.org/abs/1506.02640 IntroductionYOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。 YOLO的主要优点： 快。 全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。 泛化性能好，在艺术作品上做检测时，YOLO表现好。 DesignYOLO的大致工作流程如下：1.准备数据：将图片缩放，划分为等分的网格，每个网格按跟ground truth的IOU分配到所要预测的样本。2.卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IOU表示）。测试时，分数如下计算： 等式左边第一项由网格预测，后两项由每个box预测，综合起来变得到每个box含有不同类别物体的分数。因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，S为网格数，B为每个网格生成box个数，C为类别数。3.后处理：使用NMS过滤得到的box Loss 图片来自https://zhuanlan.zhihu.com/p/24916786 损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，loss中添加了权重并将长宽取根号。 Error Analysis 相比Fast-RCNN，YOLO的背景误检在错误中占比重小，而位置错误占比大（未采用log编码）。 LimitationsYOLO划分网格的思路还是比较粗糙的，每个网格生成的box个数也限制了其对小物体和相近物体的检测。 ConclusionYOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]SSD: Single Shot MultiBox Detector","slug":"论文笔记-SSD-Single-Shot-MultiBox-Detector","date":"2017-12-12T13:37:56.000Z","updated":"2017-12-27T13:40:01.860Z","comments":true,"path":"2017/12/12/论文笔记-SSD-Single-Shot-MultiBox-Detector/","link":"","permalink":"http://blog.ddlee.cn/2017/12/12/论文笔记-SSD-Single-Shot-MultiBox-Detector/","excerpt":"","text":"https://arxiv.org/abs/151.023325 IntroductionSSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保持较快的运行速度。 SSD 多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。 更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。 ConclusionSSD有点像多分类的RPN，生成anchor box，再对box预测分数和位置调整值。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]Feature Pyramid Networks for Object Detection","slug":"论文笔记-Feature-Pyramid-Networks-for-Object-Detection","date":"2017-12-07T13:55:59.000Z","updated":"2017-12-27T14:00:23.112Z","comments":true,"path":"2017/12/07/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/","link":"","permalink":"http://blog.ddlee.cn/2017/12/07/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/","excerpt":"","text":"https://arxiv.org/abs/1612.03144 Introduction对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。 回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。 一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的内存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。 本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。 Feature Pyramid Networks 从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行元素位置上的叠加（lateral connection），构成这一深度上的特征。 这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。 Experiments文章的主要实验结果如下： 对比不同head部分，输入feature的变化对检测精度确实有提升，而且，lateral和top-down两个操作也是缺一不可。 Conclusion特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。 个人方面，FPN跟multi-scale的区别在哪，还值得进一步探索。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]R-FCN: Object Detection via Region-based Fully Convolutinal Networks","slug":"论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks","date":"2017-12-07T13:55:48.000Z","updated":"2017-12-27T14:04:15.263Z","comments":true,"path":"2017/12/07/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/","link":"","permalink":"http://blog.ddlee.cn/2017/12/07/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/","excerpt":"","text":"https://arxiv.org/abs/1605.06409 Introduction文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等），这一点是由分类任务的平移不变性和检测任务的平移敏感性之间的矛盾导致的。换句话说，检测模型采用了分类模型的特征提取器，丢失了位置信息。这篇文章提出采用“位置敏感分数图”的方法解决这一问题。 Position-sensitive score maps &amp; Position-sensitive RoI Pooling位置敏感分数图的生成有两个重要操作，一是生成更“厚”的feature map，二是在RoI Pooling时选择性地输入feature map。 Faster R-CNN中，经过RPN得到RoI，转化成分类任务，还加入了一定量的卷积操作（ResNet中的conv5部分），而这一部分卷积操作是不能共享的。R-FCN则着眼于全卷积结构，利用卷积操作在Channel这一维度上的自由性，赋予其位置敏感的意义。下面是具体的操作： 在全卷积网络的最后一层，生成k^2(C+1)个Channel的Feature map，其中C为类别数，k^2代表k×k网格，用于分别检测目标物体的k×k个部分。即是用不同channel的feature map代表物体的不同局部（如左上部分，右下部分）。 将RPN网络得到的Proposal映射到上一步得到的feature map（厚度为k×k×(C+1)，）后，相应的，将RoI等分为k×k个bin，对第(i,j)个bin，仅考虑对应(i,j)位置的(C+1)个feature map，进行如下计算：其中(x0,y0)是这个RoI的锚点，得到的即是(i,j)号bin对C类别的相应分数。 经过上一步，每个RoI得到的结果是k^2(C+1)大小的分数张量，k×k编码着物体的局部分数信息，进行vote（平均）后得到(C+1)维的分数向量，再接入softmax得到每一类的概率。 上面第二步操作中“仅选取第(i, j)号feature map”是位置信息产生意义的关键。 这样设计的网络结构，所有可学习的参数都分布在可共享的卷积层，因而在训练和测试性能上均有提升。 小结R-FCN是对Faster R-CNN结构上的改进，部分地解决了位置不变性和位置敏感性的矛盾。通过最大化地共享卷积参数，使得在精度相当的情况下训练和测试效率都有了很大的提升。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]MegDet: A Large Mini-Batch Object Detector","slug":"论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector","date":"2017-11-21T15:30:56.000Z","updated":"2017-11-21T15:44:58.572Z","comments":true,"path":"2017/11/21/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/","link":"","permalink":"http://blog.ddlee.cn/2017/11/21/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/","excerpt":"","text":"本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。 Object Detection Progress Summay检测方法回顾：R-CNN, Fast/Faster R-CNN, Mask RCNN, RetinaNet(Focal Loss), ResNet(backbone network), 文章先指出前述方法大多是框架、loss等的更新，而均采用非常小的batch（2张图片）训练，有如下不足： training slow fails to provide accurate statistics for BN 这里涉及一个问题，检测任务的源数据，到底应该是图片还是标注框。在Fast R-CNN中，RBG提到SPPNet等每个batch采样的标注框来自不同的图片，之间不能共享卷积运算（卷积运算是以图片为单位的）。为了共享这部分计算，Fast R-CNN采用了“先选图片，再选标注框”的策略来确定每个batch，文章提到这种操作会引入相关性，但在实际中却影响不大。之后的Faster R-CNN，每张图片经过RPN产生约300个Proposal，传入RCNN做法也成了通用做法。 个人认为检测任务的数据，应该是以图片为单位的。物体在图片的背景中才会产生语义，而尽管每张图片有多个Proposal（近似分类任务中的batch大小），但它们共享的是同一个语义（场景），而单一的语义难以在同一个batch中提供多样性来供网络学习。 困境Increasing mini-batch size requires large learning rate, which may cause discovergence. 解决方案 new explanation of linear scaling rule, introduce “warmup” trick to learning rate schedule Cross GPU Batch Normalization(CGBN) ApproachVariance Equivalence explanation for Linear Scaling Rulelinear scaling rule 来自更改batch size 时，同时放缩learning rate，使得更改后的weight update相比之前小batch size， 多步的weight update类似。而本文用保持loss gradient的方差不变重新解释了linear scaling rule，并指出这一假定仅要求loss gradient是i.i.d，相比保持weight update所假设的不同batch size间loss gradient相似更弱。 参见Accurate Large Minibatch SGD: Training ImageNet in One Hour，近似时假设了求和项变化不大，这一条件在Object Detection中可能不成立，不同图片的标注框（大小、个数）差别很大。 WarmUp Strategy在训练初期，weight抖动明显，引入warmup机制来使用较小的学习率，再逐渐增大到Linear scaling rule要求的学习率。 Cross-GPU Batch NormalizationBN是使深度网络得以训练和收敛的关键技术之一，但在检测任务中，fine-tuning阶段常常固定了SOTA分类网络的BN部分参数，不进行更新。 检测中常常需要较大分辨率的图片，而GPU内存限制了单卡上的图片个数，提高batch size意味着BN要在多卡（Cross-GPU）上进行。 BN操作需要对每个batch计算均值和方差来进行标准化，对于多卡，具体做法是，单卡独立计算均值，聚合（类似Map-Reduce中的Reduce）算均值，再将均值下发到每个卡，算差，再聚合起来，计算batch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。 流程如图： Experiments在COCO数据集上的架构用预训练ResNet-50作为基础网络，FPN用于提供feature map。 结果显示，不使用BN时，较大的batch size（64,128）不能收敛。使用BN后，增大Batch size能够收敛但仅带来较小的精度提升，而BN的大小也不是越大越好，实验中，32是最好的选择。主要结果如下表： 按epoch，精度的变化如下图，小batch（16）在最初的几个epoch表现比大batch（32）要好。 小结这篇论文读起来总感觉少了些东西。对Linear scale rule的解释固然新颖，但没有引入新的trick（只是确认了检测仍是需要Linear scale rule的）。多卡的BN确实是非常厉害的工程实现（高效性），但实验的结果并没有支持到较大的batch size（128,256）比小batch精度更好的期望，而最后的COCO夺冠模型整合了多种trick，没有更进一步的错误分析，很难支撑说明CGBN带来的关键作用。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"}]},{"title":"[论文笔记]Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks","slug":"论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks","date":"2017-10-21T15:39:34.000Z","updated":"2017-12-27T14:08:20.395Z","comments":true,"path":"2017/10/21/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/","link":"","permalink":"http://blog.ddlee.cn/2017/10/21/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/","excerpt":"","text":"Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks https://arxiv.org/abs/1506.01497 OverviewFaster R-CNN是2-stage方法的主流方法，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA。 Regional Proposal Networks RPN网络将Proposal这一任务建模为二分类的问题。 第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IOU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本即是anchor box和每个anchor box是否有物体。RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的Loss统一起来，作为RPN网络的目标训练。 RPN网络可调的超参还是很多的，anchor box的大小和长宽比例、IoU的阈值、每张图片上Proposal正负样本的比例等。 Alternate Training RPN网络是在feature map上进行的，因而可以跟RCNN完全共享feature extractor部分的卷积运算。训练时，RPN和RCNN的训练可以交替进行，即交替地固定RPN和RCNN部分的参数，更新另一部分。 小结Faster R-CNN的成功之处在于用RPN网络完成了检测任务的“深度化”。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。RPN网络也成为检测2-stage方法的标准部件。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]Fast R-CNN","slug":"论文笔记-Fast-R-CNN","date":"2017-10-15T15:34:31.000Z","updated":"2017-11-21T15:47:15.805Z","comments":true,"path":"2017/10/15/论文笔记-Fast-R-CNN/","link":"","permalink":"http://blog.ddlee.cn/2017/10/15/论文笔记-Fast-R-CNN/","excerpt":"","text":"Fast R-CNN https://arxiv.org/abs/1504.08083 OverviewFast R-CNN 是对R-CNN的改进，作者栏只有RBG一人。文章先指出了R-CNN存在的问题，再介绍了自己的改进思路。文章结构堪称典范，从现存问题，到解决方案、实验细节，再到结果分析、拓展讨论，条分缕析，值得借鉴。而且，RBG开源的代码也影响了后来大部分这一领域的工作。 R-CNN的问题 训练是一个多阶段的过程（Proposal, Classification, Regression） 训练耗时耗力 推断耗时 而耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算。 Fast R-CNN ArchitectureArchitecture 上图是Fast R-CNN的架构。图片经过feature extractor产生feature map, 原图上运行Selective Search算法将RoI（Region of Interset）对应到feature map上，再对每个RoI进行RoI Pooling操作便得到等长的feature vector，最后通过FC后并行地进行Classifaction和BBox Regression。 Fast R-CNN的这一结构正是检测任务主流2-stage方法所采用的元结构的雏形。整个系统由Proposal, Feature Extractor, Object Recognition&amp;Localization几个部件组成。Proposal部分被替换成RPN(Faster R-CNN)，Feature Extractor部分使用SOTA的分类CNN网络(ResNet等），而最后的部分常常是并行的多任务结构（Mask R-CNN等）。 RoI Pooling这一操作是将不同大小的RoI（feature map上）统一的过程，具体做法是将RoI等分成目标个数的网格，在每个网格上进行max pooling，就得到等长的RoI feature vector。 Mini-batch Sampling文章指出SPPNet训练较慢的原因在于来自不同图片的RoI不能共享计算，因而Fast R-CNN采用这样的mini-batch采样策略：先采样N张图片，再在每张图片上采样R/N个RoI，构成R大小的mini-batch。 采样时，总是保持25%比例正样本（iou大于0.5），iou在0.1到0.5的作为hard example。 Multi-task Loss得到RoI feature vector后，后续的操作是一个并行的结构，Fast R-CNN将Classification和Regression的损失统一起来，并且在Regression中用更鲁棒的Smooth L1 Loss代替L2 Loss。 Fine Tuning文章还发现，对于预训练的VGG网络，开放Conv部分的参数更新有助于性能的提升，而不是只更新FC层。将proposal, classification, regression统一在一个框架 Design Evaluation文章最后还对系统结构进行了讨论： multi-loss traing相比单独训练Classification确有提升 Scale invariance方面，multi-scale相比single-scale精度略有提升，但带来的时间开销更大。一定程度上说明CNN结构可以内在地学习scale invariance 在更多的数据(VOC)上训练后，mAP是有进一步提升的 Softmax分类器比”one vs rest”型的SVM表现略好，引入了类间的竞争 更多的Proposal并不一定带来性能的提升 小结Fast R-CNN是对R-CNN的改进，也是对2-stage方法的系统化、架构化。文章将Proposal, Feature Extractor, Object Recognition&amp;Localization统一在一个整体的结构中，并推进共享卷积计算以提高效率的想法演进，是最有贡献的地方。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]Rich feature hierarchies for accurate object detection and semantic segmentation","slug":"论文笔记-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation","date":"2017-10-12T16:57:35.000Z","updated":"2017-12-27T14:10:01.998Z","comments":true,"path":"2017/10/13/论文笔记-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/","link":"","permalink":"http://blog.ddlee.cn/2017/10/13/论文笔记-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/","excerpt":"","text":"https://arxiv.org/abs/1311.2524 OverviewR-CNN系列的开山之作，2-stage的想法至今仍是精确度优先方法的主流。而且，本文中的众多做法也成为检测任务pipeline的标准配置。 摘要中提到的两大贡献：1）CNN可用于基于区域的定位和分割物体；2）监督训练样本数紧缺时，在额外的数据上预训练的模型经过fine-tuning可以取得很好的效果。 第一个贡献影响了之后几乎所有2-stage方法，而第二个贡献中用分类任务（Imagenet）中训练好的模型作为基网络，在检测问题上fine-tuning的做法也在之后的工作中一直沿用。 IntroFeatures Matter. Traditional hand-design feature(SIFT, HOG) -&gt; Learned feature(CNN). 从图像识别的经验来看，CNN网络自动习得的特征已经超出了手工设计的特征。 解决检测任务中的定位问题：”recognition using regions”，即基于区域的识别（分类）。 检测任务中样本不足的问题（对大型网络）：在大数据集上预训练分类模型，在小数据集上fine-tuning检测任务。 Object Detection with R-CNNRegion Proposal: Selective Search Feature Extraction: AlexNet(NIPS 2012), 4096-dim feature vector from every region proposal Training现在ILSVRC2012上预训练达到STOA，再在Pascal VOC上fine-tuning。根据IOU来给region proposal打标签，在每个batch中保持一定的正样本比例（背景类非常多）。这些都已成为标准做法，后续很多工作也是对这些细节进行改进（OHEM等）。 文章中特别提到，IOU的选择（即正负样例的标签准备）对结果影响显著，这里要谈两个threshold，一个用来识别正样本（IOU跟ground truth较高），另一个用来标记负样本（即背景类），而介于两者之间的则为hard negatives，若标为正类，则包含了过多的背景信息，反之又包含了要检测物体的特征，因而这些proposal便被忽略掉。 另一个重要的问题是bounding-box regression，这一过程是proposal向ground truth调整，实现时加入了log/exp变换来使loss保持在合理的量级上。 ConclusionR-CNN的想法直接明了，即是将CNN在分类上取得的成就运用在检测上，是深度学习方法在检测任务上的试水。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。尽管如此，这篇论文的很多做法仍然广泛地影响着检测任务上的深度模型革命，后续的很多工作也都是针对改进文章中的pipeline而展开，此篇可以称得上”the first paper”。 @ddlee","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"Object Detection","slug":"Object-Detection","permalink":"http://blog.ddlee.cn/tags/Object-Detection/"}]},{"title":"Linux Reborn: 个人存档","slug":"Linux软件推荐","date":"2017-09-19T13:35:09.000Z","updated":"2017-12-23T15:58:30.705Z","comments":true,"path":"2017/09/19/Linux软件推荐/","link":"","permalink":"http://blog.ddlee.cn/2017/09/19/Linux软件推荐/","excerpt":"","text":"个人的需求与应用场景我是（几乎）完全使用Linux的，它也满足了我的大部分需求，主要以下几块： 编程，主流IDE, Editor都会有Linux版本，而软件库大部分也是先支持Linux的（服务器） 上网，一个chrome几乎足够 娱乐，steam有Linux客户端，很多游戏也有相应版本（当然也有很多没有） 下面是我整理的一个列表，算是给自己的存档。 推荐编程相关Text Editor选择有很多，我用Sublime Text写代码，用Atom写博客（中文支持好），用VS Code读代码。总之精分，各取所好。 另外讲两个小技巧： 利用sshfs把服务器的文件挂在到本地用编辑器编辑，然后远程终端运行 ssh -L localhost:8888:localhost:8888 user@host命令可以将远程端口映射到本地，这样可以在服务器端开启jupyter notebook，再在本地用浏览器访问 tmux&amp;zshtmux是一个终端多窗口管理器，可以打开多个终端窗口、挂起和挂载终端回话等，利器。 sudo apt install tmux GitKrakenGit图形客户端 效率类Whatever- Evernote alternativeEvernote的第三方客户端，调用网页API，不占用免费版的客户端限制个数 Stacer- System Cleaner提供系统监视器和清理功能，也可以卸载包 synapse- App Launcher一个类似lauchy的应用启动器，可以直接用apt安装 sudo apt install synapse Gdebi- Package Installer包安装程序，比自带的安装好用一些（安装deb包等） sudo apt install gdebi Mailspring- Mail Client邮件客户端，比thunderbird, evolution等界面美观一些 Gparted- Disk Management磁盘管理程序，用于分区、格式化等等 sudo apt install gparted Okular- PDF Reader功能强大的PDF阅读器 sudo apt install okular WPS OfficeWPS的Linux版本，比Libre要好用很多 Shutter截屏软件，可以通过ubunut软件中心安装 通讯与娱乐Wewechat- Wechat client微信的第三方客户端，还有electron-wechat，wewechat界面更好，而后者可以看公众号的文章。 IeaseMusic- Netease Music Client网易云音乐的第三方客户端，界面漂亮，我一般用于听FM。功能上更全的自然是官方版本。 1Listen综合了网易，QQ，虾米三家的曲库，用于找想听的歌，建议下载chrome插件版。 SteamDota2是可以通过steam的Linux版本玩的，我买过的大部分解密游戏也可以。 @ddlee","categories":[{"name":"Individual Development","slug":"Individual-Development","permalink":"http://blog.ddlee.cn/categories/Individual-Development/"}],"tags":[{"name":"Software","slug":"Software","permalink":"http://blog.ddlee.cn/tags/Software/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/tags/Linux/"}]},{"title":"IOS Reborn: 个人APP存档","slug":"IOS-Reborn-个人APP存档","date":"2017-09-01T16:07:47.000Z","updated":"2017-11-21T16:08:28.096Z","comments":true,"path":"2017/09/02/IOS-Reborn-个人APP存档/","link":"","permalink":"http://blog.ddlee.cn/2017/09/02/IOS-Reborn-个人APP存档/","excerpt":"","text":"IOS系统个人应用列表备份，iPad主要为阅读、娱乐功能，iPod用于听歌、播客。 iPad mini 4Productivity Documents，文件中转中心，连接云服务、私有云。PDF文档中心 Google Keep，记录琐事、备忘 Pushbullet，多客户端跨平台文字、链接转发 PDF Expert，为Documents提供PDF标注编辑等功能 Git2Go，GitHub客户端，读代码 百度网盘，转存文件、电子书 Reading Reeder 3，Feedly客户端，咨讯中心 Pocket，稍后再读，配合Reeder 3 知乎，内容索引 多看阅读，电子书中心 Quora，内容索引 kindle，电子书，Amazon内容 Medium，高质量的写作社区 iBooks，少部分电子书 LifeStyle 导航犬离线地图，地图备查 Bilibili HD AVPlayer HD，本地视频，私有云视频 网易云音乐 iPodMusic 网易云音乐 QQ音乐 KUSC，南加州古典音乐电台 overcast，Podcast客户端 LifeStyle Bilibili 地铁通 @ddlee","categories":[{"name":"Individual Development","slug":"Individual-Development","permalink":"http://blog.ddlee.cn/categories/Individual-Development/"}],"tags":[{"name":"Software","slug":"Software","permalink":"http://blog.ddlee.cn/tags/Software/"},{"name":"IOS","slug":"IOS","permalink":"http://blog.ddlee.cn/tags/IOS/"}]},{"title":"Android Reborn: 个人APP存档","slug":"Android-Reborn-个人APP存档","date":"2017-08-31T15:57:23.000Z","updated":"2017-11-21T16:07:31.431Z","comments":true,"path":"2017/08/31/Android-Reborn-个人APP存档/","link":"","permalink":"http://blog.ddlee.cn/2017/08/31/Android-Reborn-个人APP存档/","excerpt":"","text":"个人使用Android的应用列表备份。 GApps Google Photos Google Chrome YouTube Google Keep Inbox by Gmail Google Pinyin Keyboard Google Now Productivity Solid File Explorer， 文件管理，云服务等中转 Steam，Steam二次验证工具 Evernote，笔记同步 WPS Office，文档阅读与编辑 Pushbullet，多端文字链接通信 Pulse Secure，学校指定VPN工具 Google Authenticator(Nutstore Rvoked)，二次验证工具 CamScanner，文档扫描 FeedMe，RSS阅读，配合feedly System Optimization Nova LancherI(config)，桌面（已备份） SD Maid，系统清理，APP管理 Tasker，自动化任务编排 Ice Box，流氓应用管理（已备份） SMS Backup，通话记录和短信备份 SuperSU，ROOT权限管理 MyAndroidTools，系统级的应用活动、服务管理（已备份） Titanium Backup，系统备份 Brevent，系统进程管理 Greenify，系统进程管理 LifeStyle Wechat，微信 Mobike，共享单车 Resplash，高质量图片，壁纸图库 Prisma，智能风格转换，滤镜 Photo Scan，旧实体照片数字化 Snapseed，图片处理 AliPay，支付宝 Max+，DOTA2资讯 C5Game，DOTA2饰品 TIM，工作版QQ Retrorika，图标包 @ddlee","categories":[{"name":"Individual Development","slug":"Individual-Development","permalink":"http://blog.ddlee.cn/categories/Individual-Development/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://blog.ddlee.cn/tags/Android/"},{"name":"Software","slug":"Software","permalink":"http://blog.ddlee.cn/tags/Software/"}]},{"title":"墓畔哀歌","slug":"墓畔哀歌","date":"2017-08-06T10:17:48.000Z","updated":"2017-08-06T10:17:48.963Z","comments":true,"path":"2017/08/06/墓畔哀歌/","link":"","permalink":"http://blog.ddlee.cn/2017/08/06/墓畔哀歌/","excerpt":"","text":"按：石评梅(1902-1928)为纪念恋人高君宇所作。初读是在高中一年级时的语文阅读材料里，读毕怅然若失，之后日日早读必大声读一遍，忘乎所以。如今重逢，只可默默抄写，竟不能放声读矣。 一 我由冬的残梦里惊醒，春正吻着我的睡靥低吟！晨曦照上了窗纱，望见往日令我醺醉的朝霞，我想让丹彩的云流，再认认我当年的颜色。 披上那件绣著蛱蝶的衣裳，姗姗地走到尘网封锁的妆台旁。呵！明镜里照见我憔悴的枯颜，像一朵颤动在风雨中苍白雕零的梨花。 我爱，我原想追回那美丽的皎容，祭献在你碧草如茵的墓旁，谁知道青春的残蕾已和你一同殉葬。 二 假如我的眼泪真凝成一粒一粒珍珠，到如今我已替你缀织成绕你玉颈的围巾。 假如我的相思真化作一颗一颗的红豆，到如今我已替你堆集永久勿忘的爱心。 哀愁深埋在我心头。 我愿燃烧我的肉身化成灰烬，我愿放浪我的热情怒涛汹涌，天呵！这蛇似的蜿蜒，蚕似的缠绵，就这样悄悄地偷去了我生命的青焰。 我爱，我吻遍了你墓头青草在日落黄昏；我祷告，就是空幻的梦吧，也让我再见见你的英魂。 三 明知道人生的尽头便是死的故乡，我将来也是一座孤冢，衰草斜阳。有一天呵！我离开繁华的人寰，悄悄入葬，这悲艳的爱情一样是烟消云散，昙花一现，梦醒后飞落在心头的都是些残泪点点。 然而我不能把记忆毁灭，把埋我心墟上的残骸抛却，只求我能永久徘徊在这垒垒荒冢之间，为了看守你的墓茔，祭献那茉莉花环。 我爱，你知否我无言的忧衷，怀想着往日轻盈之梦。梦中我低低唤着你小名，醒来只是深夜长空有孤雁哀鸣！ 四 黯淡的天幕下，没有明月也无星光这宇宙像数千年的古墓；皑皑白骨上，飞动闪映着惨绿的磷花。我匍匐哀泣于此残銹的铁栏之旁，愿烘我愤怒的心火，烧毁这黑暗丑恶的地狱之网。 命运的魔鬼有意捉弄我弱小的灵魂，罚我在冰雪寒天中，寻觅那雕零了的碎梦。求上帝饶恕我，不要再惨害我这仅有的生命，剩得此残躯在，容我杀死那狞恶的敌人！ 我爱，纵然宇宙变成烬余的战场，野烟都腥：在你给我的甜梦里，我心长系驻于虹桥之中，赞美永生！ 五 我镇天踟蹰于垒垒荒冢，看遍了春花秋月不同的风景，抛弃了一切名利虚荣，来到此无人烟的旷野，哀吟缓行。我登了高岭，向云天苍茫的西方招魂，在绚烂的彩霞里，望见了我沈落的希望之陨星。 远处是烟雾冲天的古城，火星似金箭向四方飞游！隐约的听见刀枪搏击之声，那狂热的欢呼令人震惊！在碧草萋萋的墓头，我举起了胜利的金觥，饮吧我爱，我奠祭你静寂无言的孤冢！ 星月满天时，我把你遗我的宝剑纤手轻擎，宣誓向长空： 愿此生永埋了英雄儿女的热情。 六 假如人生只是虚幻的梦影，那我这些可爱的映影，便是你赠与我的全生命。我常觉你在我身后的树林里，骑着马轻轻地走过去。常觉你停息在我的窗前，徘徊著等我的影消灯熄。常觉你随着我唤你的声音悄悄走近了我，又含泪退到了墙角。常觉你站在我低垂的雪帐外，哀哀地对月光而叹息！ 在人海尘途中，偶然逢见个像你的人，我停步凝视后，这颗心呵！便如秋风横扫落叶般冷森凄零！我默思我已经得到爱的之心，如今只是荒草夕阳下，一座静寂无语的孤冢。 我的心是深夜梦里，寒光闪灼的残月，我的情是青碧冷静，永不再流的湖水。残月照着你的墓碑，湖水环绕着你的坟，我爱，这是我的梦，也是你的梦，安息吧，敬爱的灵魂！ 七 我自从混迹到尘世间，便忘却了我自己；在你的灵魂我才知是谁？ 记得也是这样夜里。我们在河堤的柳丝中走过来，走过去。我们无语，心海的波浪也只有月儿能领会。你倚在树上望明月沈思，我枕在你胸前听你的呼吸。抬头看见黑翼飞来掩遮住月儿的清光，你抖颤著问我：假如这苍黑的翼是我们的命运时，应该怎样？ 我认识了欢乐，也随来了悲哀，接受了你的热情，同时也随来了冷酷的秋风。往日，我怕恶魔的眼睛凶，白牙如利刃；我总是藏伏在你的腋下趑趄不敢进，你一手执宝剑，一手扶着我践踏着荆棘的途径，投奔那如花的前程！ 如今，这道上还留着你斑斑血痕，恶魔的眼睛和牙齿再是那样凶狠。但是我爱，你不要怕我孤零，我愿用这一纤细的弱玉腕，建设那如意的梦境。 八 春来了，催开桃蕾又飘到柳梢，这般温柔慵懒的天气真使人恼！她似乎躲在我眼底有意缭绕，一阵阵风翼，吹起了我灵海深处的波涛。 这世界已换上了装束，如少女般那样娇娆，她披拖着浅绿的轻纱，蹁跹在她那（姹）紫嫣红中舞蹈。伫立于白杨下，我心如捣，强睁开模糊的泪眼，细认你墓头，萋萋芳草。 满腔辛酸与谁道？愿此恨吐向青空将天地包。它纠结围绕着我的心，像一堆枯黄的蔓草，我爱，我待你用宝剑来挥扫，我待你用火花来焚烧。 九 垒垒荒冢上，火光熊熊，纸灰缭绕，清明到了。这是碧草绿水的春郊。墓畔有白发老翁，有红颜年少，向这一杯黄土致不尽的怀忆和哀悼，云天苍茫处我将魂招；白杨萧条，暮鸦声声，怕孤魂归路迢迢。 逝去了，欢乐的好梦，不能随墓草而复生，明朝此日，谁知天涯何处寄此身？叹漂泊我已如落花浮萍，且高歌，且痛饮，拼一醉烧熄此心头余情。 我爱，这一杯苦酒细细斟，邀残月与孤星和泪共饮，不管黄昏，不论夜深，醉卧在你墓碑傍，任霜露侵凌吧！我再不醒。 十六年清明陶然亭畔 @ddlee","categories":[{"name":"Reading","slug":"Reading","permalink":"http://blog.ddlee.cn/categories/Reading/"}],"tags":[{"name":"Reading","slug":"Reading","permalink":"http://blog.ddlee.cn/tags/Reading/"}]},{"title":"[源码笔记]keras源码分析之Model","slug":"源码笔记-keras源码分析之Model","date":"2017-07-30T15:19:42.000Z","updated":"2017-08-03T14:15:45.668Z","comments":true,"path":"2017/07/30/源码笔记-keras源码分析之Model/","link":"","permalink":"http://blog.ddlee.cn/2017/07/30/源码笔记-keras源码分析之Model/","excerpt":"","text":"本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是/keras/engine/training.py和/keras/model.py，要观察的类是Model和Sequential。 本系列第一篇：【源码笔记】keras源码分析之Tensor, Node和Layer第二篇：【源码笔记】keras源码分析之Container Model：添加了训练信息的ContainerModel.compile()主要完成了配置optimizer, loss, metrics等操作，而要执行的fit, evaluate等则不在compile过程中配置。 def compile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None, **kwargs): loss = loss or {} self.optimizer = optimizers.get(optimizer) self.sample_weight_mode = sample_weight_mode self.loss = loss self.loss_weights = loss_weights loss_function = losses.get(loss) loss_functions = [loss_function for _ in range(len(self.outputs))] self.loss_functions = loss_functions # Prepare targets of model. self.targets = [] self._feed_targets = [] for i in range(len(self.outputs)): shape = self.internal_output_shapes[i] name = self.output_names[i] target = K.placeholder(ndim=len(shape), name=name + '_target', sparse=K.is_sparse(self.outputs[i]), dtype=K.dtype(self.outputs[i])) self.targets.append(target) self._feed_targets.append(target) # Prepare metrics. self.metrics = metrics self.metrics_names = ['loss'] self.metrics_tensors = [] # Compute total loss. total_loss = None for i in range(len(self.outputs)): y_true = self.targets[i] y_pred = self.outputs[i] loss_weight = loss_weights_list[i] if total_loss is None: total_loss = loss_weight * output_loss else: total_loss += loss_weight * output_loss for loss_tensor in self.losses: total_loss += loss_tensor self.total_loss = total_loss self.sample_weights = sample_weights Model对象的fit()方法封装了_fit_loop()内部方法，而_fit_loop()方法的关键步骤由_make_train_function()方法完成，返回history对象，用于回调函数的处理。 def fit(self, x=None, y=None, ...)： self._make_train_function() f = self.train_function return self._fit_loop(f, ins, ...) 在_fit_loop()方法中，回调函数完成了对训练过程的监控记录等任务，train_function也被应用于传入的数据： def _fit_loop(self, f, ins, out_labels=None, batch_size=32, epochs=100, verbose=1, callbacks=None, val_f=None, val_ins=None, shuffle=True, callback_metrics=None, initial_epoch=0): self.history = cbks.History() callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history] callbacks = cbks.CallbackList(callbacks) out_labels = out_labels or [] callbacks.set_model(callback_model) callbacks.set_params({ 'batch_size': batch_size, 'epochs': epochs, 'samples': num_train_samples, 'verbose': verbose, 'do_validation': do_validation, 'metrics': callback_metrics or [], }) callbacks.on_train_begin() callback_model.stop_training = False for epoch in range(initial_epoch, epochs): callbacks.on_epoch_begin(epoch) batches = _make_batches(num_train_samples, batch_size) epoch_logs = {} for batch_index, (batch_start, batch_end) in enumerate(batches): batch_ids = index_array[batch_start:batch_end] batch_logs = {} batch_logs['batch'] = batch_index batch_logs['size'] = len(batch_ids) callbacks.on_batch_begin(batch_index, batch_logs) # 应用传入的train_function outs = f(ins_batch) callbacks.on_batch_end(batch_index, batch_logs) callbacks.on_epoch_end(epoch, epoch_logs) callbacks.on_train_end() return self.history _make_train_function()方法从optimizer获取要更新的参数信息，并传入来自backend的function对象： def _make_train_function(self): if self.train_function is None: inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights training_updates = self.optimizer.get_updates( self._collected_trainable_weights, self.constraints, self.total_loss) updates = self.updates + training_updates # Gets loss and metrics. Updates weights at each call. self.train_function = K.function(inputs, [self.total_loss] + self.metrics_tensors, updates=updates, name='train_function', **self._function_kwargs) Model的其他方法evaluate()等，与fit()的结构类似。 Sequential:构建模型的外层接口Sequential对象是Model对象的进一步封装，也是用户直接面对的接口，其compile(), fit(), predict()等方法与Model几乎一致，所不同的是添加了add()方法，也是我们用于构建网络的最基本操作。 Sequential.add()方法的源码如下： def add(self, layer): # 第一层必须是InputLayer对象 if not self.outputs: if not layer.inbound_nodes: x = Input(batch_shape=layer.batch_input_shape, dtype=layer.dtype, name=layer.name + '_input') layer(x) self.outputs = [layer.inbound_nodes[0].output_tensors[0]] self.inputs = topology.get_source_inputs(self.outputs[0]) topology.Node(outbound_layer=self, ...) else: output_tensor = layer(self.outputs[0]) self.outputs = [output_tensor] self.inbound_nodes[0].output_tensors = self.outputs self.layers.append(layer) 可以看到，add()方法总是确保网络的第一层为InputLayer对象，并将新加入的层应用于outputs，使之更新。因此，从本质上讲，在Model中添加新层还是在更新模型的outputs。 @ddlee","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"},{"name":"Keras","slug":"Keras","permalink":"http://blog.ddlee.cn/tags/Keras/"}]},{"title":"[源码笔记]keras源码分析之Container","slug":"源码笔记-keras源码分析之Container","date":"2017-07-25T14:08:44.000Z","updated":"2017-08-03T14:17:13.948Z","comments":true,"path":"2017/07/25/源码笔记-keras源码分析之Container/","link":"","permalink":"http://blog.ddlee.cn/2017/07/25/源码笔记-keras源码分析之Container/","excerpt":"","text":"本篇继续讨论keras的源码结构。 第一篇源码笔记中我们观察了Layer, Tensor和Node是如何耦合在一起的，而本篇的重点是观察多层网络构成的有向无环图（DAG）。主要涉及的文件为keras/engine/topology.py， 要观察的类是Container。 Container对象：DAG的拓扑原型在第一篇中我们提到，Keras Tensor中增强的\\_keras_history属性使得我们仅通过输入和输出的Tensor，就可以构建出整张计算图。而Container对象正是实现了这样的过程。 计算图的构建DAG计算图的构建在Container对象实例化时完成，主要包括如下几个操作： 1） 记录Container的首尾连接信息 def __init__(self, inputs, outputs, name=None): for x in self.outputs: layer, node_index, tensor_index = x._keras_history self.output_layers.append(layer) self.output_layers_node_indices.append(node_index) self.output_layers_tensor_indices.append(tensor_index) for x in self.inputs: layer, node_index, tensor_index = x._keras_history self.input_layers.append(layer) self.input_layers_node_indices.append(node_index) self.input_layers_tensor_indices.append(tensor_index) 2） 从output_tensors开始反向递归构建计算图，采用广度优先的准则，本步的关键是构建nodes_in_decreasing_depth这一队列，这些Node包含的连接信息和深度信息将是后续正向传播和反向训练计算执行顺序的依据。 def build_map_of_graph(tensor, finished_nodes, nodes_in_progress): layer, node_index, tensor_index = tensor._keras_history node = layer.inbound_nodes[node_index] nodes_in_progress.add(node) # 广度优先搜索 for i in range(len(node.inbound_layers)): x = node.input_tensors[i] layer = node.inbound_layers[i] node_index = node.node_indices[i] tensor_index = node.tensor_indices[i] # 递归调用 build_map_of_graph(x, finished_nodes, nodes_in_progress, layer, node_index, tensor_index) # 维护两个队列 finished_nodes.add(node) nodes_in_progress.remove(node) nodes_in_decreasing_depth.append(node) # 反向构建DAG for x in self.outputs: build_map_of_graph(x, finished_nodes, nodes_in_progress) 3） 计算各节点的深度并按深度标定节点在DAG中的位置 # 根据队列标定各节点的深度 for node in reversed(nodes_in_decreasing_depth): depth = nodes_depths.setdefault(node, 0) previous_depth = layers_depths.get(node.outbound_layer, 0) depth = max(depth, previous_depth) layers_depths[node.outbound_layer] = depth nodes_depths[node] = depth for i in range(len(node.inbound_layers)): inbound_layer = node.inbound_layers[i] node_index = node.node_indices[i] inbound_node = inbound_layer.inbound_nodes[node_index] previous_depth = nodes_depths.get(inbound_node, 0) nodes_depths[inbound_node] = max(depth + 1, previous_depth) # 按深度标定各节点的位置 nodes_by_depth = {} for node, depth in nodes_depths.items(): if depth not in nodes_by_depth: nodes_by_depth[depth] = [] nodes_by_depth[depth].append(node) # 按深度标定各层的位置 layers_by_depth = {} for layer, depth in layers_depths.items(): if depth not in layers_by_depth: layers_by_depth[depth] = [] layers_by_depth[depth].append(layer) self.layers_by_depth = layers_by_depth self.nodes_by_depth = nodes_by_depth 4）将整个Container并入Node以保持兼容性 self.outbound_nodes = [] self.inbound_nodes = [] Node(outbound_layer=self, inbound_layers=[], node_indices=[], tensor_indices=[], input_tensors=self.inputs, output_tensors=self.outputs, ...) 计算图中的计算计算在Container对象的call()方法完成，其实现又依靠内部方法run_internal_graph()。 def run_internal_graph(self, inputs, masks=None): depth_keys = list(self.nodes_by_depth.keys()) depth_keys.sort(reverse=True) # 依据深度 for depth in depth_keys: nodes = self.nodes_by_depth[depth] # 对同一深度上的Node进行计算 for node in nodes: layer = node.outbound_layer # Node对应的layer reference_input_tensors = node.input_tensors reference_output_tensors = node.output_tensors computed_data = [] if len(computed_data) == len(reference_input_tensors): # 在Layer中进行计算 with K.name_scope(layer.name): if len(computed_data) == 1: computed_tensor, computed_mask = computed_data[0] output_tensors = _to_list(layer.call(computed_tensor, **kwargs)) computed_tensors = [computed_tensor] else: computed_tensors = [x[0] for x in computed_data] output_tensors = _to_list(layer.call(computed_tensors, **kwargs)) output_tensors = [] output_masks = [] for x in self.outputs: tensor, mask = tensor_map[str(id(x))] output_tensors.append(tensor) output_masks.append(mask) return output_tensors, output_masks 从上面的代码可以看到计算是依据深度进行的，并通过更新computed_data和output_tensor等变量完成整张图的遍历计算。 继续阅读系列第三篇：【源码笔记】keras源码分析之Model @ddlee","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"},{"name":"Keras","slug":"Keras","permalink":"http://blog.ddlee.cn/tags/Keras/"}]},{"title":"深度学习中的权重衰减","slug":"深度学习中的权重衰减","date":"2017-07-22T15:51:13.000Z","updated":"2017-08-03T14:18:06.028Z","comments":true,"path":"2017/07/22/深度学习中的权重衰减/","link":"","permalink":"http://blog.ddlee.cn/2017/07/22/深度学习中的权重衰减/","excerpt":"","text":"权重衰减（weight dacay），即L^2范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自The Deep Learning Book。 为什么要引入权重衰减机器学习的逻辑与我们最初解决问题的思维方式恰恰相反：要解决问题，一种经典的思路是把它拆成小问题，考虑之间的依赖，然后分而治之。而机器学习的哲学是“trail-error-correct”：先假设一堆可能的方案，根据结果去选择/调整这些方案，直到满意。换句话说，机器学习在假设空间中搜索最符合数据的模型：以果推因，即为最大似然的想法。随着数据量的增大，我们越来越需要表达能力更强的模型，而深度学习的优势正符合这一需要：通过分布式表示带来的指数增益，深度学习模型的扩展能力几乎是无限的（详见深度学习和分布式表示）。 有了模型（备选模型集），有了数据，就不得不面对机器学习领域的核心问题：如何保证模型能够描述数据（拟合）和生成数据（泛化）。 粗略来看，有以下三种情况： 我们假定的模型族不包含真实数据的生成过程：欠拟合/高偏差 匹配真实的数据生成过程 除了包含真实的生成过程，还包含了其他信息：过拟合/高方差 高偏差意味着我们的模型不够准确（模型族不足以描述数据），高方差意味着我们建模了不必要的信息（训练数据的随机性带来的）。前者通过提高模型的表述能力来解决（更深的网络），后者则需要合理的正则化技术来控制。这即是著名的trade-off。 深度学习模型的参数对数据建模，其实是从数据中提取我们能够理解的信息。建立的模型，是从数据分布的空间到目标变量所在空间的映射。从这个角度看，我们通过模型带来的变换获得了数据的一种表示，我们认为能够理解和操作的表示。 为了表述这一变换，深度模型的套路是线性层施加变换，非线性层固定信息（不能平移），然后将这样的结构堆叠起来，分层提取数据特征。 这让我想起实变中证明定理的套路：先证明简单函数的情形，再推广到连续函数，再到勒贝格可积的函数。 常规的套路（MLP）在拟合普通的函数任务上能够胜任，但面对更复杂的图像等数据，就需要更灵活的网络结构。 非常出色的CNN, LSTM, Inception块, ResNet, DenseNet等结构，就是加入了人类的先验知识，使之更有效的提取图像/音频数据分布空间的特征。（所以Manning有次在课堂上说，机器学习事实上还是人类在学习：机器只是在求导数、做乘法，最好的模型都是人们学习出来的。） 人们确实设计了很多巧妙的结构来解决不同的问题，但落实到网络的层和单元上，仍是最基本的矩阵乘法、加法运算。决定模型表述能力的，也正是这些普通的乘法运算中涉及的矩阵和向量了。 权重衰减如何起作用下面我们通过观察加入权重衰减后目标函数的梯度变化来讨论权重衰减是如何起作用的。可以跳过公式部分直接看最后一段。 ——————————————————————————————推导部分———————————————————————————————————————————— 简单起见，令偏置为0，模型的目标函数： $$J_{1}(w; X,y)=\\frac{\\alpha}{2} w^T w+J(w; X,y)$$ 对应的梯度为： $${\\nabla}{w} J{1}(w; X,y) = \\alpha w + {\\nabla}_{w} J(w; X,y)$$ 进行梯度下降，参数的更新规则为： $$w = w - \\epsilon (\\alpha w + {\\nabla}_{w} J(w; X,y)) $$ 也就是： $$w = (1 - \\epsilon \\alpha )w - \\epsilon {\\nabla}_{w} J(w; X,y)$$ 从上式可以发现，加入权重衰减后，先对参数进行伸缩，再沿梯度下降。下面令$$x^{(1)}$$为使目标函数达到最优的参数值，在其附近考虑目标函数的二次近似： $$J(w) \\approx J(w^{(1)}) + \\frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$ 其中$$H$$为近似目标函数在的Hessian矩阵。当近似目标函数最小时，其梯度为$$0$$，即： $${\\nabla}_{w} J(w) \\approx H(w - w^{(1)})$$ 该式也向我们说明了基于梯度的优化算法主要的信息来自Hessian矩阵。添加入权重衰减项之后，上式变为（记此时的最优点为$$w^{(2)}$$）： $${\\nabla}{w} J{1}(w) \\approx \\alpha w^{(2)} + H(w^{(2)} - w^{(1)}) = 0$$ 所以 $$w^{(2)} = (H + \\alpha I)^{-1} H w^{(1)} $$ 该式表明了了加入正则化对参数最优质点的影响，由Hessian矩阵和正则化系数$$\\alpha$$共同决定。 进一步将Hessian矩阵分解，可以得到： $$w^{(2)} = Q(\\Lambda + \\alpha I)^{-1} \\Lambda Q^T w^{(1)}$$ 其中，$$Q$$为正交矩阵，$$\\Lambda$$为对角矩阵。这样可以看到，权重衰减的效果是沿着由$$H$$的特征向量所定义的轴缩放$$w$$， 具体的伸缩因子为$$\\frac{ {\\lambda}_{i} }{ {\\lambda}i + \\alpha }$$，其中$${\\lambda}{i}$$表示第$$i$$个特征向量对应的特征值。 当特征值$$\\lambda$$很大（相比$$\\alpha$$）时，缩放因子对权重影响较小，因而更新过程中产生的变化也不大；而当特征值较小时，$$\\alpha$$的缩放作用就显现出来，将这个方向的权重衰减到0。 这种效果也可以由下图表示： ——————————————————————————推导部分结束———————————————————————————————————————————————————————————————————— 总结来说，目标函数的Hessian矩阵（显式、隐式或者近似的）是现有优化算法进行寻优的主要依据。通过控制权重衰减的$$\\alpha$$参数，我们实际上控制的是在Hessian矩阵的特征方向上以多大的幅度缩放权重，相对重要（能够显著减小目标函数）的方向上权重保留比较完好，而无助于目标函数减小的方向上权重在训练过程中逐渐地衰减掉了。而这也就是权重衰减的意义。 从宏观上来看，对目标函数来说，特征值较大的方向包含更多有关数据的信息，较小的方向则有随机性的噪声，权重衰减正是通过忽略较少信息方向的变化来对抗过拟合的。 $$L^1$$范数正则化通过类似的推导，可以得到加入了$$L^1$$范数惩罚项对参数最优解的影响如下： $$w^{(2)}{i} = sign(w^{(1)}{i}) max \\big{|w^{(1)}{i}| - \\frac{\\alpha}{H{i,i}}, 0 \\big}$$ 相比$$L^2$$范数的影响，这是一个离散的结果，因而$$L^1$$范数惩罚会将参数推向更加稀疏的解。这种稀疏性质常被用作特征选择。 权重衰减的贝叶斯解释在贝叶斯统计的框架下，常用的推断策略是最大后验点估计(Maximum A Posteriori, MAP)。有如下的推断公式（由贝叶斯定律导出）： $${\\theta}_{MAP} = argmax p(\\theta | x) = argmax (log p( x | \\theta) + log p(\\theta))$$ 上式右边第一项是标准的对数似然项，而第二项对应着先验分布。 在这样的视角下，我们只进行最大似然估计是不够的，还要考虑先验$$p(\\theta)$$的分布。而当假定参数为正态分布$$N(w; 0, \\frac{1}{\\lambda}I^2)$$时，带入上式（$$\\theta$$为参数），即可发现第二项的结果正比于权重衰减惩罚项$$\\lambda w^T w$$，加上一个不依赖于$$w$$也不影响学习过程的项。于是，具有高斯先验权重的MAP贝叶斯推断对应着权重衰减。 权重衰减与提前终止提前终止也是一种正则化技术，其想法简单粗暴：每个epoch之后在验证集上评估结果，当验证集误差不再下降的时候，我们认为模型已经尽它所能了，于是终止训练过程。 提前终止以牺牲一部分训练数据来作为验证数据来的代价来对抗过拟合，其逻辑是实证主义的。 然而，在二次近似和简单梯度下降的情形下，可以观察到提前终止可以有相当于权重衰减的效果。 我们仍考虑目标函数的二次近似： $$J(w) \\approx J(w^{(1)}) + \\frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$ 记最优参数点为$$w^{(1)}$$，其梯度为： $${\\nabla}_{w} J(w) \\approx H(w - w^{(1)})$$ 不加入正则化项，其梯度下降的更新策略（从第$$\\tau-1$$步到$$\\tau$$步）为： $$ w^{(\\tau)} = w^{\\tau - 1)} - \\epsilon H (w^{(\\tau - 1)} - w^{(1)})$$ 累加得到 $$ w^{(\\tau)} - w^{(1)} = (I - \\epsilon H) (w^{(\\tau - 1)} - w^{(1)})$$ 将Hessian矩阵分解，得到如下形式 $$ w^{(\\tau)} = Q[I - (I - \\epsilon \\Lambda) ^ {\\tau}] Q^T w^{(1)} $$ 将加入正则化项的权重影响改写为 $$ w^{(2)} = Q[I - (\\Lambda + \\alpha I) ^ {-1} \\alpha] Q^T w^{(1)} $$ 对比可以得到，如果超参数$$\\epsilon, \\alpha, \\tau$$满足 $$ (I - \\epsilon \\Lambda) ^ {\\tau} = (\\Lambda + \\alpha I) ^ {-1} \\alpha $$ 则提前终止将与权重衰减有相当的效果。具体的，即第$$\\tau$$步结束的训练过程将到达超参数为$$\\alpha$$的$$L^2$$正则化得到的最优点。 但提前终止带来的好处是，我们不再需要去找合适的超参数$$\\alpha$$，而只需要制定合理的终止策略（如3个epoch均不带来验证集误差的减小即终止训练），在训练成本的节约上，还是很值得的。 @ddlee","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://blog.ddlee.cn/tags/Machine-Learning/"}]},{"title":"[源码笔记]keras源码分析之Layer、Tensor和Node","slug":"源码笔记-keras源码分析之Layer、Tensor和Node","date":"2017-07-15T12:04:54.000Z","updated":"2017-08-03T14:13:45.334Z","comments":true,"path":"2017/07/15/源码笔记-keras源码分析之Layer、Tensor和Node/","link":"","permalink":"http://blog.ddlee.cn/2017/07/15/源码笔记-keras源码分析之Layer、Tensor和Node/","excerpt":"","text":"Keras架构的主要逻辑实现在/keras/engine/topology.py中，主要有两个基类Node()和Layer()，一个重要函数Input()。具体地， Layer()是一个计算层的抽象，完成网络中对Tensor的计算过程； Node()描述两个层之间连接关系的抽象，配合Layer()构建DAG； Input()实例化一个特殊的Layer(InputLayer)，将backend（TensorFlow或Theano）建立的Tensor对象转化为Keras Tensor对象。 Keras Tensor： 增强版Tensor相比原始的TensorFlow或者Theano的张量对象，Keras Tensor加入了如下两个属性，以使Tensor中包含了自己的来源和规模信息： _Keras_history: 保存了最近一个应用于这个Tensor的Layer _keras_shape: 标准化的Keras shape接口 当使用Keras建立深度网络时，传入的数据首先要经过Input()函数。在Input()函数中，实例化一个InputLayer()对象，并将此Layer()对象作为第一个应用于传入张量的Layer，置于_keras_history属性中。此外，InputLayer()和Input()还会对传入的数据进行规模检查和变换等，使之符合后续操作的要求。 代码上实现如下： def Input(): input_layer = InputLayer() outputs = InputLayer.inbound_nodes[0].output_tensor return outputs class InputLayer(): def __init__(): input_tensor._keras_history = (self, 0, 0) Node(self, ...) 在下面我们将看到，加入的_keras_history属性在计算图的构建上所起的作用是关键的。仅通过输入和输出的Tensor，我们可以构建出整张计算图。但这样的代价是Tensor对象太重了，包含了Layer的信息。 Node对象：层与层之间链接的抽象若考虑Layer对象抽象的是完成计算的神经元胞体，则Node对象是对神经元树突结构的抽象。其内聚的主要信息是： class Node(): def __init__(self, outbound_layer, inbound_layers, node_indices, tensor_indices, input_tensors, output_tensors, ...) 其中outbound_layer是施加计算（使input_tensors变为output_tensors）的层，inbound_layers对应了input_tensors来源的层，而node_indices和tensor_indices则记录了Node和Layer之间的标定信息。 Node对象总在outbound_layer被执行时创建，并加入outbound_layer的inbound_nodes属性中。在Node对象的表述下，A和B两个层产生连接关系时，Node对象被建立，并被加入A.outbound_nodes和B.inbound_nodes。 Layer对象：计算层的抽象Layer对象是对网络中神经元计算层的抽象，实例化需要如下参数： allowed_kwargs = {'input_shape', 'batch_input_shape', 'batch_size', 'dtype', 'name', 'trainable', 'weights', 'input_dtype', # legacy } 大部分与传入数据的类型和规模相关，trainable表征该层是否需要更新权重。此外，还有inbound_nodes和outbound_nodes属性来标定与Node对象的链接。 Layer对象最重要的方法是__call__()，主要完成如下三件事情： 验证传入数据的合法性，通过调用内部方法实现：self.assert_input_compatibility(inputs) 进行计算outputs = self.call(inputs, ...)，被其子类具体实现，如Linear, Dropout等 更新Tensor中的_keras_history属性，记录该次计算操作，通过内部方法_add_inbound_nodes()实现 方法_add_inbound_nodes()对Tensor的更新是构建Layer之间关系的关键操作，其主要代码如下： for x in input_tensors: if hasattr(x, '_keras_history'): inbound_layer, node_index, tensor_index = x._keras_history inbound_layers.append(inbound_layer) node_indices.append(node_index) tensor_indices.append(tensor_index) # Node对象的建立过程中将更新self的inbound_nodes属性 Node(self, inbound_layers=inbound_layers, node_indices=node_indices, tensor_indices=tensor_indices, ...) for i in range(len(output_tensors)): output_tensors[i]._keras_history = (self, len(self.inbound_nodes) - 1, i) 上段代码取出input_tensor的_keras_history属性，建立新的Node，并将当前Layer的信息更新到计算得到的output_tensor中。 实例：Node,Tensor和Layer间连接关系的表征下面通过代码来说明三者之间的关系，来自于测试代码： # 建立新的keras Tensor a = Input(shape=(32,), name='input_a') b = Input(shape=(32,), name='input_b') a_layer, a_node_index, a_tensor_index = a._keras_history assert len(a_layer.inbound_nodes) == 1 assert a_tensor_index is 0 # node和layer之间的关系 node = a_layer.inbound_nodes[a_node_index] assert node.outbound_layer == a_layer # 建立连接层，将Tensor传入 dense = Dense(16, name='dense_1') a_2 = dense(a) b_2 = dense(b) assert len(dense.inbound_nodes) == 2 assert len(dense.outbound_nodes) == 0 # 与张量a关联的Node assert dense.inbound_nodes[0].inbound_layers == [a_layer] assert dense.inbound_nodes[0].outbound_layer == dense assert dense.inbound_nodes[0].input_tensors == [a] # 与张量b关联的Node assert dense.inbound_nodes[1].inbound_layers == [b_layer] assert dense.inbound_nodes[1].outbound_layer == dense assert dense.inbound_nodes[1].input_tensors == [b] 总结keras利用Node对象描述Layer之间的连接关系，并在Tensor中记录其来源信息。在下篇中，我们将看到keras如何利用这些抽象和增强属性构建DAG，并实现前向传播和反向训练的。 @ddlee","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"},{"name":"Keras","slug":"Keras","permalink":"http://blog.ddlee.cn/tags/Keras/"}]},{"title":"Tensorflow最佳实践：试验管理","slug":"Tensorflow最佳实践：试验管理","date":"2017-07-11T12:53:27.000Z","updated":"2017-08-01T13:00:20.447Z","comments":true,"path":"2017/07/11/Tensorflow最佳实践：试验管理/","link":"","permalink":"http://blog.ddlee.cn/2017/07/11/Tensorflow最佳实践：试验管理/","excerpt":"","text":"本文主要记录使用TensorFlow训练模型中与试验管理相关的最佳实践，主要包括模型训练的大致代码框架、模型的保存与恢复、训练过程的监测、随机性的控制等。主要材料来自CS 20SI: Tensorflow for Deep Learning Research。 TensorFlow代码框架使用TensorFlow构建深度网络模型大致包括数据预处理、图的构建、模型训练、模型推断与评估等部分，大致的代码框架如下： import tensorflow as tf import numpy as np # Data X = tf.placeholder(\"float\", [None, n_input]) Y = tf.placeholder(\"float\", [None, n_output]) # Parameters W = tf.Variable(tf.random_normal([n_input, n_output])) b = tf.Variable(tf.random_normal([n_output])) # Define model y = tf.matmul(x, W) + b y_pred = tf.nn.relu(y) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_pred, y_true)) optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # Training with tf.Session() as sess: tf.initialize_all_variables().run() sess.run(optimizer, feed_dict={X: x_data, Y: y_data}) # Prediction y_test = tf.nn.relu(tf.matmul(x_test, W) + b)) 模型的保存与恢复一个很深的网络训练成本是比较高的，因而将模型定期保存（写入硬盘）则有必要。这里的模型，实际上是有组织的一批数据，包括图的结构描述、参数当前值等。因而我们要保存的不仅是模型，还有模型当前的运行状态，实际上每一次保存可以作为一个还原点。 tf.train.Saver类使用tf.train.Saver类需传入以下参数：tf.train.Saver.save(sess, save_path, global_step=step)。 首先定义步数变量：self​.​global_step ​=​ tf​.​Variable​(​0​,​ dtype​=​tf​.​int32​,​ trainable​=​False​,name​=​&#39;global_step&#39;) 在模型训练的过程中插入还原点的保存： self​.​optimizer ​=​ tf​.​train​.​GradientDescentOptimizer​(​self​.​lr​).​minimize​(​self​.​loss​,global_step​=​self​.​global_step) saver ​=​ tf​.​train​.​Saver​()​​ ​with​ tf​.​Session​()​​as​ sess: sess​.​run​(​tf​.​global_variables_initializer​()) average_loss ​=​​0.0 for​ index ​in​ range​(​num_train_steps​): batch ​=​ batch_gen​.​next​() loss_batch​,​ _ ​=​ sess​.​run​([​model​.​loss​,​ model​.​optimizer​], feed_dict​={...}) average_loss ​+=​ loss_batch # Save model every 1000 steps ​if​​(​index ​+​​1​)​​%​​1000​​==​​0: saver​.​save​(​sess​,​​'checkpoints/model'​,​ global_step​=​model​.​global_step) 在训练过程中，在checkpoints路径下会存储一系列的还原点文件，要恢复session到某个还原点，可使用如下代码：saver.restore(sess, &#39;checkpoints/name_of_the_checkpoint&#39;)。 Keras封装：keras.callbacks.ModelCheckpoint()Keras对TensorFlow进行了高层的封装，使用一系列回调函数keras.callbacks.Callback()来进行试验管理。 模型保存ModelCheckpoint()需要传入的参数：keras.callbacks.ModelCheckpoint(filepath, monitor=&#39;val_loss&#39;, verbose=0, save_best_only=False, save_weights_only=False, mode=&#39;auto&#39;, period=1) 实际的使用中，将上述回调函数类传入model.fit()过程即可： from keras.callbacks import ModelCheckpoint model = Sequential() model.add(Dense(10, input_dim=784, kernel_initializer='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') checkpointer = ModelCheckpoint(filepath='/checkpoints/weights.hdf5', verbose=1, save_best_only=True) model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer]) 模型训练过程的监测训练过程中，我们常常需要提取阶段性的信息来评估模型是否符合预期效果。 tf.summary首先创建想要观察指标的tf.summary对象： with tf.name_scope(\"summaries\"): tf.summary.scalar(\"loss\", self.loss) tf.summary.scalar(\"accuracy\", self.accuracy) tf.summary.histogram(\"histogram loss\", self.loss) # merge them all self.summary_op = tf.summary.merge_all() tf.summary是一种operation，因而可以随训练过程一同运行：loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict) 最后，将summary加入writer以写入文件： with tf.Session() as sess: writer ​=​ tf​.​summary​.​FileWriter​(​'./summary'​,​ sess​.​graph) for​ index ​in​ range​(​num_train_steps​): writer.add_summary(summary, global_step=step) writer.close() 这样，就可以用TensorBoard监测我们关心的指标在训练过程中的变化情况。 Keras封装：keras.callbacks.TensorBoard()Keras同样将TensorBoard封装成回调函数的形式，在模型训练时进行调用即可： from keras.callbacks import TensorBoard tensorboard = TensorBoard(log_dir=\"./logs\") model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[tensorboard] 随机性的控制TensorFlow中随机性的控制分为operation和graph两个层面。 Operation层面在Operation层面中，建立随机seed之后，新建立的Session每一次调用sess.run()都会遵循同一随机状态： c ​=​ tf​.​random_uniform​([],​​-​10​,​​10​,​ seed​=​2) with​ tf​.​Session​()​​as​ sess: print​ sess​.​run​(​c) # >> 3.57493 with​ tf​.​Session​()​​as​ sess: print​ sess​.​run​(​c) # >> 3.57493 而且，不同的operation可以保存自己的seed: c ​=​ tf​.​random_uniform​([],​​-​10​,​​10​,​ seed​=​1) d ​=​ tf​.​random_uniform​([],​​-​10​,​​10​,​ seed​=​2) with​ tf​.​Session​() ​​as​ sess: sess​.​run​(​c) sess​.​run​(​d) Graph层面在Graph层面，整张图公用一个随机状态，多次运行同一图模型的计算，其随机状态保持一致。 import​ tensorflow ​as​ tf tf​.​set_random_seed​(​2) c ​=​ tf​.​random_uniform​([],​​-​10​,​​10) d ​=​ tf​.​random_uniform​([],​​-​10​,​​10) with​ tf​.​Session​()​​ as​ sess: sess​.​run​(​c) sess​.​run​(​d) @ddlee","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://blog.ddlee.cn/tags/Tensorflow/"},{"name":"best practice","slug":"best-practice","permalink":"http://blog.ddlee.cn/tags/best-practice/"}]},{"title":"Python最佳实践：遍历列表","slug":"Python最佳实践：遍历列表","date":"2017-06-29T14:12:50.000Z","updated":"2017-08-01T13:00:37.746Z","comments":true,"path":"2017/06/29/Python最佳实践：遍历列表/","link":"","permalink":"http://blog.ddlee.cn/2017/06/29/Python最佳实践：遍历列表/","excerpt":"","text":"enumerate(): 遍历索引和值对列表进行遍历操作时，常也要用到当前遍历项的索引值： for i in range(len(flavor_list)): flavor = flaver[i] print('%d: %s' % (i + 1, flavor)) 这种写法既要取出列表长度，又要根据索引取列表值。但要改用enumerate()函数，则可同时取出索引值和遍历项值： for i, flavor in enumerate(flavor_list): print('%d: %s' % (i+1, flavor)) zip(): 并行遍历多个列表我们有多个长度相同的列表，需要在同一索引下对遍历项值进行操作： names = ['Cecilia', 'Lise', 'Marie'] letters = [len(n) for n in names] longest_name = None max_letters = 0 for i, name in enumerate(names): count = letters[i] if count > max_letters: longest_name = name max_letters = count 由上可见，name和letter通过索引值关联起来，而使用zip()函数，可免去根据索引取值的过程： for name, count in zip(names, letters): if count > max_letters: longest_name = name max_letters = count 要注意的是，当多个列表长度不一时，到达最短列表的末尾时，遍历停止。 而且，enumerate()和zip()返回的对象都是lazy generator，相对来说更加高效。 @ddlee","categories":[{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/categories/Programming/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"},{"name":"best practice","slug":"best-practice","permalink":"http://blog.ddlee.cn/tags/best-practice/"}]},{"title":"[论文笔记]Accurate, Large Minibatch SGD: Training ImageNet in One Hour","slug":"论文笔记-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour","date":"2017-06-14T14:43:41.000Z","updated":"2017-06-14T15:23:44.000Z","comments":true,"path":"2017/06/14/论文笔记-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/","link":"","permalink":"http://blog.ddlee.cn/2017/06/14/论文笔记-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/","excerpt":"","text":"论文：Accurate, Large Minibatch SGD: Training ImageNet in One Hour 这篇文章在各处都有很广泛的讨论，作为实验经验并不多的小白，将文中tricks只做些记录。 Linear Scaling Rule进行大批量的Minibatch SGD时会有批量越大，误差越大的问题。本文提出的Linear Scaling Rule正是试图解决这一问题。 Motivation设想两个情景：一是在一次参数更新中使用kn个样本梯度，二是分为k次更新，每次取n个样本梯度。 第一种情景的参数更新公式： $$w_t+1^{(1)} = w_t^{(1)} - \\mu^{(1)} \\frac{1}{kn} \\sum_{j \\leq k} \\sum \\bigtriangledown l(x, w_t)$$ 第二种情景的参数更新公式： $$w_t+k^{(2)} = w_t^{(2)} - \\mu^{(2)} \\frac{1}{n} \\sum_{j \\leq k} \\sum \\bigtriangledown l(x, w_t+j)$$ 由上面可以看出，主要的区别是梯度平均时批量的大小不同，前者为kn，后者为每次n，更新k次。 再假设双重求和号内项变化不大时，为使情景二更新k次（即使用同样数量的样本）之后参数与情景一类似，我们自然要将学习速率$\\mu$线性提升。 Gradual Warmup上面提到的Linear Scaling Rule使用的假设是梯度变化不大。但在训练初期，参数随机初始化，梯度变化很大，因而Linear Scaling Rule不再适用。在实践中，可以使学习速率在初始时较小，在经过几个epoch训练后再升至与kn批量相应的大小。 BN statistics在分布式训练的系统中，对于BN中要估计的均值和方差，文中给出的建议是对所有worker上的样本计算均值和方差，而不是每个worker单独计算。 Weight Decay由于weight decay的存在，Linear Scaling Rule最好用于学习速率，而非用于Loss Function Momentum Correction加入Linear Scaling Rule之后，适用动量加速的SGD需要进行动量更正。 Data Shuffling在分布式的系统中，先进行Data Shuffling，再分配数据到每个worker上。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"}]},{"title":"自用LaTeX中英文简历模板","slug":"自用LaTeX中英文建立模板","date":"2017-06-14T10:58:41.000Z","updated":"2017-06-14T10:58:42.000Z","comments":true,"path":"2017/06/14/自用LaTeX中英文建立模板/","link":"","permalink":"http://blog.ddlee.cn/2017/06/14/自用LaTeX中英文建立模板/","excerpt":"","text":"分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在ShareLaTeX上分享的模板。 使用 Github仓库：ddlee96/latex_cv_template 编译引擎： XeLaTeX 下载地址： v0.1 压缩包内包含.tex文件和所用字体文件，解压后修改.tex文件再编译即可。 在Ubuntu 16.04, Texlive 2016环境下测试通过。 英文字体: Fontin，中文字体：方正兰亭黑 协议 .tex代码：Apache 2.0 字体： 仅供个人使用 效果预览英文 中文","categories":[{"name":"Individual Development","slug":"Individual-Development","permalink":"http://blog.ddlee.cn/categories/Individual-Development/"}],"tags":[{"name":"Individual Development","slug":"Individual-Development","permalink":"http://blog.ddlee.cn/tags/Individual-Development/"},{"name":"LaTeX","slug":"LaTeX","permalink":"http://blog.ddlee.cn/tags/LaTeX/"}]},{"title":"[论文笔记]On the Effects and Weight Normalization in GAN","slug":"论文笔记-On-the-Effects-and-Weight-Normalization-in-GAN","date":"2017-06-10T14:01:21.000Z","updated":"2017-06-14T14:57:42.000Z","comments":true,"path":"2017/06/10/论文笔记-On-the-Effects-and-Weight-Normalization-in-GAN/","link":"","permalink":"http://blog.ddlee.cn/2017/06/10/论文笔记-On-the-Effects-and-Weight-Normalization-in-GAN/","excerpt":"","text":"论文：On the Effects and Weight Normalization in GAN 本文探索了参数标准化(Weight Normalization)这一技术在GAN中的应用。BN在mini-batch的层级上计算均值和方差，容易引入噪声，并不适用于GAN这种生成模型，而WN对参数进行重写，引入噪声更少。 我觉得本文的亮点有二： 1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变朴素的参数标准化层有如下的形式： $$y=\\frac{{w}^{T}x}{\\|w\\|}$$ 文中称这样形式的层为“strict weight-normalized layer”。若将线性层换为这样的层，网络的表达能力会下降，因而需要添加如下的affine transformation: $$y=\\frac{{w}^{T}x}{\\|w\\|} \\gamma + \\beta$$ 用于恢复网络的表达能力。 将上述变换带入ReLU，简化后可以得到如下T-ReLu:$$TReLU_\\alpha (x) = ReLU(x-\\alpha) + \\alpha$$ 文章的一个重要结论是，在网络的最后一层加入affine transformation层之后，堆叠的“线性层+ReLU”与“strict weight-normalized layer + T-ReLU”表达能力相同（在附录中给出证明）。 下面L表示线性层，R表示ReLU，TR表示TReLU，A表示affine transformation，S表示上述的strict weight-normalized layer。 证明的大致思路是，在ReLU与线性层之间加入affine transformation层，由于线性层的存在，affine transformation带来的效果会被吸收（相当于多个线性层叠在一起还是线性层），网络表达能力不变。而”L+R+A”的结构可以等价于”S+TR+A”。如此递归下去，即可得到结论。个人认为相当于把线性层中的bias转嫁成了TReLU中的threshold（即$\\alpha$）。 2. 提出对生成图形的评估指标生成式模型的生成效果常常难以评价。DcGAN给出的结果也是生成图片的对比。本文中提出一个评价生成效果的指标，且与人的主观评价一致。 评价的具体指标是生成图片与测试集图片的欧氏距离，评价的对象是生成器是Generator。有如下形式： $$\\frac{1}{m} \\sum_{i=1}^{m} min_z {\\|G(z)-x^{(i)}\\|}^2$$ 其中的$min$指使用梯度下降方法等使生成图片的效果最好。但事实上这样做开销很高。 PyTorch实现作者将他们的实现代码公布在了GitHub上。 下面是利用PyTorch对T-ReLU的实现： class TPReLU(Module): def __init__(self, num_parameters=1, init=0.25): self.num_parameters = num_parameters super(TPReLU, self).__init__() self.weight = Parameter(torch.Tensor(num_parameters).fill_(init)) self.bias = Parameter(torch.zeros(num_parameters)) def forward(self, input): bias_resize = self.bias.view(1, self.num_parameters, *((1,) * (input.dim() - 2))).expand_as(input) return F.prelu(input - bias_resize, self.weight.clamp(0, 1)) + bias_resize 对 Weigh-normalized layer 的实现： class WeightNormalizedLinear(Module): def __init__(self, in_features, out_features, scale=True, bias=True, init_factor=1, init_scale=1): super(WeightNormalizedLinear, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.Tensor(out_features, in_features)) if bias: self.bias = Parameter(torch.zeros(1, out_features)) else: self.register_parameter('bias', None) if scale: self.scale = Parameter(torch.Tensor(1, out_features).fill_(init_scale)) else: self.register_parameter('scale', None) self.reset_parameters(init_factor) def reset_parameters(self, factor): stdv = 1. * factor / math.sqrt(self.weight.size(1)) self.weight.data.uniform_(-stdv, stdv) if self.bias is not None: self.bias.data.uniform_(-stdv, stdv) def weight_norm(self): return self.weight.pow(2).sum(1).add(1e-6).sqrt() def norm_scale_bias(self, input): output = input.div(self.weight_norm().transpose(0, 1).expand_as(input)) if self.scale is not None: output = output.mul(self.scale.expand_as(input)) if self.bias is not None: output = output.add(self.bias.expand_as(input)) return output def forward(self, input): return self.norm_scale_bias(F.linear(input, self.weight)) 观察上面的forward函数可以发现，TReLU添加bias这一习得参数，而weight-normalized layer中则对传入的weight进行了标准化。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"GAN","slug":"GAN","permalink":"http://blog.ddlee.cn/tags/GAN/"}]},{"title":"[论文笔记]Large-Scale Evolution of Image Classifiers","slug":"论文笔记-Large-Scale-Evolution-of-Image-Classifiers","date":"2017-06-05T08:09:52.000Z","updated":"2017-06-05T08:09:52.000Z","comments":true,"path":"2017/06/05/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/","link":"","permalink":"http://blog.ddlee.cn/2017/06/05/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/","excerpt":"","text":"论文：Large-Scale Evolution of Image, Classifiers Abstract深层网络在图片分类问题上表现优异，但网络结构的设计上并没有统一的指导。进化是构建深度网络架构的一种方式。利用本文的自动化方法得出的深度网络结构，已经能在CIFAR-10上取得可以跟人工设计的网络相媲美的结果 MethodsEvolution Algorithm整个算法的核心是如下的tournament selection: population: 供筛选的群体 individual: 个体，带有指标fitness，特别地，指在CV集上的损失 worker: 筛选者，上帝 population 中的 individual 均已在训练集上训练完毕，带有指标 fitness worker 随机选择一对 individual，比较 fitness，较差的 individual 被舍弃 表现较好的 individual 成为parent，对其施加 mutation (变异)，得到 child 训练 child 并在CV集上得到其 fitness，归还到 population 中 Encoding and Mutation个体的网络结构和部分参数被编码为DNA。 能够施加的变异有： 改变学习率 恒等（不变） 重设参数 加入卷积层 移除卷积层 更改卷积层的stride参数 更改卷积层的Channel参数 更改卷积核大小 加入skip连接（类似ResNet) 移除skip连接 Computation计算方面采用了并行、异步、无锁的策略。 建立约为 population 数1/4的 worker，分别运行于不同的机器上，之间独立异步。population 共享，若两个 worker 在一个 individual 上产生冲突，则后一个 worker 停止并等待再次尝试。 Weight Inheritance除了架构之外，子模型还会继承父母模型未经变异影响的隐藏层参数（不仅是DNA中的），这样使子模型的训练时间大幅减小。 Experiments and Results文章的主要结果如下图： 最右边的结构是在CIFAR-10上发现的最好（CV集准确度最高）的结构，左边两个是它的祖先。其中白色块相当于简单的线性层，彩色块则带有非线性激活，可以看到，不同于人工设计的网络，某一线性层之后可能包含多个非线性层。 另外，利用本文的模型，也在CIFAR-100上做了实验，可以达到76.3%的准确率，一定程度上说明了算法的扩展性。 Analysis 上图说明随着 population 规模和训练步数的增加，模型的整体水平在变好。 在模型陷入局部最优值时，提高变异率和重设参数会使群体继续进化。这是由于变异中包含恒等映射等不改变模型架构的变异类型，再加上weight Inheritance，一些子模型只是训练次数比其他模型多很多的“活化石”。 小结Google I/O时就提到了自动筛选最优网络结构，但没有公布论文。但将网络结构自动化，必定是未来的方向。个人认为，ResNet就相当于自动化网络深度（一些层实际上被跳过了），而Inception单元似乎包含了太多的先验，而且也没有逻辑上的证据说明这样的结构更有效。网络结构本身就是先验信息，而要达到通用的人工智能，这些先验也必须由模型自行发觉。 强化学习本身也是一个进化过程，应该也有相关的工作将强化学习的框架应用于网络结构的学习上。 更进一步地，若数据是一阶信息，深度网络的隐藏层学到的表示是二阶信息，深度网络的结构则是三阶信息，从一阶到二阶的框架是不是都可以移植到二阶到三阶上来？关键之处在于我们还没有描述好深度网络的结构空间，但就现在的发展看，深度网络的一些基本结构(conv, BN)等，已经被作为基本单元（离散的）来进行构建和筛选了，也就是说，所有深度网络构成的空间之性质如何，还有大量的工作可以做。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"autoML","slug":"autoML","permalink":"http://blog.ddlee.cn/tags/autoML/"}]},{"title":"[论文笔记]An Analysis of Deep Neural Network Models for Practical Applications","slug":"论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications","date":"2017-06-03T06:27:07.000Z","updated":"2017-06-03T06:27:08.000Z","comments":true,"path":"2017/06/03/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/","link":"","permalink":"http://blog.ddlee.cn/2017/06/03/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/","excerpt":"","text":"论文：An Analysis of Deep Neural Network Models for Practical Applications 本文是对现有（论文发表于2016年5月）深度网络的比较，从以下方面入手： accuracy memory footprint parameters operations count inference time power consumption 以下图片各模型的着色是统一的：蓝色是Inception系，绿色是VGG系，粉色是ResNet系，黄色为AlexNet系。 上图是Top1准确率与模型参数数、操作数的关系。可以看到Inception系列网络以较少的参数取得相对高的准确率，而VGG系则在这一点上表现很差。 上面两图分别是推断耗时和电量消耗与批量大小的关系。可以看到，两者均与批量大小无明显的相关关系。但电量消耗在不同的模型之间也非常类似，而推断时间与模型结构关系很大（VGG再次尴尬）。 上图展示了模型占用内存大小与批量大小的关系，大部分网络都有相对固定的内存占用，随后随批量大小的上扬而上涨。 从上图可以发现推断耗时和模型的操作数大体上呈现线性关系。 电量消耗与模型的参数数、操作数并没有明显的相关性。 注意，上图中点的大小代表模型操作数，横轴代表推断效率，纵轴表示准确率。灰色区域表示模型获得了额外的推断效率或准确率，而白色区域代表非最优。 操作数越多的模型推断效率越低，大部分模型都落在相对平衡的边界上，VGG和小批量情形下的AlexNet落在了非最优区域。 小结从这篇论文的比较中可以看到，在特定的任务中对网络特定结构的设计（如Inception单元），即加入更强的先验知识，比堆叠网络层数更有效。深度网络还是需要人类的指导才能发挥更大的作用。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"Neural Network","slug":"Neural-Network","permalink":"http://blog.ddlee.cn/tags/Neural-Network/"}]},{"title":"深度学习和分布式表示","slug":"深度学习和分布式表示","date":"2017-06-01T14:52:16.000Z","updated":"2017-08-03T13:49:22.697Z","comments":true,"path":"2017/06/01/深度学习和分布式表示/","link":"","permalink":"http://blog.ddlee.cn/2017/06/01/深度学习和分布式表示/","excerpt":"","text":"本文的两个主要参考资料： Yoshua Bengio在2016年九月Deep Learning School的演讲Foundations and Challenges of Deep Learning。YouTube Deep Learning, Goodfellow et al, Section 15.4 从机器学习到人工智能在演讲中，Bengio提到从机器学习到人工智能有五个关键的飞跃： Lots of data Very flexible models Enough computing power Powerful priors that can defeat the curse of dimensionality Computationally efficient inference 第一点已经发生，到处都提大数据，到处都在招数据分析师。我在读高中时，就曾预感数据将是新时代的石油和煤炭，因为数据正是人类社会经验的总结，数据带来的知识和见解将在驱动社会进步中发挥越来越重要的作用，而自己要立志成为新时代的矿工。 第二点在我看来有两个例子，一是核技巧，通过核函数对分布空间的转换，赋予了模型更强大的表述能力；二是深度神经网络，多层的框架和非线性的引入使得模型理论上可以拟合任意函数。 第三点，借云计算的浪潮，计算力不再是一项资产而是一项可供消费的服务，我们学生也可以廉价地接触到根本负担不起的计算力资源。而GPU等芯片技术的进步也为AI的浩浩征程添砖加瓦。 第五点，近期发布的Tensorflow Lite和Caffe2等工具也有助于越来越多地将计算任务分配在终端上进行，而非作为一个发送与接收器。 最后第四点，也是这篇文章的中心话题：借助分布式表示的强大能力，深度学习正尝试解决维度带来的灾难。 没有免费的午餐简单说，没有免费的午餐定理指出找不到一个在任何问题上都表现最优的模型/算法。不同的模型都有其擅长的问题，这由该模型建立时引入的先验知识决定。 那么，深度学习加入的先验知识是什么？ Bengio用的词是Compositionality，即复合性，某一概念之意义由其组成部分的意义以及组合规则决定。复合性的原则可以用于高效地描述我们的世界，而深度学习模型中隐藏的层正是去学习其组成部分，网络的结构则代表了组合规则。这正是深度学习模型潜在的信念。 分布式表示带来的指数增益分布式表示(Distributed Representation)是连接主义的核心概念，与复合性的原理相合。整体由组成它的个体及其组合来表示。请看下面的例子： 描述一个形状，我们将其分解为不同的特征来表述。分布式表示是一种解耦，它试图复杂的概念分离成独立的部分。而这也引出了分布式表示带来的缺点：隐藏层学到的分解特征难以得到显式的解释。 传统的机器学习算法，如K-Means聚类、决策树等，大多使用的是非分布式表示，即用特定的参数去描述特定的区域。如K-Means聚类，我们要划分多少区域，就需要有多少个中心点。因而，这类算法的特点是，随着参数个数的提升，其能描述的概念线性增长。 使用分布式表示的深度网络，则可以享受到指数级的增益，即，随着参数个数的提升，其表述能力是指数级的增长。具有$k$个值的$n$个特征，可以描述${k}^{n}$个不同的概念。 分布式表示在泛化上的优势分布式的想法还可以得到额外的泛化优势。通过重新组合在原有数据中抽离出来的特征，可以表示得到原有数据中不存在的实例。在Radford et al.的工作中，生成模型区习得了性别，并能从“戴眼镜的男人”-“男人”+“女人”=“戴眼镜的女人”这样的抽象概念表达式中生成实例。 分布式表示与巻积神经网络巻积神经网络不同的滤波器习得的特征可以为分布式表示的概念分解这一特性提供一些例子。下图是VGG16不同滤波器得到结果的可视化表示，出自Francois Chollet的博文How convolutional neural networks see the world 可以看到，浅层的滤波器学到的是简单的颜色、线条走向等特征，较深的滤波器学到复杂的纹理。 量子计算机与分布式表示在我看来，量子计算机的激动人心之处也在于其表示能力。一个量子态可以表示原先两个静态表示的信息，原先需要8个单位静态存储表示的信息只需要3个量子态单位即可表示，这也是指数级的增益。在这一点上，计算模型和概念模型已然殊途同归。 小结从经验中总结原则，用原则生成套路，正是我们自己处理和解决新问题的途径。通过解耦得到的信息来消除未知和不确定性，是我们智能的一部分。我们眼中的世界，只是适合我们的一种表示而已。也许，真正的人工智能到来那一刻，会是我们创造的机器“理解”了自己的表示系统之时——我们所关注的可解释性，也就无关紧要了。","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://blog.ddlee.cn/tags/Machine-Learning/"}]},{"title":"[论文笔记]On-the-fly Operation Batching in Dynamic Computation Graphs","slug":"论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs","date":"2017-05-30T07:24:34.000Z","updated":"2017-05-30T07:24:34.000Z","comments":true,"path":"2017/05/30/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/","link":"","permalink":"http://blog.ddlee.cn/2017/05/30/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/","excerpt":"","text":"论文：On-the-fly Operation Batching in Dynamic Computaion Graphs 背景基于动态图的深度学习框架如Pytorch,DyNet提供了更为灵活的结构和数据维度的选择，但要求开发者自行将数据批量化，才能最大限度地发挥框架的并行计算优势。 当前的状况：灵活的结构与高效计算左图为循环结构，右图将序列补齐，批量化 灵活的结构和数据输入维度，采用朴素的循环结构实现，但不高效，因为尽管维度不同，在循环内数据接受的是同样的操作。 对数据做“Padding”，即用傀儡数据将输入维度对齐，进而实现向量化，但这种操作对开发者并不友好，会使开发者浪费掉很多本该投入到结构设计等方面的精力。 本文提出的方法三个部分 Graph Definition Operation Batching Computation 第一步和第三步在当前已被大部分深度学习框架较好地实现。主要特点是，构建计算图与计算的分离，即”Lazy Evaluation”。比如在Tensorflow中，一个抽象层负责解析计算图各节点之间的依赖，决定执行计算的顺序，而另一个抽象层则负责分配计算资源。 Operation BatchingComputing compatibility groups这一步是建立可以批量化计算的节点组。具体做法是，给每一个计算节点建立 signature，用于描述节点计算的特性，文中举出了如下几个例子: Component-wise operations: 直接施加在每个张量元素上的计算，跟张量的维度无关，如$tanh$,$log$ Dimension-sensitive operations: 基于维度的计算，如线性传递$Wh+b$，要求$W$和$h$维度相符，signature 中要包含维度信息 Operations with shared elements: 包含共享元素的计算，如共享的权值$W$ Unbatchable operations: 其他 Determining execution order执行顺序要满足两个目标： 每一节点的计算要在其依赖之后 带有同样 signature 且没有依赖关系的节点放在同一批量执行 但在一般情况下找到最大化批量规模的执行顺序是个NP问题。有如下两种策略： Depth-based Batching: 库Tensorflow Fold中使用的方法。某一节点的深度定义为其子节点到其本身的最大长度，同一深度的节点进行批量计算。但由于输入序列长度不一，可能会错失一些批量化的机会。 Agenda-based Batching: 本文的方法，核心的想法是维护一个 agenda 序列，所有依赖已经被解析的节点入列，每次迭代时从 agenda 序列中按 signature 相同的原则取出节点进行批量计算。 实验文章选取了四个模型：BiLSTM, BiLSTM w/char, Tree-structured LSTMs, Transition-based Dependency Parsing。 实验结果：（单位为Sentences/second） 小结本来读到题目还是蛮惊喜的，期待的是从模型构建的角度解决序列长度不一带来的计算上的不便。但通读下来发现是在计算图的计算这一层面进行的优化，有些失望但也感激，作者使用DyNet框架实现了他们的方法，希望自己也可以为Pytorch等框架该算法的实现出一份力。 感谢这些开源的框架，正一步步拉近人类构建模型和机器高效计算之间的距离。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://blog.ddlee.cn/tags/Machine-Learning/"},{"name":"Paper","slug":"Paper","permalink":"http://blog.ddlee.cn/tags/Paper/"}]},{"title":"LSTM:Pytorch实现","slug":"LSTM-Pytorch实现","date":"2017-05-28T17:06:44.000Z","updated":"2017-05-28T17:16:52.000Z","comments":true,"path":"2017/05/29/LSTM-Pytorch实现/","link":"","permalink":"http://blog.ddlee.cn/2017/05/29/LSTM-Pytorch实现/","excerpt":"","text":"本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。 LSTMLSTM是一种循环神经网络，适用于对序列化的输入建模。Chris Olah的这篇文章细致地解释了一个LSTM单元的运作方式，建议阅读。 两个想法Gate：信息流动的闸门$$it = sigmoid(W{xi} xt + W{hi}h_{t-1} + b_i)$$$$ft = sigmoid(W{xf} xt + W{hf}h_{t-1} + b_f)$$$$ot = sigmoid(W{xo} xt + W{ho}h_{t-1} + b_o)$$$x$ 表示输入，$h$表示隐藏状态，用$sigmoid$函数将输入二者的传递结果映射到$（0,1)$上，分别赋予输入门、遗忘门、输出门的含义，来控制不同神经单元（同一神经元不同时间点的状态）之间信息流动。 Cell：记忆池$$c_t = ft \\odot c{t - 1} + it \\odot tanh(W{xc} xt + W{hc}h_{t-1} + b_c)\\h_t = o_t \\odot tanh(c_t)$$$h$表示隐藏状态，$C$表示记忆池，通过Gate，上一单元（状态）的信息有控制地遗忘，当前的输入有控制地流入，记忆池中的信息有控制地流入隐藏状态。 与普通RNN的对比普通RNN只有一个自更新的隐藏状态单元。 LSTM增加了记忆池Cell，并通过几个Gate将信息有控制地更新在记忆池中，并通过记忆池中的信息来决定隐藏状态。 From Scratch下面是手动实现LSTM的代码，继承了基类nn.Module。 import torch.nn as nn import torch from torch.autograd import Variable class LSTM(nn.Module): def __init__(self, input_size, hidden_size, cell_size, output_size): super(LSTM, self).__init__() self.hidden_size = hidden_size self.cell_size = cell_size self.gate = nn.Linear(input_size + hidden_size, cell_size) self.output = nn.Linear(hidden_size, output_size) self.sigmoid = nn.Sigmoid() self.tanh = nn.Tanh() self.softmax = nn.LogSoftmax() def forward(self, input, hidden, cell): combined = torch.cat((input, hidden), 1) f_gate = self.gate(combined) i_gate = self.gate(combined) o_gate = self.gate(combined) f_gate = self.sigmoid(f_gate) i_gate = self.sigmoid(i_gate) o_gate = self.sigmoid(o_gate) cell_helper = self.gate(combined) cell_helper = self.tanh(cell_helper) cell = torch.add(torch.mul(cell, f_gate), torch.mul(cell_helper, i_gate)) hidden = torch.mul(self.tanh(cell), o_gate) output = self.output(hidden) output = self.softmax(output) return output, hidden, cell def initHidden(self): return Variable(torch.zeros(1, self.hidden_size)) def initCell(self): return Variable(torch.zeros(1, self.cell_size)) 几个关键点： Tensor的大小 信息的传递顺序 Pytorch ModulePytorch库本身对LSTM的实现封装了更多功能，类和函数的组织也非常有借鉴意义。我对其实现的理解基于以下两点展开： 胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构） 函数句柄的传递：处理好参数后返回函数句柄forward 下面开始按图索骥，源码见GitHub。 LSTM类文件：nn/modules/rnn.py # nn/modules/rnn.py class RNNBase(Module): def __init__(self, mode, input_size, output_size): pass def forward(self, input, hx=None): if hx is None: hx = torch.autograd.Variable() if self.mode == 'LSTM': hx = (hx, hx) func = self._backend.RNN() #!!! output, hidden = func(input, self.all_weights, hx) #!!! return output, hidden class LSTM(RNNBase): def __init__(self, *args, **kwargs): super(LSTM, self).__init__('LSTM', *args, **kwargs) LSTM类只是RNNBase类的一个装饰器。 在基类nn.Module中，把__call__()定义为调用forward()方法，因而真正的功能实现在_backend.RNN()中 AutogradRNN函数下面寻找_backend.RNN。文件：nn/backends/thnn.py # nn/backends/thnn.py def _initialize_backend(): from .._functions.rnn import RNN, LSTMCell 原来，_backend也是索引。 终于找到RNN()函数。文件：nn/_functions/rnn.py # nn/_functions/rnn.py def RNN(*args, **kwargs): def forward(input, *fargs, **fkwargs): func = AutogradRNN(*args, **kwargs) return func(input, *fargs, **fkwargs) return forward def AutogradRNN(mode, input_size, hidden_size): cell = LSTMCell rec_factory = Recurrent layer = (rec_factory(cell),) func = StackedRNN(layer, num_layers) def forward(input, weight, hidden): nexth, output = func(input, hidden, weight) return output, nexth return forward RNN()是一个装饰器，根据是否有cudnn库决定调用AutogradRNN()还是CudnnRNN()，这里仅观察AutogradRNN() AutogradRNN()选用了LSTMCell，用Recurrent()函数处理了Cell构成Layer，再将Layer传入StackedRNN()函数 RNN()和AutogradRNN()返回的都是其forward()函数句柄 下面是Recurrent()函数： def Recurrent(inner): def forward(input, hidden, weight): output = [] steps = range(input.size(0) - 1, -1, -1) for i in steps: hidden = inner(input[i], hidden, *weight) output.append(hidden[0]) return hidden, output return forward Recurrent()函数实现了“递归”的结构，根据输入的大小组合Cell，完成了隐藏状态和参数的迭代。 Recurrent()函数将Cell(inner)组合为Layer。 StackedRNN()函数def StackedRNN(inners, num_layers): num_directions = len(inners) total_layers = num_layers * num_directions def forward(input, hidden, weight): next_hidden = [] hidden = list(zip(*hidden)) for i in range(num_layers): all_output = [] for j, inner in enumerate(inners): hy, output = inner(input, hidden[l], weight[l]) next_hidden.append(hy) all_output.append(output) input = torch.cat(all_output, input.dim() - 1) next_h, next_c = zip(*next_hidden) next_hidden = (torch.cat(next_h, 0).view(total_layers, *next_h[0].size()), torch.cat(next_c, 0).view(total_layers, *next_c[0].size())) return next_hidden, input return forward StackedRNN()函数将Layer(inner)组合为栈 最后的最后，一个基本的LSTM单元内的计算由LSTMCell()函数实现。 LSTMCell()函数def LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None): if input.is_cuda: igates = F.linear(input, w_ih) hgates = F.linear(hidden[0], w_hh) state = fusedBackend.LSTMFused() return state(igates, hgates, hidden[1]) if b_ih is None else state(igates, hgates, hidden[1], b_ih, b_hh) hx, cx = hidden gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh) ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1) ingate = F.sigmoid(ingate) forgetgate = F.sigmoid(forgetgate) cellgate = F.tanh(cellgate) outgate = F.sigmoid(outgate) cy = (forgetgate * cx) + (ingate * cellgate) hy = outgate * F.tanh(cy) return hy, cy 观察上面的代码，即是LSTM的基本信息传递公式。至此，我们的旅程完成。 小结 没有什么是增加一层抽象不能解决的，如果不能，那就再加一层。 重复一下我对上述代码的理解： 胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构） 函数句柄的传递：处理好参数后返回函数句柄forward 如洋葱一般，我们剥到最后，发现处理的信息正是输入、隐藏状态和LSTM单元几个控制门的参数。在一层一层的抽象之中，Pytorch在不同的层面处理了不同的参数，保证了扩展性和抽象层之间的解耦。","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://blog.ddlee.cn/tags/Pytorch/"}]},{"title":"Pandas速度优化","slug":"Pandas速度优化","date":"2017-05-28T12:06:14.000Z","updated":"2017-05-28T12:17:22.000Z","comments":true,"path":"2017/05/28/Pandas速度优化/","link":"","permalink":"http://blog.ddlee.cn/2017/05/28/Pandas速度优化/","excerpt":"","text":"本文主要内容取自Sofia Heisler在PyCon 2017上的演讲No More Sad Pandas Optimizing Pandas Code for Speed and Efficiency，讲稿代码和幻灯片见GitHub。 Set Up示例数据 ean_hotel_id name address1 city state_province postal_code latitude longitude star_rating high_rate low_rate 0 269955 Hilton Garden Inn Albany/SUNY Area 1389 Washington Ave Albany NY 12206 42.68751 -73.81643 3.0 154.0272 124.0216 1 113431 Courtyard by Marriott Albany Thruway 1455 Washington Avenue Albany NY 12206 42.68971 -73.82021 3.0 179.0100 134.0000 2 108151 Radisson Hotel Albany 205 Wolf Rd Albany NY 12205 42.72410 -73.79822 3.0 134.1700 84.1600 示例函数：Haversine Distancedef haversine(lat1, lon1, lat2, lon2): miles_constant = 3959 lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2 c = 2 * np.arcsin(np.sqrt(a)) mi = miles_constant * c return mi 优化它之前，先测量它IPython Notebook的Magic Command: %timeit既可以测量某一行代码的执行时间，又可以测量整个单元格里代码快的执行时间。 Package: line_profiler记录每行代码的执行次数和执行时间。 在IPython Notebook中使用时，先运行%load_ext line_profiler， 之后可以用%lprun -f [function name]命令记录指定函数的执行情况。 实验对行做循环(Baseline)%%timeit haversine_series = [] for index, row in df.iterrows(): haversine_series.append(haversine(40.671, -73.985,\\ row['latitude'], row['longitude'])) df['distance'] = haversine_series Output: 197 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) pd.DataFrame.apply()方法%lprun -f haversine \\ df.apply(lambda row: haversine(40.671, -73.985,\\ row['latitude'], row['longitude']), axis=1) Output: 90.6 ms ± 7.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Timer unit: 1e-06 s Total time: 0.049982 s File: &lt;ipython-input-3-19c704a927b7&gt; Function: haversine at line 1 Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 def haversine(lat1, lon1, lat2, lon2): 2 1631 1535 0.9 3.1 miles_constant = 3959 3 1631 16602 10.2 33.2 lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2]) 4 1631 2019 1.2 4.0 dlat = lat2 - lat1 5 1631 1143 0.7 2.3 dlon = lon2 - lon1 6 1631 18128 11.1 36.3 a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2 7 1631 7857 4.8 15.7 c = 2 * np.arcsin(np.sqrt(a)) 8 1631 1708 1.0 3.4 mi = miles_constant * c 9 1631 990 0.6 2.0 return mi 观察Hits这一列可以看到，apply()方法还是将函数一行行地应用于每行。 向量化：将pd.Series传入函数%lprun -f haversine haversine(40.671, -73.985,\\ df[&#39;latitude&#39;], df[&#39;longitude&#39;]) Output: 2.21 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) Timer unit: 1e-06 s Total time: 0.008601 s File: &lt;ipython-input-3-19c704a927b7&gt; Function: haversine at line 1 Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 def haversine(lat1, lon1, lat2, lon2): 2 1 3 3.0 0.0 miles_constant = 3959 3 1 838 838.0 9.7 lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2]) 4 1 597 597.0 6.9 dlat = lat2 - lat1 5 1 572 572.0 6.7 dlon = lon2 - lon1 6 1 5033 5033.0 58.5 a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2 7 1 1060 1060.0 12.3 c = 2 * np.arcsin(np.sqrt(a)) 8 1 496 496.0 5.8 mi = miles_constant * c 9 1 2 2.0 0.0 return mi 向量化之后，函数内的每行操作只被访问一次，达到了行结构上的并行。 向量化：将np.array传入函数%lprun -f haversine df['distance'] = haversine(40.671, -73.985,\\ df['latitude'].values, df['longitude'].values) Output： 370 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) Timer unit: 1e-06 s Total time: 0.001382 s File: &lt;ipython-input-3-19c704a927b7&gt; Function: haversine at line 1 Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 def haversine(lat1, lon1, lat2, lon2): 2 1 3 3.0 0.2 miles_constant = 3959 3 1 292 292.0 21.1 lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2]) 4 1 40 40.0 2.9 dlat = lat2 - lat1 5 1 29 29.0 2.1 dlon = lon2 - lon1 6 1 815 815.0 59.0 a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2 7 1 183 183.0 13.2 c = 2 * np.arcsin(np.sqrt(a)) 8 1 18 18.0 1.3 mi = miles_constant * c 9 1 2 2.0 0.1 return mi 相比pd.Series，np.array不含索引等额外信息，因而更加高效。 小结 Methodology Avg. single run time Marginal performance improvement Looping with iterrows 184.00 - Looping with apply 78.10 2.4x Vectorization with Pandas series 1.79 43.6x Vectorization with NumPy arrays 0.37 4.8x 通过上面的对比，我们比最初的baseline快了近500倍。最大的提升来自于向量化。因而，实现的函数能够很方便地向量化是高效处理的关键。 用Cython优化Cython可以将python代码转化为C代码来执行，可以进行如下优化（静态化变量类型，调用C函数库） %load_ext cython %%cython -a # Haversine cythonized from libc.math cimport sin, cos, acos, asin, sqrt cdef deg2rad_cy(float deg): cdef float rad rad = 0.01745329252*deg return rad cpdef haversine_cy_dtyped(float lat1, float lon1, float lat2, float lon2): cdef: float dlon float dlat float a float c float mi lat1, lon1, lat2, lon2 = map(deg2rad_cy, [lat1, lon1, lat2, lon2]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) mi = 3959 * c return mi 嵌套于循坏中： %timeit df['distance'] =\\ df.apply(lambda row: haversine_cy_dtyped(40.671, -73.985,\\ row['latitude'], row['longitude']), axis=1) Output: 10 loops, best of 3: 68.4 ms per loop 可以看到，Cython确实带来速度上的提升，但效果不及向量化（并行化）。","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/categories/Data-Science/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/tags/Data-Science/"},{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"}]},{"title":"Python可视化工具指引","slug":"Python可视化工具指引","date":"2017-05-27T16:12:28.000Z","updated":"2017-11-18T17:45:32.127Z","comments":true,"path":"2017/05/28/Python可视化工具指引/","link":"","permalink":"http://blog.ddlee.cn/2017/05/28/Python可视化工具指引/","excerpt":"","text":"本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲Python’s Visualization Landscape Python真是越来越火了。活跃的开源社区为Python这门语言贡献着长青的活力。 子曾经曰过：轮子多了，车就稳了。 本文帮助你选好轮子，也祝愿可视化的车开得越来越稳。 The Landscape 如图。 VanderPlas在展示完这张全景图后给大家贴了这张图： 我差点笑喷。我们的表情包可能要在人民币之前走向国际化了。 回到正题，可视化工具有两个主要阵营，一是基于matplotlib，二是基于JavaScript。还有的接入了JS下著名的D3.js库。 Matplotlibnumpy, pandas, matplotlib可以说是python数据科学的三驾马车。凡以python为教学语言的数据科学相关课程必提这三个库。而matplotlib又有什么特点呢？ 先说优点： 像MATLAB的语法，对MATLAB用户好上手 稳定，久经考验 渲染后端丰富，跨平台（GTK, Qt5, svg, pdf等） 缺点也有很多： API过于繁琐 默认配色太丑 对web支持差，交互性差 对大数据集处理较慢 于是就有了很多基于matplotlib的扩展，提供了更丰富、更人性化的API。 下面是几个比较受欢迎的包： pandaspandas的DataFrame对象是有plot()方法的，如：iris.plot.scatter(&#39;petalLength&#39;, &#39;petalWidth&#39;)生成二维散点图，只需指明两个轴取自哪一列数据即可。 seabornseaborn(gallery)专注于统计数据可视化，默认配色也还可以。语法示例： import seaborn as sns sns.lmplot('petalLength', 'sepalWidth', iris, hue='species', fit_reg=False) 类ggplot对于R用户，最熟悉的可视化包可能是ggplot2，在python中可以考虑ggpy和近期上了Github Trends的plotnie。 JavaScript基于JS的包常常具有非常好的交互性，其共同点是将图形格式化为json文件，再由JS完成渲染。 BokehBokeh(Gallery)定位于绘制用于浏览器展示的交互式图形。其优点是交互性、能够处理大量数据和流数据。语法示例： p = figure() p.circle(iris.petalLength, iris.sepalWidth) show(p) PlotlyPlotly(Gallery)跟Bokeh类似。但其提供了多种语言接口(JS, R, Python, MATLAB)，并且支持3D和动画效果，缺点是有些功能需要付费。语法示例： from plotly.graph_objs import Scatter from plotly.offline import iplot p = Scatter(x=iris.petalLength, y=iris.sepalWidth, mode='markers') iplot(p) 处理大型数据集对于大型数据集，可以考虑的包包括datashader, Vaex, 基于OpenGL的Vispy和Glumpy，GlueViz等。这里介绍datashader。 datashaderdatashader是Bokeh的子项目，为处理大型数据集而生。 示例语法： from colorcet import fire export(tf.shade(agg, cmap=cm(fire, 0.2), how='eq_hist'), 'census_ds_fier_eq_hist') 最终的建议上车忠告： matplotlib必会 R用户：ggpy/plotnine 交互式：plotly(与R接口统一)/bokeh(免费)","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/categories/Data-Science/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"Visualization","slug":"Visualization","permalink":"http://blog.ddlee.cn/tags/Visualization/"}]},{"title":"[论文笔记]Deep Learning","slug":"论文笔记-Deep-Learning","date":"2017-05-23T11:09:09.000Z","updated":"2017-06-02T11:24:56.000Z","comments":true,"path":"2017/05/23/论文笔记-Deep-Learning/","link":"","permalink":"http://blog.ddlee.cn/2017/05/23/论文笔记-Deep-Learning/","excerpt":"","text":"论文：Deep Learning 这篇文章是三位大牛15年发表在Nature上有关深度学习的综述，尽管这两年深度学习又有更多的模型和成果出现，文章显得有些过时，但来自三位领军人物对深度学习的深度阐述还是值得反复回味。 Abstract摘要的第一句话可以说给深度学习下了定义。有一些观点认为深度学习就是堆叠了很多层的神经网络，因计算力的提升而迎来第二春。但请看三位是怎么说的： Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. 也就是说，深度学习是允许由 多个处理层构成的计算模型 用多个层次的 抽象 来习得 数据表示 的技术。我的解读如下： 深度学习不限于神经网络模型，其关键之处在于多层的表示 深度学习属于表示学习，目的是习得数据的某种表示，而这种表示由多个层次的抽象完成 在第一段的导言中，文章总结了深度学习技术取得突破性成果的各个领域，也再次指出了深度学习与传统学习算法的不同之处： 传统学习模型需要特征工程和领域知识来从数据构建较好的特征 深度学习中，多层的特征由通用的学习过程得到，而不需要人类工程师的参与 Supervised learning这一段概述了监督学习的一般框架、优化策略，并指出浅层学习需要Feature Extractor来提取对最适合目标问题的特征。 Backpropagation to train multilayer architectures这一段指出BP算法的关键在于目标函数关于某一子模块输入的导数可以反向通过目标函数关于该子模块输出的导数得出，而这一过程是可迭代的。BP算法曾因容易陷于局部最优解而被冷落，但对于大型网络，在实践中，理论和经验都表明尽管落于局部最优解，但这个解的效果却和全局最优解相差无几，而且几乎所有的局部最优解都可以取得类似的效果。 Convolutional neural networks巻积网络背后有四个关键想法： local connections shared weights pooling the use of many layers 巻积网络常由巻积层、池化层和激活层构成，巻积层用于提取局部特征，池化层用于整合相似的特征，激活层用于加入非线性。这样的结构有两点理由： 张量性数据的局部数值常常高度相关，局部特征容易发现 局部特征跟位置无关（平移不变性） 文章也提到了这种巻积结构的仿生学证据。 Image understanding with deep convolutional networks这一段总结了巻积网路在图像方面取得的成就。 Distributed representations and language processing分布式表示在两点上可以取得指数级增益： 习得特征的不同组合可以泛化出训练数据中不存在的类型 特征组合的个数的增加关于层数是指数级的 文章还比较了分布式表示相比传统的词频统计在表述人类语言方面的优势。 Recurrent neural networks这一段概述了循环神经网络的动态特性和LSTM等结构上的改进。 The future of deep learning作者认为在长期看来，无监督学习会更为重要，人工智能领域的重大飞跃将由组合了表示学习和复杂推理的系统取得。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"}]},{"title":"Dropout-Pytorch实现","slug":"Dropout-Pytorch实现","date":"2017-05-17T11:30:44.000Z","updated":"2017-06-03T08:32:34.000Z","comments":true,"path":"2017/05/17/Dropout-Pytorch实现/","link":"","permalink":"http://blog.ddlee.cn/2017/05/17/Dropout-Pytorch实现/","excerpt":"","text":"Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。 Dropout的洞察关于Dropout，一个流行的解释是，通过随机行为训练网络，并平均多个随机决定的结果，实现了参数共享的Bagging。如下图，通过随机地冻结/抛弃某些隐藏单元，我们得到了新的子网络，而参数共享是说，与Bagging中子模型相互独立的参数不同，深度网络中Dropout生成的子网络是串行的，后一个子模型继承了前一个子模型的某些参数。 Dropout是模型自我破坏的一种形式，这种破坏使得存活下来的部分更加鲁棒。例如，某一隐藏单元学得了脸部鼻子的特征，而在Dropout中遭到破坏，则在之后的迭代中，要么该隐藏单元重新学习到鼻子的特征，要么学到别的特征，后者则说明，鼻子特征对该任务来说是冗余的，因而，通过Dropout，保留下来的特征更加稳定和富有信息。 Hinton曾用生物学的观点解释这一点。神经网络的训练过程可以看做是生物种群逐渐适应环境的过程，在迭代中传递的模型参数可以看做种群的基因，Dropout以随机信号的方式给环境随机的干扰，使得传递的基因不得不适应更多的情况才能存活。 另一个需要指出的地方是，Dropout给隐藏单元加入的噪声是乘性的，不像Bias那样加在隐藏单元上，这样在进行反向传播时，Dropout引入的噪声仍能够起作用。 代码实现下面看在实践中，Dropout层是如何实现的。简单来说，就是生成一系列随机数作为mask，然后再用mask点乘原有的输入，达到引入噪声的效果。 From Scratch# forward pass def dropout_forward(x, dropout_param): p, mode = dropout_param['p'], dropout_param['mode'] # p: dropout rate; mode: train or test if 'seed' in dropout_param: np.random_seed(dropout_param['seed']) # seed: random seed mask = None out = None if mode == 'train': mask = (np.random.rand(*x.shape) >= p)/(1-p) # 1-p as normalization multiplier: to keep the size of input out = x * mask elif mode == 'test': # do nothing when perform inference out = x cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cache # backward pass def dropout_backward(dout, cache): dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': dx = dout * mask elif mode == 'test': dx = dout return dx Pytorch实现file: /torch/nn/_functions/dropout.py class Dropout(InplaceFunction): def __init__(self, p=0.5, train=False, inplace=False): super(Dropout, self).__init__() if p &lt; 0 or p > 1: raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p)) self.p = p self.train = train self.inplace = inplace def _make_noise(self, input): # generate random signal return input.new().resize_as_(input) def forward(self, input): if self.inplace: self.mark_dirty(input) output = input else: output = input.clone() if self.p > 0 and self.train: self.noise = self._make_noise(input) # multiply mask to input self.noise.bernoulli_(1 - self.p).div_(1 - self.p) if self.p == 1: self.noise.fill_(0) self.noise = self.noise.expand_as(input) output.mul_(self.noise) return output def backward(self, grad_output): if self.p > 0 and self.train: return grad_output.mul(self.noise) else: return grad_output","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://blog.ddlee.cn/tags/Pytorch/"}]},{"title":"[论文笔记]Visualizing and Understanding Recurrent Networks","slug":"论文笔记-Visualizing-and-Understanding-Recurrent-Networks","date":"2017-05-13T06:06:51.000Z","updated":"2017-06-03T04:35:10.000Z","comments":true,"path":"2017/05/13/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/","link":"","permalink":"http://blog.ddlee.cn/2017/05/13/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/","excerpt":"","text":"论文： Visualizing and Understanding Recurrent Networks 实验设定字母级的循环神经网络，用Torch实现，代码见GitHub。字母嵌入成One-hot向量。优化方面，采用了RMSProp算法，加入了学习速率的decay和early stopping。 数据集采用了托尔斯泰的《战争与和平》和Linux核心的代码。 可解释性激活的例子$tanh$函数激活的例子，$-1$为红色，$+1$为蓝色。 上图分别是记录了行位置、引文和if语句特征的例子和失败的例子。 上图分别是记录代码中注释、代码嵌套深度和行末标记特征的例子。 Gates数值的统计 此图信息量很大。 left-saturated和right-saturated表示各个Gates激活函数（$sigmoid$）小于0.1和大于0.9，即总是阻止信息流过和总是允许信息流过。 横轴和纵轴表示该Gate处于这两种状态的时间比例，即有多少时间是阻塞状态，有多少时间是畅通状态。 三种颜色表示不同的层。 有以下几个观察： 第一层的门总是比较中庸，既不阻塞，也不畅通 第二三层的门在这两种状态间比较分散，经常处于畅通状态的门可能记录了长期的依赖信息，而经常处于阻塞状态的门则负责了短期信息的控制。 错误来源分析在这一节，作者用了“剥洋葱”的方法，建立了不同的模型将错误进行分解。此处错误指LSTM预测下一个字母产生的错误，数据集为托尔斯泰的《战争与和平》。 n-gram Dynamic n-long memory，即对已经出现过得单词的复现。如句子”Jon yelled atMary but Mary couldn’t hear him.”中的Mary。 Rare words，不常见单词 Word model，单词首字母、新行、空格之后出现的错误 Punctuation，标点之后 Boost，其他错误 根据作者的实验，错误的来源有如下分解： 小结这篇文章是打开LSTM黑箱的尝试，提供了序列维度上共享权值的合理性证据，对Gates状态的可视化也非常值得关注，最后对误差的分解可能对新的网络结构有所启发（比如，如何将单词级别和字母级别的LSTM嵌套起来，解决首字母预测的问题？）。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/tags/AI/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://blog.ddlee.cn/tags/Machine-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"}]},{"title":"[论文笔记]Deep Residual Learning for Image Recognition","slug":"论文笔记-Deep-Residual-Learning-for-Image-Recognition","date":"2017-04-30T15:12:11.000Z","updated":"2017-05-30T13:37:56.000Z","comments":true,"path":"2017/04/30/论文笔记-Deep-Residual-Learning-for-Image-Recognition/","link":"","permalink":"http://blog.ddlee.cn/2017/04/30/论文笔记-Deep-Residual-Learning-for-Image-Recognition/","excerpt":"","text":"论文：Deep Residual Learning for Image Recognition 背景网络在堆叠到越来越深之后，由于BP算法所依赖的链式法则的连乘形式，会出现梯度消失和梯度下降的问题。初始标准化和中间标准化参数在一定程度上缓解了这一问题，但仍然存在更深的网络比浅层网络具有更大的训练误差的问题。 基本结构假设多层的网络结构能够任意接近地拟合目标映射$H(x)$，那么也能任意接近地拟合其关于恒等映射的残差函数$H(x)-x$。记$F(x)=H(x)-x$，则原来的目标映射表为$F(x)+x$。由此，可以设计如下结构。 残差单元 残差单元包含一条恒等映射的捷径，不会给原有的网络结构增添新的参数。 动机/启发层数的加深会导致更大的训练误差，但只增加恒等映射层则一定不会使训练误差增加，而若多层网络块要拟合的映射与恒等映射十分类似时，加入的捷径便可方便的发挥作用。 实验文章中列举了大量在ImagNet和CIFAR-10上的分类表现，效果很好，在此不表。 拾遗Deeper Bottleneck Architectures 两头的1 * 1巻积核先降维再升维，中间的3 * 3巻积核成为“瓶颈”，用于提取重要的特征。这样的结构跟恒等映射捷径配合，在ImageNet上有很好的分类效果。 Standard deviations of layer responses上图是在CIFAR-10数据集上训练的网络各层的相应方差（Batch-Normalization之后，激活之前）。可以看到，残差网络相对普通网络有更小的方差。这一结果支持了残差函数比非残差函数更接近于0的想法（即更接近恒等映射）。此外，还显示出网络越深，越倾向于保留流过的信息。 小结深度残差网络在当年的比赛中几乎是满贯。下面是我的一些（未经实验证实的）理解： 首先，其”跳级”的网络结构对深度网络的设计是一种启发，通过“跳级”，可以把之前网络的信息相对完整的跟后层网络结合起来，即低层次解耦得到的特征和高层次解耦得到的特征再组合。再者，这种分叉的结构可以看作网络结构层面的”Dropout”: 如果被跳过的网络块不能习得更有用的信息，就被恒等映射跳过了。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://blog.ddlee.cn/tags/Machine-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://blog.ddlee.cn/tags/Computer-Vision/"}]},{"title":"[论文笔记]Tensorflow White Paper","slug":"Tensorflow-White-Paper","date":"2017-04-20T13:32:29.000Z","updated":"2017-05-30T13:33:20.000Z","comments":true,"path":"2017/04/20/Tensorflow-White-Paper/","link":"","permalink":"http://blog.ddlee.cn/2017/04/20/Tensorflow-White-Paper/","excerpt":"","text":"论文：TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems 抽象Computation Graph整张图如同管道结构，数据流就是其中的水流。Control Dependency 描述了管道的有向结构，而反向传播可以通过增加新的管道节点来实现。 Operation即计算操作的抽象，相当于映射、函数。 Kernel执行计算的单元，CPU或GPU SessionClient-Server结构，进行计算或者调整图结构则视为一次会话 Variables特殊的Operation，返回一个句柄，指向持久化的张量，这些张量在整张图的计算中不会被释放。 Device对Kernel的封装，包含类型属性，实行注册机制维护可供使用的Device列表。 多机实现要考虑两个问题： 计算节点在Device间的分配问题 Devices之间的通信 针对这两个问题，分别建立了两个抽象层。 计算节点分配的C/S机制client提出计算请求，master负责切割计算图为子图，分配子图到Devices。分配时，会模拟执行子图，并采取贪心的策略分配。 不同Device之间的发送和接收节点 在每个Device上建立Receive和Send节点，负责与其他Device通信。 优化数据化并行 上：单线程，同步数据并行下：多线程，异步更新 拾遗文章中很多内容并没涉及到（看不懂）。 小结TensorFlow是个庞大的计算框架，不仅仅定位于深度网络。其对计算图的抽象和数据、计算资源的分配的处理是值得关注的。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/categories/Papers/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.ddlee.cn/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://blog.ddlee.cn/tags/Machine-Learning/"},{"name":"Papers","slug":"Papers","permalink":"http://blog.ddlee.cn/tags/Papers/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://blog.ddlee.cn/tags/Tensorflow/"}]},{"title":"编程方法论(一):重构","slug":"编程方法论-重构","date":"2017-04-07T18:21:19.000Z","updated":"2017-04-09T16:38:16.000Z","comments":true,"path":"2017/04/08/编程方法论-重构/","link":"","permalink":"http://blog.ddlee.cn/2017/04/08/编程方法论-重构/","excerpt":"","text":"本文内容主要整理自lynda.com课程Programming Foudations: Refactoring Code和Martin Fowler的重构。全部例子来源于refactoring.com。 内容大纲： Introduction 定义重构是在不影响软件功能的情况下，重新组织代码，使之更清晰、更容易理解的过程。 前提：功能不变 行为：改写代码 目的：提高可理解性 大白话讲，重构就是改写，造福以后需要理解这段代码的人们。 重构不是什么给一件事物下定义，有时候从反方面更好讲些。比如你难给正义下一个定义，但很容易举出什么是非正义的例子。 重构不是Debug，代码已经运行良好 重构不是优化 重构不是添加新功能 也就是说，重构对使用代码的人没有任何好处，对使用者来讲，代码是黑箱。重构是准备给要打开黑箱的人，而那个人常常是你自己。 玄学：Code Smells玄学二字是我自己加的。Martin Fowler当然没有这样说。我只是表达一下对无法精确描述的定义的敬意。 我的理解是Code Smells是best practice和code style的总和，直接和根本来源是自己的代码经验。所谓语感、文笔、血淋淋的人生道理。 准备工作：自动化测试重构当然不是breaking the code，写好测试，保证代码仍能正常运行。 重构范例：方法层面首先是一句良言：哪里加了注视，哪里可能就需要重构。 这一点的潜在信念是，好的代码是self-explained的，通过合理的命名、清晰的组织，代码应该像皇帝的新装那样一目了然。 可以用于重构的工具常见的IDE会有重构的功能，如重命名变量。另外，一个严厉的Linter加上像我这样的强迫癌患者会将风格问题扼杀在摇篮之中。 几个例子举例均以Code smell和重构建议两部分构成，较抽象(wo kan bu dong)的给出代码。 Extract MethodCode smell: 太长的方法，带注释的代码块 重构： 提取，新建，用评论命名 Remove tempsCode smell: 冗余的临时变量（本地） 重构： Replace with Query: 把表达式提取为方法（规模较大） Inline temps: 直接用表达式代替这个变量（规模较小） Add tempsCode smell: 同样的变量有多重含义 重构： Split temporary variable: 同一个临时变量在上下文赋予了不同含义（复用），拆 Remove assignment to parameters: 对参数默认值的设定，在函数内新建变量，初始化这个新变量 Remove assignment to parameters的例子： //Before int discount (int inputVal, int quantity, int yearToDate) { if (inputVal > 50) inputVal -= 2; //After Refactoring int discount (int inputVal, int quantity, int yearToDate) { int result = inputVal; if (inputVal > 50) result -= 2; 这一点基于的信念是，参数只能代表被传进来的变量，不应该在本地再赋予别的含义。 重构范例：类与方法 Move MethodCode smell: feature envy（依恋情结） 用中文来说，是指某个方法操作/使用（依恋）某一个类多过自己所处的类，我们用“出轨”这个词来表示这种现象。但这是违反婚姻法的，因而，我们的重构手段就是，把这个方法移动到它依恋的类中，圆满一段木石良缘。* 重构： 圆满木石良缘。 Extract ClassCode smell: 规模太大的类 重构： 把部分移出，自立门户 Inline ClassCode smell: 冗余的类 重构： 像我这中请天假组内运转几乎不受影响的人，应该清除掉（这是瞎话） Condition Focused（条件语句相关）Code smell: 写完判断条件自己都看不懂/看着难受 重构： Decompose conditional: 分解 Consolidate conditional expression: 多项条件指向同一段后续操作，提取这些条件为方法 Consolidate duplicate conditional fragments: 不同条件的后续操作中含有共同的部分，将共有部分提取出来（不管哪个条件总要执行） Replace condition with polymorphism: 针对有判断分支的方法，替换成多态方法 Replace type code with subclass*: 针对有判断分支的类，替换为子类 Consolidate conditional expression的例子： //Before double disabilityAmount() { if (_seniority &lt; 2) return 0; if (_monthsDisabled > 12) return 0; if (_isPartTime) return 0; // compute the disability amount //After Refactoring double disabilityAmount() { if (isNotEligableForDisability()) return 0; // compute the disability amount 重构范例：数据相关 Move fieldcode smell: inverse feature envy（自造） 某一个类使用某一数据比该数据所属的类还多。 重构：送给你了还不行吗！？ Data Clumps（数据团）code smell: 某些数据总是抱团出现 重构： Preserve whole object: 在一个方法中反复提取某个类的一些属性，将整个对象传入 Introducing parameter object: 把这些参数合并为一个类，把新建的类传入 Similifying重构： Renaming: 顾名思义 Add or remove parameters: 顾名思义 Replace parameter with explicit Method: 根据不同参数值新建专属的方法 Parameterize Method: 与上者相反，把不同方法合并，传入参数 Separate queries form modifiers: 将找到数据和更该数据两个操作拆成两个方法 Replace parameter with explicit Method的例子： //Before void setValue (String name, int value) { if (name.equals(\"height\")) { _height = value; return; } if (name.equals(\"width\")) { _width = value; return; } Assert.shouldNeverReachHere(); } //After Refactoring void setHeight(int arg) { _height = arg; } void setWidth (int arg) { _width = arg; } Pulling and pushing(升级与降级) Pull up method and pull up field Push down method and push down field 解决方法、数据归属不合理的问题。 高阶重构（大坑，大坑）Convert procedural design to objects化函数式变成为面向对象，祝好运。 拾遗写代码和改代码是一个不断被自己坑和被别人坑的旅程。且行且珍惜。 Cheers, have a good one. @ddlee","categories":[{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/categories/Programming/"}],"tags":[{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"},{"name":"Refactoring","slug":"Refactoring","permalink":"http://blog.ddlee.cn/tags/Refactoring/"}]},{"title":"ddlee约书计划（第二弹）","slug":"ddlee约书计划（第二弹）","date":"2017-04-04T16:45:28.000Z","updated":"2017-04-07T18:26:40.000Z","comments":true,"path":"2017/04/05/ddlee约书计划（第二弹）/","link":"","permalink":"http://blog.ddlee.cn/2017/04/05/ddlee约书计划（第二弹）/","excerpt":"","text":"缘起读书这种事情，每几个月都会有那么几天。 浑身难受。不干点什么，眼睛闲得团团转，双手也不知道往哪搁。 大概是愧疚吧。要立个FLAG把这压抑着的自卑和热情释放一下。 花大概十分钟的时间，下载/买下十个月都读不完的书。 书单选书原则： 我感兴趣 拒绝大部头 均为论述类，有的聊 我能提供电子版 书单： 《未来简史》尤瓦尔·赫拉利 《知识的边界》戴维·温伯格 《数字化生存》尼葛洛庞帝 《技术的本质》布莱恩•阿瑟 《科技想要什么》凯文·凯利 《枪炮、病菌与钢铁》贾雷德·戴蒙德 《言论的边界》安东尼·刘易斯 《理解媒介》马歇尔·麦克卢汉 《自私的基因》里查德.道金斯 《真实世界的脉络》戴维·多伊奇 豆列在此 剩下一些，我读过，但意犹未尽，也可以聊。 《中国近代史》徐中约 《人类简史》尤瓦尔·赫拉利 《娱乐至死》尼尔·波兹曼 《浅薄：互联网如何毒害了我们的大脑》尼古拉斯·卡尔 操作有条件的，我们用Google Docs共享想法。没条件的，用Evernote。 如果你不习惯写下来，我们可以线下聊（聊到风花雪月人生哲学就不保证了，所以最好还是写下来）。 拾遗 精力有限，同时运行三个线程，多了就溢出了。 其他书也可以推荐，如果长得足够好看的话，我会同意的。 谁也是诸事缠身，有事情不能坚持的，随时退出，我太能理解了；能坚持读完的，我陪你到最后。 这种计划似乎跟熟人约过一次，没成，向我骚扰过的人抱歉。 如果你感兴趣，私信/微信/邮箱联系我。 @ddlee","categories":[{"name":"Reading","slug":"Reading","permalink":"http://blog.ddlee.cn/categories/Reading/"}],"tags":[{"name":"Reading","slug":"Reading","permalink":"http://blog.ddlee.cn/tags/Reading/"}]},{"title":"Coroutine,Generator,Async与Await","slug":"Coroutine-Generator-Async与Await","date":"2017-04-03T14:57:54.000Z","updated":"2017-04-07T18:27:24.000Z","comments":true,"path":"2017/04/03/Coroutine-Generator-Async与Await/","link":"","permalink":"http://blog.ddlee.cn/2017/04/03/Coroutine-Generator-Async与Await/","excerpt":"","text":"GeneratorGenerator能保存自己的状态，进入一种“Paused”状态，再次调用时会继续执行。 Generator的好处之一是节省了存储空间开销，带一些”流处理”的思想。 其实，我们也可以对Generator进行传入数据的操作： def coro(): hello = yield \"Hello\" yield hello c = coro() print(next(c)) print(c.send(\"World\")) Coroutinecoroutine可以认为是generator思想的泛化： generator一个一个地吐出数据（返回值） coroutine一个一个地吃掉数据（传入参数）并返回结果，即可控地执行函数 关键点在于，generator与coroutine都能保存自己的状态，而这种特点正可以用于任务切换。yield可以看做是操作系统在进行进程管理时的traps: 实际上，coroutine可以看做”用户自定义”的进程，状态、启用和暂停都可控，David Beazley就利用这一点用coroutine实现了Python上的操作系统（参见Reference)。 Conroutine与Concurrent ProgrammingConcurrent Programming中有Task的概念，有如下特点： 独立的控制流 内部状态变量 支持计划任务（暂停、恢复执行） 与其他Task通信 @coroutine def grep(pattern): #正则匹配 print \"Looking for %s\" % pattern while True: line = (yield) if pattern in line: print line, conroutine有自己的控制流（while/if），有局部变量（pattern, line），能暂停和恢复（yield()/send()），能相互通信（send()） ====》coroutine就是一种Task！ Python Docs中提供了一个例子： import asyncio async def compute(x, y): print(\"Compute %s + %s ...\" % (x, y)) await asyncio.sleep(1.0) return x + y async def print_sum(x, y): result = await compute(x, y) print(\"%s + %s = %s\" % (x, y, result)) loop = asyncio.get_event_loop() loop.run_until_complete(print_sum(1, 2)) loop.close() 执行方式如下图： 利用coroutine，可以在一个线程(Task)上实现异步。 Impletationcoroutine有两种实现方式，基于generator和原生async, awati关键字。 generator based coroutineimport asyncio import datetime import random @asyncio.coroutine def display_date(num, loop): end_time = loop.time() + 50.0 while True: print(\"Loop: {} Time: {}\".format(num, datetime.datetime.now())) if (loop.time() + 1.0) >= end_time: break yield from asyncio.sleep(random.randint(0, 5)) loop = asyncio.get_event_loop() asyncio.ensure_future(display_date(1, loop)) asyncio.ensure_future(display_date(2, loop)) loop.run_forever() 上面的程序实现了在同一个线程里交互执行两个函数（sleep），而又能保持各自的状态 Native support(python 3.5+)只需要修改函数定义头和yield from为关键字await即可。 async def display_date(num, loop, ): end_time = loop.time() + 50.0 while True: print(\"Loop: {} Time: {}\".format(num, datetime.datetime.now())) if (loop.time() + 1.0) >= end_time: break await asyncio.sleep(random.randint(0, 5)) 拾遗Coroutine常翻译成“协程”。 Reference: David Beazley @ PyCon2009 Slides PYTHON: GENERATORS, COROUTINES, NATIVE COROUTINES AND ASYNC/AWAIT Python 3.6 Docs: Taks and coroutines @ddlee","categories":[{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/categories/Programming/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"异步","slug":"异步","permalink":"http://blog.ddlee.cn/tags/异步/"}]},{"title":"500lines项目Crawler源码阅读笔记","slug":"500lines项目Crawler阅读笔记","date":"2017-04-03T14:57:20.000Z","updated":"2017-04-07T18:27:34.000Z","comments":true,"path":"2017/04/03/500lines项目Crawler阅读笔记/","link":"","permalink":"http://blog.ddlee.cn/2017/04/03/500lines项目Crawler阅读笔记/","excerpt":"","text":"源码来自GitHub上著名的Repo: 500lines or less。 整体结构 代码结构由crawling, crawl, reporting三大部分组成。 crawl: 驱动，解析传入的参数，管理loop，调用crawler，生成report Crawling: 实现crawler类及一系列辅助函数 reporting： 生成记录 Crawler类Crawler类实现了解析网址，抓取内容等基本功能，利用asyncio库构建coroutine（parse_lings(), fetch(), work()）。 核心之处是组织管理异步的抓取任务，代码块结构如下： class Crawler: def __init__(self, roots, ....., loop): self.q = Queue(loop=self.loop) # 建立队列 @asyncio.coroutine def parse_links(self, response): '''从返回内容中解析出要抓取的链接''' body = yield from response.read() if response.status == 200: if content_type: text = yield from response.text() urls = set(re.findall()) for url in urls: if self.url_allowed(): links.add() @asynico.coroutine def fetch(self, url): '''访问链接，抓取返回结果''' while tries: try: response = yield from self.session.get(url) try: if is_redirect(): pass else: links = yield from self.parse_links(response) finally: yield from response.releas() @asyncio.coroutine def work(self): '''封装抓取过程，与队列交互''' try: while True: url = yield from self.q.get() assert url in self.seen_urls yield from self.fetch(url) self.q.task_done() @asyncio.coroutine def crawl(self): '''建立Tasks，启动Task''' workers = [asyncio.Task(self.work(), loop=self.loop) for _ in range(self.max_tasks)] yield from self.q.join() 下图是我对上述代码结构的理解： 对coroutine的进一步介绍，参见Coroutine-Generator-Async与Await。 A Web Crawler With asyncio Coroutines导读文章整体结构： 分析爬虫任务 传统方式：抢锁 异步方式的特点：无锁；单线程上同时运行多操作 回调函数：fetch(),connecte(),read_response()的实现 Coroutine Generator的工作原理 用Generator实现Coroutine Asyncio库中的Coroutine crawl() work() fetch(), handle redirections Queue() EventLoop() Task() Conclusion 文章最后，作者点明了主题思想： Increasingly often, modern programs are I/O-bound instead of CPU-bound. For such programs, Python threads are the worst of both worlds: the global interpreter lock prevents them from actually executing computations in parallel, and preemptive switching makes them prone to races. Async is often the right pattern. But as callback-based async code grows, it tends to become a dishevelled mess. Coroutines are a tidy alternative. They factor naturally into subroutines, with sane exception handling and stack traces. 大意是说，对I/O密集型的程序，Python多线程在两方面令人失望：全局锁的设定使之不能真正并行；抢占式多任务处理机制又让多个线程间形成竞争关系。异步通常是正确的选择。但持续增长的回调函数会使代码丧失可读性，Coroutine便是一种保持整洁性的替代方案。 拾遗这样说，Python的多线程效率带来的提高只是Python程序抢占了系统中非Python进程的资源（参考召集一波狐朋狗友帮你抢选修课），多个线程提高了Python作为一个整体在系统资源调配中的竞争力。 @ddlee","categories":[{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/categories/Programming/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://blog.ddlee.cn/tags/爬虫/"}]},{"title":"Feedly+Reeder3+FeedMe:信息获取与处理","slug":"Feedly-Reeder3-FeedMe-信息获取与处理","date":"2017-04-03T03:13:37.000Z","updated":"2017-06-15T05:49:44.000Z","comments":true,"path":"2017/04/03/Feedly-Reeder3-FeedMe-信息获取与处理/","link":"","permalink":"http://blog.ddlee.cn/2017/04/03/Feedly-Reeder3-FeedMe-信息获取与处理/","excerpt":"","text":"我蛮早就意识到自己被信息淹没了。于是关了票圈，屏蔽了空间，不装任何新闻APP，失去效力的微信群一概不留，知乎上的关注也缩减了很多很多。 世界终于清静了。 但仍然需要有关注的动向。我又捡起了RSS这个老朋友，建立起的信息获取跟处理流如下图： 服务与APP主要涉及的服务：Feedly（免费，有高级版） IOS APP： Reeder3（￥30） Pocket（免费） Pushbullet（免费） Evernote（免费版限制客户端个数） Android APP: FeedMe（免费） Pocket Inbox Evernote Google Keep 获取：Feedly整合Feedly是著名的信息聚合服务，能从媒体RSS、博客、YouTube Chanel等拉取文章/动态，还提供Google关键词动态提醒服务。 这里先推荐两个Chrome插件，可以更方便地将网页端想要订阅的信息整合到Feedly中。 Follow Feed: 识别跟当前网页内容相关的信息源，添加到Feedly订阅中。 Save to Feedly Board: 将当前网页添加到Feedly Board中，可以标记后分享给团队，实时更新。 信息源微信公众号先直接在Feedly中搜索公众号，若找不到订阅源，则可通过微广场等服务转成RSS。 媒体/博客RSS源很多在线媒体会在主页提供RSS地址，也可直接在Feedly中搜索媒体名。 知乎专栏有些工具可以将知乎专栏转成RSS订阅源。 关键词Feedly支持设置关键词动态提醒。 处理：Reeder3 + FeedMe支持Feedly的APP实在太多，这里是官方给出的列表，可看脸挑选。 我平常同时用Android和IOS处理订阅的信息，大屏精读，小屏浏览。 IOS: Reeder3不幸的是Reeder3是要付费的， 30块，几乎没有降过价。 替代品可以考虑自家的Feedly，Ziner等，可以参考这篇文章对比的结果选择。 Android: FeedMe这里强推FeedMe(Google Play)，抓取、缓存迅速，界面简洁，还有“中国大陆”模式。 消化：Pocket, Inbox, Evernote/Keep我个人将信息处理的结果分为三类： Read Later: 没消化 Links to save: 还想接着吃 Favorite: 想学着做 稍后再读用Pocket，接口丰富，功能专一（尽管也有了“发现”模块）。 文章中挂的一些外链，移动端不好处理，要发往PC，手机端存在Inbox中，当临时的标签栏，iPad端用Pushbullet发给Chrome，下次打开Chrome时浏览处理。 收藏的文章存到Evernote，打好tags，长篇干货/可反复参考的转到OneNote。 拾遗其他情境下遇到的好文章、信息等尽量文字存到Google Keep，链接存到Inbox，或者给自己写封邮件。 微信的Favorite尽量不用，收藏的目的就在于情景分离，在不同的上下文中，我门信息获取的效率和质量区别实在太大了。详情参考拉微信群异地参加美赛的战友们。 最后推荐几个不错的订阅源(右击复制链接)： SBNation上Mike Prada的文章: 对NBA比赛、球队战术的分析 Paper Weekly: 机器学习方面的论文解读 统计之都: 统计学及应用、R语言方面的优秀内容 GitHub Trends blog.ddlee.cn: 大言不惭 -_-！ @ddlee","categories":[{"name":"Individual Development","slug":"Individual-Development","permalink":"http://blog.ddlee.cn/categories/Individual-Development/"}],"tags":[{"name":"Individual Management","slug":"Individual-Management","permalink":"http://blog.ddlee.cn/tags/Individual-Management/"},{"name":"RSS","slug":"RSS","permalink":"http://blog.ddlee.cn/tags/RSS/"}]},{"title":"网站迁移小记：腾讯云+Debian+Vestacp","slug":"网站迁移小记：腾讯云-Debian-Vestacp","date":"2017-04-01T17:11:05.000Z","updated":"2017-04-05T13:12:58.000Z","comments":true,"path":"2017/04/02/网站迁移小记：腾讯云-Debian-Vestacp/","link":"","permalink":"http://blog.ddlee.cn/2017/04/02/网站迁移小记：腾讯云-Debian-Vestacp/","excerpt":"","text":"先贴一张文章大纲。 这是一个樱花开得正好但我很蛋疼的下午。 中午抢到了腾讯的校园优惠，便打算把网站ddlee.cn迁到国内的服务器上来。 密码管理先谈密码管理。 建站会涉及设置很多密码，之前明文保存在云笔记里的方案总觉得又土又笨，何况很多密码最好要随机生成，密码管理服务还是必要的。 搜索之后，我选择的是lastPass。主要考虑了免费和跨平台的特性。有更高要求的建议选择付费的1Password。 需要安装Chrome插件和Ubuntu下的deb包，添加Secure Note的功能深得我心，也支持自定义模板。 主机腾讯云的校园优惠力度很大。阿里云是9.9块/月，腾讯用完券1块/月。 这里多讲一句，学生真是幸福得不得了。GitHub Education Pack中既有有Digital Ocean的优惠，AWS也有150刀的礼品卡，Jetbeans大部分产品免费……这还不提学校里买的License。 腾讯的主机1核CPU，2G内存，20G系统盘（Linux），挂个网站还算够用。 SSH Key 配置建议在配置主机前创建一个SSH Key，这样访问起来安全又省心。 Linux系统下，在~/.ssh/下新建config，写入如下类似内容： Host Name HostName Host_IP User root IdentityFile path/to/ssh_private_key 这样就可以通过命令ssh Name直接访问主机。 系统选择建议选择Linux主机。具体哪一系可自行选择，我的选择是Debian，CentOS也是个不错的选择。 安全组设置建议先只开启用于SSH的22端口，之后再开HTTP访问的80端口，FTP的20,21端口和主机面板所用端口。 如果个人有代理服务器的话，也可以限制一下来源IP，这样可以通过登入代理服务器，在代理服务器上通过SSH登入WEB主机，需要迁移下SSH Private Key，可以通过命令scp usr1@host1:/path1 usr2@host2:/path2实现。 网络环境LNMP和LAMP是两种流行的结构。可以分别安装，再配置相应的config，也可以搜索得到很多一键安装脚本。另一种方案是用Docker部署。 我懒而笨，选择的是用主机面板一键安装。 主机面板在此之前，一直用的是AMH的免费4.2版本，简洁轻巧，功能也够用。付费版推出后，免费版遭到冷落，几乎没有更新，这如何能忍。 说起主机面板，我的启蒙是WDCP，其远古风格的UI仍历历在目，后来听说爆出漏洞，但那时我已转战AMH。 一番艰苦卓绝的搜索之后（其实就是检索了’best host control pannel’），我选择了Vestacp。 UI漂亮，功能不缺（建站，MAIL，备份），GitHub还算活跃，就决定是你了。 缺点是文件管理器收费，不能通过WEB管理文件。安装过程持续蛮久（半个小时，当然也包括了新主机系统包更新的时间）。 安装时注意Hostname填写IP或者已经配置好DNS解析的域名（如admin.ddlee.cn）。8083是管理面板的端口，记得在主机提供商的安全组里开放一下。 建站Vestacp支持多人管理，User身份由Package定义，安装过程会自动新建admin，拥有最高权限。 在User的设置里，可以配置用户的Package，而Package的设置里，可以配置每一用户身份的建站模板，资源上限等。如图。 建站相当容易，注意在高级选项里添加FTP账户，用于之后上传HTML文件。 FTP建站完成后，记得配置好DNS解析，开放20和21端口，就可以用FileZilla测试链接。 注意，在高级选项里配置好Default local directory，设置Default remote directory为/public_html，并启用synchronized browsing和directory comparison，以后的FTP生活会很幸福。 Mail若在建站时勾选了Mail support，可以建立个性化的邮箱名，可以设置自动回复/转发，也可以用Gmail托管。以后留邮箱的时候可以短短的了呢。 配置SSL这是无意发现的技能。 本来在我的印象里，SSL证书都是要收费的。但留心的朋友可能注意到，建站时SSL support下有Lets Encrypt Support。这一服务可以用上免费的SSL。 官网：Let’s Encrypt 要利用这项服务，需要证明自己对网站的至高无上不可侵犯的神圣权利，方法之一是运行支持[ACME protocol]的Client，官网推荐了Cerbot。 在Cerbot主页可以选择自己的操作系统，会有详细的步骤，在此不表。 下面谈两个问题，一是强制重定向至HTTPS，二是取消管理端口的HTTPS。 强制HTTPSVestacp的架构是用nginx做proxy，Apache2做HTTP Server，首先下载nginx template（proxy template）： cd /usr/local/vesta/data/templates/web wget http://c.vestacp.com/0.9.8/rhel/force-https/nginx.tar.gz tar -xzvf nginx.tar.gz rm -f nginx.tar.gz 之后在Package配置里，将proxy template配置为force-https，这样，身份由相应Package定义的用户建站时，proxy template就是用的强制HTTPS版本了。 取消管理端口的SSL用chrome访问管理页面时，会有Unsecure的警告，这里的SSL在/usr/local/vesta/nginx/conf/nginx.conf中配置。找到 # Vhost server { listen 8083; server_name _; root /usr/local/vesta/web; charset utf-8; # Fix error &quot;The plain HTTP request was sent to HTTPS port&quot; error_page 497 https://$host:$server_port$request_uri; # ssl on; # ssl_certificate /usr/local/vesta/ssl/certificate.crt; # ssl_certificate_key /usr/local/vesta/ssl/certificate.key; # ssl_session_cache shared:SSL:10m; # ssl_session_timeout 10m; 将配置SSL的几行注释掉即可。顺便，管理页面的端口也可以在这里更改。之后运行service vesta restart重启服务。 域名与DNS最后简单提一下域名注册跟DNS。要注意的几个点： 国内域名注册要备案，很烦，但cn域名好便宜。 在域名注册商那里配置DNS解析服务器（万网、DNSPod都好，不一定用自建网站的DNS） 在DNS服务商那里添加解析记录，顺便开启监控 拾遗域名备案的时候，需要签一张备案单。方案是在纸上签字后调背景为透明，用Adobe PDF Reader的签字功能签好PDF，再转成JPG。 几项操作都可以通过在线工具完成，低碳生活，人人有责。 总结 建站过程本身就其乐无穷，教程一抓一大把，难的在TROUBLE SHOOTING，所以Google是最好的伴侣。 命令行、vi编辑、必要的WEB知识等是基础，在此感谢我们的墙两年前就教给我这些东西。 @ddlee","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/tags/Linux/"},{"name":"云主机","slug":"云主机","permalink":"http://blog.ddlee.cn/tags/云主机/"},{"name":"Vestacp","slug":"Vestacp","permalink":"http://blog.ddlee.cn/tags/Vestacp/"},{"name":"博客迁移","slug":"博客迁移","permalink":"http://blog.ddlee.cn/tags/博客迁移/"}]},{"title":"The Devtools Way: devtools+RStudio快速R程序包开发","slug":"The-Devtools-Way-devtools-RStudio快速R程序包开发","date":"2017-03-31T05:50:13.000Z","updated":"2017-06-15T05:51:24.000Z","comments":true,"path":"2017/03/31/The-Devtools-Way-devtools-RStudio快速R程序包开发/","link":"","permalink":"http://blog.ddlee.cn/2017/03/31/The-Devtools-Way-devtools-RStudio快速R程序包开发/","excerpt":"","text":"本文记录我的首个R程序包MCMI的开发过程。 参考资料 两本Hadley Wickham写的书：Advanced R和R Packages。 Coursera上的课程Mastering Software Development in R Specialization和配套教材Mastering Software Development in R。 感谢开源社区，以上的资料都可以免费获取得到（Coursera课程可以申请补助）。 Preparation开发R包还需要系统中存在编译工具，编译文档需要LaTeX支持。 Linux用户：sudo apt-get install r-base-dev texlive-full Windows用户：1.RTools;2.MikTeX 另外，请确保以下两个包已安装于系统中：devtools和roxygen2。推荐使用RStudio。 Get Started在RStudio中新建项目，选择R程序包类型即可。建议同时建立Git路径以监控开发流程。 在编写代码之前，先要修改DESCRIPTION文件，要注意的几个地方： Package的命名要容易记忆和查询 Depends指你所用开发环境的R版本 慎重选择License Imports指你所要调用的其他包，但在代码中，也要明确指出函数所处的包，如ggplot2::qplot() Git Workflow建议在GitHub上为本机申请SSH密钥，并在RStudo-&gt;Tools-&gt;Global Options-&gt;Git/SVN配置好路径，这样在执行git push时不用再次输入凭据。下面是有关Git的工作流： 修改代码/文档 编译，测试 git commit git push 重复以上循环 RStudio对Git的集成很好，以上三四步操作均可在Git的操作面板里完成。 Code Workflow 修改代码 Build&amp;Reload 用命令行测试功能 重复以上循环 上述第二步可以在命令行中devtools::load_all()完成，也可以使用快捷键”Ctrl + Shift + L”，也可以在RStudio的开发面板中执行”Build&amp;Reload”命令。之后，便可在命令行中调用编写好的函数，验证其功能。 Documentation Workflow 在.R文件中添加roxygen注释 Document 使用help面板或?命令预览文档 重复以上循环 同样地，第二步有三种实现方式：1.devtools::document();2.”Ctrl + Shift + D”；3. RStudio开发面板中的”Document”命令。 Test Workflow测试方面，除了上述Coding Workflow中提到的在命令行中调用函数进行测试之外，还可以利用testthat包来使测试自动化。 首先要安装testthat包，再使用devtools::ust_testthat()命令建立testthat路径。 下面是自动化测试的工作流： 修改代码 在testthat路径下编写相应的测试语句 Build&amp;Reload Test 排除Bug，重复上述过程 以上流程第四步同样有三种实现方式：1.devtools::test();2.”Ctrl + Shift + T”;3.RStudio中开发面板的“Test”命令。 Release按照上述过程开发的R程序包，每一次git push事实上都是一次发布。使用devtools::install_github(&quot;git_repo_goest_here&quot;)命令，可以很方便地安装R程序包。 Next Step使用devtools配合RStudio和Git，开发R包的过程已经非常亲民和流水线化。但要开发高质量的R包，需要对R的数据结构和S3等对象系统有更深的理解，而Advanced R则是你通往这一方向的最好伴侣。","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/tags/Data-Science/"},{"name":"Programming","slug":"Programming","permalink":"http://blog.ddlee.cn/tags/Programming/"},{"name":"R","slug":"R","permalink":"http://blog.ddlee.cn/tags/R/"}]},{"title":"HADOOP学习速记","slug":"HADOOP学习速记","date":"2017-03-30T14:22:02.000Z","updated":"2017-04-09T15:44:10.000Z","comments":true,"path":"2017/03/30/HADOOP学习速记/","link":"","permalink":"http://blog.ddlee.cn/2017/03/30/HADOOP学习速记/","excerpt":"","text":"HDFS: 分布式文件系统NameNode, DataNode: a MetaData-Data ModelStrategy: Block split, multi-copy, distribution NameNode: High Availabilitysolution 1: backup using NFSsolution 2: Two NameNodes(Active and Standby) MapReduce: 计算框架split -&gt; Process -&gt; aggregate Deamon: Job Tracker &amp; Task Tracker Design PatternCourse Project（未完待续）@ddlee","categories":[{"name":"AI","slug":"AI","permalink":"http://blog.ddlee.cn/categories/AI/"}],"tags":[{"name":"分布式计算","slug":"分布式计算","permalink":"http://blog.ddlee.cn/tags/分布式计算/"},{"name":"Data Sicence","slug":"Data-Sicence","permalink":"http://blog.ddlee.cn/tags/Data-Sicence/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://blog.ddlee.cn/tags/Hadoop/"}]},{"title":"Kaggle比赛中EDA：流程、做法与目的","slug":"Kaggle比赛中EDA：流程、做法与目的","date":"2017-03-26T11:09:00.000Z","updated":"2017-04-02T17:47:54.000Z","comments":true,"path":"2017/03/26/Kaggle比赛中EDA：流程、做法与目的/","link":"","permalink":"http://blog.ddlee.cn/2017/03/26/Kaggle比赛中EDA：流程、做法与目的/","excerpt":"","text":"数据集大小、字段及相应的数据类型 大小：占用内存估计 字段数：维度估计，是否需要降维 数据类型：numerical, factor, string, etc. 是否需要归一化，二元化等等 了解数据的缺失值情况及分布了解数据分布情况，可用众多图形完成 bar plot histogram violin plot box plot scatter plot 主要目的： 了解整体状况，是否具有野点 结合目标变量，考察特征与目标变量间的相关性 文本数据常用的探索： 词频统计（消除stopwords之后） 词云 后记本来文章是从几个经典的EDA notebook开始，试图总结出其共性之处，但写来写去，总觉得随便一本跟数据分析相关的书中，探索性数据分析的章节也大概都会涉及到这些内容，但在读书的情景之下又难留下深刻的印象，做分析的真正见地与经验，还是要从实践中来啊。 @ddlee","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/categories/Data-Science/"}],"tags":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://blog.ddlee.cn/tags/Kaggle/"},{"name":"EDA","slug":"EDA","permalink":"http://blog.ddlee.cn/tags/EDA/"},{"name":"Data Sciencce","slug":"Data-Sciencce","permalink":"http://blog.ddlee.cn/tags/Data-Sciencce/"},{"name":"笔记","slug":"笔记","permalink":"http://blog.ddlee.cn/tags/笔记/"}]},{"title":"Udacity课程： Intro to DevOps侧记","slug":"DevOps侧记","date":"2017-03-22T15:19:07.000Z","updated":"2017-04-05T13:13:12.000Z","comments":true,"path":"2017/03/22/DevOps侧记/","link":"","permalink":"http://blog.ddlee.cn/2017/03/22/DevOps侧记/","excerpt":"","text":"文章主要内容来自Udacity的课程：Intro to DevOps CAMS: The DevOps LifecycleThe Purpose of DevOps： 产品、开发、运维之间的协作问题 Definition(from wiki) a set of practices that emphasize the collaboration and communication of both software developers and information technology (IT) professionals while automating the process of software delivery and infrastructure changes. (Source) 对比： Agile Development（敏捷软件开发） Plan-&gt;Code-&gt;Test-&gt;Release-&gt;Deploy-&gt;Operate Means of CAMS C: Culture A: Automation M: Measurement S: Sharing The DevOps EnvironmentSolving the Environment Problem: Golden Image: apps-libs-OS Configuration Management Course Project（使用Golden Image方案，引入一批DevOps工具)dependencies-&gt;build scripts-&gt;tests-&gt;web apps PackerPacker is an open source tool for creating identical machine images for multiple platforms from a single source configuration. Artifacts are the results of a single build, and are usually a set of IDs or files to represent a machine image. Builds are a single task that eventually produces an image for a single platform. Builders are components of Packer that are able to create a machine image for a single platform. Commands are sub-commands for the packer program that perform some job. Post-processors are components of Packer that take the result of a builder or another post-processor and process that to create a new artifact. Provisioners are components of Packer that install and configure software within a running machine prior to that machine being turned into a static image. Templates are JSON files which define one or more builds by configuring the various components of Packer. Example JSON File VagrantVagrant is a tool for building complete development environments. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases development/production parity, and makes the “works on my machine” excuse a relic of the past. Project Workflow: Packer-&gt;Vagrant-&gt;Virtualbox-&gt;Web ApplicationPart I: Building a box with PackerFrom the packer-templates directory on your local machine: Run packer build -only=virtualbox-iso application-server.json Troubleshooting: Find the newest version number and checksum from the Ubuntu website for this releaseEdit PACKER_BOX_NAME and iso_checksum in the template files to match that version number and checksum. Run cd virtualbox Run vagrant box add ubuntu-14.04.4-server-amd64-appserver_virtualbox.box --name devops-appserver Run vagrant up Run vagrant ssh to connect to the server Part II: Cloning, developing, and running the web application On your local machine go to the root directory of the cloned repository Run git clone https://github.com/chef/devops-kungfu.git devops-kungfu Open http://localhost:8080 from your local machine to see the app running. In the VM, run cd devops-kungfu To install app specific node packages, run sudo npm install. You may see several errors; they can be ignored for now. Now you can run tests with the command grunt -v. The tests will run, then quit with an error. On Cloud platformSimilar commands using packer: packer build -only=amazon-ebs &lt;server-name&gt;.json packer build -only=googlecompute application-server.json Continuous Integration（持续集成）CI System: JenkinsJenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks such as building, testing, and deploying software. Using command:packer build -only=&lt;cloud service target&gt; control-server.json Example control-server.json file Jenkins was configured to be installed according to Provisioners. After building and launching, access Jenkins via URL /jenkins. Testing（测试，QA） Unit Testing Regression testing Smoke testing System Integration testing Automate acceptance testing Manual QA testing Adding Manual QA step in Pipeline MonitoringMonitoring process: Additional ResourcesCourse Wiki 小结DevOps从开发和运维合作的角度审视软件开发过程，并提供了一套方法论，涉及开发、测试、部署、维护、监测各个方面。软件行业，不仅仅是写代码而已。 @ddlee","categories":[{"name":"Internet","slug":"Internet","permalink":"http://blog.ddlee.cn/categories/Internet/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.ddlee.cn/tags/DevOps/"},{"name":"技术","slug":"技术","permalink":"http://blog.ddlee.cn/tags/技术/"}]},{"title":"Python与SQL_Server的交互：pyODBC, pymssql, SQLAlchemy","slug":"Python与SQL_Server的交互：pyODBC, pymssql, SQLAlchemy","date":"2017-03-16T14:57:37.000Z","updated":"2017-04-05T13:13:08.000Z","comments":true,"path":"2017/03/16/Python与SQL_Server的交互：pyODBC, pymssql, SQLAlchemy/","link":"","permalink":"http://blog.ddlee.cn/2017/03/16/Python与SQL_Server的交互：pyODBC, pymssql, SQLAlchemy/","excerpt":"","text":"Windows平台下Python读取、写入SQL Server相关的函数库，文章结构如下： Python DriversPyODBCAnnaconda下可以用pip install pyodbc安装，也可以到这里下载。 首先建立connection对象： import pyodbc conn = pyodbc.connect( r'DRIVER={ODBC Driver 11 for SQL Server};' #or {ODBC Driver 13 for SQL Server} r'SERVER=ServerHostName;' r'DATABASE=DBName;' r'UID=user;' r'PWD=password' ) 添加游标（Cursor）对象并执行SQL查询语句： cursor = conn.cursor() cursor.execute('SQL Query Goes Here') for row in cursor.fetchall(): print(rows.[column name]) 更多信息参见MSDN DOCs。 pymssql同样可以用pip install pymssql安装，也可以到这里，然后用pip安装wheel文件。 pymssql目前还不支持Python3.6，这点要注意下。 pymssql的用法跟pyODBC很像，下面是官网给出的例子： from os import getenv import pymssql server = getenv(\"PYMSSQL_TEST_SERVER\") user = getenv(\"PYMSSQL_TEST_USERNAME\") password = getenv(\"PYMSSQL_TEST_PASSWORD\") conn = pymssql.connect(server, user, password, \"tempdb\") cursor = conn.cursor() cursor.execute(\"\"\" IF OBJECT_ID('persons', 'U') IS NOT NULL DROP TABLE persons CREATE TABLE persons ( id INT NOT NULL, name VARCHAR(100), salesrep VARCHAR(100), PRIMARY KEY(id) ) \"\"\") cursor.executemany( \"INSERT INTO persons VALUES (%d, %s, %s)\", [(1, 'John Smith', 'John Doe'), (2, 'Jane Doe', 'Joe Dog'), (3, 'Mike T.', 'Sarah H.')]) # you must call commit() to persist your data if you don't set autocommit to True conn.commit() cursor.execute('SELECT * FROM persons WHERE salesrep=%s', 'John Doe') row = cursor.fetchone() while row: print(\"ID=%d, Name=%s\" % (row[0], row[1])) row = cursor.fetchone() conn.close() 详细用法参见pymssql docs和MSDN DOCs SQLAlchemy(Python SQL Toolkit)SQLAlchemy提供了一系列丰富、完整、（我看不懂）的API用于数据库操作。这里只谈其create_engine方法。 from sqlalchemy import create_engine # pyodbc engine = create_engine('mssql+pyodbc://user:password@DSNname') #需要配置DSN，参见最后一节 # pymssql engine = create_engine('mssql+pymssql://user:password@Hostname:port/DBname') 利用创建好的engine，可以结合pandas库进行批量的读取、写入操作。 用SQLAlchemy与其他类型的数据库建立链接的方法参见这里。 Pandas利用pyODBC和pymssql拉取的对象需要进一步处理才能进行常见的数据清洗等工作，而Pandas也提供了SQL相关的方法，在SQLAlchemy的辅助下，可以将DataFrame对象直接写入table。 读取：pd.read_sql()API： pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None) 其中的con参数，可以传入SQLAlchemy建立的engine对象，也可以是pyODBC或者pymssql建立的DBAPI2 connection对象。 写入:pd.DataFrame.to_sql()API: DataFrame.to_sql(name, con, flavor=None, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None) 这里的con参数，只支持sqlite3的DBAPI2 connection对象，支持所有的SQLAlchemy engine对象。name参数传入表名，用if_exists参数控制表存在时的动作： ‘fail’: 啥也不干。 ’replace‘: 将原有表删除，新建表，插入数据。 ’append&#39;: 在表中插入数据。表不存在时新建表。 命令行利用Sqlcmd命令，也可以在命令行下执行SQL文件，用法如下： sqlcmd -U user -P password -S server -d DBName -i /path/to/myScript.sql 这样可以有如下思路，将数据写入.SQL文件，再生成.bat文件（批量）写入上述命令，之后完成执行。 DSNWindows下可以配置DSN(Data Source Names)预先存储数据库连接的信息，在Control Panel -&gt; Administrative Tools -&gt; ODBC Data Source 下添加即可。 配置好DSN后，pyODBC的连接过程可以简化为： conn = pyodbc.connect(r'DSN=DSNname;UID=user;PWD=password') #UID和PWD也可以在DSN中配置 拾遗Python与文件的IO、SQL数据库的读写时有中文字符可能会有编码问题。一种方案是在中文字符串前添加N，如N&#39;python大法好&#39;；另一种方案是传入encoding参数，常用的中文编码有GB2123，GB18030，推荐的还是统一用UTF-8编码、解码。 利用如下命令，可以在SQLAlchemy中指定编码： engine = create_engine('mssql+pymssql://user:password@HostName\\DBname', connect_args = {'charset':'utf-8'}) 其他自定义DBAPI connect()参数的方法参见这里。","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/categories/Data-Science/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.ddlee.cn/tags/Python/"},{"name":"SQL","slug":"SQL","permalink":"http://blog.ddlee.cn/tags/SQL/"},{"name":"数据库","slug":"数据库","permalink":"http://blog.ddlee.cn/tags/数据库/"}]},{"title":"再次折腾我的WNDR4300：OpenWrt文件共享","slug":"再次折腾我的WNDR4300","date":"2017-03-12T07:00:25.000Z","updated":"2017-04-03T04:31:30.000Z","comments":true,"path":"2017/03/12/再次折腾我的WNDR4300/","link":"","permalink":"http://blog.ddlee.cn/2017/03/12/再次折腾我的WNDR4300/","excerpt":"","text":"生命不惜，折腾不止。 缘起再次成为IOS用户后，访问Google和文件共享成了两大需求。问题出现了，就要解决，于是有此文记录的活动。 重新安装OpenWrtOpenWrt已经到了15.05版本，版本代号是Chaos Calmer。重装需要的-factory.img，可以在这里下载。 我的WNDR4300平台是ar71xx，可以从OpenWrt对应的硬件主页找到固件镜像文件。 TFTP重装如果你的路由器还是出厂系统的话，可以通过登入后台在线上传镜像文件进行刷机，而我的已经是OpenWrt系统，只能通过网页端升级，故选用了TFTP方式刷机。 刷机步骤摘自OpenWrt wiki &gt; set a static IP on your computer, i.e 192.168.1.35, and connect the ethernet cable to the router power on the router press and hold the RESET button as soon as the switch LEDs light up. keep holding RESET until the power LED begins to flash orange and then green. once the power LED is flashing green, release RESET start the TFTP transfer to router at 192.168.1.1. In your computer execute:tftp 192.168.1.1 -m binary -c put factory.img 总体来说是分为三步： 将电脑与路由器设置在同一内网中 令路由器进入恢复模式 利用TFTP将刷机包推入路由器 U盘挂载，文件共享安装好OpenWrt后，就可以从网页端访问路由器，设置PPPoE拨号，设置WIFI等等。 U盘挂载U盘挂载部分主要参考了跟 UMU 一起玩 OpenWRT（入门篇6）：挂接 U 盘。 首先是安装相应的包： opkg update # 核心包 opkg install kmod-usb-storage opkg install kmod-scsi-generic # 文件系统 opkg install kmod-fs-ext4 # 辅助工具 opkg install usbutils fdisk e2fsprogs 利用lsusb命令查看U盘是否已经被路由器识别。 这时可以选择用fdisk进行重新分区，不需要分区的话，可以用命令ls /dev | grep sd查看/dev分区中是否已经出现U盘。 在OpenWrt上使用U盘，建议用ext4格式，可以用下面的命令进行格式化： # sda1为上一命令得到的结果 mkfs.ext4 /dev/sda1 接下来就可以用mount命令进行挂载了： # 路径/mnt/usb/即为挂载目标点 mkdir /mnt/usb touch /mnt/usb/USB_DISK_NOT_PRESENT chmod 555 /mnt/usb chmod 444 /mnt/usb/USB_DISK_NOT_PRESENT mount /dev/sda1 /mnt/usb 这时可以测试一下，如果U盘里面存储了文件，可以通过/mnt/usb访问的到。 下面是开机自动挂载U盘的命令。 # block-mount blkid用于查看U盘的UUID opkg install block-mount blkid # 实际上要操作的是fstab的配置文件/etc/config/fstab，要将enabled值改成1 block detect > /etc/config/fstab uci set fstab.@mount[-1].target='/mnt/usb' u ci set fstab.@mount[-1].enabled=1 uci commit fstab 更详细的信息可以参见这里 文件共享文件共享可以通过FTP和SAMBA，推荐的方式是SAMBA。 SAMBA安转SAMBA： opkg update opkg install samba36-server # luci程序，可选 opkg install luci-app-samba 安装好SAMBA后，主要配置两个参数，一是共享文件夹的路径，如/mnt/usb/sambashare，可以通过更改配置文件/etc/samba/smb.conf实现，也可以通过luci实现。 示例： [sambashare] path = /mnt/usb/sambashare valid users = root read only = no guest ok = yes create mask = 0750 directory mask = 0750 第二个参数是访问账户，可以通过命令sambpasswd -a将你的当前用户加入到SAMBA的组中，需要设置一个密码。另外，可能需要将配置文件/etc/samba/smb.conf的[global]中的invalid users = root注释掉。 最后，设置SAMBA服务启动和开机自启 /etc/init.d/samba start /etc/init.d/samba enable FTPFTP可以用vsftpd包来设置，大致过程与SAMBA类似：设置路径、添加用户、设置自启。 SAMBA服务可以在Windows文件资源管理器中自动检测的到，Linux下可以通过smb://Host/sharepath访问，在IOS系统中，类似Documents的应用也支持添加SAMBA的功能。 这里强推一下Documents这个应用，结合PDF EXPERT，已经成为了我的文档中心。 访问Google这部分操作相当复杂，主要参考这里，感谢博主。 后记这天的活动，本来只有我和上帝知道，再过一个月，就只有上帝知道了。遂作笔记。 @ddlee","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/categories/Linux/"}],"tags":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://blog.ddlee.cn/tags/OpenWrt/"},{"name":"路由器","slug":"路由器","permalink":"http://blog.ddlee.cn/tags/路由器/"}]},{"title":"谁会不厌其烦地安慰那无知的少年（三）","slug":"谁会不厌其烦地安慰那无知的少年-3","date":"2017-03-03T15:41:10.000Z","updated":"2017-04-02T17:48:52.000Z","comments":true,"path":"2017/03/03/谁会不厌其烦地安慰那无知的少年-3/","link":"","permalink":"http://blog.ddlee.cn/2017/03/03/谁会不厌其烦地安慰那无知的少年-3/","excerpt":"","text":"年轻的时候可以随随便便喜欢一个人，可千万别真动情。那样的话你的余生就剩两种状态了，一种叫做想她，另一种是为克制自己想她而努力。——丁丁前舍友 丁丁的舍友告诉他，那年不懂事，一直陷于人生的错觉之中。 他觉得那女生好像喜欢她，做什么事儿都是像在针对他，总是跑来问问题呀，不懂的时候卖个萌啊，连谢谢的话都是奶声奶气的。 可他怎么能被这个给连累了呢。 他可是老师眼中最有希望的学生，早熟的他也明白合适的平台对自己的发展是多么重要。他觉得在人生的一段时间内能单纯地为一个目标而奋斗是一件幸福的事儿，任何分心的想法都是罪恶。 他在开始之前，就故作冷漠，就像结束了之后想要挽回那样。 丁丁插着话问到底什么开始什么结束的啊？ 舍友答，年少的初恋啊我的旁友！ 舍友顿了顿，眼里含着惋惜。 讲真，我是那种动情就会倾其所有的人，我真真觉得一生就只够爱一个人。但让我从没想到的是，我的故作冷漠才是动情的开始啊。 那时候我千方百计地回避她。 我特别跟组里的同学换了座位，这样就能离她远一点。 问题的时候我也爱搭不理的，不是把她推给别人就是拖着藏着。 她也算知趣，渐渐的就不来烦我了。 就这样吧，高考完了以后，我们去了不同的学校，离得八十万杆子都打不着。 但我逐渐的发现，这颗种子，已经在我的心底长成了参天大树，不管我给它什么样的脸色，它还是生长起来了。 我再也不能回避它了，我再也不能隐藏它了。 我以前听人家说暗恋一个人的时候，把她的动态错过都会有罪恶感。 我细细的品味她的日志、说说里流露出来的情感，挖空心思复原她写下这些文字时的心情，然后小心翼翼地写下我的评论，斟酌一下，再发表。然后就是每隔几个小时就刷一下，看看她回复了没有。 我也找她聊天，谈心，新的生活还适应没，高数有哪些不懂的跟我说说。 我也跟她讲我的近况，我在听什么歌，我在读哪些书。 可我从来都不敢表露我真实的心意，我也从来不敢提高中时候我的那段冷漠的时光。 可是，你知道吗，就跟吃巧克力一样，她吃到了苦的，我却吃了块甜的，德芙，带榛果颗粒的。 我终于等到了一个机会——她生病住院了。 急性胃炎，但她没跟我说，她的闺蜜告诉我的。 我买了票，赶到她所在的城市，在一个下着小雨的傍晚。 行人匆匆，从四面赶往八方。风催着云，一来一回地玩弄着月亮，雨打在肩上，我才知道我还没有方向。 我给她打电话，说我来看你来了，你在哪家医院。 她说你怎么来了，她已经快好了，明天就要出院，那你过来吧，在江东北路的那家人民医院，8号楼，324。 我说没事儿，马上就到。 不过地铁并不方便，只能在珠江路那里下，我就打算骑ofo过去。 然而我还是太年轻了，南方的冬天下着雨，可没那么好欺负，找路，问路，手冻僵，衣服也淋湿了，我想着张士超华师大的姑娘真的那么可爱吗。 等我赶到时，已经是需要照顾的人了，一副洋葱模样，就剩一层一层剥开了。 狼狈的我跑到厕所里，等个没人的空档，用烘干机吹了吹头发，把外套脱下来搭在胳膊上，这才往病房赶去。 丁丁的舍友推了下眼镜，接着说。 你可知道什么叫近乡情更怯呀，就跟查高考成绩一样啊，你再往前一步，就把那些想象过的所有美好的可能性全破除了，木已成舟，一切皆不可挽回，尽管，尽管你不往前一步，一切也早就注定了呀。 我在病房门前愣住了，万一里面还有人怎么办，她的同学在晚上应该会陪她吧，她不会有男朋友了吧？ 我跑到离门远一点的地方，又给她打了个电话，我说我快到了，你有什么想吃的我给你带点。 她说不用了，你过来就好，她也想赶快见到我。 我说好的，这么突然出现，没赶上不方便的时候吧。 她说没事儿，你直接过来吧，哪有什么方便不方便的。 挂了之后，我在楼里瞎逛了几圈，顺手把紧急逃生的路线考察了一下，发现还是很科学的，指引也做的很到位。估摸时间差不多了，我就敲门进去了。 她留起了长发，比高中的时候成熟不少，但终归有病在身，脸色有些发白，不过酒窝还是那样可爱。 我们聊起来，从病情开始，一直聊到那些在网易云音乐的歌曲下面刷评论的考研党们到底考上了没。 她似乎很开心，我也很开心。 她说上了大学就没跟别人聊这么久过，还是以前的同学好呀。 我说那当然了，以后有什么事你第一个告诉我。 要走的时候，她说谢谢我这么大老远地跑过来，不过病差不多要好了，明天亲自到车站送送你。 我说不用了，我自己走就行，你好好养着身体吧，注意一下饮食。 离开医院 ，我随便找了家旅馆住下来。心底里无限的舒适与满足。但很快，紧张与自责将我包裹起来。 太懦弱了我真是，聊那些没什么用的干啥，我该直接跟她说我喜欢你三年了我们在一起吧。 可又转念一想，这也有点趁人之危吧，还是等等再说？ 这一等就是一夜，我慢慢睡着，天刚刚破晓。 第二天，她还是来送我了，下地铁后，她用手机看了下时间，说还不晚不用着急。 她竟然用的Xperia。我心想我喜欢的女孩子就是有格调。 然后我就看到了手机桌面上男孩子的傻笑。 那个男孩子似乎不是我，我笑的时候不傻，眼睛眯成一条缝。 我说这也不早了你赶紧回去吧。 她说你开玩笑呢这才几点啊。 我说不对，我不是这个意思，我是说你不用跟我一起等了，我自己等，我自己能行。 她告诉我她当然相信我能行，不然怎么能自己跑过来看她呢。 我说也是哈，我这么催你干哈。 后面的事情我自己也记不清了。 回来的时候，出站换乘，转角碰见一家鲜花店，就进去买了一束满天星，捧着它回到寝室，摆在桌上。 我是眼睁睁地看着那一束花慢慢枯萎的。 不插在水中的话，只用了三天不到。 舍友说我那三天跟个傻逼一样。 后来她说我是她最好的朋友，跟高中的那个我完全不一样了。 原来她从来就没喜欢过我，而我也从来没承认过我那么心动，但你知道吗？这的的确确发生了。 舍友觉得可以做结了，便说出了这句丁丁永生难忘的话。 年轻的时候可以随随便便喜欢一个人，可千万别真动情。那样的话你的余生就剩一种状态了，那就是想她。 丁丁说没事儿你还有机会，天下没有不散的筵席，他们迟早会分的。 舍友说丁丁是傻逼。 ————————————————全文完————————————————— 我不再强说上面的故事是瞎编的了。它们是丁丁亲口告诉我的，在一次卧谈会上。 丁丁说在刚好记得的时候讲出来，其实是自私的。 他说他从小到大失去了很多人，从每天早到学校开门的劳动课老师到害了白血病的不幸前桌，从打架斗殴满嘴义气话的小魔王到奔走他乡借读名校的竞争对手，当好友列表里的灰色头像终于不再跳动的时候，我就不再是完整的了，他们把我的一部分带走了，而且永远找也找不回来了。这个永远是真的。 我跟丁丁说你错了，你不知道更可怕的事情。你有没有想过，即使是陪你一起长大的人，也有很多东西找不回来了。像你的父母，你的淘气和无知，早就淹没在他们眼角的层层皱纹里了。而且，是你亲手把它们埋葬进去的。你看，谁都没有失去谁，谁也失去了谁。 丁丁说是啊，我们都变了，变得都有些记不起从前的样子了。人们总是到失去了才懂得珍惜，这真是瞎话，我们就从来没有拥有过。 我记起很久以前的一个秋天，我打开了一册我昔日嗜爱的书读了下去，突然回复到十四岁时那样温柔而多感，我在那里面找到了一节写在发黄的纸上的以这样两行开始的短诗： 在你眼睛里我找到了童年的梦，如在秋天的园子里找到了迟暮的花…… @ddlee 2017年3月","categories":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ddlee.cn/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ddlee.cn/tags/随笔/"}]},{"title":"谁会不厌其烦地安慰那无知的少年（二）","slug":"谁会不厌其烦地安慰那无知的少年-2","date":"2017-02-15T12:51:08.000Z","updated":"2017-04-02T17:48:42.000Z","comments":true,"path":"2017/02/15/谁会不厌其烦地安慰那无知的少年-2/","link":"","permalink":"http://blog.ddlee.cn/2017/02/15/谁会不厌其烦地安慰那无知的少年-2/","excerpt":"","text":"不要做父母手中的烤鸭，要做一只自由的小小种马。——刘星，《（假的）家有儿女》 丁丁再回到这条老街时，又是一年的光景。 这一年，家乡添了几处新房和俏媳妇，添了几家麻将交流中心，添了几座坟头。 村后的河今年却冻住了——往年不上冻的，因为里面东西太多。 村前公路两旁的树全砍掉了。主人缺钱，不缺树。 目力所及，坑洼的油漆路向北延伸到省道上，两旁田地里丛丛的麦子依偎而息，灰蒙蒙的天，树林间掩映着冬日里小姑娘红扑扑的脸蛋，那是北方的夕阳。哎呦，还蒙了层雾气。 这次回老家，丁丁照例去拜访过道尽头被奶奶称为”二嫂“的老太太。 二嫂是帝都过来的知青，这些年没入我们的乡音，跟谁也是一口侉侉的北京话。 她最著名的话是，“我主的了疼，也主的了管”。 这是跟人家解释为什么老打孙子。一时成为村里溺爱孙子老传统中的一股清流。 可老人家现在状态不好：去年初四，脑出血，救回来之后半边失去了控制，歪了嘴，动不了腿。 我进了门，走到轮椅边。老人眼睛亮了起来，一只手撑着扶手，要站起来。 我大声说奶奶您不用起来，多累啊。 二嫂摇着头坐下，攥着我的手，晃来晃去。又赶紧把暖手袋扯过来，叫我捧着。 就像小时候那样。 二嫂是看着我长大的。奶奶经常带着我到二嫂家里串门，二嫂家里有糖吃，有奶喝。 那时候我最喜欢翻彻二嫂厚厚的影集，上面有好多我没见过的东西。 奶奶你耳朵边别着的是什么花呀，那时候你几岁。 二嫂说那年她十六，别着的花叫白玉兰。 今年她七十六。照片上的小姑娘带一点自信，含一丝羞赧，就像每个十六七岁的女孩子那样。 这让我想起妈妈。妈妈年轻的时候追邓丽君、小虎队，最喜欢的是粉红色的回忆。家里有一张她结婚时的照片，大红毛衣，傻傻的杵在那里，另一头爸爸给二叔骑在背上，向妈妈鞠躬，胸前歪着一朵大红花。 我没见过作为年轻姑娘的二嫂和妈妈是什么样子的，跟我相关的，只有她们逐渐老去的岁月。 二嫂晃动着身子，她打算站起身来。 我扶着她，走一步，拖一步，不违背，不阻挡。 五六米的距离，老人已经气喘吁吁。我也不说话，我单单陪着她。 院里的枣树上落了一只麻雀，不知为何她没回南方的家。隔壁的二层小楼开始掌起灯火，夜色也正吞下了半边天。 二嫂接着往外拖着步子，这时媳妇却迎着面从小卖部回来。 哎呀，涛你怎么让你奶奶出屋里来了？外面冷，娘咱回屋里吧。 二嫂不肯，但她做不了主。 这已不是她做主的日子了。 爷爷大二嫂好多岁，早就没了精神。多少年大大小小，一直是二嫂操持着。 去年的时候，我坐在炕头边，绕着问她年轻时候的故事。 她说她的一生就分为两部分，给大伙种地和给自己种地。前半段三十年，后半段三十年。 明明从北京赶过来，她却说这里更冷一些。村支书被打得藏在柜子底下，三千斤麦子换来的推车充了公，大雨下到把房子冲塌，夜不闭户，好冷。 二嫂说后来却是倒春寒。家乡的新媳妇，都凑不出一件体面衣裳。地里什么东西也不长。饿死的人排着队。 我问再后来呢？自己种总好些了吧？ 二嫂说自己种也要上交粮食给国家的。那年她推着小车，走了二十几里的土路，把麦子送到乡上。三十年了。 二嫂说这么多年看上去一直是我在做主支撑着这个家，但实际上我从来都没做过主，我对自己也做不了主，我对谁也做不了主。 我说还是我们这一代人幸福啊，赶上了好的时候。 二嫂说那只是看起来，长大了你就明白了。 然而我从来都长不大，二嫂却变老了。 二嫂老了，但从没老糊涂，也没装过糊涂，直到突然的疾病将糊涂的能力赐予给她。 回到屋里，二嫂就又安静地坐下来。电视里恰巧是场晚会，在希望的田野上。 夜幕已全然降临。猎户座的三星嵌在南面的而天空，月亮瘦成眉毛，挑在树枝上，除此之外，一片看不透的灰色将视野罩的密不透风。 我瞪着窗外，正出神，二嫂那边却哼了起来，摇起我的手。 呜呜声。奶奶又回到了回不去的小时候。 @ddlee 2017年2月","categories":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ddlee.cn/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ddlee.cn/tags/随笔/"}]},{"title":"谁会不厌其烦地安慰那无知的少年（一）","slug":"谁会不厌其烦地安慰那无知的少年-1","date":"2017-01-27T14:20:59.000Z","updated":"2017-04-02T17:48:34.000Z","comments":true,"path":"2017/01/27/谁会不厌其烦地安慰那无知的少年-1/","link":"","permalink":"http://blog.ddlee.cn/2017/01/27/谁会不厌其烦地安慰那无知的少年-1/","excerpt":"","text":"一个男孩要下过多少电影，才能称得上是一个男人？一只海鸥要飞过多少海洋，才能在柔柔的沙滩上安息？——鲍勃·迪伦，《答案在风中飘荡》 星星眨着眼，银河却不见。万家灯火散落在不遥远的远方，贪婪的夜色吞噬着视野，列车不紧不慢地刺破雾气的深不可测，卧铺床头的小台灯透过车窗温暖出朦胧一片，笼住返乡人的放松与期盼。 其实丁丁差点没赶上火车。亏得遇到老司机，路上没怎么堵。过检票口的时候，广播刚刚喊着“你所乘坐的班次已停止检票”。 火车终于安稳地行着。丁丁的心情也慢慢舒畅起来。 丁丁趴在铺上，翻看相册，回想这又一个人生七年。 小学到中学就是一趟火车，有起点也有终点，不慌不忙。大学是脱了轨的同一趟火车，东栽西撞，没有诗也到不了远方。 想到这里，丁丁下了铺，留意了一下安全锤的位置，然而，在回来时，他还是不可避免地被旁边的大叔注意到了。 嘿，小伙子，你也用Lumia 啊。 丁丁尴尬地讲，没，只是备用机，主力还是安卓。心想着竟然还被看出来了，不过正好，用Lumia不装逼，那跟咸鱼有什么区别。 大叔你做什么工作的呀。 大叔讲他是个半个码农，三倍的房奴，两个孩子的爹地，一个老婆坚实的依靠。 丁丁说自己是三个舍友的爸爸，五门课的开课赞助商，七个女生的备胎，九个社团的划水副总监。 大叔说你这就是我的Pro版啊，深交吗小伙子？ 丁丁说，好。 可这一开口，大叔就是从诗词歌赋到人生哲学。只不过，没有雪也没有月亮，我不是紫薇他也不叫尔康。 大叔并不大，现在在南京，江北一套房，鼓楼一套学区。两个儿子，大的刚上一年级，小的还不会撒谎。 自己公司年底出了状况，没能跟家人坐同一趟车回家。 大叔说自己本科数学，毕了业才发现自己卵没什么用。女朋友学计算机，早就找好了工作，自己只好考了研，后来拿了个硕士，主攻信号转发与缓存。 丁丁说我也数学。 大叔抿了抿嘴，嗯，有意思有意思。 大叔说那我给你介绍介绍考研经验吧。 丁丁说好啊好啊。 那年考研的形势很严峻，因为减招。 为了考研，大三那年寒假，我初五就从家里跑出来了。赶巧的是，那年跟今年一样，过年赶得好晚，我统共在家不到十天。 临走那天晚上，爸爸到单位值班，去之前又塞给我几百块钱，说穷家富路，但这种行为被我义正辞严地拒绝了。可爸爸走后，我泪湿眼底。 因为这一离开就又是半年。 考上大学第一年回家，奶奶跟我说你走后你爸来我这儿的时候哭了，说你跟小鸟一样飞走了。我说也是啊，我长大了，爸爸的一个时代也结束了呀，就在我报完到送他回去的那一刻。 那天在楼下值班室那里领钥匙，爸爸在一边摸着头笑，见我回头，他跟遇到喜欢的女生那样不好意思，红着脸。 爸爸的一个时代结束了呀。 还记得，我上小学那会儿，连午休都要家长签字确认的，还有作业也是，爸爸兢兢业业地把题都重新算一遍，马虎的地方狠狠批我一顿，这才用方方正正的钢笔给我签上“家长已检查”，现在我才知道，这叫“背书”。 那时候妈妈在一边儿踩着缝纫机，看点播台的我被爸爸叫过去，扭扭捏捏地摸着后脑勺，阳台上水仙开着，香味儿就飘到屋里来。 其实那时候的我才最懂事儿。那时我最大的梦想就是娶了班上最文静的女生，让妈妈少操点儿心。而她当时就是我的同桌，放学我们还一起走到灵石路的尽头，走过小酒馆的门口。后来四五年级，起了流言，我们就分了。 后来在外面求学，跟父母在一起的时间就越来越少了。那时候我最喜欢的时候是坐在大巴士高高的最后一排，靠窗，看路边的杨树一棵一棵闪过，我觉得我的人生康庄大道就在脚下一点一点伸展开来。 爸爸给我的支持也越来越少了。他不懂遗传平衡定律，找不到辅助线，也人脸识别不了虚拟语气。我的小小心思就像宇宙那般，无边地膨胀起来了。 高考就是碰到气球的那根针。我感觉自己是被发配到了南方，而且还被冻成了狗。 丁丁顺着说，南方确实冷的不行，尤其下雨天。 大叔说，你看，这些小事，我不说，就要一点一点埋葬在潺潺流去的岁月里了。 可我考研那年不懂事。我哭的时候，却觉得自己分分钟像个大人了，我早回去正是在做着那些英雄们不得不做的事儿。天将降大任于斯人矣。 为了呵护这个家，却要离开它。 浊酒一杯，家万里。 我觉得这就是我的燃情岁月。 后来研考上了，女朋友等了我三年，然后就媳妇也有了。后来我才知道燃情岁月才算刚刚开始。 丁丁蛮懂事，道，汪、汪、汪。 再后来，有了一室一厅，吉利帝豪，郊区的三室两厅，又因为堵车把车给卖了，再后来有了一个儿子，鼓楼的学区加户口房，又添了个儿子，就把爸妈接过来了。 这几年没有我特别想做的事儿。只有我需要做好的事儿。 两个小魔王，说实话我不觉得爸妈老年生活有多幸福。 不过多亏通了地铁，我每天八点半能到家，磕个瓜子，跟我爸聊聊我儿子和他儿子。 可是，小伙子，你知道吗？我考研那年，就是个愣头青。 那时候我对私人的时间有着近乎偏执的吝啬。我觉得自己独处的时间才是上天赐予的礼物。回家过年又烦又累，措不及防的应酬是对我神圣的私有时间的侵犯。所以，其实我早早就狠下心来，一定要早早的回学校。 我上车那天风声呼啸，暗云疾行，干燥的北风中赫赫抬起的，是我打车的一只大手。路两边白杨赤条条的，行人裹着衣，绷着脸。 风萧萧兮易水寒，众人向北我向南。 可是，小伙子，你知道吗？ 让男孩成为男人的，不是事业，是家业啊。 大叔突然不说了。他翻了个身，晚安。 丁丁也回过头，抹了眼睛，退了返程。 @ddlee 2017年1月","categories":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ddlee.cn/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.ddlee.cn/tags/随笔/"}]},{"title":"小米3变身记","slug":"小米3变身记","date":"2016-09-22T17:40:00.000Z","updated":"2017-04-05T13:13:00.000Z","comments":true,"path":"2016/09/23/小米3变身记/","link":"","permalink":"http://blog.ddlee.cn/2016/09/23/小米3变身记/","excerpt":"","text":"Across the Great Wall we can reach every corner in the world.”（越过长城，走向世界） 1.缘起——我、小米、安卓和Android知乎上有个抖机灵的回答，问题是“Nexus 5 如果不使用 VPN，会有什么影响”，回答是“android 体验变成安卓体验”。感谢无所不能的墙，让Android也有了中国特色。 各家应用市场层出不穷，应用推广不择手段，申请权限多多益善，后台活动精彩不断。更不能忍的是，小米移除了Google的服务框架，无法从Play商店推动应用。为了用Sleep Cycle alarm clock, 我得用在线工具从Play商店获取链接，同步到云盘里，再从手机里打开.apk文件来安装。因为，百度搜索出的那个结果，应用中内置了烦人的广告。 终于，我投奔了IOS阵营，手中服役的小米3也就闲置。在一个并没有那么蛋疼的午后，我拿出数据线，对它说，It’s time. 注意：本系列不作为通用教程，只做经历分享。请移步相关论坛获取教程信息。 2.基础——是什么，为什么，怎么办是什么你一定听说过，有种技术叫刷机。你也一定听说过，还有种技术叫越狱。你更一定听说过，还有种技术叫FanQiang。 这三者有什么关系呢？ 在我看来，它们都关乎我们作为用户常常忽略的两个字——权限。 换言之，我们常常关注可以用手机干什么、可以上网浏览什么，却常常不去注意，我们本来有更多事可以做，有更多信息可以获取。 刷机意味着给手机重装系统，你获得的是选择硬件所运行系统的权利；越狱获得的是掌控某一操作系统的权利；而FanQiang，获得的则是“越过长城，走向世界”的权利。 中国第一封电子邮件的内容是：Across the Great Wall we can reach every corner in the world.”（越过长城，走向世界）。这是1987年9月14日从北京向海外发出的中国第一封电子邮件，揭开了中国人使用互联网的序幕。 来源：知乎 为什么因为无聊，因为好奇，因为喜欢，因为不满足，因为我们可以。 怎么办如果把刷机比作建造楼房，你所需要准备的就是知识（图纸）、刷机包（水泥、混凝土）、调试环境（吊塔）。 知识真正重要的知识，是关于知识的知识。拿到图纸不重要，重要的是学会如何看懂图纸。以我的经历，最耗费精力的部分不是学习教程，而是TROUBLE SHOOTING, 是如何解决出现的问题。 因此，绝对不要使用某些工具的“一键刷机功能”，它们不会告诉你问题出在哪。 请保证你对整个过程的绝对控制，保证你清楚到底在哪一步无法继续进行。 而为了看懂图纸，你需要准备好你的Google.它会是你最可靠的伙伴。 下面是图纸中可能涉及的内容，请搜索并结合某些通用刷机教程理解它们发挥的作用。 卡刷、线刷：两种刷机的操作方式（体位） Root：获取Android系统管理员的过程 OTA：On The Air， 一种系统更新方式 ROM包： 刷入手机ROM的系统软件包 Recovery Mode： Android系统的一种模式，常在此模式下进行刷机操作 Fastboot Mode： Android系统的一种模式，可在此模式下刷入自定义recovery ADB： Android Debug Bridge，用PC对Android系统进行USB调教所需的环境 CM： 一家著名的ROM制作方，现已改名Lineage OS Android M： Android系统的一个版本，现在是N（Nougat，7.0） GApps： Google服务全家桶，需要刷入系统分区，包括Play和GMS等服务 3.刷机——大致的步骤，常见的坑一个负责任的教程，大概会告诉你如下几个步骤 风险警示 备份数据 如何搭建ADB环境-PC 如何进入fastboot模式-手机 如何在ADB环境下，fastboot模式中刷入自定义recovery 如何利用recovery模式清除数据，刷入ROM包（和Gapps） 如何ROOT 一次蛋疼的刷机经历，常常会遇到这些坑 找到正确的Recovery和ROM包：一定要仔细比对型号，尽量选用开源机构制作的包 搭建ADB环境：要用到命令行（请慎重选用一件脚本，即.bat文件） 连接电脑与手机：Windows系统下需要硬件驱动（请注意型号） …… 4.资源——与你同行论坛与搜索引擎你会发现，手机厂商的官方论坛和XDA等论坛会很有帮助。但真正与你同行的，还是Google。 常用网址 TWRP，著名的自定义recovery GApps，Google全家桶 Xposed，著名开源框架 ADB Guide SuperSU，用于手机Root @ddlee","categories":[{"name":"Android","slug":"Android","permalink":"http://blog.ddlee.cn/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://blog.ddlee.cn/tags/Android/"}]},{"title":"数据分析在线学习资源(Personal Archive)","slug":"数据分析资源","date":"2016-08-07T11:58:31.000Z","updated":"2017-04-09T15:44:34.000Z","comments":true,"path":"2016/08/07/数据分析资源/","link":"","permalink":"http://blog.ddlee.cn/2016/08/07/数据分析资源/","excerpt":"","text":"数据分析方向的在线资源收集。 1.Some wonderful Tutorials Data Analysis Learning Path from Springboard The Open Source Data Science Masters 2. Basic2.1 DatabaseStanford’s Database course 2.2 AgrolthmsAlgrothms from Stanford via Coursera(using Java)Booksite hereAlgorithm with Python in GItHub 2.3 AlgebraHarvard’s Massive Parralle Algebra Course on iTunes U 2.4 StatisticsPrinceton’s Statistics One 2.5 Books Pattern Recognition and Machine Learning by Bishop The Elements of Statistical Learning 3. Python3.1 Scipy &amp; Pandas &amp; sklearn Scipy Lecture Notes Pandas Doc Pandas Cookbook sklearn Doc 3.2 Python MOOCsedX courseMITx: 6.00.2x Introduction to Computational Thinking and Data Science via edX Udacity Course Design of Computer Programs with Peter Novig Intro to Machine Learning (project oriented) Machine Learning: Unsupervised Learning 3.3 Books Python for Data Analysis by Wes McKinney Programming Collective Intelligence by Toby Segaran 4. R4.1 R MOOCsedX courseMIT’s The Analytics Edge JH Data Science Specilization via Coursera Statistical Inference Regression Model Practical Machine Learning Develop Data Science Product Stanford’s Statistical Learninghereand its text book An Introduction to Statistical Learning ISLAR 4.2 R Books R Graphics Cookbook by Winston Chang ggplot2 by Hadley Wickham R in Action by Robert I. Kabacoff 5. Big Data5.1 “Big” MOOCsUdacity CourseIntro to Hadoop and MapReduce from clourdera Coursera CourseMining Massive Datasets edX CourseXserise on Spark from BerkleyX 6. Capstone Project SITP Project Health Twitter Analysis via Coursolve 7. Additional Resource Harvard’s CS109 Course: Data Science Berkley’s CS61:The Structure and Interpretation of Computer Programs Probabilistic Graphical Models via Coursera Berkeley’s Datascience’s Documentation A Gallery of IPython Notebooks A collection of Data Science Learning materials in the form of IPython Notebooks Unsupervised Feature Learing and Deep Learning @ddlee","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://blog.ddlee.cn/tags/Data-Science/"},{"name":"Data","slug":"Data","permalink":"http://blog.ddlee.cn/tags/Data/"}]},{"title":"个性化你的Ubuntu-3：主题，插件以及桌面小工具","slug":"个性化你的Ubuntu-3：主题，插件以及桌面小工具","date":"2016-06-11T14:52:49.000Z","updated":"2017-04-05T13:13:04.000Z","comments":true,"path":"2016/06/11/个性化你的Ubuntu-3：主题，插件以及桌面小工具/","link":"","permalink":"http://blog.ddlee.cn/2016/06/11/个性化你的Ubuntu-3：主题，插件以及桌面小工具/","excerpt":"","text":"个性主题依赖于扩展User themes，分为GTK主题，shell主题和icon主题。 从gnome-look.org下载喜欢的主题（压缩文件）。 将下载的主题文件复制到用户文件夹 cd ~ mkdir .themes cp file_path_to_download_file ~/.themes 并使用unzip或tar xvzf命令解压，或者： sudo cp file_path_to_download_file /usr/local/themes/ 在gnome-tweak-tool的扩展User themes中选择主题。 推荐主题我使用的是Numix系列的主题（官网） Numix-GTK3 theme Numix-like GNOME Shell theme Numix-Circle Icons Numix开发者之一Satyajit Sahoo发布的GNOME shell theme:Gnome Shell - Elegance Colors 通过PPA安装 sudo apt-add-repository ppa:numix/ppa sudo apt-get update sudo apt-get install numix-gtk-theme sudo apt-get install numix-icon-theme-circle sudo add-apt-repository ppa:satyajit-happy/themes sudo apt-get update &amp;&amp; sudo apt-get install gnome-shell-theme-elegance-colors 扩展插件我当前使用的插件： hide dash：隐藏侧边的favorite栏 Pomotodo：番茄时钟 （荐）Clipboard indicator：剪贴板切换 ToDo.txt：待办事项整理 Places indicator：文件浏览器的快捷方式 Activities configurator: 当前活动程序管理 Alternatetab: alt-tab桌面切换 Applications menu：类似Windows下开始菜单 （荐）Drop down terminal：快捷启动终端 Netspeed：网速监控 Openweather：状态栏天气预报 Removable drive menu：弹出U盘等可移除硬件 （荐）Dynamic top bar：根据窗口颜色变换顶栏颜色 其他桌面工具DOCK推荐Cairo-Dock，效果如图，扩展性很高，自定义程度也很好。 CONKY：桌面监测工具推荐Conky，皮肤也有很多，效果如图。 本系列至此完结。欢迎入坑。 @ddlee 2016年6月","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/tags/Linux/"},{"name":"Gnome","slug":"Gnome","permalink":"http://blog.ddlee.cn/tags/Gnome/"}]},{"title":"个性化你的Ubuntu-2：GNOME安装与工具","slug":"个性化你的Ubuntu-2：GNOME安装与工具","date":"2016-06-02T14:19:50.000Z","updated":"2017-04-05T13:13:02.000Z","comments":true,"path":"2016/06/02/个性化你的Ubuntu-2：GNOME安装与工具/","link":"","permalink":"http://blog.ddlee.cn/2016/06/02/个性化你的Ubuntu-2：GNOME安装与工具/","excerpt":"","text":"GNOME安装从上一篇文章，大家可以看到，GNOME是一系列软件的集合，安装时可以有不同的取舍。对于Ubuntu用户来说，可以有以下两类体验GNOME的方式。（参考：GNOME installation） 1.Ubuntu GNOME（系统）Ubuntu GNOME是Ubuntu的一个发行版本（也称Ubuntu variants），就像Ubuntu和Fedora等都是GNU/Linux的发行版那样。Ubuntu GNOME不仅包含了Ubuntu的核心部分、GNOME的核心部分，还有一系列的标准应用。 Install from DVD如果可以接受重新安装系统，请到这里下载Ubuntu GNOME。 Install with current system你也可以通过安装metapackage，这样在安装GNOME桌面环境时，你的系统中未安装的标准应用也会被同时安装。 sudo apt-get install ubuntu-gnome-desktop 2.GNOME（仅桌面环境）The “real” GNOME标准的GNOME桌面环境，没有Ubuntu的特性（尽管我区分不出哪些是Ubuntu提供的），也不安装附加的标准应用： sudo apt-get install gnome The minimux GNOMEGNOME的核心部分，不安装附加的标准应用： sudo apt-get install gnome-core GNOME shell仅安装GNOME的图形界面：sudo apt-get install gnome-shell 你还需要：sudo apt-get install gnome-session 注意在同一系统上安装不同的桌面环境可能会造成一些意料不到的问题（如锁屏界面丢失），最推荐的方案还是重新安装Ubuntu GNOME，其次，可以安装ubuntu-gnome-desktop。 使用新的桌面环境安装完毕后，重启，可在登录界面选择桌面环境。 GNOME配置工具：gnome-tweak-tool想要充分个性化GNOME桌面环境，扩展GNOME的功能，你还需要安装GNOME的配置工具：gnome tweak tool sudo apt-get install gnome-tweak-tool 利用gnome tweak tool，你可以管理桌面主题、调整窗口特性、调整显示字体、加载GNOME扩展、管理开机自启程序等等。 扩展插件在Ubuntu上，要调整桌面主题，可没有Windows上鼠标右击一下那么简单。你要先安装上面的tweak tool，然后有人告诉你需要User theme扩展插件，而你跑到extensions.gnome.org，遇到的却是这个： 我明明装了GNOME的啊！ 这是因为，extensions.gnome.org需要与浏览器通信，调用click-to-play的功能，我们需要安装GNOMNE shell intergration这个插件。 Chrome用户利用PPAsudo add-apt-repository ppa:ne0sight/chrome-gnome-shell sudo apt-get update sudo apt-get install chrome-gnome-shell 通过Chrome Web Store:GNOME Shell integration可能需要通过CMake安装native connector,请参考这一页面。 FireFox用户使用FireFox访问extensions.gnome.org时会有运行GNOME shell integration的通知，允许运行后刷新即可。 更多信息，请参考这一页面 安装好tweak-tool后，祝贺你已经打开了新世界的大门。下篇文章是关于扩展插件的推荐，欢迎继续阅读。 @ddlee","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/tags/Linux/"},{"name":"Gnome","slug":"Gnome","permalink":"http://blog.ddlee.cn/tags/Gnome/"}]},{"title":"个性化你的Ubuntu-1：GNOME桌面环境","slug":"个性化你的Ubuntu-1：GNOME桌面环境","date":"2016-05-30T11:12:10.000Z","updated":"2017-04-05T13:13:06.000Z","comments":true,"path":"2016/05/30/个性化你的Ubuntu-1：GNOME桌面环境/","link":"","permalink":"http://blog.ddlee.cn/2016/05/30/个性化你的Ubuntu-1：GNOME桌面环境/","excerpt":"","text":"我与Ubuntu我最初是Windows98用户，再到Windows2003,Windows XP,Windows 7,上了大学后用Windows 8.1,Windows 10（想不到竟然能列这么长；我从没用过Windows Vista,不知道那是什么东西），我很喜欢8.1和10的开始屏幕和动态磁贴。非常偶然的机会，我在CS50的课程中接触了GNU/Linux，才知道，原来在MS Windows和Mac OSＸ之外，还有一个GNU/Linux。换完SSD，学会了装操作系统，我便踏上了折腾GNU/Linux的不归路。 曾经被一个软院的同学安利Red Hat系的Fedora（尽管他现在已经投入了MacBook的怀抱）,普及各种内核之类的知识。然而，我只想安静的用它上上网，进行科学计算，并没有深入到考虑系统底层的需求层次。我还是安心地用Ubuntu吧。我也推荐第一次尝试GNU/Linux系统的小白从Ubuntu开始，相信我,askubuntu.com和stackoverflow.com会解决你的大部分问题的。 个性化你的Ubuntu（一）：GNOME桌面环境相信不少读者都是从Microsoft Windows转到GNU/Linux阵营的,早就习惯了用户图形界面。但是，配合桌面环境、主题和一些插件和软件，Ubuntu照样可以很酷炫。 什么是GNOME（大脚丫为什么这么大。。。） GNOME(pronounced /ɡˈnoʊm/ or /ˈnoʊm/) 最初是GNU Network Object Model Environment的缩写，但这一缩写已不再沿用（更多历史情况请参见这里）。 我们所说的GNOME，通常指的是由The GNOME Project开发的运行于Linux之上的桌面环境。 我们每天面对的，并不是全部的Microft Windows/OS X/Linux系统，而是系统提供给我们的人机接口，而桌面环境，则是统一在同一图形用户接口（GUI）之下的一揽子软件（X Window Manager, File manager, Terminal emulator, Text editor, Image viewer, E-mail client等）。 来源 Ubuntu自带的桌面环境是Unity（图形外壳）,其他流行的桌面环境还有KDE,Xfce。但我们要谈的是GNOME。 什么是X window system要谈Unix-like系统上的图形界面，就不得不提X Window System。那么，什么是X? The X Window System, commonly referred to merely as X, is a highly configurable, cross-platform, complete and free client-server system for managing graphical user interfaces (GUIs) on single computers and on networks of computers. (X窗口系统，通常简称为X，是用于管理在单个计算机和计算机网络上运行的图形用户界面（GUI）一个高度可配置的，跨平台，完整的，自由的客户端-服务器系统。） 来源：LINFO 我们试着通过X能够干什么来理解一下这句话。 X是一组规则、一套方法。它提供了从硬件（键鼠）接受用户输入、创建图形窗口、画出直线、位图等基本的图形功能（图形引擎）。 X实现了客户端-服务器的机制。通过划分Server和Client，X既能在本地计算机上运行，也能在计算计算机网络中运行。 X与操作系统独立。X可以理解为运行在操作系统之上的一套软件。如果不需要GUI，完全可以不用安装X。而在Microsoft Windows和OS X中，图形引擎是操作系统的一部分。 X Window System的结构如图。 GNOME &amp; XGNOME和X Window System是什么关系？桌面环境可以理解为一系列X client的集合，其中最重要的组件是X Window Manager。由于X Window System的client-server机制，各client之间是相对独立的，这时，需要一个特殊的client管理其他client，将他们统一在一个框架之下，这就是X Window Manager。来源 而GNOME另一个重要的组成部分是GNOME shell，它是一个图形外壳程序，也就是我们要面对的接口。 跟GNOME相关的其他组件、库、概念 GTK+：GIMP Widget toolkits，GNOME基于的GUI工具箱。KDE则基于Qt。 Display Manager:图形用户登陆管理器，为用户提供登陆界面，与session manager通信，开启新的session。GNOME使用的是GDM。 Metacity：GNOME 2使用的window manager，GNOME 3使用的是Mutter。KDE使用的是KWin。 Wayland:与X Window System对应，也是一种窗口系统 我现在的桌面 我不喜欢双击桌面图标来启动程序，更多用的是Dock和全局搜索，所以，桌面上“什么都没有”。 桌面的壁纸是电影 飞屋环游记 的海报，使用了Numix系列的主题和图标。 下方Dock使用的程序是Cairo-Dock，桌面右方运行的程序是Conky，用来监测系统运行情况和提供天气信息，上方的Topbar里添加了许多GNOME的扩展应用。 接下来的两篇文章将介绍Gnome的安装与扩展推荐，欢迎继续阅读，撒花。 @ddlee","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.ddlee.cn/tags/Linux/"},{"name":"Gnome","slug":"Gnome","permalink":"http://blog.ddlee.cn/tags/Gnome/"}]}]}