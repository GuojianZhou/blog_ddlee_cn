<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[目标检测常用评测集：Pascal VOC, MS COCO, Cityscapes]]></title>
      <url>https://blog.ddlee.cn/2018/03/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E8%AF%84%E6%B5%8B%E9%9B%86%EF%BC%9APascal-VOC-MS-COCO-Cityscapes/</url>
      <content type="html"><![CDATA[<p>本文节选自博主通过格灵深瞳公众号发表于知乎的文章：<a href="https://zhuanlan.zhihu.com/p/34179420" target="_blank" rel="external">目标检测入门（二）：模型的评测与训练技巧
</a>。</p>
<h2 id="检测模型的评测指标"><a href="#检测模型的评测指标" class="headerlink" title="检测模型的评测指标"></a>检测模型的评测指标</h2><p>目标检测模型本源上可以用统计推断的框架描述，我们关注其犯第一类错误和第二类错误的概率，通常用准确率和召回率来描述。准确率描述了模型有多准，即在预测为正例的结果中，有多少是真正例；召回率则描述了模型有多全，即在为真的样本中，有多少被我们的模型预测为正例。不同的任务，对两类错误有不同的偏好，常常在某一类错误不多于一定阈值的情况下，努力减少另一类错误。在检测中，mAP（mean Average Precision）作为一个统一的指标将这两种错误兼顾考虑。</p>
<p>具体地，对于每张图片，检测模型输出多个预测框（常常远超真实框的个数），我们使用IoU（Intersection Over Union，交并比）来标记预测框是否为预测正确。标记完成后，随着预测框的增多，召回率总会提升，在不同的召回率水平下对准确率做平均，即得到AP，最后再对所有类别按其所占比例做平均，即得到mAP。</p>
<p>在较早的Pascal VOC数据集上，常采用固定的一个IoU阈值（如0.5, 0.75）来计算mAP，现阶段较为权威的MS COCO数据集上，对不同的IoU阈值（0.5-0.95，0.05为步长）分别计算AP，再综合平均，并且给出了不同大小物体分别的AP表现，对定位准确的模型给予奖励并全面地展现不同大小物体上检测算法的性能，更为科学合理。</p>
<p>在实践中，我们不仅关注检测模型的精度，还关注其运行的速度，常常用FPS（Frame Per Second，每秒帧率）来表示检测模型能够在指定硬件上每秒处理图片的张数。通常来讲，在单块GPU上，两阶段方法的FPS一般在个位数，而单阶段方法可以达到数十。现在检测模型运行的平台并不统一，实践中也不能部署较为昂贵的GPU进行推断。事实上，很多文章并没有严谨讨论其提出模型的速度表现（加了较多的trick以使精度达到SOTA），另外，考虑到目前移动端专用芯片的发展速度和研究进展，速度方面的指标可能较难形成统一的参考标准，需要谨慎看待文章中汇报的测试结果。</p>
<h2 id="标准评测数据集"><a href="#标准评测数据集" class="headerlink" title="标准评测数据集"></a>标准评测数据集</h2><h3 id="Pascal-VOC（Pascal-Visual-Object-Classes）"><a href="#Pascal-VOC（Pascal-Visual-Object-Classes）" class="headerlink" title="Pascal VOC（Pascal Visual Object Classes）"></a>Pascal VOC（<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="external">Pascal Visual Object Classes</a>）</h3><p>自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的trainval2007和16k的trainval2012作为训练集（07+12），test2007作为测试集，用10k的trainval2007+test2007和和16k的trainval2012作为训练集（07++12），test2012作为测试集，分别汇报结果。</p>
<p>Pascal VOC对早期检测工作起到了重要的推动作用，目前提升的空间相对有限，权威评测集的交接棒也逐渐传给了下面要介绍的COCO。</p>
<h3 id="MS-COCO（Common-Objects-in-COntext）"><a href="#MS-COCO（Common-Objects-in-COntext）" class="headerlink" title="MS COCO（Common Objects in COntext）"></a>MS COCO（<a href="http://cocodataset.org" target="_blank" rel="external">Common Objects in COntext</a>）</h3><p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco.png" alt="MS COCO Roadmap，https://places-coco2017.github.io/"> <em>检测任务在COCO数据集上的进展</em></p>
<p>COCO数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/iconic-non-iconic.png" alt="iconic"> <em>iconic与non-iconic图片对比</em></p>
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p>
<p>COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco-stat.png" alt="dist"> <em>COCO数据集分布</em></p>
<p>在分布方面，COCO的每个类含有更多实例，分布也较为均衡（上图a），每张图片包含更多类和更多的实例（上图b和c，均为直方图，每张图片平均分别含3.3个类和7.7个实例），相比Pascal VOC，COCO还含有更多的小物体（下图，横轴是物体占图片的比例）。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco-size.png" alt="dist-2"> <em>COCO数据集物体大小分布</em></p>
<p>如本文第一节所述，COCO提供的评测标准更为精细化，提供的<a href="https://github.com/cocodataset/cocoapi" target="_blank" rel="external">API</a>不仅包含了可视化、评测数据的功能，还有对模型的错误来源分析脚本，能够更清晰地展现算法的不足之处。COCO所建立的这些标准也逐渐被学术界认可，成为通用的评测标准。您可以在<a href="http://cocodataset.org/#detections-leaderboard" target="_blank" rel="external">这里</a>找到目前检测任务的LeaderBoard。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco-error-analysis.jpg" alt="coco-error-breakdown"> <em>错误来源分解，详见<a href="http://cocodataset.org/#detections-eval" target="_blank" rel="external">http://cocodataset.org/#detections-eval</a></em></p>
<h3 id="Cityscapes"><a href="#Cityscapes" class="headerlink" title="Cityscapes"></a><a href="https://www.cityscapes-dataset.com/" target="_blank" rel="external">Cityscapes</a></h3><p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/cityscapes-example.png" alt="cityscapes"> <em>Cityscapes数据示例</em></p>
<p>Cityscapes数据集专注于现代城市道路场景的理解，提供了30个类的像素级标注，是自动驾驶方向较为权威的评测集。</p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[目标检测任务表述与模型基本结构]]></title>
      <url>https://blog.ddlee.cn/2018/03/05/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1%E8%A1%A8%E8%BF%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/</url>
      <content type="html"><![CDATA[<p>本文节选自博主通过格灵深瞳机构号发表在知乎上的文章：<a href="https://zhuanlan.zhihu.com/p/34142321" target="_blank" rel="external">干货 | 目标检测入门，看这篇就够了</a>。</p>
<h2 id="目标检测的任务表述"><a href="#目标检测的任务表述" class="headerlink" title="目标检测的任务表述"></a>目标检测的任务表述</h2><p>如何从图像中解析出可供计算机理解的信息，是机器视觉的中心问题。深度学习模型由于其强大的表示能力，加之数据量的积累和计算力的进步，成为机器视觉的热点研究方向。</p>
<p>那么，如何理解一张图片？根据后续任务的需要，有三个主要的层次。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测任务表述与模型基本结构/cv.jpg" alt="cv"> <em>图像理解的三个层次</em></p>
<p>一是分类（Classification），即是将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片。这一任务是最简单、最基础的图像理解任务，也是深度学习模型最先取得突破和实现大规模应用的任务。其中，ImageNet是最权威的评测集，每年的ILSVRC催生了大量的优秀深度网络结构，为其他任务提供了基础。在应用领域，人脸、场景的识别等都可以归为分类任务。</p>
<p>二是检测（Detection）。分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置），因而，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检出目标的类别和位置（常用矩形检测框的坐标表示）。</p>
<p>三是分割（Segmentation）。分割包括语义分割（semantic segmentation）和实例分割（instance segmentation），前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。</p>
<h2 id="检测模型基本特点"><a href="#检测模型基本特点" class="headerlink" title="检测模型基本特点"></a>检测模型基本特点</h2><p>深度学习方法主导下的检测模型，可以分为两阶段（two-stage）和单阶段（one-stage）。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测任务表述与模型基本结构/faster-rcnn-arch.png" alt="faster-rcnn-arch"> <em>两阶段检测模型Pipeline，<a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_blank" rel="external">来源</a></em></p>
<p>检测模型整体上由基础网络（Backbone Network）和检测头部（Detection Head）构成。前者作为特征提取器，给出图像不同大小、不同抽象层次的表示；后者则依据这些表示和监督信息学习类别和位置关联。检测头部负责的类别预测和位置回归两个任务常常是并行进行的，构成多任务的损失进行联合训练。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测任务表述与模型基本结构/faster-rcnn-head.png" alt="faster-rcnn-head"> <em>检测模型头部并行的分支，<a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_blank" rel="external">来源</a></em></p>
<p>相比单阶段，两阶段检测模型通常含有一个串行的头部结构，即完成前背景分类和回归后，把中间结果作为RCNN头部的输入再进行一次多分类和位置回归。这种设计带来了一些优点：</p>
<ul>
<li>对检测任务的解构，先进行前背景的分类，再进行物体的分类，这种解构使得监督信息在不同阶段对网络参数的学习进行指导</li>
<li>RPN网络为RCNN网络提供良好的先验，并有机会整理样本的比例，减轻RCNN网络的学习负担</li>
</ul>
<p>这种设计的缺点也很明显：中间结果常常带来空间开销，而串行的方式也使得推断速度无法跟单阶段相比；级联的位置回归则会导致RCNN部分的重复计算（如两个RoI有重叠）。</p>
<p>另一方面，单阶段模型只有一次类别预测和位置回归，卷积运算的共享程度更高，拥有更快的速度和更小的内存占用。读者将会在接下来的文章中看到，两种类型的模型也在互相吸收彼此的优点，这也使得两者的界限更为模糊。</p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]YOLO9000: Better, Faster, Stronger]]></title>
      <url>https://blog.ddlee.cn/2018/03/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-YOLO9000-Better-Faster-Stronger/</url>
      <content type="html"><![CDATA[<p><a href="https://arxiv.org/1612.08242" target="_blank" rel="external">YOLO9000: Better, Faster, Stronger</a></p>
<p>在这篇文章里，单阶段检测模型的先驱工作YOLO迎来了全面的更新：</p>
<ol>
<li>在卷积层添加BN，舍弃Dropout</li>
<li>更高尺寸的输入</li>
<li>使用Anchor Boxes，并在头部运用卷积替代全连接层</li>
<li>使用聚类方法得到更好的先验，用于生成Anchor Boxes</li>
<li>参考Fast R-CNN的方法对位置坐标进行log/exp变换使坐标回归的损失保持在合适的数量级</li>
<li>passthrough层：类似ResNet的skip-connection，将不同尺寸的feature map拼接到一起</li>
<li>多尺度训练</li>
<li>更高效的网络Darknet-19，类似VGG的网络，在ImageNet上以较少的参数量达到跟当前最佳相当的精度</li>
</ol>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-YOLO9000-Better-Faster-Stronger/yolov2.jpg" alt="yolov2"></p>
<p>此次改进后，YOLOv2吸收了很多工作的优点，达到跟SSD相当的精度和更快的推断速度。</p>
<p>作者还介绍了一种新的联合训练方式：同时训练分类任务和检测任务，使得检测模型能够泛化到检测训练集之外的目标类上。</p>
<p>YOLO9000使用了ImageNet和COCO数据集联合训练，在合并两者的标签时，根据WordNet的继承关系构建了了树状的类别预测图：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-YOLO9000-Better-Faster-Stronger/yolo9000_tree.jpg" alt="wordtree"></p>
<p>类似条件概率的方式计算每个子标签的概率值，超出一定的阈值时则选定该类作为输出，训练时也仅对其路径上的类别进行损失的计算和BP。</p>
<p>YOLO9000为我们提供了一种泛化检测模型的训练方式，文章的结果显示YOLO9000在没有COCO标注的类别上有约20的mAP表现，能够检测的物体类别超过9000种。当然，其泛化性能也受检测标注类别的制约，在有类别继承关系的类上表现不错，而在完全没有语义联系的类上表现很差。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Training Region-based Object Detectors with Online Hard Example Mining]]></title>
      <url>https://blog.ddlee.cn/2018/02/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining/</url>
      <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1604.03540" target="_blank" rel="external">Training Region-based Object Detectors with Online Hard Example Mining</a></p>
<h3 id="OHEM-Online-Hard-negative-Example-Mining，在线难例挖掘"><a href="#OHEM-Online-Hard-negative-Example-Mining，在线难例挖掘" class="headerlink" title="OHEM(Online Hard negative Example Mining，在线难例挖掘)"></a>OHEM(Online Hard negative Example Mining，在线难例挖掘)</h3><p>本文是Bootstrapping（自助采样）在深度网络中的应用。两阶段网络由于其多步的特性，在RCNN子网络的计算前会有对RoI的整理过程，早期工作中，Fast R-CNN利用随机上采样和下采样的方法来维持正负样本比例，而本文提出的方法则使得网络具有挑选“好的”正负样本的能力。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining/ohem.png" alt="ohem"></p>
<p>作者提出用R-CNN子网络对RoI Proposal预测的分数来决定每个batch选用的样本，这样，输入R-CNN子网络的RoI Proposal总为其表现不好的样本，提高了监督学习的效率。实际操作中，维护两个完全相同的R-CNN子网络，其中一个只进行前向传播来为RoI Proposal的选择提供指导，另一个则为正常的R-CNN，参与损失的计算并更新权重，并且将权重复制到前者以使两个分支权重同步。</p>
<p>OHEM以额外的R-CNN子网络的开销来改善RoI Proposal的质量，更有效地利用数据的监督信息，成为两阶段模型提升性能的常用部件之一。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[PyCharm+PipEnv本地Python开发环境配置]]></title>
      <url>https://blog.ddlee.cn/2018/02/12/PyCharm-PipEnv%E6%9C%AC%E5%9C%B0Python%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>本文记录基于PyCharm的Python工作环境配置，最终实现的效果是本地修改，远程调试，运行环境可迁移。</p>
<h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>该环境的配置需要如下准备：</p>
<ul>
<li>开发用笔记本电脑和部署用服务器</li>
<li><p>PyCharm Pro版本，如果是学生，可以免费得到这一版本</p>
</li>
<li><p>依赖管理包Pipenv</p>
</li>
</ul>
<p>逻辑是在本地编写和调试代码，调用服务器的计算资源运行，并通过PyCharm和Pipenv保证本地开发环境和服务器端运行环境的一致。</p>
<p>关于Pipenv：<br>Pipenv是一个将pip和virtualenv功能整合在一起的依赖管理包，提倡一个项目一个环境，利用Pipfile存储依赖信息并提供可迁移性。Pipenv为每个项目建立一个virtualenv，并记录pip安装依赖包的版本信息。</p>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>1.（可选）在GitHub等版本控制服务建立repo。<br>2.在服务器上建立项目文件夹，并用命令<code>pipenv install</code>初始化项目环境。<br>3.本地使用PyCharm打开项目（从GitHub，或者新建在本地文件夹），并配置部署环境（Tools-&gt;Deployment-&gt;Configurations)，以SFTP方式连接，并在本地开发项目文件夹和服务器端项目文件夹建立映射。</p>
<p><img src="https://static.ddlee.cn/static/img/PyCharm-PipEnv本地Python开发环境配置/sftp.png" alt="sftp"></p>
<p>4.打开Files-&gt;settings-&gt;project settings-&gt;project interpreter，选择add remote，勾选Deployment configuration到上一步建立的配置，选择Move this server to IDE settings。</p>
<p><img src="https://static.ddlee.cn/static/img/PyCharm-PipEnv本地Python开发环境配置/interpreter.png" alt="interpreter"></p>
<p>最后指定好上一步pipenv建立的vitualenv（默认目录为~/.local/share/virtualenvs/）为Python解释器的路径。</p>
<p><img src="https://static.ddlee.cn/static/img/PyCharm-PipEnv本地Python开发环境配置/patch.png" alt="path"></p>
<p>5.在Tools-&gt;Deployment-&gt;Download from将服务器代码下载到本地（主要是Pipfile），之后右击项目，将项目文件上传到服务器（本地代码文件），并在Tools-&gt;Deployment勾选Automatic Upload，使本地代码跟服务器代码保持同步。</p>
<p>6.在Tools-&gt;Start SSH Session利用部署配置登录服务器，使用Pipenv来安装需要的依赖后，在本地新建脚本文件进行测试。</p>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p>1.本地修改代码，自动上传到服务器<br>2.利用SSH处理数据存取路径等问题<br>3.运行脚本（使用远程环境）并调试<br>4.解决报错问题<br>5.调试成功，将代码提交到版本控制服务<br>6.重复</p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Software </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation]]></title>
      <url>https://blog.ddlee.cn/2018/01/18/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/</url>
      <content type="html"><![CDATA[<p>本文是MobileNets的第二版。<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">第一版</a>中，MobileNets全面应用了Depth-wise Seperable Convolution并提出两个超参来控制网络容量，在保持移动端可接受的模型复杂性的基础上达到了相当的精度。而第二版中，MobileNets应用了新的单元：Inverted residual with linear bottleneck，主要的改动是添加了线性Bottleneck和将skip-connection转移到低维bottleneck层。</p>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>本篇比较丰富的地方是对网络中bottleneck结构的探讨。</p>
<p>在最早的Network in Network工作中，1x1卷积被作为一个降维的操作而引入，后来逐渐发展为Depth-wise Seperable Convolution（可分离卷积）并被广泛应用，堪称跟skip-connection同样具有影响力的网络部件。在<a href="https://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/">Inception单元</a>最初提出之时，具有较多channel的feature map被认为是可供压缩的，作者引入1x1卷积将它们映射到低维（较少channel数）空间上并添加多路径处理的范式。之后的<a href="https://blog.ddlee.cn/2018/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/">Xception</a>、MobileNets等工作则将可分离卷积应用到极致：前者指出可分离卷积背后的假设是跨channel相关性和跨spatial相关性的解耦，后者则利用可控的两个超参来获得在效率和精度上取得较好平衡的网络。</p>
<p>文中，经过激活层后的张量被称为兴趣流形，具有维HxWxD，其中D即为通常意义的channel数，部分文章也将其称为网络的宽度（width）。</p>
<p>根据之前的研究，兴趣流形可能仅分布在激活空间的一个低维子空间里，利用这一点很容易使用1x1卷积将张量降维（即MobileNet V1的工作），但由于ReLU的存在，这种降维实际上会损失较多的信息。下图是一个例子。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/collapse.png" alt="envolve"></p>
<p>上图中，利用MxN的矩阵B将张量（2D，即N=2）变换到M维的空间中，通过ReLUctant后（y=ReLU(Bx)），再用此矩阵之逆恢复原来的张量。可以看到，当M较小时，恢复后的张量坍缩严重，M较大时则恢复较好。</p>
<p>这意味着，在较低维度的张量表示（兴趣流形）上进行ReLU等线性变换会有很大的信息损耗。因而本文提出使用线性变换替代Bottleneck的激活层，而在需要激活的卷积层中，使用较大的M使张量在进行激活前先扩张，整个单元的输入输出是低维张量，而中间的层则用较高维的张量。文中所用单元的演化过程如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/envolve.png" alt="envolve"></p>
<p>。图a中普通卷积将channel和spatial的信息同时进行映射，参数量较大；图b为可分离卷积，解耦了channel和spatial，化乘法为加法，有一定比例的参数节省；图c中进行可分离卷积后又添加了bottleneck，映射到低维空间中；图d则是从低维空间开始，进行可分离卷积时扩张到较高的维度（前后维度之比被称为expansion factor，扩张系数），之后再通过1x1卷积降到原始维度。</p>
<p>实际上，图c和图d的结构在堆叠时是等价的，只是观察起点的不同。但基于兴趣流形应该分布在一个低维子空间上的假设，这引出了文章的第二个关键点：将skip-connection转移到低维表达间，即Inverted residual block。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/invert_residual.png" alt="inverted"></p>
<p>综合以上两点，文章中网络所用的基本单元如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/module.png" alt="envolve"></p>
<p>文章指出，这种设计将层输入、输出空间跟层变换分离，即网络容量（capacity）和表达力（expressIveness）的解耦。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/module2.png" alt="envolve"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>MobileNet V2的整体结构如下表：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/arch.png" alt="envolve"></p>
<p>上图中，t代表单元的扩张系数，c代表channel数，n为单元重复个数，s为stride数。可见，网络整体上遵循了重复相同单元和加深则变宽等设计范式。也不免有人工设计的成分（如28^2*64单元的stride，单元重复数等）。</p>
<h4 id="ImageNet-Classification"><a href="#ImageNet-Classification" class="headerlink" title="ImageNet Classification"></a>ImageNet Classification</h4><p>MoblieNets V2仍然集成了V1版本的两个超参数，在ImageNet上的实验结果如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/imagenet.png" alt="envolve"></p>
<p>可以看到相比V1版本优势明显，在精度方面跟NAS搜索出的结构有相当的表现。</p>
<h4 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h4><p>文章还提出SSDLite来更好适应移动端需求，改动是将head部分的普通卷积都替换为了可分离卷积。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/coco.png" alt="envolve"></p>
<p>上面是在COCO上的表现，可以看到精度方面跟YOLOv2和SSD300相当（尽管很低，相比SOTA差距还很大），但模型参数和运算复杂度都有一个数量级的减少。最后的CPU时间是在Pixel上测得，可以到5FPS，达不到真正移动实时的要求，但也是不小的推进了（并没有给出GPU上的推断时间，而Pixel+TF-Lite的benchmark又跟其他网络难以产生有效的比较）。</p>
<h4 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h4><p>下图是在VOC上分割的结果：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/seg.png" alt="envolve"></p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>文章还做了关于线性变换bottleneck替代ReLU和skip-connection位置的实验，进一步支撑之前的分析。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/ablation.png" alt="envolve"></p>
<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p>本篇文章的附录部分提供了紧的n维流形在经过升维线性变化加ReLU后被映射到子集的期望大小的界，这个界说明在扩张到足够高的维度后，升维线性变换加ReLu能以较高的概率可逆（保持信息）并加入非线性。</p>
<p>上面的结论是非常拗口的。自己的理解是，使用ReLU引入非线性的同时会导致信息损失（非线性指不会被卷积、全连接等线性映射吸收掉，信息损失则是指ReLU将&lt;0的输入置0，输入变得稀疏，而若所有输入的某一维度都被置0，则会使输入空间本身降维），我们要对抗这一可能的信息损失，需要将输入先扩张，即y=ReLU(Bx)，x为R^n空间上的输入，B为m×n矩阵，我们期望m足够大，以达到扩张的效果并在经过ReLU后保持y跟x的信息量同样多（文中的引理二，即是证此变换可逆性的一个条件，应该是借用了代数的概念，矩阵在经过可逆变换后不会降秩，秩成为衡量信息损失的指标）。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/distribution.png" alt="envolve"></p>
<p>在ReLU(Bx)算符可逆性的问题上，作者做了一些经验性实验，如上图。a和b分别为训练前后，每层正激活channel数（可逆性条件）和其占总channel数比例的分布。图a和图b的左图，随网络加深，channel数增多，即变宽；训练前后，方差增大，且有两层低于了可逆性条件阈值（图b左图中绿色线低于紫色阈值的部分）。右图是一个比例，由于随机初始化，均值在0.5附近，训练后同样方差增大，而可逆性条件阈值一直为1/6（即为MobileNet V2扩张系数的倒数）。</p>
<p>附录的Theorem 1则证明了ReLU(Bx)算符将输入x压缩后的空间维度(n-volume)的界，此界在扩张系数较大时可以跟原空间相当，即信息损失很小。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>能够看到附录里给出文中假定或观察的数学证明的论文还是开心的。太多论文只是终于实验的SOTA而避而不谈Insights，况且给出证明。尽管本篇附录中的证明仍有经验主义的部分，且并没有完全定义清楚问题和结论，其对后续工作的启发价值还是有的。</p>
<p>这也暴露了当前领域的通病，我们没有共通的一套语言来描述自己的网络，譬如，如何定义网络的容量、表达力，如何衡量信息的损失。没有通用的定义造成了论文表述常常有令经验少者难以理解的表达。去定义这样一套语言和标准来为网络设计提供参考，希望成为以后的研究热点，也是我自己的一个思考方向。</p>
<p>总体来看，本篇文章提供的两点改进都是有启发性的，但并不完整，需要更多工作来补充。另外源码没有给出，会尝试复现。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Chrome扩展推荐]]></title>
      <url>https://blog.ddlee.cn/2018/01/16/Chrome%E6%89%A9%E5%B1%95%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<p>本篇是一个Chrome扩展的推荐列表。所有扩展插件在<a href="https://chrome.google.com/webstore/" target="_blank" rel="external">Chrome Web Store</a>都可搜索得到。</p>
<ul>
<li>Adblock Plus，用于屏蔽广告，效果出众，必备</li>
<li>Chrono Download Manager，下载任务管理，必备</li>
<li>Draw.io Desktop，流程图编辑，轻量方便</li>
<li>Emoji Keyboard (2016) by EmojiOne，表情输入</li>
<li>Evernote Web Clipper，保存有价值的信息到Evernote</li>
<li>Fatkun Batch Download Image，批量下载图片的插件，配合Google搜图</li>
<li>Follow Feed(by Feedly)，在当前网页搜索RSS订阅源并订阅至Feedly，用于订阅浏览到的价值博客等</li>
<li>GNOME Shell Integration，GNOME插件集成，用于在extension.gnome.org给GNOME安装插件，GNOME用户必备（有关GNOME可参考<a href="https://blog.ddlee.cn/tags/Gnome/">这里</a>）</li>
<li>Google Dictionary (by Google)，字典，设置快捷键Ctrl+D，搜词很方便</li>
<li>Google Input Tools，Google输入工具，应急之用</li>
<li>Google Scholar Button，用于在当前网页识别论文并在Google Scholar上检索相关内容</li>
<li>Google Translate，选取网页内容，可进行方便翻译</li>
<li>Inbox by Gmail，用于保存连接等内容到邮箱，方便分享</li>
<li>LastPass，跨平台的免费密码管理</li>
<li>Mega，如名</li>
<li>Mendeley Importer，导入文章到Mendeley Library</li>
<li>Mercury Reader，渲染网页到阅读模式，清爽干净</li>
<li>Momentum，增强新标签页，显示时钟、天气、To Do List等</li>
<li>Octotree，显示Github Repo的目录结构，必备</li>
<li>One-Click Extensions Manager，管理扩展用，用于节省内存</li>
<li>PDF Viewer，用于阅读PDF（免于直接下载），记得勾选Allow access to file URLs</li>
<li>Proxy SwitchyOmega，代理，善用auto switch和备份等功能</li>
<li>Pushbullet，跨设备文字通信，精分（PC用Ubuntu，平板iOS，手机Android）推荐</li>
<li>Quick QRCode，利器，用于把文字、连接等转成二维码，方便分享</li>
<li>Save to Google，保存网页到Google</li>
<li>Save to Pocket，保存到Pocket，稍后再读工具</li>
<li>Secure Shell，网页版SSH</li>
<li>Tab Snooze，折叠暂时不必要的标签页，利器</li>
<li>Telegram，如名</li>
<li>Text，轻量文本编辑器</li>
<li>Turn Off the Lights，利器，关灯。YouTube和Bilibili可用，其他未测试</li>
<li>Vimium，利器，脱离鼠标的网页浏览体验</li>
<li>微软雅黑字体，强迫症推荐</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Chrome </tag>
            
            <tag> Software </tag>
            
            <tag> Digital Life </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Laptop Reborn: 系统重装侧记]]></title>
      <url>https://blog.ddlee.cn/2018/01/15/Laptop-Reborn-%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A3%85%E4%BE%A7%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>本篇是个人向的系统重装记录，以便自己日后参考。</p>
<p>我使用的系统是Win10+Ubuntu16.04。</p>
<p>Ubuntu为主力，Windows在两种情况下发挥作用：</p>
<ol>
<li>用只支持win平台下解锁的移动硬盘存取文件</li>
<li>用iTunes备份iPad和传输文件</li>
</ol>
<h2 id="备份-Backup"><a href="#备份-Backup" class="headerlink" title="备份 Backup"></a>备份 Backup</h2><p>备份必然是第一步，几乎只需要备份用户文件夹就好。</p>
<p>Ubuntu:<br>/home/Documents/<br>/home/Pictures(Music, Video, etc.)<br>/home/Downloads(Maybe)</p>
<p>Windows<br>~/Documents/(Music, Picture, Video, Desktop)</p>
<h2 id="镜像准备-ISO-Preparation"><a href="#镜像准备-ISO-Preparation" class="headerlink" title="镜像准备 ISO Preparation"></a>镜像准备 ISO Preparation</h2><p>准备系统镜像。这次用的刻录软件分别是Ruff(Windows)和Etcher(Ubuntu)。</p>
<p>Ubuntu: Ubuntu Gnome 16.04(From TUNA Mirror)</p>
<p>Windows: Win10-multi-ver1709(msdn.itellyou.cn)</p>
<p>Burning Tool: etcher portable</p>
<h2 id="硬盘分区-Disk-Partition"><a href="#硬盘分区-Disk-Partition" class="headerlink" title="硬盘分区 Disk Partition"></a>硬盘分区 Disk Partition</h2><p>双系统共存的话，要先装Windows(反之Windows安装时会影响已存在的Ubuntu)，因此分区这一部分在安装Windows的时候完成。</p>
<p>我的SSD只有240G，因此选择50+50给Windows，剩下的给Ubuntu(包括swap)</p>
<p>Windows(sys 50G, data 50G)<br>Ubuntu(sys 120G)</p>
<h2 id="驱动准备-Driver-Preparation"><a href="#驱动准备-Driver-Preparation" class="headerlink" title="驱动准备 Driver Preparation"></a>驱动准备 Driver Preparation</h2><p>Ubuntu还好，全新的Windows可能没法驱动网卡，这样就没办法更新和后续操作。我之前的方案是用驱动人生或者驱动精灵的网卡版，用完之后，良犬烹。但这次洁癖发作，因此选择去联想官网下载网卡驱动。再用相对干净的Driver boost更新其他驱动。</p>
<p>netcard: <a href="https://pcsupport.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-t-series-laptops/thinkpad-t460p/downloads" target="_blank" rel="external">https://pcsupport.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-t-series-laptops/thinkpad-t460p/downloads</a></p>
<p>Driver update tool(Driver boost 3): <a href="http://download.cnet.com/Driver-Booster-2/3000-18513_4-75992725.html?part=dl-&amp;subj=dl&amp;tag=button" target="_blank" rel="external">http://download.cnet.com/Driver-Booster-2/3000-18513_4-75992725.html?part=dl-&amp;subj=dl&amp;tag=button</a></p>
<h2 id="软件安装-Software-Installation"><a href="#软件安装-Software-Installation" class="headerlink" title="软件安装 Software Installation"></a>软件安装 Software Installation</h2><p>这一部分只是一个list，主要给自己参考用。</p>
<p>Windows:</p>
<ul>
<li>Minimal-Net-Pack</li>
<li>Office 2016</li>
<li>iTunes</li>
<li>CCleaner</li>
<li>Picasa</li>
<li>PotPlayer</li>
<li>mini-Thunder</li>
<li>Launchy</li>
</ul>
<p>Ubuntu:</p>
<ul>
<li>Minimal-Net-Pack</li>
<li>Google Chrome</li>
<li>Sougou Pinyin, Nutstore, Stacer</li>
<li>Netease Music, Variety</li>
<li>Gnome themes, Telegram, Xmind, FileZilla</li>
</ul>
<h2 id="开发环境配置-Environment-Configuration"><a href="#开发环境配置-Environment-Configuration" class="headerlink" title="开发环境配置 Environment Configuration"></a>开发环境配置 Environment Configuration</h2><p>这一部分完成自己开发环境的配置。</p>
<h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h4><p>SSH key: ssh-keygen and ssh-add, git config name&amp;email</p>
<h4 id="Python-Annaconda"><a href="#Python-Annaconda" class="headerlink" title="Python(Annaconda)"></a>Python(Annaconda)</h4><p>(pypi: 修改 ~/.config/pip/pip.conf (Linux), %APPDATA%\pip\pip.ini (Windows 10) 或 $HOME/Library/Application Support/pip/pip.conf (macOS) (没有就创建一个)， 修改 index-url至tuna，例如<br>[global]<br>index-url = <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="external">https://pypi.tuna.tsinghua.edu.cn/simple</a> )<br>(conda TUNA 还提供了 Anaconda 仓库的镜像，运行以下命令:<br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a></p>
<p>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a></p>
<p>conda config –set show_channel_urls yes<br>即可添加 Anaconda Python 免费仓库。)</p>
<h4 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h4><p>install: <a href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/</a></p>
<p>To verify: sudo docker run hello-world</p>
<p>mirror: <a href="https://www.docker-cn.com/registry-mirror" target="_blank" rel="external">https://www.docker-cn.com/registry-mirror</a><br>chmod 755 /etc/docker<br>为了永久性保留更改，您可以修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。<br>{ “registry-mirrors”: [“<a href="https://registry.docker-cn.com" target="_blank" rel="external">https://registry.docker-cn.com</a>“] }<br>修改保存后重启 Docker 以使配置生效</p>
<p>ref: <a href="https://docker_practice.gitee.io" target="_blank" rel="external">https://docker_practice.gitee.io</a></p>
<h4 id="R"><a href="#R" class="headerlink" title="R"></a>R</h4><p>sudo apt install r-base r-base-dev</p>
<h4 id="Hexo-Node-js"><a href="#Hexo-Node-js" class="headerlink" title="Hexo(Node.js)"></a>Hexo(Node.js)</h4><p>Node.js:<br>curl -sL <a href="https://deb.nodesource.com/setup_8.x" target="_blank" rel="external">https://deb.nodesource.com/setup_8.x</a> | sudo -E bash -<br>sudo apt-get install -y nodejs<br>hexo:<br>sudo npm install -g hexo-cli<br>rebuild bind in hexo directory:<br>npm rebuild node-sass</p>
<h2 id="杂项-Minor-Changes"><a href="#杂项-Minor-Changes" class="headerlink" title="杂项 Minor Changes"></a>杂项 Minor Changes</h2><p>一些需要操作的小地方备忘。</p>
<p>1.install Nvidia Driver<br><a href="http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux" target="_blank" rel="external">http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux</a><br><a href="https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07" target="_blank" rel="external">https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07</a></p>
<p>sudo add-apt-repository ppa:graphics-drivers/ppa<br>sudo apt-get update<br>sudo apt-get install nvidia-384<br>(ref at <a href="http://www.nvidia.com/object/unix.html" target="_blank" rel="external">http://www.nvidia.com/object/unix.html</a> to determine driver version)<br>optional: pip install gpustat</p>
<p>Proxy for ppa.launchpad.net<br>修改/etc/apt/sources.list.d下面需要代理的仓库，将ppa.launchpad.net换成代理地址，执行sudo apt update更新即可。 可用代理地址：<a href="http://launchpad.proxy.ustclug.org/" target="_blank" rel="external">http://launchpad.proxy.ustclug.org/</a></p>
<p>2.Grub skip time<br><a href="https://askubuntu.com/questions/157925/how-do-i-skip-the-grub-menu-on-a-dual-boot-system" target="_blank" rel="external">https://askubuntu.com/questions/157925/how-do-i-skip-the-grub-menu-on-a-dual-boot-system</a></p>
<p>Edit /etc/default/grub to contain</p>
<p>if you prefer to see the menu for 1 second:<br>GRUB_HIDDEN_TIMEOUT=<br>GRUB_TIMEOUT=1</p>
<p>When you’re done, run sudo update-grub to save your changes.</p>
<p>3.cpufrequtils(set performance &amp; get temperature)<br><a href="http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Power-Management-with-cpufrequtils" target="_blank" rel="external">http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Power-Management-with-cpufrequtils</a><br><a href="https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature" target="_blank" rel="external">https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature</a></p>
<ul>
<li>Disable intel turbo boost technology: terminal:<br>sudo -i<br>echo “1” | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo<br>exit</li>
</ul>
<p>4.tracke-miner-fs bug<br><a href="https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup" target="_blank" rel="external">https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup</a></p>
<p>echo -e “\nHidden=true\n” | sudo tee –append /etc/xdg/autostart/tracker-extract.desktop /etc/xdg/autostart/tracker-miner-apps.desktop /etc/xdg/autostart/tracker-miner-fs.desktop /etc/xdg/autostart/tracker-miner-user-guides.desktop /etc/xdg/autostart/tracker-store.desktop &gt; /dev/null<br>gsettings set org.freedesktop.Tracker.Miner.Files crawling-interval -2<br>gsettings set org.freedesktop.Tracker.Miner.Files enable-monitors false<br>tracker reset –hard</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Digital Life </tag>
            
            <tag> Linux </tag>
            
            <tag> OS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[FlintOS轻体验]]></title>
      <url>https://blog.ddlee.cn/2018/01/10/FlintOS%E8%BD%BB%E4%BD%93%E9%AA%8C/</url>
      <content type="html"><![CDATA[<p>呃，我在<a href="https://ddlee.cn/index_ver2016.html" target="_blank" rel="external">2016版的个人主页</a>里曾表达自己特别想拥有一台Chromebook，可终于还是屈服在了网络环境面前。</p>
<p>离了我的路由器，我的Chromebook几乎就是个废物了。</p>
<p>不过，FlintOS的出现让我重拾了这个想法，我可能还是需要一台轻便的上网本。</p>
<p>FlintOS是Chromium OS的中文本地化项目，而后者正是Chromebook的操作系统，在美国的低端笔记本市场和教育市场占有很大的份额。</p>
<p>只不过在大陆呵呵。</p>
<p>言归正传，FlintOS背后的公司是成立不久的燧炻科技，这里是他们的<a href="https://flintos.com" target="_blank" rel="external">官网</a>，可以在这个<a href="https://flintos.com/faq/" target="_blank" rel="external">页面</a>了解更多FlintOS的信息，本篇中的版本是在<a href="https://flintos.com/community/topic/flint-os-for-pc-dev-3-1-%e5%8f%91%e5%b8%83%e9%80%9a%e7%9f%a5/" target="_blank" rel="external">论坛</a>的DEV3.2中国版。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>DEV3.2中国版的最大亮点是支持本地账户。这意味着在大陆的网络环境下也有使用的可能性。而且，这个版本内置了可供科学上网的服务，测试可用。另外，FlintOS settings里面也包括了安装Flash的快捷方式。</p>
<p>目前建议是在U盘上体验，安装教程在<a href="https://flintos.com/instructions-pc" target="_blank" rel="external">这里</a>。</p>
<p>也可以选择安装双启动和硬盘独占安装，可以参考社区的<a href="https://flintos.com/community/forum/flint-os-for-pc/" target="_blank" rel="external">置顶帖</a>。</p>
<h2 id="界面UI"><a href="#界面UI" class="headerlink" title="界面UI"></a>界面UI</h2><p>桌面和右下角的通知栏，遵从了Material Design，看着舒服。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/desktop.png" alt="desktop"></p>
<p>APP Launcher，跟Gnome的风格很像。搜索栏可以直接Google搜索，也可以搜索文件和APP。实际上大部分APP可以当做交互逻辑级别更高的书签，打开即是新建相应网站的标签页（如YouTube）。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/apps.png" alt="desktop"></p>
<p>这个是Chrome Web Store，跟作为浏览器的Chrome完全一致，只不过这个平台上可没有homebrew也没有dpkg。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/webstore.png" alt="desktop"></p>
<p>文件管理应用，跟Google Drive深度集成。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/files.png" alt="desktop"></p>
<h2 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h2><p>我也尝试探索用于开发的可能性，由于ssh的存在，可操作性还是很高的。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/ssh.png" alt="desktop"></p>
<p>Texts是一个比较轻亮的文本编辑器。当然也有仿atom的付费APP。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/texts.png" alt="desktop"></p>
<p>crosh（通过Crtl+Shift+T打开）是内置的shell，功能上还有待探索，图为运行top的效果。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/crosh.png" alt="desktop"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Chromium OS的理念是web为王，浏览器即一切。这种理念配合Google相当完整的生态使得Chromebook在廉价本市场几乎是统治地位。去年的MS build上，微软也发布了对标的Windows版本，demo用的场景就是老师为每个学生配备PC。</p>
<p>相比之下，大陆的这个市场还是空白（或许不一定有，毕竟教育方面并不普及）。看到有把这种理念本地化的公司出现还是很惊艳，在此默默祝福他们。</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Software </tag>
            
            <tag> Chrome OS </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Densely Connected Convolutional Networks]]></title>
      <url>https://blog.ddlee.cn/2018/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/</url>
      <content type="html"><![CDATA[<p>DenseNet将shortcut-connection的思路发挥到极致。在一个DenseBlock内部，每一层的输出均跟后面的层建立shortcut，特别需要注意的是，不同于ResNet中的相加，DenseNet连接shortcut的方式是Concat，这样越深的层则输入channel数越大。</p>
<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/arch.png" alt="arch"></p>
<p>整个网络被分为Dense Block和Transition Layer，前者内部进行密集连接，保持同样大小的feature map，后者为DenseBlock之间的连接层，完成下采样操作。</p>
<p>在每个DenseBlock内部，接受的数据维度会随层数加深而变大（因为不断拼接了之前层的输出），增长的速率即为初始的channel数，文章称这一channel数为growth rate，作为模型的一个超参数。初始的growth rate为32时，在DenseNet121架构下，最后一层的channel数将增长到1024。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/56cb18697f42eb0374d933446f45b151" target="_blank" rel="external">Netscope Vis</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>作者在CIFAR和ImageNet上都做了实验，DenseNet取得了跟ResNet相当的表现，加入Bottleneck和一部分压缩技巧后，用较少的参数就能达到跟ResNet相当的效果：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/result.png" alt="arch"></p>
<p>论文链接：<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">https://arxiv.org/abs/1608.06993</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Aggregated Residual Transformations for Deep Neural Networks]]></title>
      <url>https://blog.ddlee.cn/2018/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/</url>
      <content type="html"><![CDATA[<p>本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为”cardinality”。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也添加了层与层间的shortcut connection。但由于其多路径的设计特征，我将其归为Inception系网络。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>深度网络结构上的设计已经有三种经典的范式：</p>
<ul>
<li>Repeat. 由AlexNet和VGG等开拓，几乎被之后所有的网络采用。即堆叠相同的拓扑结构，整个网络成为模块化的结构。</li>
<li>Multi-path. 由Inception系列发扬，将前一层的输入分割到不同的路径上进行变换，最后拼接结果。</li>
<li>Skip-connection. 最初出现于Highway Network，由ResNet发扬并成为标配。即建立浅层信息与深层信息的传递通道，改变原有的单一线性结构。</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>文章将残差函数表示为：</p>
<p>其中，C为本层进行的变换数目，即”cardinality”。</p>
<p>相比Inception-ResNet，ResNeXt相当于将其Inception Module的每条路径规范化了，并将规范后的路径数目作为新的超参数。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/multi-path.png" alt="multi-path"></p>
<p>上图中，路径被扩展为多条，而每条路径的宽度（channel数）也变窄了（64-&gt;4）。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/c2ba521fcb60520abb0b0da0e9c0f2ef" target="_blank" rel="external">NetScope Vis</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiements"><a href="#Experiements" class="headerlink" title="Experiements"></a>Experiements</h3><p>ResNeXt试图在保持参数数目的情况下提高网络性能，提升cardinality的同时使每条路径的宽度变窄。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/setting.png" alt="setting"></p>
<p>对比其他网络的结果：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/result.png" alt="result"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>ResNeXt较为突出的是把Inception单元规范化了，摆脱了需要精心设计Inception单元中卷积结构的问题，更好地组织了参数。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">https://arxiv.org/abs/1611.05431</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications]]></title>
      <url>https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/</url>
      <content type="html"><![CDATA[<p>MobileNets系列可以看做是继Xception之后对Depthwise Separable Convolution的又一推动。利用深度可分离的特征，MobileNets系列引入两个模型精度和大小的超参，在保持相当精度的同时享有非常小的计算消耗，适用于移动端情形，因而被命名为”MobileNets”。</p>
<h3 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a>Depthwise Separable Convolution</h3><p>深度可分离卷积是近期深度网络设计的重要趋势。最早见于L. Sifre的PhD论文Rigid-motion scattering for image classification，其1×1卷积在Inception, ResNet, SqueezeNet等网络中作为降维bottleneck使用。Xception指出，Inception单元本质上假设了跨通道和跨空间相关性的解耦关系，并将这一解耦关系推向极端，用Depthwise Separable Convolution改造了Inception结构。</p>
<p>深度可分离卷积受欢迎的另一重要原因是其参数高效性。将原有卷积换成深度可分离卷积后，可以享受到模型压缩的增益。</p>
<p>标准的卷积操作，可以认为是大小为DK的窗口在DF大小的特征图上滑动计算，计算复杂性为：</p>
<p>DK×DK × M×N × DF×DF</p>
<p>其中，M和N分别代表输入channel数和输出channel数。</p>
<p>替换为深度可分离卷积后，先进行Depthwise Convolution，再进行1×1 Pointwise Convolution，计算复杂性为：</p>
<p>DK×DK × M × DF×DF + M×N × DF×DF</p>
<p>相比下，深度可分离卷积的计算复杂性约为原来的(1/N+1/DK^2)。</p>
<p>进一步地，MobileNet添加两个超参来控制这一压缩程度，alpha为channel数压缩系数，rho为分辨率压缩系数：</p>
<p>DK×DK × alpha×M × rho×DF× rho×DF + alpha×M × alpha×N × rho×DF × rho×DF</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/depthwise-seperable.png" alt="depthwise-seperable"></p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>MobileNet的基本结构如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/arch.png" alt="arch"></p>
<p>论文链接：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Xception: Deep Learning with Depthwise Seperable Convolutions]]></title>
      <url>https://blog.ddlee.cn/2018/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/</url>
      <content type="html"><![CDATA[<p>本篇是keras库作者的文章，对Inception结构进行了改进：用Depth-wise seperable convolution替换了Inception单元中的1×1卷积和3×3卷积。</p>
<p>文章对Inception结构的评论非常有见地。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>文章指出，Inception单元背后的假设是跨Channel和跨空间的相关性可以充分解耦，类似的还有长度和高度方向上的卷积结构（在Inception-v3里的3×3卷积被1×3和3×1卷积替代）。</p>
<p>进一步的，Xception基于更强的假设：跨channel和跨空间的相关性完全解耦。这也是Depthwise Separable Convolution所建模的理念。</p>
<p>一个简化的Inception单元：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception.png" alt="inception"></p>
<p>等价于：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception2.png" alt="inception"></p>
<p>将channel推向极端，即每个channel都由独立的3×3卷积处理：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception3.png" alt="inception"></p>
<p>这样就得到了Depthwise Separable Convolution。</p>
<h3 id="Architectrue"><a href="#Architectrue" class="headerlink" title="Architectrue"></a>Architectrue</h3><p>简单讲，Xception是线性堆叠的Depthwise Separable卷积，附加了Skip-connection。</p>
<p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/931d7c91b22109f83bbbb7ff1a215f5f" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/arch.png" alt="inception"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文的实验部分并没有像其他论文那样集成一个在ImageNet上SOTA的结果，而是以Inception-v3为基线，对比了参数数量和性能，认为提升正来自于更合理的参数利用。文章还对比了Residual的作用，在Xception网络中，Skip-connection不仅能提高训练速度，还能增强模型的性能。</p>
<h3 id="Concolusion"><a href="#Concolusion" class="headerlink" title="Concolusion"></a>Concolusion</h3><p>本文贡献主要对Inception单元的解读和引入Depthwise Seperable Convolution。更多对于Depthwise Seperable Convolution的描述，请参考<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">MobileNets</a>的笔记。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="external">https://arxiv.org/abs/1610.02357</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning]]></title>
      <url>https://blog.ddlee.cn/2017/12/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/</url>
      <content type="html"><![CDATA[<p>在15年，ResNet成为那年最耀眼的卷积网络结构，skip-connection的结构也成为避不开的考虑选项。Inception系列也参考ResNet更新了自己的结构。同时推出了第四代和跟ResNet的结合版：Inception-v4和Inception-ResNet。</p>
<p>然而，这是一篇几乎都是图的论文。</p>
<p>所以，上图。</p>
<h3 id="Inception-v4-Architecture"><a href="#Inception-v4-Architecture" class="headerlink" title="Inception-v4 Architecture"></a>Inception-v4 Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/e0ac64013b167844053184d97b380978" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch1.jpg" alt="arch1"></p>
<h3 id="Inception-ResNet-v2-Architecture"><a href="#Inception-ResNet-v2-Architecture" class="headerlink" title="Inception-ResNet(v2) Architecture"></a>Inception-ResNet(v2) Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/aadd97383baccabb8b827ba507c24162" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch2.jpg" alt="arch1"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章在实验部分提到，不借助Skip-connection的结构也可以将Inception网络提升到SOTA的水准，但加入Skip-connection可以有效增加训练速度。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>卷积网络结构的演进遇到了瓶颈，在ImageNet上的提升边界似乎碰到天花板，且更多来自训练技巧和集成。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">https://arxiv.org/abs/1602.07261</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Speed/accuracy trade-offs for modern convolutional object detectors]]></title>
      <url>https://blog.ddlee.cn/2017/12/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/</url>
      <content type="html"><![CDATA[<p>这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。</p>
<p>文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了Proposal个数、输入图片尺寸，生成Feature map的大小等作为超参，并行实验，探索精度和速度方面的trade-off。</p>
<p>文章也将源码公开，作为Tensorflow的Object Detection API。</p>
<p>下图是三种元结构的图示：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/meta-arch.png" alt="meta-arch"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig2.png" alt="meta-arch"></p>
<p>信息量非常大的一张图。</p>
<ul>
<li>横纵两个维度分别代表速度和准确度，横轴越靠左说明用时越少，纵轴越靠上说明mAP表现越好，因而，sweet spot应分布在左上角</li>
<li>两个超维是元结构和特征提取网络，元结构由形状代表，特征提取网络由颜色代表</li>
<li>虚线代表理想中的trade-off边界</li>
</ul>
<p>分析：</p>
<ul>
<li>准确度最高的由Faster-RCNN元结构、Inception-ResNet提取网络，高分图片，使用较大的feature map达到，如图右上角</li>
<li>较快的网络中准确度表现最好的由使用Inception和Mobilenet的SSD达到</li>
<li>sweet spot区特征提取网络由ResNet统治，较少Proposal的Faster-RCNN可以跟R-FCN相当</li>
<li>特征提取网络方面，Inception V2和MobileNet在高速度区，Incep-ResNet和ResNet在sweet spot和高精度区，Inception V3和VGG则远离理想边界（虚线）</li>
</ul>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig3.png" alt="meta-arch"></p>
<p>上图是特征提取网络对三种元结构的影响，横轴是特征提取网络的分类准确率，纵轴是检测任务上的mAP表现，可以看到，SSD在纵轴方向上方差最小，而Faster-RCNN和R-FCN对特征提取网络更为敏感。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig4.png" alt="meta-arch"></p>
<p>上图的横轴是不同的特征提取网络，组内是三种元结构的对比，纵轴是不同尺寸物体的mAP。</p>
<p>可以看到，在大物体的检测上，使用较小的网络时，SSD的效果跟两阶段方法相当，更深的特征提取网络则对两阶段方法的中型和小型物体的检测提升较大（ResNet101和Incep-ResNet都显现了两阶段方法在小物体上的提升）</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig5.png" alt="meta-arch"></p>
<p>上图显示了输入图片尺寸对mAP的影响。高分的图片对小物体检测帮助明显，因而拥有更高的精度，但相对运行速度会变慢。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig6.png" alt="meta-arch"></p>
<p>上图探究了两阶段方法中Proposal个数的影响，左边是Faster-RCNN，右边是R-FCN，实线是mAP，虚线是推断时间。<br>分析：</p>
<ul>
<li>相比R-FCN，Faster-RCNN推断时间对Proposal个数相当敏感（因为有per ROI的计算）</li>
<li>减少Proposal的个数，并不会给精度带来致命的下降</li>
</ul>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig7.png" alt="meta-arch"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig8.png" alt="meta-arch"></p>
<p>上面两图是对FLOPS的记录，相对GPU时间更为中立，在图8中，GPU部分显现了ResNet跟Inception的分野（关于45度线，此时FLOPS跟GPU时间相当），文章认为分解操作(Factorization)减少了FLOPs，但增加了内存的IO时间，或者是GPU指令集更适合密集的卷积计算。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig9.png" alt="meta-arch"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig10.png" alt="meta-arch"></p>
<p>上两图是对内存占用的分析，总体来说，特征提取网络越精简、feature map尺寸越小，占用内存越少，运行时间也越短。</p>
<p>最后，文章描述了他们ensemble的思路，在一系列不同stride、loss和配置的Faster-RCNN中（ResNet和Incep-ResNet为特征提取网络），贪心地选择验证集上AP较高的，并且去除类AP相似的模型。选择的5个用于ensemble的模型如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/table5.png" alt="meta-arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇文章是不错的实验结果报告，测试了足够多的模型，也得出了合理的和有启发的结论。几点想法：</p>
<ul>
<li>RFCN并没有很好的解决定位跟分类的矛盾，per ROI的子网络最好还是要有，但要限制Proposal的个数（实际大部分都是负样本）来减少冗余</li>
<li>小物体的检测仍然是最大的难点，增大分辨率和更深的网络确有帮助，但不是实质的。</li>
</ul>
<p>论文链接： <a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="external">https://arxiv.org/abs/1611.10012</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Light-Head R-CNN: In Defense of Two-Stage Object Detector]]></title>
      <url>https://blog.ddlee.cn/2017/12/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/</url>
      <content type="html"><![CDATA[<p>文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。</p>
<p>针对这两种元结构(Faster-RCNN和RFCN)，文章提出了“头”轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。</p>
<h2 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/arch.png" alt="arch"></p>
<p>如上图，虚线框出的部分是三种结构的RCNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为sperable convolution，产生的Score map也减少至10×p×p（相比原先的#class×p×p）。</p>
<p>一个可能的解释是，“瘦”（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较“厚”的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。</p>
<p>进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。</p>
<p>light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。</p>
<p>粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分，文章验证了较“瘦”的Score map不会对精度产生太大损害，也展现了ROI Align, Multiscale train等技巧对基线的提升过程。</p>
<p>文章的主要结果如下面两图（第一个为高精度，第二个为高速度）：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result1.png" alt="result1"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result2.png" alt="result2"></p>
<p>只能说这样的对比比较诡异。</p>
<p>第一张图中三个light-head结果并不能跟上面的其他结构构成多少有效的对照组，要么scale不同，要么FPN, multi-scale, ROI Align不同。唯一的有效对照是跟Mask-RCNN。</p>
<p>在高精度方面，基础网络不同，采用的scale也不同，没有有效的对照组。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我并不觉得这是对两阶段方法的Defense。文章对两阶段方法在精度和速度方面的分析比较有见地，但实验的结果并不能可靠地支撑light-head的有效性。相比之下Google的那篇trade-off可能更有参考价值。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="external">https://arxiv.org/abs/1711.07264</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]You Only Look Once: Unified, Real Time Object Detection]]></title>
      <url>https://blog.ddlee.cn/2017/12/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-You-Only-Look-Once-Unified-Real-Time-Object-Detection/</url>
      <content type="html"><![CDATA[<p>YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。</p>
<p>YOLO的主要优点：</p>
<ul>
<li>快。</li>
<li>全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。</li>
<li>泛化性能好，在艺术作品上做检测时，YOLO表现好。</li>
</ul>
<h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><p>YOLO的大致工作流程如下：<br>1.准备数据：将图片缩放，划分为等分的网格，每个网格按跟ground truth的IOU分配到所要预测的样本。<br>2.卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IOU表示）。测试时，分数如下计算：</p>
<p>等式左边第一项由网格预测，后两项由每个box预测，综合起来变得到每个box含有不同类别物体的分数。<br>因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，S为网格数，B为每个网格生成box个数，C为类别数。<br>3.后处理：使用NMS过滤得到的box</p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><img src="https://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/loss.jpg" alt="loss-function"></p>
<p>图片来自<a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/24916786</a></p>
<p>损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，loss中添加了权重并将长宽取根号。</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/error.png" alt="error"></p>
<p>相比Fast-RCNN，YOLO的背景误检在错误中占比重小，而位置错误占比大（未采用log编码）。</p>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>YOLO划分网格的思路还是比较粗糙的，每个网格生成的box个数也限制了其对小物体和相近物体的检测。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">https://arxiv.org/abs/1506.02640</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Rethinking the Inception Architecture for Computer Vision]]></title>
      <url>https://blog.ddlee.cn/2017/12/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/</url>
      <content type="html"><![CDATA[<p>本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一网络结构，v3版本相比v2版本加入了RMSProp，Label Smoothing等技巧。</p>
<p>文章表述了Inception系列的几个设计原则，并根据这些原则改进了GoogLeNet的结构。</p>
<h3 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h3><ul>
<li>Avoid representational bottlenecks, especially early in the network. 建议不要在过浅的阶段进行特征压缩，而维度只是一个表达复杂性的参考，并不能作为特征复杂性的绝对衡量标准。</li>
<li>Higher dimensional representations are easier to process locally with a network. 高阶的表示更有局部描述力，增加非线性有助于固化这些描述力。</li>
<li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. 基于空间的聚合信息可以在低维空间里处理，而不必担心有太多信息损失。这一点也佐证了1×1卷积的降维作用。</li>
<li>Balance the width and depth of the network.  宽度和深度的增加都有助于网络的表达能力，最好的做法是同时在这两个方向上推进，而非只顾及一个。</li>
</ul>
<h3 id="Factorizing-Convolution"><a href="#Factorizing-Convolution" class="headerlink" title="Factorizing Convolution"></a>Factorizing Convolution</h3><p>分解一直是计算数学里经典的思路。从牛顿法到BFGS，就是把Hessian矩阵（或其逆）用一系列的向量操作来表示和近似，避免矩阵的计算。</p>
<p>本文提出了两种卷积结构方面的分解，一个是在卷积核的层面，另一个是在空间方面。</p>
<p>第一种分解是将大核卷积分解成串联的小核卷积。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor3.png" alt="factor5"></p>
<p>用两个3×3的卷积代替5×5的卷积，带来的参数减少为(9+9)/(5×5).</p>
<p>第二种分解是在卷积核本身上，引入非对称卷积：用3×1和1×3的卷积串联代替3×3卷积。如下图所示。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor1.png" alt="factor3"></p>
<p>这种分解也可以推广到n维情况，且n越大，带来的收益越明显。</p>
<p>空间上的卷积分解建模了这样的情形：两个方向上的卷积参数互相正交，便被空间分解卷积解耦。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/spatial-seperable.png" alt="factor5"></p>
<h3 id="Utility-of-Auxiliary-Classifiers"><a href="#Utility-of-Auxiliary-Classifiers" class="headerlink" title="Utility of Auxiliary Classifiers"></a>Utility of Auxiliary Classifiers</h3><p>在GoogLeNet中，作者用loss监督了低维的特征图的学习，但进一步的实验发现，加入BN层后，这些增益被抵消了，于是Auxiliary Classifier可被看做是某种正则化技术，在加入BN的网络中便不再应用。</p>
<h3 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h3><p>这一节讨论网络中的特征降维，即下采样的过程，通常由卷积层或Pooling层的stride参数控制。文章为避免原则一中提到的Representation Bottleneck，在进行Pooling之前将网络加宽（通过Channel数的增加），这也对应了平衡宽度和深度的原则。</p>
<p>最终结合了Inception结构和下采样需求的单元如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/downsample.png" alt="factor5"></p>
<p>不同于Inception单元，上面的1×1卷积扩展了Channel，并且3×3卷积采用了stride=2。</p>
<h3 id="Inception-v2-amp-Inception-v3-Architecture"><a href="#Inception-v2-amp-Inception-v3-Architecture" class="headerlink" title="Inception-v2 &amp; Inception-v3 Architecture"></a>Inception-v2 &amp; Inception-v3 Architecture</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/arch.png" alt="factor5"></p>
<p>可以看到随深度增加，Channel数也在扩展，而Inception单元也遵从了堆叠的范式。</p>
<p>其中三种Inception单元分别为：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptiona.png" alt="factor5"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionb.png" alt="factor5"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionc.png" alt="factor5"></p>
<p>另外，也可以查看<a href="http://ethereon.github.io/netscope/#gist/a2394c1c4a9738469078f096a8979346" target="_blank" rel="external">NetScope Vis</a>来熟悉Inception-v3的结构，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下面是Inception结构演化带来的增益分解：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/experiment.png" alt="factor5"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本篇是对Inception系网络的推进，其分解的思想成为又一网络设计的指导原则。</p>
<p>对卷积的进一步理解，可以参考这个<a href="https://graphics.stanford.edu/courses/cs178-10/applets/convolution.html" target="_blank" rel="external">页面</a>，这一工具可视化了不同卷积核对输入的处理，给出的例子都是在早期人们手工设计的滤波器，而深度网络隐式地学习到了这些滤波器的卷积表达。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">https://arxiv.org/abs/1512.00567</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]SSD: Single Shot MultiBox Detector]]></title>
      <url>https://blog.ddlee.cn/2017/12/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-SSD-Single-Shot-MultiBox-Detector/</url>
      <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保持较快的运行速度。</p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-SSD-Single-Shot-MultiBox-Detector/arch.jpg" alt="arch"></p>
<ul>
<li><p>多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。</p>
</li>
<li><p>更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SSD有点像多分类的RPN，生成anchor box，再对box预测分数和位置调整值。</p>
<p>论文链接：<a href="https://arxiv.org/abs/151.023325" target="_blank" rel="external">https://arxiv.org/abs/151.023325</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Feature Pyramid Networks for Object Detection]]></title>
      <url>https://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Feature-Pyramid-Networks-for-Object-Detection/</url>
      <content type="html"><![CDATA[<p>对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。</p>
<p>回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。</p>
<p>一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的内存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。</p>
<p>本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。</p>
<h3 id="Feature-Pyramid-Networks"><a href="#Feature-Pyramid-Networks" class="headerlink" title="Feature Pyramid Networks"></a>Feature Pyramid Networks</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/arch.png" alt="arch"></p>
<p>从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行元素位置上的叠加（lateral connection），构成这一深度上的特征。</p>
<p>这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章的主要实验结果如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/result.png" alt="Experiments results"></p>
<p>对比不同head部分，输入feature的变化对检测精度确实有提升，而且，lateral和top-down两个操作也是缺一不可。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。</p>
<p>个人方面，FPN跟multi-scale的区别在哪，还值得进一步探索。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">https://arxiv.org/abs/1612.03144</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]R-FCN: Object Detection via Region-based Fully Convolutinal Networks]]></title>
      <url>https://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/</url>
      <content type="html"><![CDATA[<p>文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等），这一点是由分类任务的平移不变性和检测任务的平移敏感性之间的矛盾导致的。换句话说，检测模型采用了分类模型的特征提取器，丢失了位置信息。这篇文章提出采用“位置敏感分数图”的方法解决这一问题。</p>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI Pooling"></a>Position-sensitive score maps &amp; Position-sensitive RoI Pooling</h3><p>位置敏感分数图的生成有两个重要操作，一是生成更“厚”的feature map，二是在RoI Pooling时选择性地输入feature map。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/rfcn.png" alt="arch"></p>
<p>Faster R-CNN中，经过RPN得到RoI，转化成分类任务，还加入了一定量的卷积操作（ResNet中的conv5部分），而这一部分卷积操作是不能共享的。R-FCN则着眼于全卷积结构，利用卷积操作在Channel这一维度上的自由性，赋予其位置敏感的意义。下面是具体的操作：</p>
<ul>
<li>在全卷积网络的最后一层，生成k^2(C+1)个Channel的Feature map，其中C为类别数，k^2代表k×k网格，用于分别检测目标物体的k×k个部分。即是用不同channel的feature map代表物体的不同局部（如左上部分，右下部分）。</li>
<li>将RPN网络得到的Proposal映射到上一步得到的feature map（厚度为k×k×(C+1)，）后，相应的，将RoI等分为k×k个bin，对第(i,j)个bin，仅考虑对应(i,j)位置的(C+1)个feature map，进行如下计算：其中(x0,y0)是这个RoI的锚点，得到的即是(i,j)号bin对C类别的相应分数。</li>
<li>经过上一步，每个RoI得到的结果是k^2(C+1)大小的分数张量，k×k编码着物体的局部分数信息，进行vote（平均）后得到(C+1)维的分数向量，再接入softmax得到每一类的概率。</li>
</ul>
<p>上面第二步操作中“仅选取第(i, j)号feature map”是位置信息产生意义的关键。</p>
<p>这样设计的网络结构，所有可学习的参数都分布在可共享的卷积层，因而在训练和测试性能上均有提升。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>R-FCN是对Faster R-CNN结构上的改进，部分地解决了位置不变性和位置敏感性的矛盾。通过最大化地共享卷积参数，使得在精度相当的情况下训练和测试效率都有了很大的提升。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="external">https://arxiv.org/abs/1605.06409</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Going deeper with convolutions]]></title>
      <url>https://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/</url>
      <content type="html"><![CDATA[<p>本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的GoogLeNet拿下了ILSVRC14分类任务的头名。文章也探讨了网络在不断加深的情况下如何更好地利用计算资源，这一理念也是Inception系列网络的核心。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Inception单元的启发主要来自Network in Network结构和Arora等人在神经科学方面的工作。</p>
<p>提高深度模型的一个简单想法是增加深度，但这样带来过拟合的风险和巨大的计算资源消耗，对数据量和计算力的要求可能会超过网络加深带来的收益。</p>
<p>解决这些问题的基本思路是使用稀疏连接的网络，而这也跟Arora等人工作中的Hebbian principle吻合：共同激活的神经元常常集聚在一起。换句话说，某一层激活的神经元只向下一层中特定的几个神经元传递激活信号，而向其他神经元几乎不传递信息，即仅有少部分连接是真正有效的，这也是稀疏的含义。</p>
<p>然而另一方面，现代计算架构对稀疏的计算非常低效，更适合的是密集的计算，这样便产生了矛盾。而Inception单元的提出就是为了用密集的结构来近似稀疏结构，在建模稀疏连接的同时又能利用密集计算的优势。</p>
<p>很多文章认为inception结构的意义在于将不同大小核的卷积并行连接，然后让网络自行决定采用哪种卷积来提取特征，有些无监督的意味，然后将1×1的卷积解释为降维操作。这种想法有待验证，是否在5×5卷积有较强激活的时候，3×3卷积大部分没有激活，还是两者能够同时有较强的激活？不同的处理阶段这两种卷积核的选择有没有规律？</p>
<p>在此提出一个个人的理解，欢迎讨论。</p>
<p>首先是channel的意义。我们知道，卷积之所以有效，是因为它建模了张量数据在空间上的局部相关性，加之Pooling操作，将这些相关性赋予平移不变性（即泛化能力）。而channel则是第三维，它实际上是卷积结构中的隐藏单元，是中间神经元的个数。卷积层在事实上是全连接的：每个Input channel都会和output channel互动，互动的信息只不过从全连接层的weight和bias变成了卷积核的weight。</p>
<p>这种全连接是冗余的，本质上应是一个稀疏的结构。Inception单元便在channel这个维度上做文章，采用的是类似矩阵分块的思想。</p>
<p>根据Hebbian principle，跨channel的这些神经元，应是高度相关的，于是有信息压缩的空间，因而使用跨channel的1×1的卷积将它们嵌入到低维的空间里（比如，Inception4a单元的输入channel是512，不同分支的1×1卷积输出channel则是192,96,16和64，见下面GoogLeNet结构表），在这个低维空间里，用密集的全连接建模（即3×5和5×5卷积），它们的输出channel相加也再恢复到原来的输入channel维度（Inception4a分别是192+208+48+64），最后的连接由Concat操作完成（分块矩阵的合并），这样就完成了分块密集矩阵对稀疏矩阵的近似。</p>
<p>这样来看，3×3和5×5大小的选择并不是本质的，本质的是分块低维嵌入和concat的分治思路。而在ResNeXt的工作中，这里的分块被认为是新的维度（称为cardinality），采用相同的拓扑结构。</p>
<h3 id="Stacked-Inception-Module"><a href="#Stacked-Inception-Module" class="headerlink" title="Stacked Inception Module"></a>Stacked Inception Module</h3><p>在GoogLeNet中，借鉴了AlexNet和VGG的stack(repeat)策略，将Inception单元重复串联起来，构成基本的特征提取结构。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/arch.png" alt="arch"></p>
<h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><p>朴素版本的Inception单元会带来Channel维数的不断增长，加入的1×1卷积则起到低维嵌入的作用，使Inception单元前后channel数保持稳定。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/inception.png" alt="arch"></p>
<h3 id="Auxililary-Classifier"><a href="#Auxililary-Classifier" class="headerlink" title="Auxililary Classifier"></a>Auxililary Classifier</h3><p>这里是本文的另一个贡献，将监督信息传入中间的feature map，构成一个整合loss，作者认为这样有助于浅层特征的学习。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/auxililary.png" alt="arch"></p>
<h3 id="Architecture-of-GoogLeNet"><a href="#Architecture-of-GoogLeNet" class="headerlink" title="Architecture of GoogLeNet"></a>Architecture of GoogLeNet</h3><p>下面的表显示了GoogLeNet的整体架构，可以留意到Inception单元的堆叠和Channel数在子路径中的变化。NetScope可视化可参见<a href="http://ethereon.github.io/netscope/#/gist/db8754ee4b239920b3df5ab93220a84b" target="_blank" rel="external">GoogLeNet Vis</a>。源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/table.png" alt="arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>文章是对NiN思想的继承和推进，不同于AlexNet和VGG，网络的模块化更加凸显，多路径的结构也成为新的网络设计范本，启发了众多后续网络结构的设计。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]MegDet: A Large Mini-Batch Object Detector]]></title>
      <url>https://blog.ddlee.cn/2017/11/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MegDet-A-Large-Mini-Batch-Object-Detector/</url>
      <content type="html"><![CDATA[<p>本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。</p>
<h2 id="Object-Detection-Progress-Summay"><a href="#Object-Detection-Progress-Summay" class="headerlink" title="Object Detection Progress Summay"></a>Object Detection Progress Summay</h2><p>检测方法回顾：R-CNN, Fast/Faster R-CNN, Mask RCNN, RetinaNet(Focal Loss), ResNet(backbone network),</p>
<p>文章先指出前述方法大多是框架、loss等的更新，而均采用非常小的batch（2张图片）训练，有如下不足：</p>
<ul>
<li>training slow</li>
<li>fails to provide accurate statistics for BN</li>
</ul>
<p>这里涉及一个问题，检测任务的源数据，到底应该是图片还是标注框。在Fast R-CNN中，RBG提到SPPNet等每个batch采样的标注框来自不同的图片，之间不能共享卷积运算（卷积运算是以图片为单位的）。为了共享这部分计算，Fast R-CNN采用了“先选图片，再选标注框”的策略来确定每个batch，文章提到这种操作会引入相关性，但在实际中却影响不大。之后的Faster R-CNN，每张图片经过RPN产生约300个Proposal，传入RCNN做法也成了通用做法。</p>
<p>个人认为检测任务的数据，应该是以图片为单位的。物体在图片的背景中才会产生语义，而尽管每张图片有多个Proposal（近似分类任务中的batch大小），但它们共享的是同一个语义（场景），而单一的语义难以在同一个batch中提供多样性来供网络学习。</p>
<h5 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h5><p>Increasing mini-batch size requires large learning rate, which may cause discovergence.</p>
<h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>new explanation of linear scaling rule, introduce “warmup” trick to learning rate schedule</li>
<li>Cross GPU Batch Normalization(CGBN)</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="Variance-Equivalence-explanation-for-Linear-Scaling-Rule"><a href="#Variance-Equivalence-explanation-for-Linear-Scaling-Rule" class="headerlink" title="Variance Equivalence explanation for Linear Scaling Rule"></a>Variance Equivalence explanation for Linear Scaling Rule</h3><p>linear scaling rule 来自更改batch size 时，同时放缩learning rate，使得更改后的weight update相比之前小batch size， 多步的weight update类似。而本文用保持loss gradient的方差不变重新解释了linear scaling rule，并指出这一假定仅要求loss gradient是i.i.d，相比保持weight update所假设的不同batch size间loss gradient相似更弱。</p>
<p>参见<a href="https://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/#Motivation">Accurate Large Minibatch SGD: Training ImageNet in One Hour</a>，近似时假设了求和项变化不大，这一条件在Object Detection中可能不成立，不同图片的标注框（大小、个数）差别很大。</p>
<h3 id="WarmUp-Strategy"><a href="#WarmUp-Strategy" class="headerlink" title="WarmUp Strategy"></a>WarmUp Strategy</h3><p>在训练初期，weight抖动明显，引入warmup机制来使用较小的学习率，再逐渐增大到Linear scaling rule要求的学习率。</p>
<h3 id="Cross-GPU-Batch-Normalization"><a href="#Cross-GPU-Batch-Normalization" class="headerlink" title="Cross-GPU Batch Normalization"></a>Cross-GPU Batch Normalization</h3><p>BN是使深度网络得以训练和收敛的关键技术之一，但在检测任务中，fine-tuning阶段常常固定了SOTA分类网络的BN部分参数，不进行更新。</p>
<p>检测中常常需要较大分辨率的图片，而GPU内存限制了单卡上的图片个数，提高batch size意味着BN要在多卡（Cross-GPU）上进行。</p>
<p>BN操作需要对每个batch计算均值和方差来进行标准化，对于多卡，具体做法是，单卡独立计算均值，聚合（类似Map-Reduce中的Reduce）算均值，再将均值下发到每个卡，算差，再聚合起来，计算batch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。</p>
<p>流程如图：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/cgbn.png" alt="cgbn"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在COCO数据集上的架构用预训练ResNet-50作为基础网络，FPN用于提供feature map。</p>
<p>结果显示，不使用BN时，较大的batch size（64,128）不能收敛。使用BN后，增大Batch size能够收敛但仅带来较小的精度提升，而BN的大小也不是越大越好，实验中，32是最好的选择。主要结果如下表：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/results.png" alt="results"></p>
<p>按epoch，精度的变化如下图，小batch（16）在最初的几个epoch表现比大batch（32）要好。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/byepoch.png" alt="accuracy-by-epoch"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇论文读起来总感觉少了些东西。对Linear scale rule的解释固然新颖，但没有引入新的trick（只是确认了检测仍是需要Linear scale rule的）。多卡的BN确实是非常厉害的工程实现（高效性），但实验的结果并没有支持到较大的batch size（128,256）比小batch精度更好的期望，而最后的COCO夺冠模型整合了多种trick，没有更进一步的错误分析，很难支撑说明CGBN带来的关键作用。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks]]></title>
      <url>https://blog.ddlee.cn/2017/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/</url>
      <content type="html"><![CDATA[<p>Faster R-CNN是2-stage方法的主流方法，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA。</p>
<h2 id="Regional-Proposal-Networks"><a href="#Regional-Proposal-Networks" class="headerlink" title="Regional Proposal Networks"></a>Regional Proposal Networks</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/rpn.png" alt="faster_rcnn_arch"></p>
<p>RPN网络将Proposal这一任务建模为二分类的问题。</p>
<p>第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IOU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本即是anchor box和每个anchor box是否有物体。RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的Loss统一起来，作为RPN网络的目标训练。</p>
<p>RPN网络可调的超参还是很多的，anchor box的大小和长宽比例、IoU的阈值、每张图片上Proposal正负样本的比例等。</p>
<h2 id="Alternate-Training"><a href="#Alternate-Training" class="headerlink" title="Alternate Training"></a>Alternate Training</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/faster_rcnn_netwrok.png" alt="faster_rcnn_arch"></p>
<p>RPN网络是在feature map上进行的，因而可以跟RCNN完全共享feature extractor部分的卷积运算。训练时，RPN和RCNN的训练可以交替进行，即交替地固定RPN和RCNN部分的参数，更新另一部分。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Faster R-CNN的成功之处在于用RPN网络完成了检测任务的“深度化”。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。RPN网络也成为检测2-stage方法的标准部件。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Fast R-CNN]]></title>
      <url>https://blog.ddlee.cn/2017/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Fast-R-CNN/</url>
      <content type="html"><![CDATA[<p>Fast R-CNN 是对R-CNN的改进，作者栏只有RBG一人。文章先指出了R-CNN存在的问题，再介绍了自己的改进思路。文章结构堪称典范，从现存问题，到解决方案、实验细节，再到结果分析、拓展讨论，条分缕析，值得借鉴。而且，RBG开源的代码也影响了后来大部分这一领域的工作。</p>
<h2 id="R-CNN的问题"><a href="#R-CNN的问题" class="headerlink" title="R-CNN的问题"></a>R-CNN的问题</h2><ul>
<li>训练是一个多阶段的过程（Proposal, Classification, Regression）</li>
<li>训练耗时耗力</li>
<li>推断耗时</li>
</ul>
<p>而耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算。</p>
<h2 id="Fast-R-CNN-Architecture"><a href="#Fast-R-CNN-Architecture" class="headerlink" title="Fast R-CNN Architecture"></a>Fast R-CNN Architecture</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/fast-rcnn-arch.png" alt="arch"></p>
<p>上图是Fast R-CNN的架构。图片经过feature extractor产生feature map, 原图上运行Selective Search算法将RoI（Region of Interset）对应到feature map上，再对每个RoI进行RoI Pooling操作便得到等长的feature vector，最后通过FC后并行地进行Classifaction和BBox Regression。</p>
<p>Fast R-CNN的这一结构正是检测任务主流2-stage方法所采用的元结构的雏形。整个系统由Proposal, Feature Extractor, Object Recognition&amp;Localization几个部件组成。Proposal部分被替换成RPN(Faster R-CNN)，Feature Extractor部分使用SOTA的分类CNN网络(ResNet等），而最后的部分常常是并行的多任务结构（Mask R-CNN等）。</p>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><p>这一操作是将不同大小的RoI（feature map上）统一的过程，具体做法是将RoI等分成目标个数的网格，在每个网格上进行max pooling，就得到等长的RoI feature vector。</p>
<h3 id="Mini-batch-Sampling"><a href="#Mini-batch-Sampling" class="headerlink" title="Mini-batch Sampling"></a>Mini-batch Sampling</h3><p>文章指出SPPNet训练较慢的原因在于来自不同图片的RoI不能共享计算，因而Fast R-CNN采用这样的mini-batch采样策略：先采样N张图片，再在每张图片上采样R/N个RoI，构成R大小的mini-batch。</p>
<p>采样时，总是保持25%比例正样本（iou大于0.5），iou在0.1到0.5的作为hard example。</p>
<h3 id="Multi-task-Loss"><a href="#Multi-task-Loss" class="headerlink" title="Multi-task Loss"></a>Multi-task Loss</h3><p>得到RoI feature vector后，后续的操作是一个并行的结构，Fast R-CNN将Classification和Regression的损失统一起来，并且在Regression中用更鲁棒的Smooth L1 Loss代替L2 Loss。</p>
<h3 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h3><p>文章还发现，对于预训练的VGG网络，开放Conv部分的参数更新有助于性能的提升，而不是只更新FC层。<br>将proposal, classification, regression统一在一个框架</p>
<h2 id="Design-Evaluation"><a href="#Design-Evaluation" class="headerlink" title="Design Evaluation"></a>Design Evaluation</h2><p>文章最后还对系统结构进行了讨论：</p>
<ul>
<li>multi-loss traing相比单独训练Classification确有提升</li>
<li>Scale invariance方面，multi-scale相比single-scale精度略有提升，但带来的时间开销更大。一定程度上说明CNN结构可以内在地学习scale invariance</li>
<li>在更多的数据(VOC)上训练后，mAP是有进一步提升的</li>
<li>Softmax分类器比”one vs rest”型的SVM表现略好，引入了类间的竞争</li>
<li>更多的Proposal并不一定带来性能的提升</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Fast R-CNN是对R-CNN的改进，也是对2-stage方法的系统化、架构化。文章将Proposal, Feature Extractor, Object Recognition&amp;Localization统一在一个整体的结构中，并推进共享卷积计算以提高效率的想法演进，是最有贡献的地方。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">https://arxiv.org/abs/1504.08083</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Rich feature hierarchies for accurate object detection and semantic segmentation]]></title>
      <url>https://blog.ddlee.cn/2017/10/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</url>
      <content type="html"><![CDATA[<p>R-CNN系列的开山之作，2-stage的想法至今仍是精确度优先方法的主流。而且，本文中的众多做法也成为检测任务pipeline的标准配置。</p>
<p>摘要中提到的两大贡献：1）CNN可用于基于区域的定位和分割物体；2）监督训练样本数紧缺时，在额外的数据上预训练的模型经过fine-tuning可以取得很好的效果。</p>
<p>第一个贡献影响了之后几乎所有2-stage方法，而第二个贡献中用分类任务（Imagenet）中训练好的模型作为基网络，在检测问题上fine-tuning的做法也在之后的工作中一直沿用。</p>
<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Features Matter. Traditional hand-design feature(SIFT, HOG) -&gt; Learned feature(CNN). 从图像识别的经验来看，CNN网络自动习得的特征已经超出了手工设计的特征。</p>
<p>解决检测任务中的定位问题：”recognition using regions”，即基于区域的识别（分类）。</p>
<p>检测任务中样本不足的问题（对大型网络）：在大数据集上预训练分类模型，在小数据集上fine-tuning检测任务。</p>
<h3 id="Object-Detection-with-R-CNN"><a href="#Object-Detection-with-R-CNN" class="headerlink" title="Object Detection with R-CNN"></a>Object Detection with R-CNN</h3><p>Region Proposal: Selective Search</p>
<p>Feature Extraction: AlexNet(NIPS 2012), 4096-dim feature vector from every region proposal</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/rcnn.png" alt="arch"></p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>现在ILSVRC2012上预训练达到STOA，再在Pascal VOC上fine-tuning。根据IOU来给region proposal打标签，在每个batch中保持一定的正样本比例（背景类非常多）。这些都已成为标准做法，后续很多工作也是对这些细节进行改进（OHEM等）。</p>
<p>文章中特别提到，IOU的选择（即正负样例的标签准备）对结果影响显著，这里要谈两个threshold，一个用来识别正样本（IOU跟ground truth较高），另一个用来标记负样本（即背景类），而介于两者之间的则为hard negatives，若标为正类，则包含了过多的背景信息，反之又包含了要检测物体的特征，因而这些proposal便被忽略掉。</p>
<p>另一个重要的问题是bounding-box regression，这一过程是proposal向ground truth调整，实现时加入了log/exp变换来使loss保持在合理的量级上。</p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>R-CNN的想法直接明了，即是将CNN在分类上取得的成就运用在检测上，是深度学习方法在检测任务上的试水。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。尽管如此，这篇论文的很多做法仍然广泛地影响着检测任务上的深度模型革命，后续的很多工作也都是针对改进文章中的pipeline而展开，此篇可以称得上”the first paper”。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Object Detection </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux Reborn: 个人存档]]></title>
      <url>https://blog.ddlee.cn/2017/09/19/Linux%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/</url>
      <content type="html"><![CDATA[<h3 id="个人的需求与应用场景"><a href="#个人的需求与应用场景" class="headerlink" title="个人的需求与应用场景"></a>个人的需求与应用场景</h3><p>我是（几乎）完全使用Linux的，它也满足了我的大部分需求，主要以下几块：</p>
<ol>
<li>编程，主流IDE, Editor都会有Linux版本，而软件库大部分也是先支持Linux的（服务器）</li>
<li>上网，一个chrome几乎足够</li>
<li>娱乐，steam有Linux客户端，很多游戏也有相应版本（当然也有很多没有）</li>
</ol>
<p>下面是我整理的一个列表，算是给自己的存档。</p>
<h3 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h3><h4 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h4><h5 id="Text-Editor"><a href="#Text-Editor" class="headerlink" title="Text Editor"></a>Text Editor</h5><p>选择有很多，我用Sublime Text写代码，用Atom写博客（中文支持好），用VS Code读代码。总之精分，各取所好。</p>
<p>另外讲两个小技巧：</p>
<ol>
<li>利用<code>sshfs</code>把服务器的文件挂在到本地用编辑器编辑，然后远程终端运行</li>
<li><code>ssh -L localhost:8888:localhost:8888 user@host</code>命令可以将远程端口映射到本地，这样可以在服务器端开启<code>jupyter notebook</code>，再在本地用浏览器访问</li>
</ol>
<h5 id="tmux-amp-zsh"><a href="#tmux-amp-zsh" class="headerlink" title="tmux&amp;zsh"></a>tmux&amp;zsh</h5><p>tmux是一个终端多窗口管理器，可以打开多个终端窗口、挂起和挂载终端回话等，利器。</p>
<pre><code>sudo apt install tmux
</code></pre><h5 id="GitKraken"><a href="#GitKraken" class="headerlink" title="GitKraken"></a>GitKraken</h5><p>Git图形客户端</p>
<h4 id="效率类"><a href="#效率类" class="headerlink" title="效率类"></a>效率类</h4><h5 id="Whatever-Evernote-alternative"><a href="#Whatever-Evernote-alternative" class="headerlink" title="Whatever- Evernote alternative"></a><a href="https://cellard0-0r.github.io/whatever/" target="_blank" rel="external">Whatever- Evernote alternative</a></h5><p>Evernote的第三方客户端，调用网页API，不占用免费版的客户端限制个数</p>
<h5 id="Stacer-System-Cleaner"><a href="#Stacer-System-Cleaner" class="headerlink" title="Stacer- System Cleaner"></a><a href="https://github.com/oguzhaninan/Stacer/releases" target="_blank" rel="external">Stacer- System Cleaner</a></h5><p>提供系统监视器和清理功能，也可以卸载包</p>
<h5 id="synapse-App-Launcher"><a href="#synapse-App-Launcher" class="headerlink" title="synapse- App Launcher"></a>synapse- App Launcher</h5><p>一个类似lauchy的应用启动器，可以直接用apt安装</p>
<pre><code>sudo apt install synapse
</code></pre><h5 id="Gdebi-Package-Installer"><a href="#Gdebi-Package-Installer" class="headerlink" title="Gdebi- Package Installer"></a>Gdebi- Package Installer</h5><p>包安装程序，比自带的安装好用一些（安装deb包等）</p>
<pre><code>sudo apt install gdebi
</code></pre><h5 id="Mailspring-Mail-Client"><a href="#Mailspring-Mail-Client" class="headerlink" title="Mailspring- Mail Client"></a><a href="https://getmailspring.com/" target="_blank" rel="external">Mailspring- Mail Client</a></h5><p>邮件客户端，比thunderbird, evolution等界面美观一些</p>
<h5 id="Gparted-Disk-Management"><a href="#Gparted-Disk-Management" class="headerlink" title="Gparted- Disk Management"></a>Gparted- Disk Management</h5><p>磁盘管理程序，用于分区、格式化等等</p>
<pre><code>sudo apt install gparted
</code></pre><h4 id="Okular-PDF-Reader"><a href="#Okular-PDF-Reader" class="headerlink" title="Okular- PDF Reader"></a>Okular- PDF Reader</h4><p>功能强大的PDF阅读器</p>
<pre><code>sudo apt install okular
</code></pre><h4 id="WPS-Office"><a href="#WPS-Office" class="headerlink" title="WPS Office"></a><a href="https://www.wps.com/linux" target="_blank" rel="external">WPS Office</a></h4><p>WPS的Linux版本，比Libre要好用很多</p>
<h4 id="Shutter"><a href="#Shutter" class="headerlink" title="Shutter"></a>Shutter</h4><p>截屏软件，可以通过ubunut软件中心安装</p>
<h3 id="通讯与娱乐"><a href="#通讯与娱乐" class="headerlink" title="通讯与娱乐"></a>通讯与娱乐</h3><h5 id="Wewechat-Wechat-client"><a href="#Wewechat-Wechat-client" class="headerlink" title="Wewechat- Wechat client"></a><a href="https://github.com/trazyn/weweChat/releases" target="_blank" rel="external">Wewechat- Wechat client</a></h5><p>微信的第三方客户端，还有<a href="https://github.com/geeeeeeeeek/electronic-wechat" target="_blank" rel="external">electron-wechat</a>，wewechat界面更好，而后者可以看公众号的文章。</p>
<h5 id="IeaseMusic-Netease-Music-Client"><a href="#IeaseMusic-Netease-Music-Client" class="headerlink" title="IeaseMusic- Netease Music Client"></a><a href="https://github.com/trazyn/ieaseMusic/releases" target="_blank" rel="external">IeaseMusic- Netease Music Client</a></h5><p>网易云音乐的第三方客户端，界面漂亮，我一般用于听FM。功能上更全的自然是<a href="http://music.163.com/#/download" target="_blank" rel="external">官方版本</a>。</p>
<h5 id="1Listen"><a href="#1Listen" class="headerlink" title="1Listen"></a><a href="https://listen1.github.io/listen1/" target="_blank" rel="external">1Listen</a></h5><p>综合了网易，QQ，虾米三家的曲库，用于找想听的歌，建议下载chrome插件版。</p>
<h5 id="Steam"><a href="#Steam" class="headerlink" title="Steam"></a><a href="http://store.steampowered.com/linux" target="_blank" rel="external">Steam</a></h5><p>Dota2是可以通过steam的Linux版本玩的，我买过的大部分解密游戏也可以。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Software </tag>
            
            <tag> Digital Life </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[IOS Reborn: 个人APP存档]]></title>
      <url>https://blog.ddlee.cn/2017/09/02/IOS-Reborn-%E4%B8%AA%E4%BA%BAAPP%E5%AD%98%E6%A1%A3/</url>
      <content type="html"><![CDATA[<p>IOS系统个人应用列表备份，iPad主要为阅读、娱乐功能，iPod用于听歌、播客。</p>
<h3 id="iPad-mini-4"><a href="#iPad-mini-4" class="headerlink" title="iPad mini 4"></a>iPad mini 4</h3><h4 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h4><ul>
<li>Documents，文件中转中心，连接云服务、私有云。PDF文档中心</li>
<li>Google Keep，记录琐事、备忘</li>
<li>Pushbullet，多客户端跨平台文字、链接转发</li>
<li>PDF Expert，为Documents提供PDF标注编辑等功能</li>
<li>Git2Go，GitHub客户端，读代码</li>
<li>百度网盘，转存文件、电子书</li>
</ul>
<h4 id="Reading"><a href="#Reading" class="headerlink" title="Reading"></a>Reading</h4><ul>
<li>Reeder 3，Feedly客户端，咨讯中心</li>
<li>Pocket，稍后再读，配合Reeder 3</li>
<li>知乎，内容索引</li>
<li>多看阅读，电子书中心</li>
<li>Quora，内容索引</li>
<li>kindle，电子书，Amazon内容</li>
<li>Medium，高质量的写作社区</li>
<li>iBooks，少部分电子书</li>
</ul>
<h4 id="LifeStyle"><a href="#LifeStyle" class="headerlink" title="LifeStyle"></a>LifeStyle</h4><ul>
<li>导航犬离线地图，地图备查</li>
<li>Bilibili HD</li>
<li>AVPlayer HD，本地视频，私有云视频</li>
<li>网易云音乐</li>
</ul>
<h3 id="iPod"><a href="#iPod" class="headerlink" title="iPod"></a>iPod</h3><h4 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h4><ul>
<li>网易云音乐</li>
<li>QQ音乐</li>
<li>KUSC，南加州古典音乐电台</li>
<li>overcast，Podcast客户端</li>
</ul>
<h4 id="LifeStyle-1"><a href="#LifeStyle-1" class="headerlink" title="LifeStyle"></a>LifeStyle</h4><ul>
<li>Bilibili</li>
<li>地铁通</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Software </tag>
            
            <tag> Digital Life </tag>
            
            <tag> IOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Android Reborn: 个人APP存档]]></title>
      <url>https://blog.ddlee.cn/2017/08/31/Android-Reborn-%E4%B8%AA%E4%BA%BAAPP%E5%AD%98%E6%A1%A3/</url>
      <content type="html"><![CDATA[<p>个人使用Android的应用列表备份。</p>
<h3 id="GApps"><a href="#GApps" class="headerlink" title="GApps"></a>GApps</h3><ul>
<li>Google Photos</li>
<li>Google Chrome</li>
<li>YouTube</li>
<li>Google Keep</li>
<li>Inbox by Gmail</li>
<li>Google Pinyin Keyboard</li>
<li>Google Now</li>
</ul>
<h3 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h3><ul>
<li>Solid File Explorer， 文件管理，云服务等中转</li>
<li>Steam，Steam二次验证工具</li>
<li>Evernote，笔记同步</li>
<li>WPS Office，文档阅读与编辑</li>
<li>Pushbullet，多端文字链接通信</li>
<li>Pulse Secure，学校指定VPN工具</li>
<li>Google Authenticator(Nutstore Rvoked)，二次验证工具</li>
<li>CamScanner，文档扫描</li>
<li>FeedMe，RSS阅读，配合feedly</li>
</ul>
<h3 id="System-Optimization"><a href="#System-Optimization" class="headerlink" title="System Optimization"></a>System Optimization</h3><ul>
<li>Nova LancherI(config)，桌面（已备份）</li>
<li>SD Maid，系统清理，APP管理</li>
<li>Tasker，自动化任务编排</li>
<li>Ice Box，流氓应用管理（已备份）</li>
<li>SMS Backup，通话记录和短信备份</li>
<li>SuperSU，ROOT权限管理</li>
<li>MyAndroidTools，系统级的应用活动、服务管理（已备份）</li>
<li>Titanium Backup，系统备份</li>
<li>Brevent，系统进程管理</li>
<li>Greenify，系统进程管理</li>
</ul>
<h3 id="LifeStyle"><a href="#LifeStyle" class="headerlink" title="LifeStyle"></a>LifeStyle</h3><ul>
<li>Wechat，微信</li>
<li>Mobike，共享单车</li>
<li>Resplash，高质量图片，壁纸图库</li>
<li>Prisma，智能风格转换，滤镜</li>
<li>Photo Scan，旧实体照片数字化</li>
<li>Snapseed，图片处理</li>
<li>AliPay，支付宝</li>
<li>Max+，DOTA2资讯</li>
<li>C5Game，DOTA2饰品</li>
<li>TIM，工作版QQ</li>
<li>Retrorika，图标包</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Software </tag>
            
            <tag> Digital Life </tag>
            
            <tag> Android </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[墓畔哀歌]]></title>
      <url>https://blog.ddlee.cn/2017/08/06/%E5%A2%93%E7%95%94%E5%93%80%E6%AD%8C/</url>
      <content type="html"><![CDATA[<p>按：石评梅(1902-1928)为纪念恋人高君宇所作。初读是在高中一年级时的语文阅读材料里，读毕怅然若失，之后日日早读必大声读一遍，忘乎所以。如今重逢，只可默默抄写，竟不能放声读矣。</p>
<p>一</p>
<p>我由冬的残梦里惊醒，春正吻着我的睡靥低吟！晨曦照上了窗纱，望见往日令我醺醉的朝霞，我想让丹彩的云流，再认认我当年的颜色。</p>
<p>披上那件绣著蛱蝶的衣裳，姗姗地走到尘网封锁的妆台旁。呵！明镜里照见我憔悴的枯颜，像一朵颤动在风雨中苍白雕零的梨花。</p>
<p>我爱，我原想追回那美丽的皎容，祭献在你碧草如茵的墓旁，谁知道青春的残蕾已和你一同殉葬。</p>
<p>二</p>
<p>假如我的眼泪真凝成一粒一粒珍珠，到如今我已替你缀织成绕你玉颈的围巾。</p>
<p>假如我的相思真化作一颗一颗的红豆，到如今我已替你堆集永久勿忘的爱心。</p>
<p>哀愁深埋在我心头。</p>
<p>我愿燃烧我的肉身化成灰烬，我愿放浪我的热情怒涛汹涌，天呵！这蛇似的蜿蜒，蚕似的缠绵，就这样悄悄地偷去了我生命的青焰。</p>
<p>我爱，我吻遍了你墓头青草在日落黄昏；我祷告，就是空幻的梦吧，也让我再见见你的英魂。</p>
<p>三</p>
<p>明知道人生的尽头便是死的故乡，我将来也是一座孤冢，衰草斜阳。有一天呵！我离开繁华的人寰，悄悄入葬，这悲艳的爱情一样是烟消云散，昙花一现，梦醒后飞落在心头的都是些残泪点点。</p>
<p>然而我不能把记忆毁灭，把埋我心墟上的残骸抛却，只求我能永久徘徊在这垒垒荒冢之间，为了看守你的墓茔，祭献那茉莉花环。</p>
<p>我爱，你知否我无言的忧衷，怀想着往日轻盈之梦。梦中我低低唤着你小名，醒来只是深夜长空有孤雁哀鸣！</p>
<p>四</p>
<p>黯淡的天幕下，没有明月也无星光这宇宙像数千年的古墓；皑皑白骨上，飞动闪映着惨绿的磷花。我匍匐哀泣于此残銹的铁栏之旁，愿烘我愤怒的心火，烧毁这黑暗丑恶的地狱之网。</p>
<p>命运的魔鬼有意捉弄我弱小的灵魂，罚我在冰雪寒天中，寻觅那雕零了的碎梦。求上帝饶恕我，不要再惨害我这仅有的生命，剩得此残躯在，容我杀死那狞恶的敌人！</p>
<p>我爱，纵然宇宙变成烬余的战场，野烟都腥：在你给我的甜梦里，我心长系驻于虹桥之中，赞美永生！</p>
<p>五</p>
<p>我镇天踟蹰于垒垒荒冢，看遍了春花秋月不同的风景，抛弃了一切名利虚荣，来到此无人烟的旷野，哀吟缓行。我登了高岭，向云天苍茫的西方招魂，在绚烂的彩霞里，望见了我沈落的希望之陨星。</p>
<p>远处是烟雾冲天的古城，火星似金箭向四方飞游！隐约的听见刀枪搏击之声，那狂热的欢呼令人震惊！在碧草萋萋的墓头，我举起了胜利的金觥，饮吧我爱，我奠祭你静寂无言的孤冢！</p>
<p>星月满天时，我把你遗我的宝剑纤手轻擎，宣誓向长空：</p>
<p>愿此生永埋了英雄儿女的热情。</p>
<p>六</p>
<p>假如人生只是虚幻的梦影，那我这些可爱的映影，便是你赠与我的全生命。我常觉你在我身后的树林里，骑着马轻轻地走过去。常觉你停息在我的窗前，徘徊著等我的影消灯熄。常觉你随着我唤你的声音悄悄走近了我，又含泪退到了墙角。常觉你站在我低垂的雪帐外，哀哀地对月光而叹息！</p>
<p>在人海尘途中，偶然逢见个像你的人，我停步凝视后，这颗心呵！便如秋风横扫落叶般冷森凄零！我默思我已经得到爱的之心，如今只是荒草夕阳下，一座静寂无语的孤冢。</p>
<p>我的心是深夜梦里，寒光闪灼的残月，我的情是青碧冷静，永不再流的湖水。残月照着你的墓碑，湖水环绕着你的坟，我爱，这是我的梦，也是你的梦，安息吧，敬爱的灵魂！</p>
<p>七</p>
<p>我自从混迹到尘世间，便忘却了我自己；在你的灵魂我才知是谁？</p>
<p>记得也是这样夜里。我们在河堤的柳丝中走过来，走过去。我们无语，心海的波浪也只有月儿能领会。你倚在树上望明月沈思，我枕在你胸前听你的呼吸。抬头看见黑翼飞来掩遮住月儿的清光，你抖颤著问我：假如这苍黑的翼是我们的命运时，应该怎样？</p>
<p>我认识了欢乐，也随来了悲哀，接受了你的热情，同时也随来了冷酷的秋风。往日，我怕恶魔的眼睛凶，白牙如利刃；我总是藏伏在你的腋下趑趄不敢进，你一手执宝剑，一手扶着我践踏着荆棘的途径，投奔那如花的前程！</p>
<p>如今，这道上还留着你斑斑血痕，恶魔的眼睛和牙齿再是那样凶狠。但是我爱，你不要怕我孤零，我愿用这一纤细的弱玉腕，建设那如意的梦境。</p>
<p>八</p>
<p>春来了，催开桃蕾又飘到柳梢，这般温柔慵懒的天气真使人恼！她似乎躲在我眼底有意缭绕，一阵阵风翼，吹起了我灵海深处的波涛。</p>
<p>这世界已换上了装束，如少女般那样娇娆，她披拖着浅绿的轻纱，蹁跹在她那（姹）紫嫣红中舞蹈。伫立于白杨下，我心如捣，强睁开模糊的泪眼，细认你墓头，萋萋芳草。</p>
<p>满腔辛酸与谁道？愿此恨吐向青空将天地包。它纠结围绕着我的心，像一堆枯黄的蔓草，我爱，我待你用宝剑来挥扫，我待你用火花来焚烧。</p>
<p>九</p>
<p>垒垒荒冢上，火光熊熊，纸灰缭绕，清明到了。这是碧草绿水的春郊。墓畔有白发老翁，有红颜年少，向这一杯黄土致不尽的怀忆和哀悼，云天苍茫处我将魂招；白杨萧条，暮鸦声声，怕孤魂归路迢迢。</p>
<p>逝去了，欢乐的好梦，不能随墓草而复生，明朝此日，谁知天涯何处寄此身？叹漂泊我已如落花浮萍，且高歌，且痛饮，拼一醉烧熄此心头余情。</p>
<p>我爱，这一杯苦酒细细斟，邀残月与孤星和泪共饮，不管黄昏，不论夜深，醉卧在你墓碑傍，任霜露侵凌吧！我再不醒。</p>
<p>十六年清明陶然亭畔</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Reading </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Reading </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[源码笔记]keras源码分析之Model]]></title>
      <url>https://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/</url>
      <content type="html"><![CDATA[<p>本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是<a href="https://github.com/fchollet/keras/blob/master/keras/engine/training.py" target="_blank" rel="external">/keras/engine/training.py</a>和<a href="https://github.com/fchollet/keras/blob/master/keras/models.py" target="_blank" rel="external">/keras/model.py</a>，要观察的类是<code>Model</code>和<code>Sequential</code>。</p>
<p>本系列第一篇：<a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">【源码笔记】keras源码分析之Tensor, Node和Layer</a><br>第二篇：<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">【源码笔记】keras源码分析之Container</a></p>
<h3 id="Model：添加了训练信息的Container"><a href="#Model：添加了训练信息的Container" class="headerlink" title="Model：添加了训练信息的Container"></a><code>Model</code>：添加了训练信息的<code>Container</code></h3><p><code>Model.compile()</code>主要完成了配置<code>optimizer</code>, <code>loss</code>, <code>metrics</code>等操作，而要执行的<code>fit</code>, <code>evaluate</code>等则不在<code>compile</code>过程中配置。</p>
<pre><code class="python">def compile(self, optimizer, loss, metrics=None, loss_weights=None,
            sample_weight_mode=None, **kwargs):
    loss = loss or {}
    self.optimizer = optimizers.get(optimizer)
    self.sample_weight_mode = sample_weight_mode
    self.loss = loss
    self.loss_weights = loss_weights

    loss_function = losses.get(loss)
    loss_functions = [loss_function for _ in range(len(self.outputs))]
    self.loss_functions = loss_functions

    # Prepare targets of model.
    self.targets = []
    self._feed_targets = []
    for i in range(len(self.outputs)):
        shape = self.internal_output_shapes[i]
        name = self.output_names[i]
        target = K.placeholder(ndim=len(shape),
                               name=name + &#39;_target&#39;,
                               sparse=K.is_sparse(self.outputs[i]),
                               dtype=K.dtype(self.outputs[i]))
        self.targets.append(target)
        self._feed_targets.append(target)

    # Prepare metrics.
    self.metrics = metrics
    self.metrics_names = [&#39;loss&#39;]
    self.metrics_tensors = []

    # Compute total loss.
    total_loss = None
    for i in range(len(self.outputs)):
        y_true = self.targets[i]
        y_pred = self.outputs[i]
        loss_weight = loss_weights_list[i]
        if total_loss is None:
            total_loss = loss_weight * output_loss
        else:
            total_loss += loss_weight * output_loss

    for loss_tensor in self.losses:
        total_loss += loss_tensor

    self.total_loss = total_loss
    self.sample_weights = sample_weights
</code></pre>
<p><code>Model</code>对象的<code>fit()</code>方法封装了<code>_fit_loop()</code>内部方法，而<code>_fit_loop()</code>方法的关键步骤由<code>_make_train_function()</code>方法完成，返回<code>history</code>对象，用于回调函数的处理。</p>
<pre><code class="python">def fit(self, x=None, y=None, ...)：
      self._make_train_function()
      f = self.train_function
      return self._fit_loop(f, ins, ...)
</code></pre>
<p>在<code>_fit_loop()</code>方法中，回调函数完成了对训练过程的监控记录等任务，<code>train_function</code>也被应用于传入的数据：</p>
<pre><code class="python">def _fit_loop(self, f, ins, out_labels=None, batch_size=32,
              epochs=100, verbose=1, callbacks=None,
              val_f=None, val_ins=None, shuffle=True,
              callback_metrics=None, initial_epoch=0):
    self.history = cbks.History()
    callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]
    callbacks = cbks.CallbackList(callbacks)
    out_labels = out_labels or []
    callbacks.set_model(callback_model)
    callbacks.set_params({
        &#39;batch_size&#39;: batch_size,
        &#39;epochs&#39;: epochs,
        &#39;samples&#39;: num_train_samples,
        &#39;verbose&#39;: verbose,
        &#39;do_validation&#39;: do_validation,
        &#39;metrics&#39;: callback_metrics or [],
    })
    callbacks.on_train_begin()
    callback_model.stop_training = False

    for epoch in range(initial_epoch, epochs):
        callbacks.on_epoch_begin(epoch)
        batches = _make_batches(num_train_samples, batch_size)
        epoch_logs = {}
        for batch_index, (batch_start, batch_end) in enumerate(batches):
            batch_ids = index_array[batch_start:batch_end]
            batch_logs = {}
            batch_logs[&#39;batch&#39;] = batch_index
            batch_logs[&#39;size&#39;] = len(batch_ids)
            callbacks.on_batch_begin(batch_index, batch_logs)
            # 应用传入的train_function
            outs = f(ins_batch)
            callbacks.on_batch_end(batch_index, batch_logs)
        callbacks.on_epoch_end(epoch, epoch_logs)
    callbacks.on_train_end()
    return self.history
</code></pre>
<p><code>_make_train_function()</code>方法从<code>optimizer</code>获取要更新的参数信息，并传入来自<code>backend</code>的<code>function</code>对象：</p>
<pre><code class="python">def _make_train_function(self):
    if self.train_function is None:
        inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights
        training_updates = self.optimizer.get_updates(
            self._collected_trainable_weights,
            self.constraints,
            self.total_loss)
        updates = self.updates + training_updates
        # Gets loss and metrics. Updates weights at each call.
        self.train_function = K.function(inputs,
                                         [self.total_loss] + self.metrics_tensors,
                                         updates=updates,
                                         name=&#39;train_function&#39;,
                                         **self._function_kwargs)
</code></pre>
<p><code>Model</code>的其他方法<code>evaluate()</code>等，与<code>fit()</code>的结构类似。</p>
<h3 id="Sequential-构建模型的外层接口"><a href="#Sequential-构建模型的外层接口" class="headerlink" title="Sequential:构建模型的外层接口"></a><code>Sequential</code>:构建模型的外层接口</h3><p><code>Sequential</code>对象是<code>Model</code>对象的进一步封装，也是用户直接面对的接口，其<code>compile()</code>, <code>fit()</code>, <code>predict()</code>等方法与<code>Model</code>几乎一致，所不同的是添加了<code>add()</code>方法，也是我们用于构建网络的最基本操作。</p>
<p><code>Sequential.add()</code>方法的源码如下：</p>
<pre><code class="python">def add(self, layer):
    # 第一层必须是InputLayer对象
    if not self.outputs:
        if not layer.inbound_nodes:
            x = Input(batch_shape=layer.batch_input_shape,
                      dtype=layer.dtype, name=layer.name + &#39;_input&#39;)
            layer(x)

        self.outputs = [layer.inbound_nodes[0].output_tensors[0]]
        self.inputs = topology.get_source_inputs(self.outputs[0])

        topology.Node(outbound_layer=self, ...)
    else:
        output_tensor = layer(self.outputs[0])
        self.outputs = [output_tensor]
        self.inbound_nodes[0].output_tensors = self.outputs

    self.layers.append(layer)
</code></pre>
<p>可以看到，<code>add()</code>方法总是确保网络的第一层为<code>InputLayer</code>对象，并将新加入的层应用于<code>outputs</code>，使之更新。因此，从本质上讲，在<code>Model</code>中添加新层还是在更新模型的<code>outputs</code>。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Programming </tag>
            
            <tag> Keras </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[源码笔记]keras源码分析之Container]]></title>
      <url>https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/</url>
      <content type="html"><![CDATA[<p>本篇继续讨论keras的源码结构。</p>
<p><a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">第一篇源码笔记</a>中我们观察了<code>Layer</code>, <code>Tensor</code>和<code>Node</code>是如何耦合在一起的，而本篇的重点是观察多层网络构成的有向无环图（DAG）。主要涉及的文件为<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">keras/engine/topology.py</a>， 要观察的类是<code>Container</code>。</p>
<h3 id="Container对象：DAG的拓扑原型"><a href="#Container对象：DAG的拓扑原型" class="headerlink" title="Container对象：DAG的拓扑原型"></a><code>Container</code>对象：DAG的拓扑原型</h3><p>在第一篇中我们提到，Keras Tensor中增强的<code>\_keras_history</code>属性使得我们仅通过输入和输出的Tensor，就可以构建出整张计算图。而<code>Container</code>对象正是实现了这样的过程。</p>
<h4 id="计算图的构建"><a href="#计算图的构建" class="headerlink" title="计算图的构建"></a>计算图的构建</h4><p>DAG计算图的构建在<code>Container</code>对象实例化时完成，主要包括如下几个操作：</p>
<p>1） 记录<code>Container</code>的首尾连接信息</p>
<pre><code class="python">def __init__(self, inputs, outputs, name=None):
  for x in self.outputs:
      layer, node_index, tensor_index = x._keras_history
      self.output_layers.append(layer)
      self.output_layers_node_indices.append(node_index)
      self.output_layers_tensor_indices.append(tensor_index)

  for x in self.inputs:
      layer, node_index, tensor_index = x._keras_history
      self.input_layers.append(layer)
      self.input_layers_node_indices.append(node_index)
      self.input_layers_tensor_indices.append(tensor_index)
</code></pre>
<p>2） 从<code>output_tensors</code>开始反向递归构建计算图，采用广度优先的准则，本步的关键是构建<code>nodes_in_decreasing_depth</code>这一队列，这些<code>Node</code>包含的连接信息和深度信息将是后续正向传播和反向训练计算执行顺序的依据。</p>
<pre><code class="python">  def build_map_of_graph(tensor, finished_nodes, nodes_in_progress):
      layer, node_index, tensor_index = tensor._keras_history
      node = layer.inbound_nodes[node_index]
      nodes_in_progress.add(node)

      # 广度优先搜索
      for i in range(len(node.inbound_layers)):
          x = node.input_tensors[i]
          layer = node.inbound_layers[i]
          node_index = node.node_indices[i]
          tensor_index = node.tensor_indices[i]
          # 递归调用
          build_map_of_graph(x, finished_nodes, nodes_in_progress,
                             layer, node_index, tensor_index)

      # 维护两个队列
      finished_nodes.add(node)
      nodes_in_progress.remove(node)
      nodes_in_decreasing_depth.append(node)

  # 反向构建DAG
  for x in self.outputs:
      build_map_of_graph(x, finished_nodes, nodes_in_progress)
</code></pre>
<p>3） 计算各节点的深度并按深度标定节点在DAG中的位置</p>
<pre><code class="python">  # 根据队列标定各节点的深度
  for node in reversed(nodes_in_decreasing_depth):
      depth = nodes_depths.setdefault(node, 0)
      previous_depth = layers_depths.get(node.outbound_layer, 0)
      depth = max(depth, previous_depth)
      layers_depths[node.outbound_layer] = depth
      nodes_depths[node] = depth

      for i in range(len(node.inbound_layers)):
          inbound_layer = node.inbound_layers[i]
          node_index = node.node_indices[i]
          inbound_node = inbound_layer.inbound_nodes[node_index]
          previous_depth = nodes_depths.get(inbound_node, 0)
          nodes_depths[inbound_node] = max(depth + 1, previous_depth)

  # 按深度标定各节点的位置
  nodes_by_depth = {}
  for node, depth in nodes_depths.items():
      if depth not in nodes_by_depth:
          nodes_by_depth[depth] = []
      nodes_by_depth[depth].append(node)

  # 按深度标定各层的位置
  layers_by_depth = {}
  for layer, depth in layers_depths.items():
      if depth not in layers_by_depth:
          layers_by_depth[depth] = []
      layers_by_depth[depth].append(layer)

  self.layers_by_depth = layers_by_depth
  self.nodes_by_depth = nodes_by_depth
</code></pre>
<p>4）将整个<code>Container</code>并入<code>Node</code>以保持兼容性</p>
<pre><code class="python">  self.outbound_nodes = []
  self.inbound_nodes = []
  Node(outbound_layer=self,
       inbound_layers=[],
       node_indices=[],
       tensor_indices=[],
       input_tensors=self.inputs,
       output_tensors=self.outputs,
       ...)
</code></pre>
<h3 id="计算图中的计算"><a href="#计算图中的计算" class="headerlink" title="计算图中的计算"></a>计算图中的计算</h3><p>计算在<code>Container</code>对象的<code>call()</code>方法完成，其实现又依靠内部方法<code>run_internal_graph()</code>。</p>
<pre><code class="python">def run_internal_graph(self, inputs, masks=None):
       depth_keys = list(self.nodes_by_depth.keys())
       depth_keys.sort(reverse=True)
       # 依据深度
       for depth in depth_keys:
           nodes = self.nodes_by_depth[depth]
           # 对同一深度上的Node进行计算
           for node in nodes:
               layer = node.outbound_layer # Node对应的layer
               reference_input_tensors = node.input_tensors
               reference_output_tensors = node.output_tensors
               computed_data = []
               if len(computed_data) == len(reference_input_tensors):
                   # 在Layer中进行计算
                   with K.name_scope(layer.name):
                       if len(computed_data) == 1:
                           computed_tensor, computed_mask = computed_data[0]
                           output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
                           computed_tensors = [computed_tensor]
                       else:
                           computed_tensors = [x[0] for x in computed_data]
                           output_tensors = _to_list(layer.call(computed_tensors, **kwargs))
       output_tensors = []
       output_masks = []
       for x in self.outputs:
           tensor, mask = tensor_map[str(id(x))]
           output_tensors.append(tensor)
           output_masks.append(mask)
       return output_tensors, output_masks
</code></pre>
<p>从上面的代码可以看到计算是依据深度进行的，并通过更新<code>computed_data</code>和<code>output_tensor</code>等变量完成整张图的遍历计算。</p>
<p>继续阅读系列第三篇：<a href="https://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/">【源码笔记】keras源码分析之Model</a></p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Programming </tag>
            
            <tag> Keras </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习中的权重衰减]]></title>
      <url>https://blog.ddlee.cn/2017/07/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</url>
      <content type="html"><![CDATA[<p>权重衰减（weight dacay），即L^2范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自<a href="https://deeplearningbook.org" target="_blank" rel="external">The Deep Learning Book</a>。</p>
<h3 id="为什么要引入权重衰减"><a href="#为什么要引入权重衰减" class="headerlink" title="为什么要引入权重衰减"></a>为什么要引入权重衰减</h3><p>机器学习的逻辑与我们最初解决问题的思维方式恰恰相反：要解决问题，一种经典的思路是把它拆成小问题，考虑之间的依赖，然后分而治之。而机器学习的哲学是“<em>trail-error-correct</em>”：先假设一堆可能的方案，根据结果去选择/调整这些方案，直到满意。换句话说，机器学习在假设空间中搜索最符合数据的模型：以果推因，即为最大似然的想法。随着数据量的增大，我们越来越需要表达能力更强的模型，而深度学习的优势正符合这一需要：通过分布式表示带来的指数增益，深度学习模型的扩展能力几乎是无限的（详见<a href="https://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/">深度学习和分布式表示</a>）。</p>
<p>有了模型（备选模型集），有了数据，就不得不面对机器学习领域的核心问题：如何保证模型能够描述数据（拟合）和生成数据（泛化）。</p>
<p>粗略来看，有以下三种情况：</p>
<ul>
<li>我们假定的模型族不包含真实数据的生成过程：欠拟合/高偏差</li>
<li>匹配真实的数据生成过程</li>
<li>除了包含真实的生成过程，还包含了其他信息：过拟合/高方差</li>
</ul>
<p>高偏差意味着我们的模型不够准确（模型族不足以描述数据），高方差意味着我们建模了不必要的信息（训练数据的随机性带来的）。前者通过提高模型的表述能力来解决（更深的网络），后者则需要合理的正则化技术来控制。这即是著名的trade-off。</p>
<h3 id="深度学习模型的参数"><a href="#深度学习模型的参数" class="headerlink" title="深度学习模型的参数"></a>深度学习模型的参数</h3><p>对数据建模，其实是从数据中提取我们能够理解的信息。建立的模型，是从数据分布的空间到目标变量所在空间的映射。从这个角度看，我们通过模型带来的变换获得了数据的一种表示，我们认为能够理解和操作的表示。</p>
<p>为了表述这一变换，深度模型的套路是线性层施加变换，非线性层固定信息（不能平移），然后将这样的结构堆叠起来，分层提取数据特征。</p>
<p>这让我想起实变中证明定理的套路：先证明简单函数的情形，再推广到连续函数，再到勒贝格可积的函数。</p>
<p>常规的套路（MLP）在拟合普通的函数任务上能够胜任，但面对更复杂的图像等数据，就需要更灵活的网络结构。</p>
<p>非常出色的CNN, LSTM, Inception块, ResNet, DenseNet等结构，就是加入了人类的先验知识，使之更有效的提取图像/音频数据分布空间的特征。（所以Manning有次在课堂上说，机器学习事实上还是人类在学习：机器只是在求导数、做乘法，最好的模型都是人们学习出来的。）</p>
<p>人们确实设计了很多巧妙的结构来解决不同的问题，但落实到网络的层和单元上，仍是最基本的矩阵乘法、加法运算。决定模型表述能力的，也正是这些普通的乘法运算中涉及的矩阵和向量了。</p>
<h3 id="权重衰减如何起作用"><a href="#权重衰减如何起作用" class="headerlink" title="权重衰减如何起作用"></a>权重衰减如何起作用</h3><p>下面我们通过观察加入权重衰减后目标函数的梯度变化来讨论权重衰减是如何起作用的。可以跳过公式部分直接看最后一段。</p>
<p>————————————推导部分—————————</p>
<p>简单起见，令偏置为0，模型的目标函数：</p>
<p>$$J_{1}(w; X,y)=\frac{\alpha}{2} w^T w+J(w; X,y)$$</p>
<p>对应的梯度为：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w; X,y) = \alpha w + {\nabla}_{w} J(w; X,y)$$</p>
<p>进行梯度下降，参数的更新规则为：</p>
<p>$$w = w - \epsilon (\alpha w + {\nabla}_{w} J(w; X,y)) $$</p>
<p>也就是：</p>
<p>$$w = (1 - \epsilon \alpha )w - \epsilon {\nabla}_{w} J(w; X,y)$$</p>
<p>从上式可以发现，加入权重衰减后，先对参数进行伸缩，再沿梯度下降。下面令 $x^{(1)}$ 为使目标函数达到最优的参数值，在其附近考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>其中 $H$ 为近似目标函数在的Hessian矩阵。当近似目标函数最小时，其梯度为 $0$ ，即：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>该式也向我们说明了基于梯度的优化算法主要的信息来自Hessian矩阵。添加入权重衰减项之后，上式变为（记此时的最优点为 $w^{(2)}$ ）：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w) \approx \alpha w^{(2)} + H(w^{(2)} - w^{(1)}) = 0$$</p>
<p>所以</p>
<p>$$w^{(2)} = (H + \alpha I)^{-1} H w^{(1)} $$</p>
<p>该式表明了了加入正则化对参数最优质点的影响，由Hessian矩阵和正则化系数 $\alpha$ 共同决定。</p>
<p>进一步将Hessian矩阵分解，可以得到：</p>
<p>$$w^{(2)} = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^{(1)}$$</p>
<p>其中， $Q$ 为正交矩阵，$\Lambda$ 为对角矩阵。这样可以看到，权重衰减的效果是沿着由 $H$ 的特征向量所定义的轴缩放 $w$， 具体的伸缩因子为 ${\frac{ {\lambda}_{i} }{ {\lambda}<em>i + \alpha }}$ ，其中 ${\lambda}</em>{i}$ 表示第 $i$ 个特征向量对应的特征值。</p>
<p>当特征值 $\lambda$很大（相比 $\alpha$）时，缩放因子对权重影响较小，因而更新过程中产生的变化也不大；而当特征值较小时， $\alpha$的缩放作用就显现出来，将这个方向的权重衰减到0。</p>
<p>这种效果也可以由下图表示：</p>
<p><img src="https://static.ddlee.cn/static/img/深度学习中的权重衰减/transform.png" alt="transform"></p>
<p>———————————————————推导部分结束———————————————————</p>
<p><em>总结来说，目标函数的Hessian矩阵（显式、隐式或者近似的）是现有优化算法进行寻优的主要依据。通过控制权重衰减的 $\alpha$ 参数，我们实际上控制的是在Hessian矩阵的特征方向上以多大的幅度缩放权重，相对重要（能够显著减小目标函数）的方向上权重保留比较完好，而无助于目标函数减小的方向上权重在训练过程中逐渐地衰减掉了。而这也就是权重衰减的意义。</em></p>
<p>从宏观上来看，对目标函数来说，特征值较大的方向包含更多有关数据的信息，较小的方向则有随机性的噪声，权重衰减正是通过忽略较少信息方向的变化来对抗过拟合的。</p>
<h3 id="L-1-范数正则化"><a href="#L-1-范数正则化" class="headerlink" title="$L^1$ 范数正则化"></a>$L^1$ 范数正则化</h3><p>通过类似的推导，可以得到加入了 $L^1$ 范数惩罚项对参数最优解的影响如下：</p>
<p>$$w^{(2)}<em>{i} = sign(w^{(1)}</em>{i}) max \big{|w^{(1)}<em>{i}| - \frac{\alpha}{H</em>{i,i}}, 0 \big}$$</p>
<p>相比 $L^2$ 范数的影响，这是一个离散的结果，因而 $L^1$ 范数惩罚会将参数推向更加稀疏的解。这种稀疏性质常被用作特征选择。</p>
<h3 id="权重衰减的贝叶斯解释"><a href="#权重衰减的贝叶斯解释" class="headerlink" title="权重衰减的贝叶斯解释"></a>权重衰减的贝叶斯解释</h3><p>在贝叶斯统计的框架下，常用的推断策略是最大后验点估计(Maximum A Posteriori, MAP)。有如下的推断公式（由贝叶斯定律导出）：</p>
<p>$${\theta}_{MAP} = argmax p(\theta | x) = argmax (log p( x | \theta) + log p(\theta))$$</p>
<p>上式右边第一项是标准的对数似然项，而第二项对应着先验分布。</p>
<p>在这样的视角下，我们只进行最大似然估计是不够的，还要考虑先验 $p(\theta)$ 的分布。而当假定参数为正态分布 $N(w; 0, \frac{1}{\lambda}I^2)$ 时，带入上式（ $\theta$ 为参数），即可发现第二项的结果正比于权重衰减惩罚项 $\lambda w^T w$ ，加上一个不依赖于 $w$ 也不影响学习过程的项。于是，具有高斯先验权重的MAP贝叶斯推断对应着权重衰减。</p>
<h3 id="权重衰减与提前终止"><a href="#权重衰减与提前终止" class="headerlink" title="权重衰减与提前终止"></a>权重衰减与提前终止</h3><p>提前终止也是一种正则化技术，其想法简单粗暴：每个epoch之后在验证集上评估结果，当验证集误差不再下降的时候，我们认为模型已经尽它所能了，于是终止训练过程。</p>
<p>提前终止以牺牲一部分训练数据来作为验证数据来的代价来对抗过拟合，其逻辑是实证主义的。</p>
<p>然而，在二次近似和简单梯度下降的情形下，可以观察到提前终止可以有相当于权重衰减的效果。</p>
<p>我们仍考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>记最优参数点为 $w^{(1)}$ ，其梯度为：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>不加入正则化项，其梯度下降的更新策略（从第 $\tau-1$ 步到 $\tau$ 步）为：</p>
<p>$$ w^{(\tau)} = w^{\tau - 1)} - \epsilon H (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>累加得到</p>
<p>$$ w^{(\tau)} - w^{(1)} = (I - \epsilon H) (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>将Hessian矩阵分解，得到如下形式</p>
<p>$$ w^{(\tau)} = Q[I - (I - \epsilon \Lambda) ^ {\tau}] Q^T w^{(1)} $$</p>
<p>将加入正则化项的权重影响改写为</p>
<p>$$ w^{(2)} = Q[I - (\Lambda + \alpha I) ^ {-1} \alpha] Q^T w^{(1)} $$</p>
<p>对比可以得到，如果超参数 $\epsilon, \alpha, \tau$ 满足</p>
<p>$$ (I - \epsilon \Lambda) ^ {\tau} = (\Lambda + \alpha I) ^ {-1} \alpha $$</p>
<p>则提前终止将与权重衰减有相当的效果。具体的，即第 $\tau$ 步结束的训练过程将到达超参数为 $\alpha$ 的 $L^2$ 正则化得到的最优点。</p>
<p>但提前终止带来的好处是，我们不再需要去找合适的超参数 $\alpha$ ，而只需要制定合理的终止策略（如3个epoch均不带来验证集误差的减小即终止训练），在训练成本的节约上，还是很值得的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[源码笔记]keras源码分析之Layer、Tensor和Node]]></title>
      <url>https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/</url>
      <content type="html"><![CDATA[<p>Keras架构的主要逻辑实现在<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">/keras/engine/topology.py</a>中，主要有两个基类<code>Node()</code>和<code>Layer()</code>，一个重要函数<code>Input()</code>。具体地，</p>
<ul>
<li><code>Layer()</code>是一个计算层的抽象，完成网络中对Tensor的计算过程；</li>
<li><code>Node()</code>描述两个层之间连接关系的抽象，配合<code>Layer()</code>构建DAG；</li>
<li><code>Input()</code>实例化一个特殊的<code>Layer</code>(<code>InputLayer</code>)，将<code>backend</code>（TensorFlow或Theano）建立的Tensor对象转化为Keras Tensor对象。</li>
</ul>
<h3 id="Keras-Tensor：-增强版Tensor"><a href="#Keras-Tensor：-增强版Tensor" class="headerlink" title="Keras Tensor： 增强版Tensor"></a>Keras Tensor： 增强版Tensor</h3><p>相比原始的TensorFlow或者Theano的张量对象，Keras Tensor加入了如下两个属性，以使Tensor中包含了自己的来源和规模信息：</p>
<ul>
<li>_Keras_history: 保存了最近一个应用于这个Tensor的Layer</li>
<li>_keras_shape: 标准化的Keras shape接口</li>
</ul>
<p>当使用Keras建立深度网络时，传入的数据首先要经过<code>Input()</code>函数。在<code>Input()</code>函数中，实例化一个<code>InputLayer()</code>对象，并将此<code>Layer()</code>对象作为第一个应用于传入张量的Layer，置于<code>_keras_history</code>属性中。此外，<code>InputLayer()</code>和<code>Input()</code>还会对传入的数据进行规模检查和变换等，使之符合后续操作的要求。</p>
<p>代码上实现如下：</p>
<pre><code class="python">def Input():
  input_layer = InputLayer()
  outputs = InputLayer.inbound_nodes[0].output_tensor
  return outputs

class InputLayer():
  def __init__():
    input_tensor._keras_history = (self, 0, 0)
    Node(self, ...)
</code></pre>
<p>在下面我们将看到，加入的<code>_keras_history</code>属性在计算图的构建上所起的作用是关键的。仅通过输入和输出的Tensor，我们可以构建出整张计算图。但这样的代价是Tensor对象太重了，包含了Layer的信息。</p>
<h3 id="Node对象：层与层之间链接的抽象"><a href="#Node对象：层与层之间链接的抽象" class="headerlink" title="Node对象：层与层之间链接的抽象"></a><code>Node</code>对象：层与层之间链接的抽象</h3><p>若考虑<code>Layer</code>对象抽象的是完成计算的神经元胞体，则<code>Node</code>对象是对神经元树突结构的抽象。其内聚的主要信息是：</p>
<pre><code class="python">class Node():
  def __init__(self, outbound_layer,
              inbound_layers, node_indices, tensor_indices,
              input_tensors, output_tensors, ...)
</code></pre>
<p>其中<code>outbound_layer</code>是施加计算（使<code>input_tensors</code>变为<code>output_tensors</code>）的层，<code>inbound_layers</code>对应了<code>input_tensors</code>来源的层，而<code>node_indices</code>和<code>tensor_indices</code>则记录了<code>Node</code>和<code>Layer</code>之间的标定信息。</p>
<p><code>Node</code>对象总在<code>outbound_layer</code>被执行时创建，并加入<code>outbound_layer</code>的<code>inbound_nodes</code>属性中。在<code>Node</code>对象的表述下，A和B两个层产生连接关系时，<code>Node</code>对象被建立，并被加入<code>A.outbound_nodes</code>和<code>B.inbound_nodes</code>。</p>
<h3 id="Layer对象：计算层的抽象"><a href="#Layer对象：计算层的抽象" class="headerlink" title="Layer对象：计算层的抽象"></a><code>Layer</code>对象：计算层的抽象</h3><p><code>Layer</code>对象是对网络中神经元计算层的抽象，实例化需要如下参数：</p>
<pre><code class="python">allowed_kwargs = {&#39;input_shape&#39;,
                  &#39;batch_input_shape&#39;,
                  &#39;batch_size&#39;,
                  &#39;dtype&#39;,
                  &#39;name&#39;,
                  &#39;trainable&#39;,
                  &#39;weights&#39;,
                  &#39;input_dtype&#39;,  # legacy
                  }
</code></pre>
<p>大部分与传入数据的类型和规模相关，<code>trainable</code>表征该层是否需要更新权重。此外，还有<code>inbound_nodes</code>和<code>outbound_nodes</code>属性来标定与<code>Node</code>对象的链接。</p>
<p><code>Layer</code>对象最重要的方法是<code>__call__()</code>，主要完成如下三件事情：</p>
<ol>
<li><p>验证传入数据的合法性，通过调用内部方法实现：<code>self.assert_input_compatibility(inputs)</code></p>
</li>
<li><p>进行计算<code>outputs = self.call(inputs, ...)</code>，被其子类具体实现，如<code>Linear</code>, <code>Dropout</code>等</p>
</li>
<li><p>更新Tensor中的<code>_keras_history</code>属性，记录该次计算操作，通过内部方法<code>_add_inbound_nodes()</code>实现</p>
</li>
</ol>
<p>方法<code>_add_inbound_nodes()</code>对Tensor的更新是构建<code>Layer</code>之间关系的关键操作，其主要代码如下：</p>
<pre><code class="python">for x in input_tensors:
    if hasattr(x, &#39;_keras_history&#39;):
        inbound_layer, node_index, tensor_index = x._keras_history
        inbound_layers.append(inbound_layer)
        node_indices.append(node_index)
        tensor_indices.append(tensor_index)

# Node对象的建立过程中将更新self的inbound_nodes属性
Node(self,
    inbound_layers=inbound_layers,
    node_indices=node_indices,
    tensor_indices=tensor_indices,
    ...)

for i in range(len(output_tensors)):
     output_tensors[i]._keras_history = (self, len(self.inbound_nodes) - 1, i)
</code></pre>
<p>上段代码取出<code>input_tensor</code>的<code>_keras_history</code>属性，建立新的<code>Node</code>，并将当前<code>Layer</code>的信息更新到计算得到的<code>output_tensor</code>中。</p>
<h3 id="实例：Node-Tensor和Layer间连接关系的表征"><a href="#实例：Node-Tensor和Layer间连接关系的表征" class="headerlink" title="实例：Node,Tensor和Layer间连接关系的表征"></a>实例：<code>Node</code>,<code>Tensor</code>和<code>Layer</code>间连接关系的表征</h3><p>下面通过代码来说明三者之间的关系，来自于测试代码：</p>
<pre><code class="python"># 建立新的keras Tensor
a = Input(shape=(32,), name=&#39;input_a&#39;)
b = Input(shape=(32,), name=&#39;input_b&#39;)

a_layer, a_node_index, a_tensor_index = a._keras_history
assert len(a_layer.inbound_nodes) == 1
assert a_tensor_index is 0

# node和layer之间的关系
node = a_layer.inbound_nodes[a_node_index]
assert node.outbound_layer == a_layer

# 建立连接层，将Tensor传入
dense = Dense(16, name=&#39;dense_1&#39;)
a_2 = dense(a)
b_2 = dense(b)

assert len(dense.inbound_nodes) == 2
assert len(dense.outbound_nodes) == 0

# 与张量a关联的Node
assert dense.inbound_nodes[0].inbound_layers == [a_layer]
assert dense.inbound_nodes[0].outbound_layer == dense
assert dense.inbound_nodes[0].input_tensors == [a]

# 与张量b关联的Node
assert dense.inbound_nodes[1].inbound_layers == [b_layer]
assert dense.inbound_nodes[1].outbound_layer == dense
assert dense.inbound_nodes[1].input_tensors == [b]
</code></pre>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>keras利用<code>Node</code>对象描述<code>Layer</code>之间的连接关系，并在<code>Tensor</code>中记录其来源信息。在<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">下篇</a>中，我们将看到keras如何利用这些抽象和增强属性构建DAG，并实现前向传播和反向训练的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Programming </tag>
            
            <tag> Keras </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow最佳实践：试验管理]]></title>
      <url>https://blog.ddlee.cn/2017/07/11/Tensorflow%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E8%AF%95%E9%AA%8C%E7%AE%A1%E7%90%86/</url>
      <content type="html"><![CDATA[<p>本文主要记录使用TensorFlow训练模型中与试验管理相关的最佳实践，主要包括模型训练的大致代码框架、模型的保存与恢复、训练过程的监测、随机性的控制等。主要材料来自<a href="https://web.stanford.edu/class/cs20si/index.html" target="_blank" rel="external">CS 20SI: Tensorflow for Deep Learning Research</a>。</p>
<h3 id="TensorFlow代码框架"><a href="#TensorFlow代码框架" class="headerlink" title="TensorFlow代码框架"></a>TensorFlow代码框架</h3><p>使用TensorFlow构建深度网络模型大致包括数据预处理、图的构建、模型训练、模型推断与评估等部分，大致的代码框架如下：</p>
<pre><code class="python">import tensorflow as tf
import numpy as np

# Data
X = tf.placeholder(&quot;float&quot;, [None, n_input])
Y = tf.placeholder(&quot;float&quot;, [None, n_output])

# Parameters
W = tf.Variable(tf.random_normal([n_input, n_output]))
b = tf.Variable(tf.random_normal([n_output]))

# Define model
y = tf.matmul(x, W) + b
y_pred = tf.nn.relu(y)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_pred, y_true))
optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(cost)

# Training
with tf.Session() as sess:
    tf.initialize_all_variables().run()
    sess.run(optimizer, feed_dict={X: x_data, Y: y_data})

# Prediction
y_test = tf.nn.relu(tf.matmul(x_test, W) + b))
</code></pre>
<h3 id="模型的保存与恢复"><a href="#模型的保存与恢复" class="headerlink" title="模型的保存与恢复"></a>模型的保存与恢复</h3><p>一个很深的网络训练成本是比较高的，因而将模型定期保存（写入硬盘）则有必要。这里的模型，实际上是有组织的一批数据，包括图的结构描述、参数当前值等。因而我们要保存的不仅是模型，还有模型当前的运行状态，实际上每一次保存可以作为一个还原点。</p>
<h4 id="tf-train-Saver类"><a href="#tf-train-Saver类" class="headerlink" title="tf.train.Saver类"></a>tf.train.Saver类</h4><p>使用<code>tf.train.Saver</code>类需传入以下参数：<code>tf.train.Saver.save(sess, save_path, global_step=step)</code>。</p>
<p>首先定义步数变量：<code>self​.​global_step ​=​ tf​.​Variable​(​0​,​ dtype​=​tf​.​int32​,​ trainable​=​False​,name​=​&#39;global_step&#39;)</code></p>
<p>在模型训练的过程中插入还原点的保存：</p>
<pre><code class="python">self​.​optimizer ​=​ tf​.​train​.​GradientDescentOptimizer​(​self​.​lr​).​minimize​(​self​.​loss​,global_step​=​self​.​global_step)
saver ​=​ tf​.​train​.​Saver​()​​
​with​ tf​.​Session​()​​as​ sess:
 sess​.​run​(​tf​.​global_variables_initializer​())
 average_loss ​=​​0.0
 for​ index ​in​ range​(​num_train_steps​):
    batch ​=​ batch_gen​.​next​()
    loss_batch​,​ _ ​=​ sess​.​run​([​model​.​loss​,​ model​.​optimizer​], feed_dict​={...})
    average_loss ​+=​ loss_batch
    # Save model every 1000 steps
    ​if​​(​index ​+​​1​)​​%​​1000​​==​​0:
      saver​.​save​(​sess​,​​&#39;checkpoints/model&#39;​,​ global_step​=​model​.​global_step)
</code></pre>
<p>在训练过程中，在<code>checkpoints</code>路径下会存储一系列的还原点文件，要恢复session到某个还原点，可使用如下代码：<code>saver.restore(sess, &#39;checkpoints/name_of_the_checkpoint&#39;)</code>。</p>
<h4 id="Keras封装：keras-callbacks-ModelCheckpoint"><a href="#Keras封装：keras-callbacks-ModelCheckpoint" class="headerlink" title="Keras封装：keras.callbacks.ModelCheckpoint()"></a>Keras封装：<code>keras.callbacks.ModelCheckpoint()</code></h4><p>Keras对TensorFlow进行了高层的封装，使用一系列回调函数<code>keras.callbacks.Callback()</code>来进行试验管理。</p>
<p>模型保存<code>ModelCheckpoint()</code>需要传入的参数：<br><code>keras.callbacks.ModelCheckpoint(filepath, monitor=&#39;val_loss&#39;, verbose=0, save_best_only=False, save_weights_only=False, mode=&#39;auto&#39;, period=1)</code></p>
<p>实际的使用中，将上述回调函数类传入<code>model.fit()</code>过程即可：</p>
<pre><code class="python">from keras.callbacks import ModelCheckpoint

model = Sequential()
model.add(Dense(10, input_dim=784, kernel_initializer=&#39;uniform&#39;))
model.add(Activation(&#39;softmax&#39;))
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)
checkpointer = ModelCheckpoint(filepath=&#39;/checkpoints/weights.hdf5&#39;, verbose=1, save_best_only=True)
model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])
</code></pre>
<h3 id="模型训练过程的监测"><a href="#模型训练过程的监测" class="headerlink" title="模型训练过程的监测"></a>模型训练过程的监测</h3><p>训练过程中，我们常常需要提取阶段性的信息来评估模型是否符合预期效果。</p>
<h4 id="tf-summary"><a href="#tf-summary" class="headerlink" title="tf.summary"></a><code>tf.summary</code></h4><p>首先创建想要观察指标的<code>tf.summary</code>对象：</p>
<pre><code class="python">with tf.name_scope(&quot;summaries&quot;):
  tf.summary.scalar(&quot;loss&quot;, self.loss)
  tf.summary.scalar(&quot;accuracy&quot;, self.accuracy)
  tf.summary.histogram(&quot;histogram loss&quot;, self.loss)
  # merge them all
  self.summary_op = tf.summary.merge_all()
</code></pre>
<p><code>tf.summary</code>是一种operation，因而可以随训练过程一同运行：<br><code>loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)</code></p>
<p>最后，将summary加入writer以写入文件：</p>
<pre><code class="python">with tf.Session() as sess:
  writer ​=​ tf​.​summary​.​FileWriter​(​&#39;./summary&#39;​,​ sess​.​graph)
  for​ index ​in​ range​(​num_train_steps​):
    writer.add_summary(summary, global_step=step)
  writer.close()
</code></pre>
<p>这样，就可以用TensorBoard监测我们关心的指标在训练过程中的变化情况。</p>
<h4 id="Keras封装：keras-callbacks-TensorBoard"><a href="#Keras封装：keras-callbacks-TensorBoard" class="headerlink" title="Keras封装：keras.callbacks.TensorBoard()"></a>Keras封装：<code>keras.callbacks.TensorBoard()</code></h4><p>Keras同样将TensorBoard封装成回调函数的形式，在模型训练时进行调用即可：</p>
<pre><code class="python">from keras.callbacks import TensorBoard

tensorboard = TensorBoard(log_dir=&quot;./logs&quot;)
model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[tensorboard]
</code></pre>
<h3 id="随机性的控制"><a href="#随机性的控制" class="headerlink" title="随机性的控制"></a>随机性的控制</h3><p>TensorFlow中随机性的控制分为operation和graph两个层面。</p>
<h4 id="Operation层面"><a href="#Operation层面" class="headerlink" title="Operation层面"></a>Operation层面</h4><p>在Operation层面中，建立随机seed之后，新建立的Session每一次调用<code>sess.run()</code>都会遵循同一随机状态：</p>
<pre><code class="python">c ​=​ tf​.​random_uniform​([],​​-​10​,​​10​,​ seed​=​2)

with​ tf​.​Session​()​​as​ sess:
  print​ sess​.​run​(​c) # &gt;&gt; 3.57493

with​ tf​.​Session​()​​as​ sess:
  print​ sess​.​run​(​c) # &gt;&gt; 3.57493
</code></pre>
<p>而且，不同的operation可以保存自己的seed:</p>
<pre><code class="python">c ​=​ tf​.​random_uniform​([],​​-​10​,​​10​,​ seed​=​1)
d ​=​ tf​.​random_uniform​([],​​-​10​,​​10​,​ seed​=​2)
with​ tf​.​Session​() ​​as​ sess:
  sess​.​run​(​c)
  sess​.​run​(​d)
</code></pre>
<h4 id="Graph层面"><a href="#Graph层面" class="headerlink" title="Graph层面"></a>Graph层面</h4><p>在Graph层面，整张图公用一个随机状态，多次运行同一图模型的计算，其随机状态保持一致。</p>
<pre><code class="python">import​ tensorflow ​as​ tf

tf​.​set_random_seed​(​2)

c ​=​ tf​.​random_uniform​([],​​-​10​,​​10)
d ​=​ tf​.​random_uniform​([],​​-​10​,​​10)
with​ tf​.​Session​()​​ as​ sess:
  sess​.​run​(​c)
  sess​.​run​(​d)
</code></pre>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> best practice </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python最佳实践：遍历列表]]></title>
      <url>https://blog.ddlee.cn/2017/06/29/Python%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E9%81%8D%E5%8E%86%E5%88%97%E8%A1%A8/</url>
      <content type="html"><![CDATA[<h3 id="enumerate-遍历索引和值"><a href="#enumerate-遍历索引和值" class="headerlink" title="enumerate(): 遍历索引和值"></a>enumerate(): 遍历索引和值</h3><p>对列表进行遍历操作时，常也要用到当前遍历项的索引值：</p>
<pre><code class="python">for i in range(len(flavor_list)):
  flavor = flaver[i]
  print(&#39;%d: %s&#39; % (i + 1, flavor))
</code></pre>
<p>这种写法既要取出列表长度，又要根据索引取列表值。但要改用<code>enumerate()</code>函数，则可同时取出索引值和遍历项值：</p>
<pre><code class="python">for i, flavor in enumerate(flavor_list):
  print(&#39;%d: %s&#39; % (i+1, flavor))
</code></pre>
<h3 id="zip-并行遍历多个列表"><a href="#zip-并行遍历多个列表" class="headerlink" title="zip(): 并行遍历多个列表"></a>zip(): 并行遍历多个列表</h3><p>我们有多个长度相同的列表，需要在同一索引下对遍历项值进行操作：</p>
<pre><code class="python">names = [&#39;Cecilia&#39;, &#39;Lise&#39;, &#39;Marie&#39;]
letters = [len(n) for n in names]

longest_name = None
max_letters = 0

for i, name in enumerate(names):
  count = letters[i]
  if count &gt; max_letters:
    longest_name = name
    max_letters = count
</code></pre>
<p>由上可见，name和letter通过索引值关联起来，而使用<code>zip()</code>函数，可免去根据索引取值的过程：</p>
<pre><code class="python">for name, count in zip(names, letters):
  if count &gt; max_letters:
    longest_name = name
    max_letters = count
</code></pre>
<p>要注意的是，当多个列表长度不一时，到达最短列表的末尾时，遍历停止。</p>
<p>而且，<code>enumerate()</code>和<code>zip()</code>返回的对象都是lazy generator，相对来说更加高效。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Programming </tag>
            
            <tag> best practice </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Accurate, Large Minibatch SGD: Training ImageNet in One Hour]]></title>
      <url>https://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/</url>
      <content type="html"><![CDATA[<p>这篇文章在各处都有很广泛的讨论，作为实验经验并不多的小白，将文中tricks只做些记录。</p>
<h3 id="Linear-Scaling-Rule"><a href="#Linear-Scaling-Rule" class="headerlink" title="Linear Scaling Rule"></a>Linear Scaling Rule</h3><p>进行大批量的Minibatch SGD时会有批量越大，误差越大的问题。本文提出的Linear Scaling Rule正是试图解决这一问题。</p>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>设想两个情景：一是在一次参数更新中使用kn个样本梯度，二是分为k次更新，每次取n个样本梯度。</p>
<p>第一种情景的参数更新公式：<br>
$$w_t+1^{(1)} = w_t^{(1)} - \mu^{(1)} \frac{1}{kn} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t)$$
</p>
<p>第二种情景的参数更新公式：<br>
$$w_t+k^{(2)} = w_t^{(2)} - \mu^{(2)} \frac{1}{n} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t+j)$$
</p>
<p>由上面可以看出，主要的区别是梯度平均时批量的大小不同，前者为kn，后者为每次n，更新k次。</p>
<p>再假设双重求和号内项变化不大时，为使情景二更新k次（即使用同样数量的样本）之后参数与情景一类似，我们自然要将学习速率$\mu$线性提升。</p>
<h3 id="Gradual-Warmup"><a href="#Gradual-Warmup" class="headerlink" title="Gradual Warmup"></a>Gradual Warmup</h3><p>上面提到的Linear Scaling Rule使用的假设是梯度变化不大。但在训练初期，参数随机初始化，梯度变化很大，因而Linear Scaling Rule不再适用。在实践中，可以使学习速率在初始时较小，在经过几个epoch训练后再升至与kn批量相应的大小。</p>
<h3 id="BN-statistics"><a href="#BN-statistics" class="headerlink" title="BN statistics"></a>BN statistics</h3><p>在分布式训练的系统中，对于BN中要估计的均值和方差，文中给出的建议是对所有worker上的样本计算均值和方差，而不是每个worker单独计算。</p>
<h3 id="Weight-Decay"><a href="#Weight-Decay" class="headerlink" title="Weight Decay"></a>Weight Decay</h3><p>由于weight decay的存在，Linear Scaling Rule最好用于学习速率，而非用于Loss Function</p>
<h3 id="Momentum-Correction"><a href="#Momentum-Correction" class="headerlink" title="Momentum Correction"></a>Momentum Correction</h3><p>加入Linear Scaling Rule之后，适用动量加速的SGD需要进行动量更正。</p>
<h3 id="Data-Shuffling"><a href="#Data-Shuffling" class="headerlink" title="Data Shuffling"></a>Data Shuffling</h3><p>在分布式的系统中，先进行Data Shuffling，再分配数据到每个worker上。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="external">Accurate, Large Minibatch SGD: Training ImageNet in One Hour</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[自用LaTeX中英文简历模板]]></title>
      <url>https://blog.ddlee.cn/2017/06/14/%E8%87%AA%E7%94%A8LaTeX%E4%B8%AD%E8%8B%B1%E6%96%87%E5%BB%BA%E7%AB%8B%E6%A8%A1%E6%9D%BF/</url>
      <content type="html"><![CDATA[<p>分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在<a href="https://www.sharelatex.com/templates/cv-or-resume/professional-cv" target="_blank" rel="external">ShareLaTeX</a>上分享的模板。</p>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li>Github仓库：<a href="https://github.com/ddlee96/latex_cv_template" target="_blank" rel="external">ddlee96/latex_cv_template</a></li>
<li>编译引擎： XeLaTeX</li>
<li>下载地址： <a href="https://github.com/ddlee96/latex_cv_template/releases/tag/0.1" target="_blank" rel="external">v0.1</a></li>
<li>压缩包内包含.tex文件和所用字体文件，解压后修改.tex文件再编译即可。</li>
<li>在Ubuntu 16.04, Texlive 2016环境下测试通过。</li>
<li>英文字体: Fontin，中文字体：方正兰亭黑</li>
</ul>
<h4 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h4><ul>
<li>.tex代码：Apache 2.0</li>
<li>字体： 仅供个人使用</li>
</ul>
<h4 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h4><h5 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h5><p><img src="https://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv-1.png" alt="en"></p>
<h5 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h5><p><img src="https://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv_zh-1.png" alt="zh"></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Individual Development </tag>
            
            <tag> LaTeX </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]On the Effects and Weight Normalization in GAN]]></title>
      <url>https://blog.ddlee.cn/2017/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-Effects-and-Weight-Normalization-in-GAN/</url>
      <content type="html"><![CDATA[<p>本文探索了参数标准化(Weight Normalization)这一技术在GAN中的应用。BN在mini-batch的层级上计算均值和方差，容易引入噪声，并不适用于GAN这种生成模型，而WN对参数进行重写，引入噪声更少。</p>
<p>我觉得本文的亮点有二：</p>
<h3 id="1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变"><a href="#1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变" class="headerlink" title="1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变"></a>1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变</h3><p>朴素的参数标准化层有如下的形式：<br>
$$y=\frac{{w}^{T}x}{\|w\|}$$
<br>文中称这样形式的层为“strict weight-normalized layer”。若将线性层换为这样的层，网络的表达能力会下降，因而需要添加如下的affine transformation:</p>

$$y=\frac{{w}^{T}x}{\|w\|} \gamma + \beta$$

<p>用于恢复网络的表达能力。</p>
<p>将上述变换带入ReLU，简化后可以得到如下T-ReLu:<br>$$TReLU_\alpha (x) = ReLU(x-\alpha) + \alpha$$</p>
<p>文章的一个重要结论是，在网络的最后一层加入affine transformation层之后，堆叠的“线性层+ReLU”与“strict weight-normalized layer + T-ReLU”表达能力相同（在附录中给出证明）。</p>
<p>下面L表示线性层，R表示ReLU，TR表示TReLU，A表示affine transformation，S表示上述的strict weight-normalized layer。</p>
<p>证明的大致思路是，在ReLU与线性层之间加入affine transformation层，由于线性层的存在，affine transformation带来的效果会被吸收（相当于多个线性层叠在一起还是线性层），网络表达能力不变。而”L+R+A”的结构可以等价于”S+TR+A”。如此递归下去，即可得到结论。个人认为相当于把线性层中的bias转嫁成了TReLU中的threshold（即$\alpha$）。</p>
<h3 id="2-提出对生成图形的评估指标"><a href="#2-提出对生成图形的评估指标" class="headerlink" title="2. 提出对生成图形的评估指标"></a>2. 提出对生成图形的评估指标</h3><p>生成式模型的生成效果常常难以评价。DcGAN给出的结果也是生成图片的对比。本文中提出一个评价生成效果的指标，且与人的主观评价一致。</p>
<p>评价的具体指标是生成图片与测试集图片的欧氏距离，评价的对象是生成器是Generator。有如下形式：</p>

$$\frac{1}{m} \sum_{i=1}^{m} min_z {\|G(z)-x^{(i)}\|}^2$$

<p>其中的$min$指使用梯度下降方法等使生成图片的效果最好。但事实上这样做开销很高。</p>
<h3 id="PyTorch实现"><a href="#PyTorch实现" class="headerlink" title="PyTorch实现"></a>PyTorch实现</h3><p>作者将他们的实现代码公布在了<a href="https://github.com/stormraiser/GAN-weight-norm" target="_blank" rel="external">GitHub</a>上。</p>
<p>下面是利用PyTorch对T-ReLU的实现：</p>
<pre><code class="python">class TPReLU(Module):

    def __init__(self, num_parameters=1, init=0.25):
        self.num_parameters = num_parameters
        super(TPReLU, self).__init__()
        self.weight = Parameter(torch.Tensor(num_parameters).fill_(init))
        self.bias = Parameter(torch.zeros(num_parameters))

    def forward(self, input):
        bias_resize = self.bias.view(1, self.num_parameters, *((1,) * (input.dim() - 2))).expand_as(input)
        return F.prelu(input - bias_resize, self.weight.clamp(0, 1)) + bias_resize
</code></pre>
<p>对 Weigh-normalized layer 的实现：</p>
<pre><code class="python">class WeightNormalizedLinear(Module):

    def __init__(self, in_features, out_features, scale=True, bias=True, init_factor=1, init_scale=1):
        super(WeightNormalizedLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = Parameter(torch.zeros(1, out_features))
        else:
            self.register_parameter(&#39;bias&#39;, None)
        if scale:
            self.scale = Parameter(torch.Tensor(1, out_features).fill_(init_scale))
        else:
            self.register_parameter(&#39;scale&#39;, None)
        self.reset_parameters(init_factor)

    def reset_parameters(self, factor):
        stdv = 1. * factor / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def weight_norm(self):
        return self.weight.pow(2).sum(1).add(1e-6).sqrt()

    def norm_scale_bias(self, input):
        output = input.div(self.weight_norm().transpose(0, 1).expand_as(input))
        if self.scale is not None:
            output = output.mul(self.scale.expand_as(input))
        if self.bias is not None:
            output = output.add(self.bias.expand_as(input))
        return output

    def forward(self, input):
        return self.norm_scale_bias(F.linear(input, self.weight))
</code></pre>
<p>观察上面的forward函数可以发现，TReLU添加bias这一习得参数，而weight-normalized layer中则对传入的weight进行了标准化。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1704.03971" target="_blank" rel="external">On the Effects and Weight Normalization in GAN</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> GAN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Large-Scale Evolution of Image Classifiers]]></title>
      <url>https://blog.ddlee.cn/2017/06/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Large-Scale-Evolution-of-Image-Classifiers/</url>
      <content type="html"><![CDATA[<p>深层网络在图片分类问题上表现优异，但网络结构的设计上并没有统一的指导。进化是构建深度网络架构的一种方式。利用本文的自动化方法得出的深度网络结构，已经能在CIFAR-10上取得可以跟人工设计的网络相媲美的结果</p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Evolution-Algorithm"><a href="#Evolution-Algorithm" class="headerlink" title="Evolution Algorithm"></a>Evolution Algorithm</h3><p>整个算法的核心是如下的tournament selection:</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/tournament.jpg" alt="tournament"></p>
<ul>
<li>population: 供筛选的群体</li>
<li>individual: 个体，带有指标fitness，特别地，指在CV集上的损失</li>
<li>worker: 筛选者，上帝</li>
</ul>
<ol>
<li><em>population</em> 中的 <em>individual</em> 均已在训练集上训练完毕，带有指标 <em>fitness</em></li>
<li><em>worker</em> 随机选择一对 <em>individual</em>，比较 <em>fitness</em>，较差的 <em>individual</em> 被舍弃</li>
<li>表现较好的 <em>individual</em> 成为parent，对其施加 <em>mutation</em> (变异)，得到 <em>child</em></li>
<li>训练 <em>child</em> 并在CV集上得到其 <em>fitness</em>，归还到 <em>population</em> 中</li>
</ol>
<h3 id="Encoding-and-Mutation"><a href="#Encoding-and-Mutation" class="headerlink" title="Encoding and Mutation"></a>Encoding and Mutation</h3><p>个体的网络结构和部分参数被编码为DNA。</p>
<p>能够施加的变异有：</p>
<ul>
<li>改变学习率</li>
<li>恒等（不变）</li>
<li>重设参数</li>
<li>加入卷积层</li>
<li>移除卷积层</li>
<li>更改卷积层的stride参数</li>
<li>更改卷积层的Channel参数</li>
<li>更改卷积核大小</li>
<li>加入skip连接（类似ResNet)</li>
<li>移除skip连接</li>
</ul>
<h3 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h3><p>计算方面采用了并行、异步、无锁的策略。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/comp.jpg" alt="comp"></p>
<p>建立约为 <em>population</em> 数1/4的 <em>worker</em>，分别运行于不同的机器上，之间独立异步。<em>population</em> 共享，若两个 <em>worker</em> 在一个 <em>individual</em> 上产生冲突，则后一个 <em>worker</em> 停止并等待再次尝试。</p>
<h3 id="Weight-Inheritance"><a href="#Weight-Inheritance" class="headerlink" title="Weight Inheritance"></a>Weight Inheritance</h3><p>除了架构之外，子模型还会继承父母模型未经变异影响的隐藏层参数（不仅是DNA中的），这样使子模型的训练时间大幅减小。</p>
<h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><p>文章的主要结果如下图：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/results.png" alt="results"></p>
<p>最右边的结构是在CIFAR-10上发现的最好（CV集准确度最高）的结构，左边两个是它的祖先。其中白色块相当于简单的线性层，彩色块则带有非线性激活，可以看到，不同于人工设计的网络，某一线性层之后可能包含多个非线性层。</p>
<p>另外，利用本文的模型，也在CIFAR-100上做了实验，可以达到76.3%的准确率，一定程度上说明了算法的扩展性。</p>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/popu.png" alt="popu"></p>
<p>上图说明随着 <em>population</em> 规模和训练步数的增加，模型的整体水平在变好。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/mutation.png" alt="mutation"></p>
<p>在模型陷入局部最优值时，提高变异率和重设参数会使群体继续进化。这是由于变异中包含恒等映射等不改变模型架构的变异类型，再加上weight Inheritance，一些子模型只是训练次数比其他模型多很多的“活化石”。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Google I/O时就提到了自动筛选最优网络结构，但没有公布论文。但将网络结构自动化，必定是未来的方向。个人认为，ResNet就相当于自动化网络深度（一些层实际上被跳过了），而Inception单元似乎包含了太多的先验，而且也没有逻辑上的证据说明这样的结构更有效。网络结构本身就是先验信息，而要达到通用的人工智能，这些先验也必须由模型自行发觉。</p>
<p>强化学习本身也是一个进化过程，应该也有相关的工作将强化学习的框架应用于网络结构的学习上。</p>
<p>更进一步地，若数据是一阶信息，深度网络的隐藏层学到的表示是二阶信息，深度网络的结构则是三阶信息，从一阶到二阶的框架是不是都可以移植到二阶到三阶上来？关键之处在于我们还没有描述好深度网络的结构空间，但就现在的发展看，深度网络的一些基本结构(conv, BN)等，已经被作为基本单元（离散的）来进行构建和筛选了，也就是说，所有深度网络构成的空间之性质如何，还有大量的工作可以做。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">Large-Scale Evolution of Image, Classifiers</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> CNN </tag>
            
            <tag> autoML </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]An Analysis of Deep Neural Network Models for Practical Applications]]></title>
      <url>https://blog.ddlee.cn/2017/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/</url>
      <content type="html"><![CDATA[<p>本文是对现有（论文发表于2016年5月）深度网络的比较，从以下方面入手：</p>
<ul>
<li>accuracy</li>
<li>memory footprint</li>
<li>parameters</li>
<li>operations count</li>
<li>inference time</li>
<li>power consumption</li>
</ul>
<p>以下图片各模型的着色是统一的：蓝色是Inception系，绿色是VGG系，粉色是ResNet系，黄色为AlexNet系。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/top1.png" alt="top1"></p>
<p>上图是Top1准确率与模型参数数、操作数的关系。可以看到Inception系列网络以较少的参数取得相对高的准确率，而VGG系则在这一点上表现很差。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/infer-batch.png" alt="infer-batch"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/power-batch.png" alt="power-batch"></p>
<p>上面两图分别是推断耗时和电量消耗与批量大小的关系。可以看到，两者均与批量大小无明显的相关关系。但电量消耗在不同的模型之间也非常类似，而推断时间与模型结构关系很大（VGG再次尴尬）。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/mem-batch.png" alt="mem-batch"></p>
<p>上图展示了模型占用内存大小与批量大小的关系，大部分网络都有相对固定的内存占用，随后随批量大小的上扬而上涨。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-infer.png" alt="infer-ops"></p>
<p>从上图可以发现推断耗时和模型的操作数大体上呈现线性关系。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-power.png" alt="ops-power"></p>
<p>电量消耗与模型的参数数、操作数并没有明显的相关性。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/accuracy-infer.png" alt="accuracy-infer"></p>
<p>注意，上图中点的大小代表模型操作数，横轴代表推断效率，纵轴表示准确率。灰色区域表示模型获得了额外的推断效率或准确率，而白色区域代表非最优。</p>
<p>操作数越多的模型推断效率越低，大部分模型都落在相对平衡的边界上，VGG和小批量情形下的AlexNet落在了非最优区域。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从这篇论文的比较中可以看到，在特定的任务中对网络特定结构的设计（如Inception单元），即加入更强的先验知识，比堆叠网络层数更有效。深度网络还是需要人类的指导才能发挥更大的作用。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1605.07678" target="_blank" rel="external">An Analysis of Deep Neural Network Models for Practical Applications</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习和分布式表示]]></title>
      <url>https://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/</url>
      <content type="html"><![CDATA[<p>本文的两个主要参考资料：</p>
<ol>
<li>Yoshua Bengio在2016年九月<a href="https://www.bayareadlschool.org/" target="_blank" rel="external">Deep Learning School</a>的演讲Foundations and Challenges of Deep Learning。<a href="https://www.youtube.com/watch?v=11rsu_WwZTc" target="_blank" rel="external">YouTube</a></li>
<li><a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning</a>, Goodfellow et al, Section 15.4</li>
</ol>
<h3 id="从机器学习到人工智能"><a href="#从机器学习到人工智能" class="headerlink" title="从机器学习到人工智能"></a>从机器学习到人工智能</h3><p>在演讲中，Bengio提到从机器学习到人工智能有五个关键的飞跃：</p>
<ol>
<li>Lots of data</li>
<li>Very flexible models</li>
<li>Enough computing power</li>
<li>Powerful priors that can defeat the curse of dimensionality</li>
<li>Computationally efficient inference</li>
</ol>
<p>第一点已经发生，到处都提大数据，到处都在招数据分析师。<br>我在读高中时，就曾预感数据将是新时代的石油和煤炭，因为数据正是人类社会经验的总结，数据带来的知识和见解将在驱动社会进步中发挥越来越重要的作用，而自己要立志成为新时代的矿工。</p>
<p>第二点在我看来有两个例子，一是核技巧，通过核函数对分布空间的转换，赋予了模型更强大的表述能力；二是深度神经网络，多层的框架和非线性的引入使得模型理论上可以拟合任意函数。</p>
<p>第三点，借云计算的浪潮，计算力不再是一项资产而是一项可供消费的服务，我们学生也可以廉价地接触到根本负担不起的计算力资源。而GPU等芯片技术的进步也为AI的浩浩征程添砖加瓦。</p>
<p>第五点，近期发布的Tensorflow Lite和Caffe2等工具也有助于越来越多地将计算任务分配在终端上进行，而非作为一个发送与接收器。</p>
<p>最后第四点，也是这篇文章的中心话题：借助分布式表示的强大能力，深度学习正尝试解决维度带来的灾难。</p>
<h3 id="没有免费的午餐"><a href="#没有免费的午餐" class="headerlink" title="没有免费的午餐"></a>没有免费的午餐</h3><p>简单说，没有免费的午餐定理指出找不到一个在任何问题上都表现最优的模型/算法。不同的模型都有其擅长的问题，这由该模型建立时引入的先验知识决定。</p>
<p>那么，深度学习加入的先验知识是什么？</p>
<p>Bengio用的词是Compositionality，即复合性，<em>某一概念之意义由其组成部分的意义以及组合规则决定</em>。复合性的原则可以用于高效地描述我们的世界，而深度学习模型中隐藏的层正是去学习其组成部分，网络的结构则代表了组合规则。这正是深度学习模型潜在的信念。</p>
<h3 id="分布式表示带来的指数增益"><a href="#分布式表示带来的指数增益" class="headerlink" title="分布式表示带来的指数增益"></a>分布式表示带来的指数增益</h3><p>分布式表示(Distributed Representation)是连接主义的核心概念，与复合性的原理相合。整体由组成它的个体及其组合来表示。请看下面的例子：</p>
<p><img src="https://static.ddlee.cn/static/img/深度学习和分布式表示/distributed.webp" alt="Distributed"></p>
<p>描述一个形状，我们将其分解为不同的特征来表述。分布式表示是一种解耦，它试图复杂的概念分离成独立的部分。而这也引出了分布式表示带来的缺点：隐藏层学到的分解特征难以得到显式的解释。</p>
<p>传统的机器学习算法，如K-Means聚类、决策树等，大多使用的是非分布式表示，即用特定的参数去描述特定的区域。如K-Means聚类，我们要划分多少区域，就需要有多少个中心点。因而，这类算法的特点是，随着参数个数的提升，其能描述的概念线性增长。</p>
<p><img src="https://static.ddlee.cn/static/img/深度学习和分布式表示/non-dist.png" alt="non-dist"><br>使用分布式表示的深度网络，则可以享受到指数级的增益，即，随着参数个数的提升，其表述能力是指数级的增长。具有$k$个值的$n$个特征，可以描述${k}^{n}$个不同的概念。</p>
<p><img src="https://static.ddlee.cn/static/img/深度学习和分布式表示/dist.png" alt="dist"></p>
<h3 id="分布式表示在泛化上的优势"><a href="#分布式表示在泛化上的优势" class="headerlink" title="分布式表示在泛化上的优势"></a>分布式表示在泛化上的优势</h3><p>分布式的想法还可以得到额外的泛化优势。通过重新组合在原有数据中抽离出来的特征，可以表示得到原有数据中不存在的实例。在Radford et al.的工作中，生成模型区习得了性别，并能从“戴眼镜的男人”-“男人”+“女人”=“戴眼镜的女人”这样的抽象概念表达式中生成实例。</p>
<p><img src="https://static.ddlee.cn/static/img/深度学习和分布式表示/generative.png" alt="generative"></p>
<h3 id="分布式表示与巻积神经网络"><a href="#分布式表示与巻积神经网络" class="headerlink" title="分布式表示与巻积神经网络"></a>分布式表示与巻积神经网络</h3><p>巻积神经网络不同的滤波器习得的特征可以为分布式表示的概念分解这一特性提供一些例子。下图是VGG16不同滤波器得到结果的可视化表示，<br>出自Francois Chollet的博文<a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" target="_blank" rel="external">How convolutional neural networks see the world</a></p>
<p><img src="https://static.ddlee.cn/static/img/深度学习和分布式表示/filters.jpg" alt="filters"></p>
<p>可以看到，浅层的滤波器学到的是简单的颜色、线条走向等特征，较深的滤波器学到复杂的纹理。</p>
<h3 id="量子计算机与分布式表示"><a href="#量子计算机与分布式表示" class="headerlink" title="量子计算机与分布式表示"></a>量子计算机与分布式表示</h3><p>在我看来，量子计算机的激动人心之处也在于其表示能力。一个量子态可以表示原先两个静态表示的信息，原先需要8个单位静态存储表示的信息只需要3个量子态单位即可表示，这也是指数级的增益。在这一点上，计算模型和概念模型已然殊途同归。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从经验中总结原则，用原则生成套路，正是我们自己处理和解决新问题的途径。通过解耦得到的信息来消除未知和不确定性，是我们智能的一部分。我们眼中的世界，只是适合我们的一种表示而已。也许，真正的人工智能到来那一刻，会是我们创造的机器“理解”了自己的表示系统之时——我们所关注的可解释性，也就无关紧要了。</p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]On-the-fly Operation Batching in Dynamic Computation Graphs]]></title>
      <url>https://blog.ddlee.cn/2017/05/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/</url>
      <content type="html"><![CDATA[<p>基于动态图的深度学习框架如<code>Pytorch</code>,<code>DyNet</code>提供了更为灵活的结构和数据维度的选择，但要求开发者自行将数据批量化，才能最大限度地发挥框架的并行计算优势。</p>
<h2 id="当前的状况：灵活的结构与高效计算"><a href="#当前的状况：灵活的结构与高效计算" class="headerlink" title="当前的状况：灵活的结构与高效计算"></a>当前的状况：灵活的结构与高效计算</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/comparison.png" alt="左图为循环结构，右图将序列补齐，批量化"><br>左图为循环结构，右图将序列补齐，批量化</p>
<ol>
<li>灵活的结构和数据输入维度，采用朴素的循环结构实现，但不高效，因为尽管维度不同，在循环内数据接受的是同样的操作。</li>
</ol>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/loop.png" alt="loop"></p>
<ol>
<li>对数据做“Padding”，即用傀儡数据将输入维度对齐，进而实现向量化，但这种操作对开发者并不友好，会使开发者浪费掉很多本该投入到结构设计等方面的精力。</li>
</ol>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/padding.png" alt="padding"></p>
<h2 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h2><h3 id="三个部分"><a href="#三个部分" class="headerlink" title="三个部分"></a>三个部分</h3><ol>
<li>Graph Definition</li>
<li>Operation Batching</li>
<li>Computation</li>
</ol>
<p>第一步和第三步在当前已被大部分深度学习框架较好地实现。主要特点是，构建计算图与计算的分离，即”Lazy Evaluation”。比如在<code>Tensorflow</code>中，一个抽象层负责解析计算图各节点之间的依赖，决定执行计算的顺序，而另一个抽象层则负责分配计算资源。</p>
<h3 id="Operation-Batching"><a href="#Operation-Batching" class="headerlink" title="Operation Batching"></a>Operation Batching</h3><h4 id="Computing-compatibility-groups"><a href="#Computing-compatibility-groups" class="headerlink" title="Computing compatibility groups"></a>Computing compatibility groups</h4><p>这一步是建立可以批量化计算的节点组。具体做法是，给每一个计算节点建立 <em>signature</em>，用于描述节点计算的特性，文中举出了如下几个例子:</p>
<ol>
<li>Component-wise operations: 直接施加在每个张量元素上的计算，跟张量的维度无关，如$tanh$,$log$</li>
<li>Dimension-sensitive operations: 基于维度的计算，如线性传递$Wh+b$，要求$W$和$h$维度相符，<em>signature</em> 中要包含维度信息</li>
<li>Operations with shared elements: 包含共享元素的计算，如共享的权值$W$</li>
<li>Unbatchable operations: 其他</li>
</ol>
<h4 id="Determining-execution-order"><a href="#Determining-execution-order" class="headerlink" title="Determining execution order"></a>Determining execution order</h4><p>执行顺序要满足两个目标：</p>
<ol>
<li>每一节点的计算要在其依赖之后</li>
<li>带有同样 <em>signature</em> 且没有依赖关系的节点放在同一批量执行</li>
</ol>
<p>但在一般情况下找到最大化批量规模的执行顺序是个NP问题。有如下两种策略：</p>
<ol>
<li>Depth-based Batching: 库<code>Tensorflow Fold</code>中使用的方法。某一节点的深度定义为其子节点到其本身的最大长度，同一深度的节点进行批量计算。但由于输入序列长度不一，可能会错失一些批量化的机会。</li>
<li>Agenda-based Batching: 本文的方法，核心的想法是维护一个 <em>agenda</em> 序列，所有依赖已经被解析的节点入列，每次迭代时从 <em>agenda</em> 序列中按 <em>signature</em> 相同的原则取出节点进行批量计算。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章选取了四个模型：BiLSTM, BiLSTM w/char, Tree-structured LSTMs, Transition-based Dependency Parsing。</p>
<p>实验结果：（单位为Sentences/second）<br><img src="https://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/result.png" alt="result"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本来读到题目还是蛮惊喜的，期待的是从模型构建的角度解决序列长度不一带来的计算上的不便。但通读下来发现是在计算图的计算这一层面进行的优化，有些失望但也感激，作者使用<code>DyNet</code>框架实现了他们的方法，希望自己也可以为<code>Pytorch</code>等框架该算法的实现出一份力。</p>
<p>感谢这些开源的框架，正一步步拉近人类构建模型和机器高效计算之间的距离。</p>
<p>论文链接：<a href="http://arxiv.org/abs/1705.07860" target="_blank" rel="external">On-the-fly Operation Batching in Dynamic Computaion Graphs</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[LSTM:Pytorch实现]]></title>
      <url>https://blog.ddlee.cn/2017/05/29/LSTM-Pytorch%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p>本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM是一种循环神经网络，适用于对序列化的输入建模。Chris Olah的这篇<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">文章</a>细致地解释了一个LSTM单元的运作方式，建议阅读。</p>
<h3 id="两个想法"><a href="#两个想法" class="headerlink" title="两个想法"></a>两个想法</h3><h4 id="Gate：信息流动的闸门"><a href="#Gate：信息流动的闸门" class="headerlink" title="Gate：信息流动的闸门"></a>Gate：信息流动的闸门</h4><p>$$i<em>t = sigmoid(W</em>{xi} x<em>t  +  W</em>{hi}h_{t-1} + b_i)$$<br>$$f<em>t = sigmoid(W</em>{xf} x<em>t  +  W</em>{hf}h_{t-1} + b_f)$$<br>$$o<em>t = sigmoid(W</em>{xo} x<em>t  +  W</em>{ho}h_{t-1} + b_o)$$<br>$x$ 表示输入，$h$表示隐藏状态，用$sigmoid$函数将输入二者的传递结果映射到$（0,1)$上，分别赋予输入门、遗忘门、输出门的含义，来控制不同神经单元（同一神经元不同时间点的状态）之间信息流动。</p>
<h4 id="Cell：记忆池"><a href="#Cell：记忆池" class="headerlink" title="Cell：记忆池"></a>Cell：记忆池</h4><p>$$c_t = f<em>t \odot c</em>{t - 1} + i<em>t \odot tanh(W</em>{xc} x<em>t  +  W</em>{hc}h_{t-1} + b_c)\<br>h_t = o_t \odot tanh(c_t)$$<br>$h$表示隐藏状态，$C$表示记忆池，通过Gate，上一单元（状态）的信息有控制地遗忘，当前的输入有控制地流入，记忆池中的信息有控制地流入隐藏状态。</p>
<h3 id="与普通RNN的对比"><a href="#与普通RNN的对比" class="headerlink" title="与普通RNN的对比"></a>与普通RNN的对比</h3><p><img src="https://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM3-SimpleRNN.png" alt="RNN"><br>普通RNN只有一个自更新的隐藏状态单元。</p>
<p><img src="https://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM.jpg" alt="LSTM"><br>LSTM增加了记忆池Cell，并通过几个Gate将信息有控制地更新在记忆池中，并通过记忆池中的信息来决定隐藏状态。</p>
<h2 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h2><p>下面是手动实现LSTM的代码，继承了基类<code>nn.Module</code>。</p>
<pre><code class="python">import torch.nn as nn
import torch
from torch.autograd import Variable

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, cell_size, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.cell_size = cell_size
        self.gate = nn.Linear(input_size + hidden_size, cell_size)
        self.output = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
        self.tanh = nn.Tanh()
        self.softmax = nn.LogSoftmax()

    def forward(self, input, hidden, cell):
        combined = torch.cat((input, hidden), 1)
        f_gate = self.gate(combined)
        i_gate = self.gate(combined)
        o_gate = self.gate(combined)
        f_gate = self.sigmoid(f_gate)
        i_gate = self.sigmoid(i_gate)
        o_gate = self.sigmoid(o_gate)
        cell_helper = self.gate(combined)
        cell_helper = self.tanh(cell_helper)
        cell = torch.add(torch.mul(cell, f_gate), torch.mul(cell_helper, i_gate))
        hidden = torch.mul(self.tanh(cell), o_gate)
        output = self.output(hidden)
        output = self.softmax(output)
        return output, hidden, cell

    def initHidden(self):
        return Variable(torch.zeros(1, self.hidden_size))

    def initCell(self):
        return Variable(torch.zeros(1, self.cell_size))
</code></pre>
<p>几个关键点：</p>
<ol>
<li>Tensor的大小</li>
<li>信息的传递顺序</li>
</ol>
<h2 id="Pytorch-Module"><a href="#Pytorch-Module" class="headerlink" title="Pytorch Module"></a>Pytorch Module</h2><p>Pytorch库本身对LSTM的实现封装了更多功能，类和函数的组织也非常有借鉴意义。我对其实现的理解基于以下两点展开：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p>下面开始按图索骥，源码见<a href="https://github.com/pytorch/pytorch/tree/master/torch" target="_blank" rel="external">GitHub</a>。</p>
<h4 id="LSTM类"><a href="#LSTM类" class="headerlink" title="LSTM类"></a>LSTM类</h4><p>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" target="_blank" rel="external">nn/modules/rnn.py</a></p>
<pre><code class="Python"># nn/modules/rnn.py
class RNNBase(Module):
  def __init__(self, mode, input_size, output_size):
      pass
  def forward(self, input, hx=None):
      if hx is None:
          hx = torch.autograd.Variable()
      if self.mode == &#39;LSTM&#39;:
          hx = (hx, hx)
      func = self._backend.RNN() #!!!
      output, hidden = func(input, self.all_weights, hx) #!!!
      return output, hidden

class LSTM(RNNBase):
    def __init__(self, *args, **kwargs):
        super(LSTM, self).__init__(&#39;LSTM&#39;, *args, **kwargs)
</code></pre>
<ol>
<li><code>LSTM</code>类只是<code>RNNBase</code>类的一个装饰器。</li>
<li>在基类<code>nn.Module</code>中，把<code>__call__()</code>定义为调用<code>forward()</code>方法，因而真正的功能实现在<code>_backend.RNN()</code>中</li>
</ol>
<h4 id="AutogradRNN函数"><a href="#AutogradRNN函数" class="headerlink" title="AutogradRNN函数"></a>AutogradRNN函数</h4><p>下面寻找<code>_backend.RNN</code>。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/backends/thnn.py" target="_blank" rel="external">nn/backends/thnn.py</a></p>
<pre><code class="python"># nn/backends/thnn.py
def _initialize_backend():
    from .._functions.rnn import RNN, LSTMCell
</code></pre>
<p>原来，<code>_backend</code>也是索引。</p>
<p>终于找到<code>RNN()</code>函数。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py" target="_blank" rel="external">nn/_functions/rnn.py</a></p>
<pre><code class="python"># nn/_functions/rnn.py
def RNN(*args, **kwargs):
    def forward(input, *fargs, **fkwargs):
        func = AutogradRNN(*args, **kwargs)
        return func(input, *fargs, **fkwargs)
    return forward

def AutogradRNN(mode, input_size, hidden_size):
    cell = LSTMCell
    rec_factory = Recurrent
    layer = (rec_factory(cell),)
    func = StackedRNN(layer, num_layers)
    def forward(input, weight, hidden):
        nexth, output = func(input, hidden, weight)
        return output, nexth
    return forward
</code></pre>
<ol>
<li><code>RNN()</code>是一个装饰器，根据是否有<code>cudnn</code>库决定调用<code>AutogradRNN()</code>还是<code>CudnnRNN()</code>，这里仅观察<code>AutogradRNN()</code></li>
<li><code>AutogradRNN()</code>选用了<code>LSTMCell</code>，用<code>Recurrent()</code>函数处理了<code>Cell</code>构成<code>Layer</code>，再将<code>Layer</code>传入<code>StackedRNN()</code>函数</li>
<li><code>RNN()</code>和<code>AutogradRNN()</code>返回的都是其<code>forward()</code>函数句柄</li>
</ol>
<p>下面是<code>Recurrent()</code>函数：</p>
<pre><code class="python">def Recurrent(inner):
    def forward(input, hidden, weight):
        output = []
        steps = range(input.size(0) - 1, -1, -1)
        for i in steps:
            hidden = inner(input[i], hidden, *weight)
            output.append(hidden[0])
        return hidden, output
    return forward
</code></pre>
<ol>
<li><code>Recurrent()</code>函数实现了“递归”的结构，根据输入的大小组合<code>Cell</code>，完成了隐藏状态和参数的迭代。</li>
<li><code>Recurrent()</code>函数将<code>Cell(inner)</code>组合为<code>Layer</code>。</li>
</ol>
<h4 id="StackedRNN-函数"><a href="#StackedRNN-函数" class="headerlink" title="StackedRNN()函数"></a>StackedRNN()函数</h4><pre><code class="python">def StackedRNN(inners, num_layers):
    num_directions = len(inners)
    total_layers = num_layers * num_directions
    def forward(input, hidden, weight):
        next_hidden = []
        hidden = list(zip(*hidden))
        for i in range(num_layers):
          all_output = []
          for j, inner in enumerate(inners):
              hy, output = inner(input, hidden[l], weight[l])
              next_hidden.append(hy)
              all_output.append(output)
          input = torch.cat(all_output, input.dim() - 1)
        next_h, next_c = zip(*next_hidden)
        next_hidden = (torch.cat(next_h, 0).view(total_layers, *next_h[0].size()),
                  torch.cat(next_c, 0).view(total_layers, *next_c[0].size()))
        return next_hidden, input
    return forward
</code></pre>
<ol>
<li><code>StackedRNN()</code>函数将<code>Layer(inner)</code>组合为栈</li>
</ol>
<p>最后的最后，一个基本的LSTM单元内的计算由<code>LSTMCell()</code>函数实现。</p>
<h4 id="LSTMCell-函数"><a href="#LSTMCell-函数" class="headerlink" title="LSTMCell()函数"></a>LSTMCell()函数</h4><pre><code class="python">def LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):
    if input.is_cuda:
        igates = F.linear(input, w_ih)
        hgates = F.linear(hidden[0], w_hh)
        state = fusedBackend.LSTMFused()
        return state(igates, hgates, hidden[1]) if b_ih is None else state(igates, hgates, hidden[1], b_ih, b_hh)

    hx, cx = hidden
    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)

    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)

    ingate = F.sigmoid(ingate)
    forgetgate = F.sigmoid(forgetgate)
    cellgate = F.tanh(cellgate)
    outgate = F.sigmoid(outgate)

    cy = (forgetgate * cx) + (ingate * cellgate)
    hy = outgate * F.tanh(cy)

    return hy, cy
</code></pre>
<p>观察上面的代码，即是LSTM的基本信息传递公式。至此，我们的旅程完成。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><blockquote>
<p>没有什么是增加一层抽象不能解决的，如果不能，那就再加一层。</p>
</blockquote>
<p>重复一下我对上述代码的理解：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p><img src="https://static.ddlee.cn/static/img/LSTM-Pytorch实现/str.jpg" alt="str"></p>
<p>如洋葱一般，我们剥到最后，发现处理的信息正是输入、隐藏状态和LSTM单元几个控制门的参数。在一层一层的抽象之中，Pytorch在不同的层面处理了不同的参数，保证了扩展性和抽象层之间的解耦。</p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> AI </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Pandas速度优化]]></title>
      <url>https://blog.ddlee.cn/2017/05/28/Pandas%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<p>本文主要内容取自Sofia Heisler在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=HN5d490_KKk" target="_blank" rel="external">No More Sad Pandas Optimizing Pandas Code for Speed and Efficiency</a>，讲稿代码和幻灯片见<a href="https://github.com/sversh/pycon2017-optimizing-pandas" target="_blank" rel="external">GitHub</a>。</p>
<h2 id="Set-Up"><a href="#Set-Up" class="headerlink" title="Set Up"></a>Set Up</h2><h4 id="示例数据"><a href="#示例数据" class="headerlink" title="示例数据"></a>示例数据</h4><table>
<thead>
<tr>
<th></th>
<th>ean_hotel_id</th>
<th>name</th>
<th>address1</th>
<th>city</th>
<th>state_province</th>
<th>postal_code</th>
<th>latitude</th>
<th>longitude</th>
<th>star_rating</th>
<th>high_rate</th>
<th>low_rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>269955</td>
<td>Hilton Garden Inn Albany/SUNY Area</td>
<td>1389 Washington Ave</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68751</td>
<td>-73.81643</td>
<td>3.0</td>
<td>154.0272</td>
<td>124.0216</td>
</tr>
<tr>
<td>1</td>
<td>113431</td>
<td>Courtyard by Marriott Albany Thruway</td>
<td>1455 Washington Avenue</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68971</td>
<td>-73.82021</td>
<td>3.0</td>
<td>179.0100</td>
<td>134.0000</td>
</tr>
<tr>
<td>2</td>
<td>108151</td>
<td>Radisson Hotel Albany</td>
<td>205 Wolf Rd</td>
<td>Albany</td>
<td>NY</td>
<td>12205</td>
<td>42.72410</td>
<td>-73.79822</td>
<td>3.0</td>
<td>134.1700</td>
<td>84.1600</td>
</tr>
</tbody>
</table>
<h4 id="示例函数：Haversine-Distance"><a href="#示例函数：Haversine-Distance" class="headerlink" title="示例函数：Haversine Distance"></a>示例函数：Haversine Distance</h4><pre><code class="Python">def haversine(lat1, lon1, lat2, lon2):
    miles_constant = 3959
    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    mi = miles_constant * c
    return mi
</code></pre>
<h2 id="优化它之前，先测量它"><a href="#优化它之前，先测量它" class="headerlink" title="优化它之前，先测量它"></a>优化它之前，先测量它</h2><h4 id="IPython-Notebook的Magic-Command-timeit"><a href="#IPython-Notebook的Magic-Command-timeit" class="headerlink" title="IPython Notebook的Magic Command: %timeit"></a>IPython Notebook的Magic Command: <code>%timeit</code></h4><p>既可以测量某一行代码的执行时间，又可以测量整个单元格里代码快的执行时间。</p>
<h4 id="Package-line-profiler"><a href="#Package-line-profiler" class="headerlink" title="Package: line_profiler"></a>Package: line_profiler</h4><p>记录每行代码的执行次数和执行时间。</p>
<p>在IPython Notebook中使用时，先运行<code>%load_ext line_profiler</code>， 之后可以用<code>%lprun -f [function name]</code>命令记录指定函数的执行情况。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h4 id="对行做循环-Baseline"><a href="#对行做循环-Baseline" class="headerlink" title="对行做循环(Baseline)"></a>对行做循环(Baseline)</h4><pre><code class="python">%%timeit
haversine_series = []
for index, row in df.iterrows():
    haversine_series.append(haversine(40.671, -73.985,\
                                      row[&#39;latitude&#39;], row[&#39;longitude&#39;]))
df[&#39;distance&#39;] = haversine_series
</code></pre>
<p>Output:</p>
<pre><code>197 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><h4 id="pd-DataFrame-apply-方法"><a href="#pd-DataFrame-apply-方法" class="headerlink" title="pd.DataFrame.apply()方法"></a>pd.DataFrame.apply()方法</h4><pre><code class="python">%lprun -f haversine \
df.apply(lambda row: haversine(40.671, -73.985,\
                               row[&#39;latitude&#39;], row[&#39;longitude&#39;]), axis=1)
</code></pre>
<p>Output:</p>
<pre><code>90.6 ms ± 7.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.049982 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2      1631         1535      0.9      3.1      miles_constant = 3959
     3      1631        16602     10.2     33.2      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4      1631         2019      1.2      4.0      dlat = lat2 - lat1
     5      1631         1143      0.7      2.3      dlon = lon2 - lon1
     6      1631        18128     11.1     36.3      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7      1631         7857      4.8     15.7      c = 2 * np.arcsin(np.sqrt(a))
     8      1631         1708      1.0      3.4      mi = miles_constant * c
     9      1631          990      0.6      2.0      return mi
</code></pre><p>观察Hits这一列可以看到，<code>apply()</code>方法还是将函数一行行地应用于每行。</p>
<h4 id="向量化：将pd-Series传入函数"><a href="#向量化：将pd-Series传入函数" class="headerlink" title="向量化：将pd.Series传入函数"></a>向量化：将pd.Series传入函数</h4><pre><code>%lprun -f haversine haversine(40.671, -73.985,\
                              df[&#39;latitude&#39;], df[&#39;longitude&#39;])
</code></pre><p>Output:</p>
<pre><code>2.21 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.008601 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.0      miles_constant = 3959
     3         1          838    838.0      9.7      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1          597    597.0      6.9      dlat = lat2 - lat1
     5         1          572    572.0      6.7      dlon = lon2 - lon1
     6         1         5033   5033.0     58.5      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1         1060   1060.0     12.3      c = 2 * np.arcsin(np.sqrt(a))
     8         1          496    496.0      5.8      mi = miles_constant * c
     9         1            2      2.0      0.0      return mi
</code></pre><p>向量化之后，函数内的每行操作只被访问一次，达到了行结构上的并行。</p>
<h3 id="向量化：将np-array传入函数"><a href="#向量化：将np-array传入函数" class="headerlink" title="向量化：将np.array传入函数"></a>向量化：将np.array传入函数</h3><pre><code class="python">%lprun -f haversine df[&#39;distance&#39;] = haversine(40.671, -73.985,\
                        df[&#39;latitude&#39;].values, df[&#39;longitude&#39;].values)
</code></pre>
<p>Output：</p>
<pre><code>370 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.001382 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.2      miles_constant = 3959
     3         1          292    292.0     21.1      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1           40     40.0      2.9      dlat = lat2 - lat1
     5         1           29     29.0      2.1      dlon = lon2 - lon1
     6         1          815    815.0     59.0      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1          183    183.0     13.2      c = 2 * np.arcsin(np.sqrt(a))
     8         1           18     18.0      1.3      mi = miles_constant * c
     9         1            2      2.0      0.1      return mi
</code></pre><p>相比<code>pd.Series</code>，<code>np.array</code>不含索引等额外信息，因而更加高效。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><table>
<thead>
<tr>
<th>Methodology</th>
<th>Avg.    single    run    time</th>
<th>Marginal    performance    improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Looping    with    iterrows</td>
<td>184.00</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Looping    with    apply</td>
<td>78.10</td>
<td>2.4x</td>
</tr>
<tr>
<td>Vectorization    with    Pandas    series</td>
<td>1.79</td>
<td>43.6x</td>
</tr>
<tr>
<td>Vectorization    with    NumPy    arrays</td>
<td>0.37</td>
<td>4.8x</td>
</tr>
</tbody>
</table>
<p>通过上面的对比，我们比最初的baseline快了近500倍。最大的提升来自于向量化。因而，实现的函数能够很方便地向量化是高效处理的关键。</p>
<h2 id="用Cython优化"><a href="#用Cython优化" class="headerlink" title="用Cython优化"></a>用<code>Cython</code>优化</h2><p><code>Cython</code>可以将<code>python</code>代码转化为<code>C</code>代码来执行，可以进行如下优化（静态化变量类型，调用C函数库）</p>
<pre><code class="python">%load_ext cython

%%cython -a
# Haversine cythonized
from libc.math cimport sin, cos, acos, asin, sqrt

cdef deg2rad_cy(float deg):
    cdef float rad
    rad = 0.01745329252*deg
    return rad

cpdef haversine_cy_dtyped(float lat1, float lon1, float lat2, float lon2):
    cdef:
        float dlon
        float dlat
        float a
        float c
        float mi

    lat1, lon1, lat2, lon2 = map(deg2rad_cy, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    mi = 3959 * c
    return mi
</code></pre>
<p>嵌套于循坏中：</p>
<pre><code class="python">%timeit df[&#39;distance&#39;] =\
df.apply(lambda row: haversine_cy_dtyped(40.671, -73.985,\
                              row[&#39;latitude&#39;], row[&#39;longitude&#39;]), axis=1)
</code></pre>
<p>Output:</p>
<pre><code>10 loops, best of 3: 68.4 ms per loop
</code></pre><p>可以看到，<code>Cython</code>确实带来速度上的提升，但效果不及向量化（并行化）。</p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Data Science </tag>
            
            <tag> Programming </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python可视化工具指引]]></title>
      <url>https://blog.ddlee.cn/2017/05/28/Python%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E6%8C%87%E5%BC%95/</url>
      <content type="html"><![CDATA[<p>本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=FytuB8nFHPQ" target="_blank" rel="external">Python’s Visualization Landscape</a></p>
<p>Python真是越来越火了。活跃的开源社区为Python这门语言贡献着长青的活力。</p>
<p>子曾经曰过：轮子多了，车就稳了。</p>
<p>本文帮助你选好轮子，也祝愿可视化的车开得越来越稳。</p>
<h2 id="The-Landscape"><a href="#The-Landscape" class="headerlink" title="The Landscape"></a>The Landscape</h2><p><img src="https://static.ddlee.cn/static/img/Python可视化工具指引/landscape.png" alt="landscape"></p>
<p>如图。</p>
<p>VanderPlas在展示完这张全景图后给大家贴了这张图：</p>
<p><img src="https://static.ddlee.cn/static/img/Python可视化工具指引/chan.png" alt="chan"></p>
<p>我差点笑喷。我们的表情包可能要在人民币之前走向国际化了。</p>
<p>回到正题，可视化工具有两个主要阵营，一是基于matplotlib，二是基于JavaScript。还有的接入了JS下著名的D3.js库。</p>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>numpy, pandas, matplotlib可以说是python数据科学的三驾马车。凡以python为教学语言的数据科学相关课程必提这三个库。而matplotlib又有什么特点呢？</p>
<p>先说优点：</p>
<ol>
<li>像MATLAB的语法，对MATLAB用户好上手</li>
<li>稳定，久经考验</li>
<li>渲染后端丰富，跨平台（GTK, Qt5, svg, pdf等）</li>
</ol>
<p>缺点也有很多：</p>
<ol>
<li>API过于繁琐</li>
<li>默认配色太丑</li>
<li>对web支持差，交互性差</li>
<li>对大数据集处理较慢</li>
</ol>
<p>于是就有了很多基于matplotlib的扩展，提供了更丰富、更人性化的API。</p>
<p><img src="https://static.ddlee.cn/static/img/./Python可视化工具指引/matplotlib.png" alt="matplotlib"></p>
<p>下面是几个比较受欢迎的包：</p>
<h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p>pandas的DataFrame对象是有plot()方法的，如：<br><code>iris.plot.scatter(&#39;petalLength&#39;, &#39;petalWidth&#39;)</code>生成二维散点图，只需指明两个轴取自哪一列数据即可。</p>
<h3 id="seaborn"><a href="#seaborn" class="headerlink" title="seaborn"></a>seaborn</h3><p>seaborn(<a href="http://seaborn.pydata.org/examples/" target="_blank" rel="external">gallery</a>)专注于统计数据可视化，默认配色也还可以。语法示例：</p>
<pre><code class="Python">import seaborn as sns
sns.lmplot(&#39;petalLength&#39;, &#39;sepalWidth&#39;, iris, hue=&#39;species&#39;, fit_reg=False)
</code></pre>
<h3 id="类ggplot"><a href="#类ggplot" class="headerlink" title="类ggplot"></a>类ggplot</h3><p>对于R用户，最熟悉的可视化包可能是ggplot2，在python中可以考虑<a href="https://github.com/yhat/ggpy" target="_blank" rel="external">ggpy</a>和近期上了Github Trends的<a href="https://github.com/has2k1/plotnine" target="_blank" rel="external">plotnie</a>。</p>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>基于JS的包常常具有非常好的交互性，其共同点是将图形格式化为json文件，再由JS完成渲染。</p>
<p><img src="https://static.ddlee.cn/static/img/./Python可视化工具指引/js.png" alt="js"></p>
<h3 id="Bokeh"><a href="#Bokeh" class="headerlink" title="Bokeh"></a>Bokeh</h3><p>Bokeh(<a href="http://bokeh.pydata.org/en/latest/docs/gallery.html" target="_blank" rel="external">Gallery</a>)定位于绘制用于浏览器展示的交互式图形。其优点是交互性、能够处理大量数据和流数据。语法示例：</p>
<pre><code class="python">p = figure()
p.circle(iris.petalLength, iris.sepalWidth)
show(p)
</code></pre>
<h3 id="Plotly"><a href="#Plotly" class="headerlink" title="Plotly"></a>Plotly</h3><p>Plotly(<a href="https://plot.ly/python/" target="_blank" rel="external">Gallery</a>)跟Bokeh类似。但其提供了多种语言接口(JS, R, Python, MATLAB)，并且支持3D和动画效果，缺点是有些功能需要付费。<br>语法示例：</p>
<pre><code class="python">from plotly.graph_objs import Scatter
from plotly.offline import iplot
p = Scatter(x=iris.petalLength,
            y=iris.sepalWidth,
            mode=&#39;markers&#39;)
iplot(p)
</code></pre>
<h2 id="处理大型数据集"><a href="#处理大型数据集" class="headerlink" title="处理大型数据集"></a>处理大型数据集</h2><p>对于大型数据集，可以考虑的包包括datashader, Vaex, 基于OpenGL的Vispy和Glumpy，GlueViz等。这里介绍datashader。</p>
<h3 id="datashader"><a href="#datashader" class="headerlink" title="datashader"></a>datashader</h3><p><a href="https://github.com/bokeh/datashader" target="_blank" rel="external">datashader</a>是Bokeh的子项目，为处理大型数据集而生。</p>
<p>示例语法：</p>
<pre><code class="Python">from colorcet import fire
export(tf.shade(agg, cmap=cm(fire, 0.2), how=&#39;eq_hist&#39;), &#39;census_ds_fier_eq_hist&#39;)
</code></pre>
<p><img src="https://static.ddlee.cn/static/img/./Python可视化工具指引/fire.jpg" alt="fire"></p>
<h2 id="最终的建议"><a href="#最终的建议" class="headerlink" title="最终的建议"></a>最终的建议</h2><p>上车忠告：</p>
<ol>
<li>matplotlib必会</li>
<li>R用户：ggpy/plotnine</li>
<li>交互式：plotly(与R接口统一)/bokeh(免费)</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Visualization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Deep Learning]]></title>
      <url>https://blog.ddlee.cn/2017/05/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Learning/</url>
      <content type="html"><![CDATA[<p>这篇文章是三位大牛15年发表在Nature上有关深度学习的综述，尽管这两年深度学习又有更多的模型和成果出现，文章显得有些过时，但来自三位领军人物对深度学习的深度阐述还是值得反复回味。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>摘要的第一句话可以说给深度学习下了定义。有一些观点认为深度学习就是堆叠了很多层的神经网络，因计算力的提升而迎来第二春。但请看三位是怎么说的：</p>
<blockquote>
<p>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.</p>
</blockquote>
<p>也就是说，深度学习是允许由 <em>多个处理层构成的计算模型</em> 用多个层次的 <em>抽象</em> 来习得 <em>数据表示</em> 的技术。我的解读如下：</p>
<ol>
<li>深度学习不限于神经网络模型，其关键之处在于多层的表示</li>
<li>深度学习属于表示学习，目的是习得数据的某种表示，而这种表示由多个层次的抽象完成</li>
</ol>
<p>在第一段的导言中，文章总结了深度学习技术取得突破性成果的各个领域，也再次指出了深度学习与传统学习算法的不同之处：</p>
<ul>
<li>传统学习模型需要特征工程和领域知识来从数据构建较好的特征</li>
<li>深度学习中，多层的特征由通用的学习过程得到，而不需要人类工程师的参与</li>
</ul>
<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><p>这一段概述了监督学习的一般框架、优化策略，并指出浅层学习需要Feature Extractor来提取对最适合目标问题的特征。</p>
<h2 id="Backpropagation-to-train-multilayer-architectures"><a href="#Backpropagation-to-train-multilayer-architectures" class="headerlink" title="Backpropagation to train multilayer architectures"></a>Backpropagation to train multilayer architectures</h2><p>这一段指出BP算法的关键在于目标函数关于某一子模块输入的导数可以反向通过目标函数关于该子模块输出的导数得出，而这一过程是可迭代的。BP算法曾因容易陷于局部最优解而被冷落，但对于大型网络，在实践中，理论和经验都表明尽管落于局部最优解，但这个解的效果却和全局最优解相差无几，而且几乎所有的局部最优解都可以取得类似的效果。</p>
<h2 id="Convolutional-neural-networks"><a href="#Convolutional-neural-networks" class="headerlink" title="Convolutional neural networks"></a>Convolutional neural networks</h2><p>巻积网络背后有四个关键想法：</p>
<ul>
<li>local connections</li>
<li>shared weights</li>
<li>pooling</li>
<li>the use of many layers</li>
</ul>
<p>巻积网络常由巻积层、池化层和激活层构成，巻积层用于提取局部特征，池化层用于整合相似的特征，激活层用于加入非线性。这样的结构有两点理由：</p>
<ol>
<li>张量性数据的局部数值常常高度相关，局部特征容易发现</li>
<li>局部特征跟位置无关（平移不变性）</li>
</ol>
<p>文章也提到了这种巻积结构的仿生学证据。</p>
<h2 id="Image-understanding-with-deep-convolutional-networks"><a href="#Image-understanding-with-deep-convolutional-networks" class="headerlink" title="Image understanding with deep convolutional networks"></a>Image understanding with deep convolutional networks</h2><p>这一段总结了巻积网路在图像方面取得的成就。</p>
<h2 id="Distributed-representations-and-language-processing"><a href="#Distributed-representations-and-language-processing" class="headerlink" title="Distributed representations and language processing"></a>Distributed representations and language processing</h2><p>分布式表示在两点上可以取得指数级增益：</p>
<ol>
<li>习得特征的不同组合可以泛化出训练数据中不存在的类型</li>
<li>特征组合的个数的增加关于层数是指数级的</li>
</ol>
<p>文章还比较了分布式表示相比传统的词频统计在表述人类语言方面的优势。</p>
<h2 id="Recurrent-neural-networks"><a href="#Recurrent-neural-networks" class="headerlink" title="Recurrent neural networks"></a>Recurrent neural networks</h2><p>这一段概述了循环神经网络的动态特性和LSTM等结构上的改进。</p>
<h2 id="The-future-of-deep-learning"><a href="#The-future-of-deep-learning" class="headerlink" title="The future of deep learning"></a>The future of deep learning</h2><p>作者认为在长期看来，无监督学习会更为重要，人工智能领域的重大飞跃将由组合了表示学习和复杂推理的系统取得。</p>
<p>论文链接：<a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" target="_blank" rel="external">Deep Learning</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Dropout-Pytorch实现]]></title>
      <url>https://blog.ddlee.cn/2017/05/17/Dropout-Pytorch%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p>Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。</p>
<h3 id="Dropout的洞察"><a href="#Dropout的洞察" class="headerlink" title="Dropout的洞察"></a>Dropout的洞察</h3><p>关于Dropout，一个流行的解释是，通过随机行为训练网络，并平均多个随机决定的结果，实现了参数共享的Bagging。如下图，通过随机地冻结/抛弃某些隐藏单元，我们得到了新的子网络，而参数共享是说，与Bagging中子模型相互独立的参数不同，深度网络中Dropout生成的子网络是串行的，后一个子模型继承了前一个子模型的某些参数。</p>
<p><img src="https://static.ddlee.cn/static/img/Dropout-Pytorch实现/dropout.jpg" alt="dropout"></p>
<p>Dropout是模型自我破坏的一种形式，这种破坏使得存活下来的部分更加鲁棒。例如，某一隐藏单元学得了脸部鼻子的特征，而在Dropout中遭到破坏，则在之后的迭代中，要么该隐藏单元重新学习到鼻子的特征，要么学到别的特征，后者则说明，鼻子特征对该任务来说是冗余的，因而，通过Dropout，保留下来的特征更加稳定和富有信息。</p>
<p>Hinton曾用生物学的观点解释这一点。神经网络的训练过程可以看做是生物种群逐渐适应环境的过程，在迭代中传递的模型参数可以看做种群的基因，Dropout以随机信号的方式给环境随机的干扰，使得传递的基因不得不适应更多的情况才能存活。</p>
<p>另一个需要指出的地方是，Dropout给隐藏单元加入的噪声是乘性的，不像Bias那样加在隐藏单元上，这样在进行反向传播时，Dropout引入的噪声仍能够起作用。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面看在实践中，Dropout层是如何实现的。简单来说，就是生成一系列随机数作为mask，然后再用mask点乘原有的输入，达到引入噪声的效果。</p>
<h4 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h4><pre><code class="python"># forward pass
def dropout_forward(x, dropout_param):
  p, mode = dropout_param[&#39;p&#39;], dropout_param[&#39;mode&#39;]
  # p: dropout rate; mode: train or test
  if &#39;seed&#39; in dropout_param:
    np.random_seed(dropout_param[&#39;seed&#39;])
  # seed: random seed
  mask = None
  out = None
  if mode == &#39;train&#39;:
    mask = (np.random.rand(*x.shape) &gt;= p)/(1-p)
    # 1-p as normalization multiplier: to keep the size of input
    out = x * mask
  elif mode == &#39;test&#39;:
    # do nothing when perform inference
    out = x
  cache = (dropout_param, mask)
  out = out.astype(x.dtype, copy=False)
  return out, cache

# backward pass
def dropout_backward(dout, cache):
  dropout_param, mask = cache
  mode = dropout_param[&#39;mode&#39;]

  dx = None
  if mode == &#39;train&#39;:
    dx = dout * mask
  elif mode == &#39;test&#39;:
    dx = dout
  return dx
</code></pre>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><p>file: <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/dropout.py" target="_blank" rel="external">/torch/nn/_functions/dropout.py</a></p>
<pre><code class="python">class Dropout(InplaceFunction):

    def __init__(self, p=0.5, train=False, inplace=False):
        super(Dropout, self).__init__()
        if p &lt; 0 or p &gt; 1:
            raise ValueError(&quot;dropout probability has to be between 0 and 1, &quot;
                             &quot;but got {}&quot;.format(p))
        self.p = p
        self.train = train
        self.inplace = inplace

    def _make_noise(self, input):
    # generate random signal
        return input.new().resize_as_(input)

    def forward(self, input):
        if self.inplace:
            self.mark_dirty(input)
            output = input
        else:
            output = input.clone()

        if self.p &gt; 0 and self.train:
            self.noise = self._make_noise(input)
            # multiply mask to input
            self.noise.bernoulli_(1 - self.p).div_(1 - self.p)
            if self.p == 1:
                self.noise.fill_(0)
            self.noise = self.noise.expand_as(input)
            output.mul_(self.noise)

        return output

    def backward(self, grad_output):
        if self.p &gt; 0 and self.train:
            return grad_output.mul(self.noise)
        else:
            return grad_output
</code></pre>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> AI </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Visualizing and Understanding Recurrent Networks]]></title>
      <url>https://blog.ddlee.cn/2017/05/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Visualizing-and-Understanding-Recurrent-Networks/</url>
      <content type="html"><![CDATA[<p>论文： <a href="http://arxiv.org/abs/1506.02078" target="_blank" rel="external">Visualizing and Understanding Recurrent Networks</a></p>
<h2 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h2><p>字母级的循环神经网络，用Torch实现，代码见<a href="http://github.com/karpathy/char-rnn" target="_blank" rel="external">GitHub</a>。字母嵌入成One-hot向量。优化方面，采用了RMSProp算法，加入了学习速率的decay和early stopping。</p>
<p>数据集采用了托尔斯泰的《战争与和平》和Linux核心的代码。</p>
<h2 id="可解释性激活的例子"><a href="#可解释性激活的例子" class="headerlink" title="可解释性激活的例子"></a>可解释性激活的例子</h2><p>$tanh$函数激活的例子，$-1$为红色，$+1$为蓝色。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane1.png" alt="pane1"></p>
<p>上图分别是记录了行位置、引文和if语句特征的例子和失败的例子。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane2.png" alt="pane2"></p>
<p>上图分别是记录代码中注释、代码嵌套深度和行末标记特征的例子。</p>
<h2 id="Gates数值的统计"><a href="#Gates数值的统计" class="headerlink" title="Gates数值的统计"></a>Gates数值的统计</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/gates.png" alt="gates"></p>
<p>此图信息量很大。</p>
<ol>
<li>left-saturated和right-saturated表示各个Gates激活函数（$sigmoid$）小于0.1和大于0.9，即总是阻止信息流过和总是允许信息流过。</li>
<li>横轴和纵轴表示该Gate处于这两种状态的时间比例，即有多少时间是阻塞状态，有多少时间是畅通状态。</li>
<li>三种颜色表示不同的层。</li>
</ol>
<p>有以下几个观察：</p>
<ol>
<li>第一层的门总是比较中庸，既不阻塞，也不畅通</li>
<li>第二三层的门在这两种状态间比较分散，经常处于畅通状态的门可能记录了长期的依赖信息，而经常处于阻塞状态的门则负责了短期信息的控制。</li>
</ol>
<h2 id="错误来源分析"><a href="#错误来源分析" class="headerlink" title="错误来源分析"></a>错误来源分析</h2><p>在这一节，作者用了“剥洋葱”的方法，建立了不同的模型将错误进行分解。此处错误指LSTM预测下一个字母产生的错误，数据集为托尔斯泰的《战争与和平》。</p>
<ol>
<li>n-gram</li>
<li>Dynamic n-long memory，即对已经出现过得单词的复现。如句子”Jon yelled at<br>Mary but Mary couldn’t hear him.”中的Mary。</li>
<li>Rare words，不常见单词</li>
<li>Word model，单词首字母、新行、空格之后出现的错误</li>
<li>Punctuation，标点之后</li>
<li>Boost，其他错误</li>
</ol>
<p>根据作者的实验，错误的来源有如下分解：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/error.png" alt="error"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇文章是打开LSTM黑箱的尝试，提供了序列维度上共享权值的合理性证据，对Gates状态的可视化也非常值得关注，最后对误差的分解可能对新的网络结构有所启发（比如，如何将单词级别和字母级别的LSTM嵌套起来，解决首字母预测的问题？）。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> RNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Deep Residual Learning for Image Recognition]]></title>
      <url>https://blog.ddlee.cn/2017/04/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/</url>
      <content type="html"><![CDATA[<p>网络在堆叠到越来越深之后，由于BP算法所依赖的链式法则的连乘形式，会出现梯度消失和梯度下降的问题。初始标准化和中间标准化参数在一定程度上缓解了这一问题，但仍然存在更深的网络比浅层网络具有更大的训练误差的问题。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/error.png" alt="error"></p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><p>多层的网络结构能够任意接近地拟合目标映射$H(x)$，那么也能任意接近地拟合其关于恒等映射的残差函数$H(x)-x$。记$F(x)=H(x)-x$，则原来的目标映射表为$F(x)+x$。由此，可以设计如下结构。</p>
<h3 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/block.jpg" alt="Residual Learning: a building block"></p>
<p>残差单元包含一条恒等映射的捷径，不会给原有的网络结构增添新的参数。</p>
<h2 id="动机-启发"><a href="#动机-启发" class="headerlink" title="动机/启发"></a>动机/启发</h2><p>层数的加深会导致更大的训练误差，但只增加恒等映射层则一定不会使训练误差增加，而若多层网络块要拟合的映射与恒等映射十分类似时，加入的捷径便可方便的发挥作用。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章中列举了大量在ImagNet和CIFAR-10上的分类表现，效果很好，在此不表。</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><h4 id="Deeper-Bottleneck-Architectures"><a href="#Deeper-Bottleneck-Architectures" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h4><p><img src="https://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/Bottleneck.png" alt="Bottleneck"></p>
<p>两头的1 * 1巻积核先降维再升维，中间的3 * 3巻积核成为“瓶颈”，用于提取重要的特征。这样的结构跟恒等映射捷径配合，在ImageNet上有很好的分类效果。</p>
<h4 id="Standard-deviations-of-layer-responses"><a href="#Standard-deviations-of-layer-responses" class="headerlink" title="Standard deviations of layer responses"></a>Standard deviations of layer responses</h4><p><img src="https://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/std.png" alt="std"><br>上图是在CIFAR-10数据集上训练的网络各层的相应方差（Batch-Normalization之后，激活之前）。可以看到，残差网络相对普通网络有更小的方差。这一结果支持了残差函数比非残差函数更接近于0的想法（即更接近恒等映射）。此外，还显示出网络越深，越倾向于保留流过的信息。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>深度残差网络在当年的比赛中几乎是满贯。<br>下面是我的一些（未经实验证实的）理解：</p>
<p>首先，其”跳级”的网络结构对深度网络的设计是一种启发，通过“跳级”，可以把之前网络的信息相对完整的跟后层网络结合起来，即低层次解耦得到的特征和高层次解耦得到的特征再组合。<br>再者，这种分叉的结构可以看作网络结构层面的”Dropout”: 如果被跳过的网络块不能习得更有用的信息，就被恒等映射跳过了。</p>
<p>论文链接：<a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a></p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Tensorflow White Paper]]></title>
      <url>https://blog.ddlee.cn/2017/04/20/Tensorflow-White-Paper/</url>
      <content type="html"><![CDATA[<p>论文：<a href="https://www.tensorflow.org/about/bib" target="_blank" rel="external">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</a></p>
<h2 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h2><h3 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h3><p>整张图如同管道结构，数据流就是其中的水流。<em>Control Dependency</em> 描述了管道的有向结构，而反向传播可以通过增加新的管道节点来实现。</p>
<h3 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h3><p>即计算操作的抽象，相当于映射、函数。</p>
<h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><p>执行计算的单元，CPU或GPU</p>
<h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>Client-Server结构，进行计算或者调整图结构则视为一次会话</p>
<h3 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h3><p>特殊的Operation，返回一个句柄，指向持久化的张量，这些张量在整张图的计算中不会被释放。</p>
<h3 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h3><p>对Kernel的封装，包含类型属性，实行注册机制维护可供使用的Device列表。</p>
<h2 id="多机实现"><a href="#多机实现" class="headerlink" title="多机实现"></a>多机实现</h2><p>要考虑两个问题：</p>
<ol>
<li>计算节点在Device间的分配问题</li>
<li>Devices之间的通信</li>
</ol>
<p>针对这两个问题，分别建立了两个抽象层。</p>
<h3 id="计算节点分配的C-S机制"><a href="#计算节点分配的C-S机制" class="headerlink" title="计算节点分配的C/S机制"></a>计算节点分配的C/S机制</h3><p><img src="https://static.ddlee.cn/static/img/Tensorflow-White-Paper/master.png" alt="master"><br>client提出计算请求，master负责切割计算图为子图，分配子图到Devices。分配时，会模拟执行子图，并采取贪心的策略分配。</p>
<h3 id="不同Device之间的发送和接收节点"><a href="#不同Device之间的发送和接收节点" class="headerlink" title="不同Device之间的发送和接收节点"></a>不同Device之间的发送和接收节点</h3><p><img src="https://static.ddlee.cn/static/img/Tensorflow-White-Paper/rev-send.png" alt="rec-send"></p>
<p>在每个Device上建立Receive和Send节点，负责与其他Device通信。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="数据化并行"><a href="#数据化并行" class="headerlink" title="数据化并行"></a>数据化并行</h3><p><img src="https://static.ddlee.cn/static/img/Tensorflow-White-Paper/Parallelize.png" alt="Parallelize"></p>
<p>上：单线程，同步数据并行<br>下：多线程，异步更新</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><p>文章中很多内容并没涉及到（看不懂）。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>TensorFlow是个庞大的计算框架，不仅仅定位于深度网络。其对计算图的抽象和数据、计算资源的分配的处理是值得关注的。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Tensorflow </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Papers </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[编程方法论(一):重构]]></title>
      <url>https://blog.ddlee.cn/2017/04/08/%E7%BC%96%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA-%E9%87%8D%E6%9E%84/</url>
      <content type="html"><![CDATA[<p>本文内容主要整理自lynda.com课程<a href="https://www.lynda.com/Developer-Programming-Foundations-tutorials/Foundations-Programming-Refactoring-Code/122457-2.html" target="_blank" rel="external">Programming Foudations: Refactoring Code</a>和<em>Martin Fowler</em>的<a href="https://martinfowler.com/books/refactoring.html" target="_blank" rel="external">重构</a>。全部例子来源于<a href="https://refactoring.com" target="_blank" rel="external">refactoring.com</a>。</p>
<p>内容大纲：<br><img src="https://static.ddlee.cn/static/img/编程方法论-重构/Refactoring.png" alt="structure"></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><img src="https://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-1.png" alt="intro-method"></p>
<h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>重构是在不影响软件功能的情况下，重新组织代码，使之更清晰、更容易理解的过程。</p>
<ul>
<li>前提：功能不变</li>
<li>行为：改写代码</li>
<li>目的：提高可理解性</li>
</ul>
<p>大白话讲，重构就是改写，造福以后需要理解这段代码的人们。</p>
<h5 id="重构不是什么"><a href="#重构不是什么" class="headerlink" title="重构不是什么"></a>重构不是什么</h5><p>给一件事物下定义，有时候从反方面更好讲些。比如你难给正义下一个定义，但很容易举出什么是非正义的例子。</p>
<ul>
<li>重构不是Debug，代码已经运行良好</li>
<li>重构不是优化</li>
<li>重构不是添加新功能</li>
</ul>
<p>也就是说，重构对使用代码的人没有任何好处，对使用者来讲，代码是黑箱。重构是准备给要打开黑箱的人，而那个人常常是你自己。</p>
<h5 id="玄学：Code-Smells"><a href="#玄学：Code-Smells" class="headerlink" title="玄学：Code Smells"></a>玄学：Code Smells</h5><p>玄学二字是我自己加的。Martin Fowler当然没有这样说。我只是表达一下对无法精确描述的定义的敬意。</p>
<p>我的理解是Code Smells是best practice和code style的总和，直接和根本来源是自己的代码经验。所谓语感、文笔、血淋淋的人生道理。</p>
<h5 id="准备工作：自动化测试"><a href="#准备工作：自动化测试" class="headerlink" title="准备工作：自动化测试"></a>准备工作：自动化测试</h5><p>重构当然不是breaking the code，写好测试，保证代码仍能正常运行。</p>
<h3 id="重构范例：方法层面"><a href="#重构范例：方法层面" class="headerlink" title="重构范例：方法层面"></a>重构范例：方法层面</h3><p>首先是一句良言：哪里加了注视，哪里可能就需要重构。</p>
<p>这一点的潜在信念是，好的代码是self-explained的，通过合理的命名、清晰的组织，代码应该像皇帝的新装那样一目了然。</p>
<h4 id="可以用于重构的工具"><a href="#可以用于重构的工具" class="headerlink" title="可以用于重构的工具"></a>可以用于重构的工具</h4><p>常见的IDE会有重构的功能，如重命名变量。另外，一个严厉的Linter加上像我这样的强迫癌患者会将风格问题扼杀在摇篮之中。</p>
<h4 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h4><p>举例均以Code smell和重构建议两部分构成，较抽象(wo kan bu dong)的给出代码。</p>
<h5 id="Extract-Method"><a href="#Extract-Method" class="headerlink" title="Extract Method"></a><em>Extract Method</em></h5><p>Code smell: 太长的方法，带注释的代码块</p>
<p>重构： 提取，新建，用评论命名</p>
<h5 id="Remove-temps"><a href="#Remove-temps" class="headerlink" title="Remove temps"></a>Remove temps</h5><p>Code smell: 冗余的临时变量（本地）</p>
<p>重构：</p>
<ul>
<li><em>Replace with Query</em>: 把表达式提取为方法（规模较大）</li>
<li><em>Inline temps</em>: 直接用表达式代替这个变量（规模较小）</li>
</ul>
<h5 id="Add-temps"><a href="#Add-temps" class="headerlink" title="Add temps"></a>Add temps</h5><p>Code smell: 同样的变量有多重含义</p>
<p>重构：</p>
<ul>
<li><em>Split temporary variable</em>: 同一个临时变量在上下文赋予了不同含义（复用），拆</li>
<li><em>Remove assignment to parameters</em>: 对参数默认值的设定，在函数内新建变量，初始化这个新变量</li>
</ul>
<p>Remove assignment to parameters的例子：</p>
<pre><code class="java">//Before
int discount (int inputVal, int quantity, int yearToDate) {
  if (inputVal &gt; 50) inputVal -= 2;

//After Refactoring
int discount (int inputVal, int quantity, int yearToDate) {
  int result = inputVal;
  if (inputVal &gt; 50) result -= 2;
</code></pre>
<p>这一点基于的信念是，参数只能代表被传进来的变量，不应该在本地再赋予别的含义。</p>
<h3 id="重构范例：类与方法"><a href="#重构范例：类与方法" class="headerlink" title="重构范例：类与方法"></a>重构范例：类与方法</h3><p><img src="https://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-2.png" alt="class and method"></p>
<h4 id="Move-Method"><a href="#Move-Method" class="headerlink" title="Move Method"></a><em>Move Method</em></h4><p>Code smell: feature envy（依恋情结）</p>
<p>用中文来说，是指某个方法操作/使用（依恋）某一个类多过自己所处的类，我们用“出轨”这个词来表示这种现象。但这是违反婚姻法的，因而，我们的重构手段就是，把这个方法移动到它依恋的类中，圆满一段木石良缘。*</p>
<p>重构： 圆满木石良缘。</p>
<h4 id="Extract-Class"><a href="#Extract-Class" class="headerlink" title="Extract Class"></a><em>Extract Class</em></h4><p>Code smell: 规模太大的类</p>
<p>重构： 把部分移出，自立门户</p>
<h4 id="Inline-Class"><a href="#Inline-Class" class="headerlink" title="Inline Class"></a><em>Inline Class</em></h4><p>Code smell: 冗余的类</p>
<p>重构： 像我这中请天假组内运转几乎不受影响的人，应该清除掉（这是瞎话）</p>
<h4 id="Condition-Focused（条件语句相关）"><a href="#Condition-Focused（条件语句相关）" class="headerlink" title="Condition Focused（条件语句相关）"></a>Condition Focused（条件语句相关）</h4><p>Code smell: 写完判断条件自己都看不懂/看着难受</p>
<p>重构：</p>
<ul>
<li><em>Decompose conditional</em>: 分解</li>
<li><em>Consolidate conditional expression</em>: 多项条件指向同一段后续操作，提取这些条件为方法</li>
<li><em>Consolidate duplicate conditional fragments</em>: 不同条件的后续操作中含有共同的部分，将共有部分提取出来（不管哪个条件总要执行）</li>
<li><em>Replace condition with polymorphism</em>: 针对有判断分支的方法，替换成多态方法</li>
<li><em>Replace type code with subclass*</em>: 针对有判断分支的类，替换为子类</li>
</ul>
<p>Consolidate conditional expression的例子：</p>
<pre><code class="java">//Before
double disabilityAmount() {
  if (_seniority &lt; 2) return 0;
  if (_monthsDisabled &gt; 12) return 0;
  if (_isPartTime) return 0;
  // compute the disability amount
//After Refactoring
double disabilityAmount() {
  if (isNotEligableForDisability()) return 0;
  // compute the disability amount
</code></pre>
<h3 id="重构范例：数据相关"><a href="#重构范例：数据相关" class="headerlink" title="重构范例：数据相关"></a>重构范例：数据相关</h3><p><img src="https://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-3.png" alt="class and method"></p>
<h4 id="Move-field"><a href="#Move-field" class="headerlink" title="Move field"></a><em>Move field</em></h4><p>code smell: inverse feature envy（自造）</p>
<p>某一个类使用某一数据比该数据所属的类还多。</p>
<p>重构：送给你了还不行吗！？</p>
<h4 id="Data-Clumps（数据团）"><a href="#Data-Clumps（数据团）" class="headerlink" title="Data Clumps（数据团）"></a>Data Clumps（数据团）</h4><p>code smell: 某些数据总是抱团出现</p>
<p>重构：</p>
<ul>
<li><em>Preserve whole object</em>: 在一个方法中反复提取某个类的一些属性，将整个对象传入</li>
<li><em>Introducing parameter object</em>: 把这些参数合并为一个类，把新建的类传入</li>
</ul>
<h4 id="Similifying"><a href="#Similifying" class="headerlink" title="Similifying"></a>Similifying</h4><p>重构：</p>
<ul>
<li><em>Renaming</em>: 顾名思义</li>
<li><em>Add or remove parameters</em>: 顾名思义</li>
<li><em>Replace parameter with explicit Method</em>: 根据不同参数值新建专属的方法</li>
<li><em>Parameterize Method</em>: 与上者相反，把不同方法合并，传入参数</li>
<li><em>Separate queries form modifiers</em>: 将找到数据和更该数据两个操作拆成两个方法</li>
</ul>
<p>Replace parameter with explicit Method的例子：</p>
<pre><code class="java">//Before
void setValue (String name, int value) {
  if (name.equals(&quot;height&quot;)) {
    _height = value;
    return;
  }
  if (name.equals(&quot;width&quot;)) {
    _width = value;
    return;
  }
  Assert.shouldNeverReachHere();
}
//After Refactoring
void setHeight(int arg) {
  _height = arg;
}
void setWidth (int arg) {
  _width = arg;
}
</code></pre>
<h4 id="Pulling-and-pushing-升级与降级"><a href="#Pulling-and-pushing-升级与降级" class="headerlink" title="Pulling and pushing(升级与降级)"></a>Pulling and pushing(升级与降级)</h4><ul>
<li><em>Pull up method</em> and <em>pull up field</em></li>
<li><em>Push down method</em> and <em>push down field</em></li>
</ul>
<p>解决方法、数据归属不合理的问题。</p>
<h3 id="高阶重构（大坑，大坑）"><a href="#高阶重构（大坑，大坑）" class="headerlink" title="高阶重构（大坑，大坑）"></a>高阶重构（大坑，大坑）</h3><h4 id="Convert-procedural-design-to-objects"><a href="#Convert-procedural-design-to-objects" class="headerlink" title="Convert procedural design to objects"></a><em>Convert procedural design to objects</em></h4><p>化函数式变成为面向对象，祝好运。</p>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>写代码和改代码是一个不断被自己坑和被别人坑的旅程。且行且珍惜。</p>
<p>Cheers, have a good one.</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Programming </tag>
            
            <tag> Refactoring </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Coroutine,Generator,Async与Await]]></title>
      <url>https://blog.ddlee.cn/2017/04/03/Coroutine-Generator-Async%E4%B8%8EAwait/</url>
      <content type="html"><![CDATA[<h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>Generator能保存自己的状态，进入一种“Paused”状态，再次调用时会继续执行。</p>
<p>Generator的好处之一是节省了存储空间开销，带一些”流处理”的思想。</p>
<p>其实，我们也可以对Generator进行传入数据的操作：</p>
<pre><code class="python">def coro():
    hello = yield &quot;Hello&quot;
    yield hello

c = coro()
print(next(c))
print(c.send(&quot;World&quot;))
</code></pre>
<h4 id="Coroutine"><a href="#Coroutine" class="headerlink" title="Coroutine"></a>Coroutine</h4><p>coroutine可以认为是generator思想的泛化：</p>
<ul>
<li>generator一个一个地吐出数据（返回值）</li>
<li>coroutine一个一个地吃掉数据（传入参数）并返回结果，即可控地执行函数</li>
</ul>
<p>关键点在于，generator与coroutine都能保存自己的状态，而这种特点正可以用于任务切换。yield可以看做是操作系统在进行进程管理时的traps:</p>
<p><img src="https://static.ddlee.cn/static/img/coroutine/os.png" alt="traps"></p>
<p>实际上，coroutine可以看做”用户自定义”的进程，状态、启用和暂停都可控，David Beazley就利用这一点用coroutine实现了Python上的操作系统（参见Reference)。</p>
<h4 id="Conroutine与Concurrent-Programming"><a href="#Conroutine与Concurrent-Programming" class="headerlink" title="Conroutine与Concurrent Programming"></a>Conroutine与Concurrent Programming</h4><p>Concurrent Programming中有Task的概念，有如下特点：</p>
<ul>
<li>独立的控制流</li>
<li>内部状态变量</li>
<li>支持计划任务（暂停、恢复执行）</li>
<li>与其他Task通信</li>
</ul>
<pre><code class="python">@coroutine
def grep(pattern):  #正则匹配
    print &quot;Looking for %s&quot; % pattern
    while True:
        line = (yield)
        if pattern in line:
            print line,
</code></pre>
<p>conroutine有自己的控制流（while/if），有局部变量（pattern, line），能暂停和恢复（yield()/send()），能相互通信（send()）</p>
<p>====》coroutine就是一种Task！</p>
<p>Python Docs中提供了一个例子：</p>
<pre><code class="python">import asyncio

async def compute(x, y):
    print(&quot;Compute %s + %s ...&quot; % (x, y))
    await asyncio.sleep(1.0)
    return x + y

async def print_sum(x, y):
    result = await compute(x, y)
    print(&quot;%s + %s = %s&quot; % (x, y, result))

loop = asyncio.get_event_loop()
loop.run_until_complete(print_sum(1, 2))
loop.close()
</code></pre>
<p>执行方式如下图：</p>
<p><img src="https://static.ddlee.cn/static/img/coroutine/tulip_coro.png" alt="Chaining coroutines"></p>
<p>利用coroutine，可以在一个线程(Task)上实现异步。</p>
<h4 id="Impletation"><a href="#Impletation" class="headerlink" title="Impletation"></a>Impletation</h4><p>coroutine有两种实现方式，基于generator和原生async, awati关键字。</p>
<h5 id="generator-based-coroutine"><a href="#generator-based-coroutine" class="headerlink" title="generator based coroutine"></a>generator based coroutine</h5><pre><code class="python">import asyncio
import datetime
import random

@asyncio.coroutine
def display_date(num, loop):
    end_time = loop.time() + 50.0
    while True:
        print(&quot;Loop: {} Time: {}&quot;.format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) &gt;= end_time:
            break
        yield from asyncio.sleep(random.randint(0, 5))

loop = asyncio.get_event_loop()

asyncio.ensure_future(display_date(1, loop))
asyncio.ensure_future(display_date(2, loop))

loop.run_forever()
</code></pre>
<p>上面的程序实现了在同一个线程里交互执行两个函数（sleep），而又能保持各自的状态</p>
<h5 id="Native-support-python-3-5"><a href="#Native-support-python-3-5" class="headerlink" title="Native support(python 3.5+)"></a>Native support(python 3.5+)</h5><p>只需要修改函数定义头和<code>yield from</code>为关键字<code>await</code>即可。</p>
<pre><code class="python">async def display_date(num, loop, ):
    end_time = loop.time() + 50.0
    while True:
        print(&quot;Loop: {} Time: {}&quot;.format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) &gt;= end_time:
            break
        await asyncio.sleep(random.randint(0, 5))
</code></pre>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><p>Coroutine常翻译成“协程”。</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h4><ul>
<li><a href="http://www.dabeaz.com/coroutines/Coroutines.pdf" target="_blank" rel="external">David Beazley @ PyCon2009 Slides</a></li>
<li><a href="http://masnun.com/2015/11/13/python-generators-coroutines-native-coroutines-and-async-await.html" target="_blank" rel="external">PYTHON: GENERATORS, COROUTINES, NATIVE COROUTINES AND ASYNC/AWAIT</a></li>
<li><a href="https://docs.python.org/3/library/asyncio-task.html" target="_blank" rel="external">Python 3.6 Docs: Taks and coroutines</a></li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 异步计算 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[500lines项目Crawler源码阅读笔记]]></title>
      <url>https://blog.ddlee.cn/2017/04/03/500lines%E9%A1%B9%E7%9B%AECrawler%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>源码来自GitHub上著名的Repo: <a href="https://github.com/aosabook/500lines" target="_blank" rel="external">500lines or less</a>。</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p><img src="https://static.ddlee.cn/static/img/500lines_crawler/500lines_crawler.png" alt="structure"></p>
<p>代码结构由crawling, crawl, reporting三大部分组成。</p>
<ul>
<li>crawl: 驱动，解析传入的参数，管理loop，调用crawler，生成report</li>
<li>Crawling: 实现crawler类及一系列辅助函数</li>
<li>reporting： 生成记录</li>
</ul>
<h3 id="Crawler类"><a href="#Crawler类" class="headerlink" title="Crawler类"></a>Crawler类</h3><p>Crawler类实现了解析网址，抓取内容等基本功能，利用<code>asyncio</code>库构建<code>coroutine</code>（parse_lings(), fetch(), work()）。</p>
<p><img src="https://static.ddlee.cn/static/img/500lines_crawler/500lines_crawler_class.png" alt="Class Crawler"></p>
<p>核心之处是组织管理异步的抓取任务，代码块结构如下：</p>
<pre><code class="python">class Crawler:
  def __init__(self, roots, ....., loop):
    self.q = Queue(loop=self.loop) # 建立队列

  @asyncio.coroutine
  def parse_links(self, response):
    &#39;&#39;&#39;从返回内容中解析出要抓取的链接&#39;&#39;&#39;
    body = yield from response.read()
    if response.status == 200:
      if content_type:
        text = yield from response.text()
        urls = set(re.findall())
        for url in urls:
          if self.url_allowed():
            links.add()
  @asynico.coroutine
  def fetch(self, url):
    &#39;&#39;&#39;访问链接，抓取返回结果&#39;&#39;&#39;
    while tries:
      try:
        response = yield from self.session.get(url)
    try:
      if is_redirect():
        pass
      else:
        links = yield from self.parse_links(response)
    finally:
      yield from response.releas()
  @asyncio.coroutine
  def work(self):
    &#39;&#39;&#39;封装抓取过程，与队列交互&#39;&#39;&#39;
    try:
      while True:
        url = yield from self.q.get()
        assert url in self.seen_urls
        yield from self.fetch(url)
        self.q.task_done()
  @asyncio.coroutine
  def crawl(self):
    &#39;&#39;&#39;建立Tasks，启动Task&#39;&#39;&#39;
    workers = [asyncio.Task(self.work(), loop=self.loop)
                for _ in range(self.max_tasks)]
    yield from self.q.join()
</code></pre>
<p>下图是我对上述代码结构的理解：</p>
<p><img src="https://static.ddlee.cn/static/img/500lines_crawler/coroutine.JPG" alt="coroutines"></p>
<p>对coroutine的进一步介绍，参见<a href="/2017/04/03/Coroutine-Generator-Async与Await/">Coroutine-Generator-Async与Await</a>。</p>
<h3 id="A-Web-Crawler-With-asyncio-Coroutines导读"><a href="#A-Web-Crawler-With-asyncio-Coroutines导读" class="headerlink" title="A Web Crawler With asyncio Coroutines导读"></a><a href="http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html" target="_blank" rel="external">A Web Crawler With asyncio Coroutines</a>导读</h3><p>文章整体结构：</p>
<ul>
<li>分析爬虫任务</li>
<li><ul>
<li>传统方式：抢锁</li>
</ul>
</li>
<li><ul>
<li>异步方式的特点：无锁；单线程上同时运行多操作</li>
</ul>
</li>
<li>回调函数：fetch(),connecte(),read_response()的实现</li>
<li>Coroutine</li>
<li><ul>
<li>Generator的工作原理</li>
</ul>
</li>
<li><ul>
<li>用Generator实现Coroutine</li>
</ul>
</li>
<li>Asyncio库中的Coroutine</li>
<li><ul>
<li>crawl()</li>
</ul>
</li>
<li><ul>
<li>work()</li>
</ul>
</li>
<li><ul>
<li>fetch(), handle redirections</li>
</ul>
</li>
<li><ul>
<li>Queue()</li>
</ul>
</li>
<li><ul>
<li>EventLoop()</li>
</ul>
</li>
<li><ul>
<li>Task()</li>
</ul>
</li>
<li>Conclusion</li>
</ul>
<p>文章最后，作者点明了主题思想：</p>
<blockquote>
<p>Increasingly often, modern programs are I/O-bound instead of CPU-bound. For such programs, Python threads are the worst of both worlds: the global interpreter lock prevents them from actually executing computations in parallel, and preemptive switching makes them prone to races. Async is often the right pattern. But as callback-based async code grows, it tends to become a dishevelled mess. Coroutines are a tidy alternative. They factor naturally into subroutines, with sane exception handling and stack traces.</p>
</blockquote>
<p>大意是说，对I/O密集型的程序，Python多线程在两方面令人失望：全局锁的设定使之不能真正并行；抢占式多任务处理机制又让多个线程间形成竞争关系。异步通常是正确的选择。但持续增长的回调函数会使代码丧失可读性，Coroutine便是一种保持整洁性的替代方案。</p>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><p>这样说，Python的多线程效率带来的提高只是Python程序抢占了系统中非Python进程的资源（参考召集一波狐朋狗友帮你抢选修课），多个线程提高了Python作为一个整体在系统资源调配中的竞争力。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Feedly+Reeder3+FeedMe:信息获取与处理]]></title>
      <url>https://blog.ddlee.cn/2017/04/03/Feedly-Reeder3-FeedMe-%E4%BF%A1%E6%81%AF%E8%8E%B7%E5%8F%96%E4%B8%8E%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<p>我蛮早就意识到自己被信息淹没了。于是关了票圈，屏蔽了空间，不装任何新闻APP，失去效力的微信群一概不留，知乎上的关注也缩减了很多很多。</p>
<blockquote>
<p>世界终于清静了。</p>
</blockquote>
<p>但仍然需要有关注的动向。我又捡起了RSS这个老朋友，建立起的信息获取跟处理流如下图：</p>
<p><img src="https://static.ddlee.cn/static/img/Feedly_Reeder/Feedly+Reeder.png" alt="Feedly+Reeder3"></p>
<h3 id="服务与APP"><a href="#服务与APP" class="headerlink" title="服务与APP"></a>服务与APP</h3><p>主要涉及的服务：Feedly（免费，有高级版）</p>
<p>IOS APP：</p>
<ul>
<li>Reeder3（￥30）</li>
<li>Pocket（免费）</li>
<li>Pushbullet（免费）</li>
<li>Evernote（免费版限制客户端个数）</li>
</ul>
<p>Android APP:</p>
<ul>
<li>FeedMe（免费）</li>
<li>Pocket</li>
<li>Inbox</li>
<li>Evernote</li>
<li>Google Keep</li>
</ul>
<h3 id="获取：Feedly整合"><a href="#获取：Feedly整合" class="headerlink" title="获取：Feedly整合"></a>获取：Feedly整合</h3><p><a href="https://feedly.com/i/welcome" target="_blank" rel="external">Feedly</a>是著名的信息聚合服务，能从媒体RSS、博客、YouTube Chanel等拉取文章/动态，还提供Google关键词动态提醒服务。</p>
<p>这里先推荐两个Chrome插件，可以更方便地将网页端想要订阅的信息整合到Feedly中。</p>
<ul>
<li><a href="https://chrome.google.com/webstore/detail/save-to-feedly-board/hdhblphcdjcicefneapkhmleapfaocih?hl=en-US" target="_blank" rel="external">Follow Feed</a>: 识别跟当前网页内容相关的信息源，添加到Feedly订阅中。</li>
<li><a href="https://chrome.google.com/webstore/detail/save-to-feedly-board/hdhblphcdjcicefneapkhmleapfaocih?hl=en-US" target="_blank" rel="external">Save to Feedly Board</a>: 将当前网页添加到Feedly Board中，可以标记后分享给团队，实时更新。</li>
</ul>
<h4 id="信息源"><a href="#信息源" class="headerlink" title="信息源"></a>信息源</h4><h5 id="微信公众号"><a href="#微信公众号" class="headerlink" title="微信公众号"></a>微信公众号</h5><p>先直接在Feedly中搜索公众号，若找不到订阅源，则可通过<a href="http://www.iwgc.cn/" target="_blank" rel="external">微广场</a>等服务转成RSS。</p>
<h5 id="媒体-博客RSS源"><a href="#媒体-博客RSS源" class="headerlink" title="媒体/博客RSS源"></a>媒体/博客RSS源</h5><p>很多在线媒体会在主页提供RSS地址，也可直接在Feedly中搜索媒体名。</p>
<h5 id="知乎专栏"><a href="#知乎专栏" class="headerlink" title="知乎专栏"></a>知乎专栏</h5><p>有些<a href="https://rss.lilydjwg.me/" target="_blank" rel="external">工具</a>可以将知乎专栏转成RSS订阅源。</p>
<h5 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h5><p>Feedly支持设置关键词动态提醒。</p>
<h3 id="处理：Reeder3-FeedMe"><a href="#处理：Reeder3-FeedMe" class="headerlink" title="处理：Reeder3 + FeedMe"></a>处理：Reeder3 + FeedMe</h3><p>支持Feedly的APP实在太多，<a href="https://feedly.com/apps.html" target="_blank" rel="external">这里</a>是官方给出的列表，可看脸挑选。</p>
<p>我平常同时用Android和IOS处理订阅的信息，大屏精读，小屏浏览。</p>
<h4 id="IOS-Reeder3"><a href="#IOS-Reeder3" class="headerlink" title="IOS: Reeder3"></a>IOS: Reeder3</h4><p>不幸的是Reeder3是要付费的， 30块，几乎没有降过价。</p>
<p>替代品可以考虑自家的Feedly，Ziner等，可以参考<a href="http://www.makeuseof.com/tag/5-best-ipad-rss-readers/" target="_blank" rel="external">这篇文章</a>对比的结果选择。</p>
<h4 id="Android-FeedMe"><a href="#Android-FeedMe" class="headerlink" title="Android: FeedMe"></a>Android: FeedMe</h4><p>这里强推FeedMe(<a href="https://play.google.com/store/apps/details?id=com.seazon.feedme&amp;hl=en" target="_blank" rel="external">Google Play</a>)，抓取、缓存迅速，界面简洁，还有“中国大陆”模式。</p>
<h3 id="消化：Pocket-Inbox-Evernote-Keep"><a href="#消化：Pocket-Inbox-Evernote-Keep" class="headerlink" title="消化：Pocket, Inbox, Evernote/Keep"></a>消化：Pocket, Inbox, Evernote/Keep</h3><p>我个人将信息处理的结果分为三类：</p>
<ul>
<li>Read Later: 没消化</li>
<li>Links to save: 还想接着吃</li>
<li>Favorite:　想学着做</li>
</ul>
<p>稍后再读用Pocket，接口丰富，功能专一（尽管也有了“发现”模块）。</p>
<p>文章中挂的一些外链，移动端不好处理，要发往PC，手机端存在Inbox中，当临时的标签栏，iPad端用Pushbullet发给Chrome，下次打开Chrome时浏览处理。</p>
<p>收藏的文章存到Evernote，打好tags，长篇干货/可反复参考的转到OneNote。</p>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>其他情境下遇到的好文章、信息等尽量文字存到Google Keep，链接存到Inbox，或者给自己写封邮件。</p>
<p>微信的Favorite尽量不用，收藏的目的就在于情景分离，在不同的上下文中，我门信息获取的效率和质量区别实在太大了。详情参考拉微信群异地参加美赛的战友们。</p>
<p>最后推荐几个不错的订阅源(右击复制链接)：</p>
<ul>
<li><a href="http://www.sbnation.com/authors/mike-prada/rss" target="_blank" rel="external">SBNation上Mike Prada的文章</a>: 对NBA比赛、球队战术的分析</li>
<li><a href="http://rsarxiv.github.io/atom.xml" target="_blank" rel="external">Paper Weekly</a>: 机器学习方面的论文解读</li>
<li><a href="http://cos.name/feed/" target="_blank" rel="external">统计之都</a>: 统计学及应用、R语言方面的优秀内容</li>
<li><a href="http://github-trends.ryotarai.info/rss/github_trends_all_weekly.rss" target="_blank" rel="external">GitHub Trends</a></li>
<li><a href="https://blog.ddlee.cn/atom.xml">blog.ddlee.cn</a>: 大言不惭 -_-！</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 方法论 </tag>
            
            <tag> 信息处理 </tag>
            
            <tag> RSS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[网站迁移小记：腾讯云+Debian+Vestacp]]></title>
      <url>https://blog.ddlee.cn/2017/04/02/%E7%BD%91%E7%AB%99%E8%BF%81%E7%A7%BB%E5%B0%8F%E8%AE%B0%EF%BC%9A%E8%85%BE%E8%AE%AF%E4%BA%91-Debian-Vestacp/</url>
      <content type="html"><![CDATA[<p>先贴一张文章大纲。</p>
<p><img src="https://static.ddlee.cn/static/img/Website-Migeration/structure.png" alt="structure"></p>
<p>这是一个樱花开得正好但我很蛋疼的下午。</p>
<p>中午抢到了腾讯的校园优惠，便打算把网站<code>ddlee.cn</code>迁到国内的服务器上来。</p>
<h3 id="密码管理"><a href="#密码管理" class="headerlink" title="密码管理"></a>密码管理</h3><p>先谈密码管理。</p>
<p>建站会涉及设置很多密码，之前明文保存在云笔记里的方案总觉得又土又笨，何况很多密码最好要随机生成，密码管理服务还是必要的。</p>
<p>搜索之后，我选择的是<a href="https://www.lastpass.com/2" target="_blank" rel="external">lastPass</a>。主要考虑了免费和跨平台的特性。有更高要求的建议选择付费的<a href="https://1password.com/" target="_blank" rel="external">1Password</a>。</p>
<p>需要安装Chrome插件和Ubuntu下的deb包，添加Secure Note的功能深得我心，也支持自定义模板。</p>
<h3 id="主机"><a href="#主机" class="headerlink" title="主机"></a>主机</h3><p>腾讯云的校园优惠力度很大。阿里云是9.9块/月，腾讯用完券1块/月。</p>
<p>这里多讲一句，学生真是幸福得不得了。GitHub Education Pack中既有有Digital Ocean的优惠，AWS也有150刀的礼品卡，Jetbeans大部分产品免费……这还不提学校里买的License。</p>
<p>腾讯的主机1核CPU，2G内存，20G系统盘（Linux），挂个网站还算够用。</p>
<h4 id="SSH-Key-配置"><a href="#SSH-Key-配置" class="headerlink" title="SSH Key 配置"></a>SSH Key 配置</h4><p>建议在配置主机前创建一个SSH Key，这样访问起来安全又省心。</p>
<p>Linux系统下，在<code>~/.ssh/</code>下新建<code>config</code>，写入如下类似内容：</p>
<pre><code>Host Name
  HostName Host_IP
  User root
  IdentityFile path/to/ssh_private_key
</code></pre><p>这样就可以通过命令<code>ssh Name</code>直接访问主机。</p>
<h4 id="系统选择"><a href="#系统选择" class="headerlink" title="系统选择"></a>系统选择</h4><p>建议选择Linux主机。具体哪一系可自行选择，我的选择是Debian，CentOS也是个不错的选择。</p>
<h4 id="安全组设置"><a href="#安全组设置" class="headerlink" title="安全组设置"></a>安全组设置</h4><p>建议先只开启用于SSH的22端口，之后再开HTTP访问的80端口，FTP的20,21端口和主机面板所用端口。</p>
<p>如果个人有代理服务器的话，也可以限制一下来源IP，这样可以通过登入代理服务器，在代理服务器上通过SSH登入WEB主机，需要迁移下SSH Private Key，可以通过命令<code>scp usr1@host1:/path1 usr2@host2:/path2</code>实现。</p>
<h4 id="网络环境"><a href="#网络环境" class="headerlink" title="网络环境"></a>网络环境</h4><p>LNMP和LAMP是两种流行的结构。可以分别安装，再配置相应的<code>config</code>，也可以搜索得到很多一键安装脚本。另一种方案是用Docker部署。</p>
<p>我懒而笨，选择的是用主机面板一键安装。</p>
<h4 id="主机面板"><a href="#主机面板" class="headerlink" title="主机面板"></a>主机面板</h4><p>在此之前，一直用的是AMH的免费4.2版本，简洁轻巧，功能也够用。付费版推出后，免费版遭到冷落，几乎没有更新，这如何能忍。</p>
<p>说起主机面板，我的启蒙是WDCP，其远古风格的UI仍历历在目，后来听说爆出漏洞，但那时我已转战AMH。</p>
<p>一番艰苦卓绝的搜索之后（其实就是检索了’best host control pannel’），我选择了<a href="https://vestacp.com/" target="_blank" rel="external">Vestacp</a>。</p>
<p>UI漂亮，功能不缺（建站，MAIL，备份），GitHub还算活跃，就决定是你了。</p>
<p>缺点是文件管理器收费，不能通过WEB管理文件。安装过程持续蛮久（半个小时，当然也包括了新主机系统包更新的时间）。</p>
<p>安装时注意Hostname填写IP或者已经配置好DNS解析的域名（如<code>admin.ddlee.cn</code>）。8083是管理面板的端口，记得在主机提供商的安全组里开放一下。</p>
<h5 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h5><p>Vestacp支持多人管理，User身份由Package定义，安装过程会自动新建admin，拥有最高权限。</p>
<p>在User的设置里，可以配置用户的Package，而Package的设置里，可以配置每一用户身份的建站模板，资源上限等。如图。</p>
<p><img src="https://static.ddlee.cn/static/img/Website-Migeration/Package.png" alt="Package"></p>
<p>建站相当容易，注意在高级选项里添加FTP账户，用于之后上传HTML文件。</p>
<h5 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h5><p>建站完成后，记得配置好DNS解析，开放20和21端口，就可以用FileZilla测试链接。</p>
<p>注意，在高级选项里配置好Default local directory，设置Default remote directory为<code>/public_html</code>，并启用synchronized browsing和directory comparison，以后的FTP生活会很幸福。</p>
<h5 id="Mail"><a href="#Mail" class="headerlink" title="Mail"></a>Mail</h5><p>若在建站时勾选了Mail support，可以建立个性化的邮箱名，可以设置自动回复/转发，也可以用Gmail托管。以后留邮箱的时候可以短短的了呢。</p>
<h5 id="配置SSL"><a href="#配置SSL" class="headerlink" title="配置SSL"></a>配置SSL</h5><p>这是无意发现的技能。</p>
<p>本来在我的印象里，SSL证书都是要收费的。但留心的朋友可能注意到，建站时SSL support下有Lets Encrypt Support。这一服务可以用上免费的SSL。</p>
<p>官网：<a href="https://letsencrypt.org/" target="_blank" rel="external">Let’s Encrypt</a></p>
<p>要利用这项服务，需要证明自己对网站的至高无上不可侵犯的神圣权利，方法之一是运行支持[ACME protocol]的Client，官网推荐了<a href="https://certbot.eff.org/" target="_blank" rel="external">Cerbot</a>。</p>
<p>在Cerbot主页可以选择自己的操作系统，会有详细的步骤，在此不表。</p>
<p>下面谈两个问题，一是强制重定向至HTTPS，二是取消管理端口的HTTPS。</p>
<h6 id="强制HTTPS"><a href="#强制HTTPS" class="headerlink" title="强制HTTPS"></a>强制HTTPS</h6><p>Vestacp的架构是用nginx做proxy，Apache2做HTTP Server，首先下载nginx template（proxy template）：</p>
<pre><code class="bash">cd /usr/local/vesta/data/templates/web
wget http://c.vestacp.com/0.9.8/rhel/force-https/nginx.tar.gz
tar -xzvf nginx.tar.gz
rm -f nginx.tar.gz
</code></pre>
<p>之后在Package配置里，将proxy template配置为force-https，这样，身份由相应Package定义的用户建站时，proxy template就是用的强制HTTPS版本了。</p>
<h6 id="取消管理端口的SSL"><a href="#取消管理端口的SSL" class="headerlink" title="取消管理端口的SSL"></a>取消管理端口的SSL</h6><p>用chrome访问管理页面时，会有Unsecure的警告，这里的SSL在<code>/usr/local/vesta/nginx/conf/nginx.conf</code>中配置。找到</p>
<pre><code># Vhost
server {
    listen          8083;
    server_name     _;
    root            /usr/local/vesta/web;
    charset         utf-8;

    # Fix error &quot;The plain HTTP request was sent to HTTPS port&quot;
    error_page      497 https://$host:$server_port$request_uri;

    # ssl                  on;
    # ssl_certificate      /usr/local/vesta/ssl/certificate.crt;
    # ssl_certificate_key  /usr/local/vesta/ssl/certificate.key;
    # ssl_session_cache    shared:SSL:10m;
    # ssl_session_timeout  10m;
</code></pre><p>将配置SSL的几行注释掉即可。顺便，管理页面的端口也可以在这里更改。之后运行<code>service vesta restart</code>重启服务。</p>
<h3 id="域名与DNS"><a href="#域名与DNS" class="headerlink" title="域名与DNS"></a>域名与DNS</h3><p>最后简单提一下域名注册跟DNS。要注意的几个点：</p>
<ul>
<li>国内域名注册要备案，很烦，但cn域名好便宜。</li>
<li>在域名注册商那里配置DNS解析服务器（万网、DNSPod都好，不一定用自建网站的DNS）</li>
<li>在DNS服务商那里添加解析记录，顺便开启监控</li>
</ul>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>域名备案的时候，需要签一张备案单。方案是在纸上签字后调背景为透明，用Adobe PDF Reader的签字功能签好PDF，再转成JPG。</p>
<p>几项操作都可以通过在线工具完成，低碳生活，人人有责。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>建站过程本身就其乐无穷，教程一抓一大把，难的在TROUBLE SHOOTING，所以Google是最好的伴侣。</p>
</li>
<li><p>命令行、vi编辑、必要的WEB知识等是基础，在此感谢我们的墙两年前就教给我这些东西。</p>
</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Internet </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 云主机 </tag>
            
            <tag> Vestacp </tag>
            
            <tag> 博客 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[The Devtools Way: devtools+RStudio快速R程序包开发]]></title>
      <url>https://blog.ddlee.cn/2017/03/31/The-Devtools-Way-devtools-RStudio%E5%BF%AB%E9%80%9FR%E7%A8%8B%E5%BA%8F%E5%8C%85%E5%BC%80%E5%8F%91/</url>
      <content type="html"><![CDATA[<p>本文记录我的首个R程序包<a href="https://ddlee.cn/projects/mcmi.html" target="_blank" rel="external">MCMI</a>的开发过程。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol>
<li>两本Hadley Wickham写的书：<a href="http://adv-r.had.co.nz/" target="_blank" rel="external">Advanced R</a>和<a href="http://r-pkgs.had.co.nz/" target="_blank" rel="external">R Packages</a>。</li>
<li>Coursera上的课程<a href="https://www.coursera.org/specializations/r" target="_blank" rel="external">Mastering Software Development in R Specialization</a>和配套教材<a href="https://bookdown.org/rdpeng/RProgDA/" target="_blank" rel="external">Mastering Software Development in R</a>。</li>
</ol>
<p>感谢开源社区，以上的资料都可以免费获取得到（Coursera课程可以申请补助）。</p>
<h3 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h3><p>开发R包还需要系统中存在编译工具，编译文档需要LaTeX支持。</p>
<ul>
<li>Linux用户：<code>sudo apt-get install r-base-dev texlive-full</code></li>
<li>Windows用户：1.<a href="https://cran.rstudio.com/bin/windows/Rtools/" target="_blank" rel="external">RTools</a>;2.<a href="http://miktex.org/download" target="_blank" rel="external">MikTeX</a></li>
</ul>
<p>另外，请确保以下两个包已安装于系统中：<code>devtools</code>和<code>roxygen2</code>。推荐使用RStudio。</p>
<h3 id="Get-Started"><a href="#Get-Started" class="headerlink" title="Get Started"></a>Get Started</h3><p>在RStudio中新建项目，选择R程序包类型即可。建议同时建立Git路径以监控开发流程。</p>
<p>在编写代码之前，先要修改DESCRIPTION文件，要注意的几个地方：</p>
<ol>
<li>Package的命名要容易记忆和查询</li>
<li>Depends指你所用开发环境的R版本</li>
<li>慎重选择License</li>
<li>Imports指你所要调用的其他包，但在代码中，也要明确指出函数所处的包，如<code>ggplot2::qplot()</code></li>
</ol>
<h3 id="Git-Workflow"><a href="#Git-Workflow" class="headerlink" title="Git Workflow"></a>Git Workflow</h3><p>建议在GitHub上为本机申请SSH密钥，并在RStudo-&gt;Tools-&gt;Global Options-&gt;Git/SVN配置好路径，这样在执行<code>git push</code>时不用再次输入凭据。下面是有关Git的工作流：</p>
<ol>
<li>修改代码/文档</li>
<li>编译，测试</li>
<li>git commit</li>
<li>git push</li>
<li>重复以上循环</li>
</ol>
<p>RStudio对Git的集成很好，以上三四步操作均可在Git的操作面板里完成。</p>
<h3 id="Code-Workflow"><a href="#Code-Workflow" class="headerlink" title="Code Workflow"></a>Code Workflow</h3><ol>
<li>修改代码</li>
<li>Build&amp;Reload</li>
<li>用命令行测试功能</li>
<li>重复以上循环</li>
</ol>
<p>上述第二步可以在命令行中<code>devtools::load_all()</code>完成，也可以使用快捷键”Ctrl + Shift + L”，也可以在RStudio的开发面板中执行”Build&amp;Reload”命令。之后，便可在命令行中调用编写好的函数，验证其功能。</p>
<h3 id="Documentation-Workflow"><a href="#Documentation-Workflow" class="headerlink" title="Documentation Workflow"></a>Documentation Workflow</h3><ol>
<li>在.R文件中添加roxygen注释</li>
<li>Document</li>
<li>使用help面板或?命令预览文档</li>
<li>重复以上循环</li>
</ol>
<p>同样地，第二步有三种实现方式：1.<code>devtools::document()</code>;2.”Ctrl + Shift + D”；3. RStudio开发面板中的”Document”命令。</p>
<h3 id="Test-Workflow"><a href="#Test-Workflow" class="headerlink" title="Test Workflow"></a>Test Workflow</h3><p>测试方面，除了上述Coding Workflow中提到的在命令行中调用函数进行测试之外，还可以利用<code>testthat</code>包来使测试自动化。</p>
<p>首先要安装<code>testthat</code>包，再使用<code>devtools::ust_testthat()</code>命令建立<code>testthat</code>路径。</p>
<p>下面是自动化测试的工作流：</p>
<ol>
<li>修改代码</li>
<li>在<code>testthat</code>路径下编写相应的测试语句</li>
<li>Build&amp;Reload</li>
<li>Test</li>
<li>排除Bug，重复上述过程</li>
</ol>
<p>以上流程第四步同样有三种实现方式：1.<code>devtools::test()</code>;2.”Ctrl + Shift + T”;3.RStudio中开发面板的“Test”命令。</p>
<h3 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h3><p>按照上述过程开发的R程序包，每一次<code>git push</code>事实上都是一次发布。使用<code>devtools::install_github(&quot;git_repo_goest_here&quot;)</code>命令，可以很方便地安装R程序包。</p>
<h3 id="Next-Step"><a href="#Next-Step" class="headerlink" title="Next Step"></a>Next Step</h3><p>使用<code>devtools</code>配合RStudio和Git，开发R包的过程已经非常亲民和流水线化。但要开发高质量的R包，需要对R的数据结构和S3等对象系统有更深的理解，而Advanced R则是你通往这一方向的最好伴侣。</p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Data Science </tag>
            
            <tag> Programming </tag>
            
            <tag> R </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HADOOP学习速记]]></title>
      <url>https://blog.ddlee.cn/2017/03/30/HADOOP%E5%AD%A6%E4%B9%A0%E9%80%9F%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h4 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS: 分布式文件系统"></a>HDFS: 分布式文件系统</h4><p>NameNode, DataNode: a MetaData-Data Model<br>Strategy: Block split, multi-copy, distribution</p>
<p>NameNode: High Availability<br>solution 1: backup using NFS<br>solution 2: Two NameNodes(Active and Standby)</p>
<h4 id="MapReduce-计算框架"><a href="#MapReduce-计算框架" class="headerlink" title="MapReduce: 计算框架"></a>MapReduce: 计算框架</h4><p>split -&gt; Process -&gt; aggregate</p>
<p>Deamon: Job Tracker &amp; Task Tracker</p>
<h4 id="Design-Pattern"><a href="#Design-Pattern" class="headerlink" title="Design Pattern"></a>Design Pattern</h4><h3 id="Course-Project"><a href="#Course-Project" class="headerlink" title="Course Project"></a>Course Project</h3><p>（未完待续）<br>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分布式计算 </tag>
            
            <tag> Data Sicence </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle比赛中EDA：流程、做法与目的]]></title>
      <url>https://blog.ddlee.cn/2017/03/26/Kaggle%E6%AF%94%E8%B5%9B%E4%B8%ADEDA%EF%BC%9A%E6%B5%81%E7%A8%8B%E3%80%81%E5%81%9A%E6%B3%95%E4%B8%8E%E7%9B%AE%E7%9A%84/</url>
      <content type="html"><![CDATA[<h4 id="数据集大小、字段及相应的数据类型"><a href="#数据集大小、字段及相应的数据类型" class="headerlink" title="数据集大小、字段及相应的数据类型"></a>数据集大小、字段及相应的数据类型</h4><ul>
<li>大小：占用内存估计</li>
<li>字段数：维度估计，是否需要降维</li>
<li>数据类型：numerical, factor, string, etc. 是否需要归一化，二元化等等</li>
</ul>
<h4 id="了解数据的缺失值情况及分布"><a href="#了解数据的缺失值情况及分布" class="headerlink" title="了解数据的缺失值情况及分布"></a>了解数据的缺失值情况及分布</h4><h4 id="了解数据分布情况，可用众多图形完成"><a href="#了解数据分布情况，可用众多图形完成" class="headerlink" title="了解数据分布情况，可用众多图形完成"></a>了解数据分布情况，可用众多图形完成</h4><ul>
<li>bar plot</li>
<li>histogram</li>
<li>violin plot</li>
<li>box plot</li>
<li>scatter plot</li>
</ul>
<h5 id="主要目的："><a href="#主要目的：" class="headerlink" title="主要目的："></a>主要目的：</h5><ol>
<li>了解整体状况，是否具有野点</li>
<li>结合目标变量，考察特征与目标变量间的相关性</li>
</ol>
<h5 id="文本数据常用的探索："><a href="#文本数据常用的探索：" class="headerlink" title="文本数据常用的探索："></a>文本数据常用的探索：</h5><ul>
<li>词频统计（消除stopwords之后）</li>
<li>词云</li>
</ul>
<h4 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h4><p>本来文章是从几个经典的EDA notebook开始，试图总结出其共性之处，但写来写去，总觉得随便一本跟数据分析相关的书中，探索性数据分析的章节也大概都会涉及到这些内容，但在读书的情景之下又难留下深刻的印象，做分析的真正见地与经验，还是要从实践中来啊。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> EDA </tag>
            
            <tag> Data Sciencce </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Udacity课程： Intro to DevOps侧记]]></title>
      <url>https://blog.ddlee.cn/2017/03/22/DevOps%E4%BE%A7%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>文章主要内容来自Udacity的课程：<a href="https://www.udacity.com/course/intro-to-devops--ud611" target="_blank" rel="external">Intro to DevOps</a></p>
<h3 id="CAMS-The-DevOps-Lifecycle"><a href="#CAMS-The-DevOps-Lifecycle" class="headerlink" title="CAMS: The DevOps Lifecycle"></a>CAMS: The DevOps Lifecycle</h3><p>The Purpose of DevOps： 产品、开发、运维之间的协作问题</p>
<h4 id="Definition-from-wiki"><a href="#Definition-from-wiki" class="headerlink" title="Definition(from wiki)"></a>Definition(from wiki)</h4><blockquote>
<p>a set of practices that emphasize the collaboration and communication of both software developers and information technology (IT) professionals while automating the process of software delivery and infrastructure changes.</p>
</blockquote>
<p><img src="https://static.ddlee.cn/static/img/DevOps/Devops_venn.png" alt="DevOps_Venn"><br>(<a href="https://commons.wikimedia.org/wiki/File:Devops.png" target="_blank" rel="external">Source</a>)</p>
<p>对比： Agile Development（敏捷软件开发）</p>
<p>Plan-&gt;Code-&gt;Test-&gt;Release-&gt;Deploy-&gt;Operate</p>
<h4 id="Means-of-CAMS"><a href="#Means-of-CAMS" class="headerlink" title="Means of CAMS"></a>Means of CAMS</h4><ul>
<li>C: Culture</li>
<li>A: Automation</li>
<li>M: Measurement</li>
<li>S: Sharing</li>
</ul>
<h3 id="The-DevOps-Environment"><a href="#The-DevOps-Environment" class="headerlink" title="The DevOps Environment"></a>The DevOps Environment</h3><p>Solving the Environment Problem:</p>
<ul>
<li>Golden Image: apps-libs-OS</li>
<li>Configuration Management</li>
</ul>
<h4 id="Course-Project（使用Golden-Image方案，引入一批DevOps工具"><a href="#Course-Project（使用Golden-Image方案，引入一批DevOps工具" class="headerlink" title="Course Project（使用Golden Image方案，引入一批DevOps工具)"></a>Course Project（使用Golden Image方案，引入一批DevOps工具)</h4><p>dependencies-&gt;build scripts-&gt;tests-&gt;web apps</p>
<h5 id="Packer"><a href="#Packer" class="headerlink" title="Packer"></a>Packer</h5><p>Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.</p>
<ul>
<li><code>Artifacts</code> are the results of a single build, and are usually a set of IDs or files to represent a machine image.</li>
<li><code>Builds</code> are a single task that eventually produces an image for a single platform.</li>
<li><code>Builders</code> are components of Packer that are able to create a machine image for a single platform.</li>
<li><code>Commands</code> are sub-commands for the packer program that perform some job.</li>
<li><code>Post-processors</code> are components of Packer that take the result of a builder or another post-processor and process that to create a new artifact.</li>
<li><code>Provisioners</code> are components of Packer that install and configure software within a running machine prior to that machine being turned into a static image.</li>
<li><code>Templates</code> are JSON files which define one or more builds by configuring the various components of Packer.</li>
</ul>
<p><a href="https://github.com/ddlee96/devops-intro-project/blob/master/packer-templates/application-server.json" target="_blank" rel="external">Example JSON File</a></p>
<h5 id="Vagrant"><a href="#Vagrant" class="headerlink" title="Vagrant"></a>Vagrant</h5><p>Vagrant is a tool for building complete development environments. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases development/production parity, and makes the “works on my machine” excuse a relic of the past.</p>
<h5 id="Project-Workflow-Packer-gt-Vagrant-gt-Virtualbox-gt-Web-Application"><a href="#Project-Workflow-Packer-gt-Vagrant-gt-Virtualbox-gt-Web-Application" class="headerlink" title="Project Workflow: Packer-&gt;Vagrant-&gt;Virtualbox-&gt;Web Application"></a>Project Workflow: Packer-&gt;Vagrant-&gt;Virtualbox-&gt;Web Application</h5><h6 id="Part-I-Building-a-box-with-Packer"><a href="#Part-I-Building-a-box-with-Packer" class="headerlink" title="Part I: Building a box with Packer"></a>Part I: Building a box with Packer</h6><p>From the packer-templates directory on your local machine:</p>
<ul>
<li><p>Run <code>packer build -only=virtualbox-iso application-server.json</code></p>
<p>Troubleshooting: <em>Find the newest version number and checksum from the <a href="http://releases.ubuntu.com/trusty/" target="_blank" rel="external">Ubuntu website for this release</a></em><br><em>Edit <code>PACKER_BOX_NAME</code> and <code>iso_checksum</code> in the template files to match that version number and checksum.</em></p>
</li>
<li>Run <code>cd virtualbox</code></li>
<li>Run <code>vagrant box add ubuntu-14.04.4-server-amd64-appserver_virtualbox.box --name devops-appserver</code></li>
<li>Run <code>vagrant up</code></li>
<li>Run <code>vagrant ssh</code> to connect to the server</li>
</ul>
<h6 id="Part-II-Cloning-developing-and-running-the-web-application"><a href="#Part-II-Cloning-developing-and-running-the-web-application" class="headerlink" title="Part II: Cloning, developing, and running the web application"></a>Part II: Cloning, developing, and running the web application</h6><ul>
<li>On your local machine go to the root directory of the cloned repository</li>
<li>Run <code>git clone https://github.com/chef/devops-kungfu.git devops-kungfu</code></li>
<li>Open <a href="http://localhost:8080" target="_blank" rel="external">http://localhost:8080</a> from your local machine to see the app running.</li>
<li>In the VM, run <code>cd devops-kungfu</code></li>
<li>To install app specific node packages, run <code>sudo npm install</code>. You may see several errors; they can be ignored for now.</li>
<li>Now you can run tests with the command <code>grunt -v</code>. The tests will run, then quit with an error.</li>
</ul>
<h6 id="On-Cloud-platform"><a href="#On-Cloud-platform" class="headerlink" title="On Cloud platform"></a>On Cloud platform</h6><p>Similar commands using packer:</p>
<ul>
<li><code>packer build -only=amazon-ebs &lt;server-name&gt;.json</code></li>
<li><code>packer build -only=googlecompute application-server.json</code></li>
</ul>
<h3 id="Continuous-Integration（持续集成）"><a href="#Continuous-Integration（持续集成）" class="headerlink" title="Continuous Integration（持续集成）"></a>Continuous Integration（持续集成）</h3><p>CI System:</p>
<p><img src="https://static.ddlee.cn/static/img/DevOps/CI.png" alt="CI"></p>
<h4 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h4><p>Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks such as building, testing, and deploying software.</p>
<p>Using command:<br><code>packer build -only=&lt;cloud service target&gt; control-server.json</code></p>
<p><a href="https://github.com/ddlee96/devops-intro-project/blob/master/packer-templates/control-server.json" target="_blank" rel="external">Example control-server.json file</a></p>
<p>Jenkins was configured to be installed according to <code>Provisioners</code>.</p>
<p>After building and launching, access Jenkins via URL <code>/jenkins</code>.</p>
<h4 id="Testing（测试，QA）"><a href="#Testing（测试，QA）" class="headerlink" title="Testing（测试，QA）"></a>Testing（测试，QA）</h4><ul>
<li>Unit Testing</li>
<li>Regression testing</li>
<li>Smoke testing</li>
<li>System Integration testing</li>
<li>Automate acceptance testing</li>
<li>Manual QA testing</li>
</ul>
<p>Adding Manual QA step in Pipeline</p>
<p><img src="https://static.ddlee.cn/static/img/DevOps/pipline.png" alt="Pipeline"></p>
<h4 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h4><p>Monitoring process:</p>
<p><img src="https://static.ddlee.cn/static/img/DevOps/Monitoring.png" alt="Monitoring"></p>
<h3 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h3><p><a href="https://www.udacity.com/wiki/ud611#!#additional-resources" target="_blank" rel="external">Course Wiki</a></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>DevOps从开发和运维合作的角度审视软件开发过程，并提供了一套方法论，涉及开发、测试、部署、维护、监测各个方面。软件行业，不仅仅是写代码而已。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Internet </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> MOOCs </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python与SQL_Server的交互：pyODBC, pymssql, SQLAlchemy]]></title>
      <url>https://blog.ddlee.cn/2017/03/16/Python%E4%B8%8ESQL_Server%E7%9A%84%E4%BA%A4%E4%BA%92%EF%BC%9ApyODBC,%20pymssql,%20SQLAlchemy/</url>
      <content type="html"><![CDATA[<p>Windows平台下Python读取、写入SQL Server相关的函数库，文章结构如下：</p>
<p><img src="https://static.ddlee.cn/static/img/Python+SQLserver/Python+SQL_Server.png" alt="Python+SQLserver"></p>
<h3 id="Python-Drivers"><a href="#Python-Drivers" class="headerlink" title="Python Drivers"></a>Python Drivers</h3><h4 id="PyODBC"><a href="#PyODBC" class="headerlink" title="PyODBC"></a>PyODBC</h4><p>Annaconda下可以用<code>pip install pyodbc</code>安装，也可以到<a href="https://www.microsoft.com/en-us/download/details.aspx?id=50420" target="_blank" rel="external">这里</a>下载。</p>
<p>首先建立<code>connection</code>对象：</p>
<pre><code class="python">import pyodbc
conn = pyodbc.connect(
    r&#39;DRIVER={ODBC Driver 11 for SQL Server};&#39;  #or {ODBC Driver 13 for SQL Server}
    r&#39;SERVER=ServerHostName;&#39;
    r&#39;DATABASE=DBName;&#39;
    r&#39;UID=user;&#39;
    r&#39;PWD=password&#39;
    )
</code></pre>
<p>添加游标（Cursor）对象并执行SQL查询语句：</p>
<pre><code class="python">cursor = conn.cursor()
cursor.execute(&#39;SQL Query Goes Here&#39;)
for row in cursor.fetchall():
  print(rows.[column name])
</code></pre>
<p>更多信息参见<a href="https://docs.microsoft.com/en-us/sql/connect/python/pyodbc/python-sql-driver-pyodbc" target="_blank" rel="external">MSDN DOCs</a>。</p>
<h4 id="pymssql"><a href="#pymssql" class="headerlink" title="pymssql"></a>pymssql</h4><p>同样可以用<code>pip install pymssql</code>安装，也可以到<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#pymssql" target="_blank" rel="external">这里</a>，然后用<code>pip</code>安装<code>wheel</code>文件。</p>
<p>pymssql目前还不支持Python3.6，这点要注意下。</p>
<p>pymssql的用法跟pyODBC很像，下面是官网给出的例子：</p>
<pre><code class="python">from os import getenv
import pymssql

server = getenv(&quot;PYMSSQL_TEST_SERVER&quot;)
user = getenv(&quot;PYMSSQL_TEST_USERNAME&quot;)
password = getenv(&quot;PYMSSQL_TEST_PASSWORD&quot;)

conn = pymssql.connect(server, user, password, &quot;tempdb&quot;)
cursor = conn.cursor()
cursor.execute(&quot;&quot;&quot;
IF OBJECT_ID(&#39;persons&#39;, &#39;U&#39;) IS NOT NULL
    DROP TABLE persons
CREATE TABLE persons (
    id INT NOT NULL,
    name VARCHAR(100),
    salesrep VARCHAR(100),
    PRIMARY KEY(id)
)
&quot;&quot;&quot;)
cursor.executemany(
    &quot;INSERT INTO persons VALUES (%d, %s, %s)&quot;,
    [(1, &#39;John Smith&#39;, &#39;John Doe&#39;),
     (2, &#39;Jane Doe&#39;, &#39;Joe Dog&#39;),
     (3, &#39;Mike T.&#39;, &#39;Sarah H.&#39;)])
# you must call commit() to persist your data if you don&#39;t set autocommit to True
conn.commit()

cursor.execute(&#39;SELECT * FROM persons WHERE salesrep=%s&#39;, &#39;John Doe&#39;)
row = cursor.fetchone()
while row:
    print(&quot;ID=%d, Name=%s&quot; % (row[0], row[1]))
    row = cursor.fetchone()

conn.close()
</code></pre>
<p>详细用法参见<a href="http://www.pymssql.org/en/stable/index.html" target="_blank" rel="external">pymssql docs</a>和<a href="https://docs.microsoft.com/en-us/sql/connect/python/pymssql/python-sql-driver-pymssql" target="_blank" rel="external">MSDN DOCs</a></p>
<h4 id="SQLAlchemy-Python-SQL-Toolkit"><a href="#SQLAlchemy-Python-SQL-Toolkit" class="headerlink" title="SQLAlchemy(Python SQL Toolkit)"></a><a href="https://www.sqlalchemy.org/" target="_blank" rel="external">SQLAlchemy</a>(Python SQL Toolkit)</h4><p>SQLAlchemy提供了一系列丰富、完整、（我看不懂）的API用于数据库操作。这里只谈其<code>create_engine</code>方法。</p>
<pre><code class="python">from sqlalchemy import create_engine
# pyodbc
engine = create_engine(&#39;mssql+pyodbc://user:password@DSNname&#39;) #需要配置DSN，参见最后一节

# pymssql
engine = create_engine(&#39;mssql+pymssql://user:password@Hostname:port/DBname&#39;)
</code></pre>
<p>利用创建好的<code>engine</code>，可以结合pandas库进行批量的读取、写入操作。</p>
<p>用SQLAlchemy与其他类型的数据库建立链接的方法参见<a href="http://docs.sqlalchemy.org/en/latest/core/engines.html" target="_blank" rel="external">这里</a>。</p>
<h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><p>利用pyODBC和pymssql拉取的对象需要进一步处理才能进行常见的数据清洗等工作，而Pandas也提供了SQL相关的方法，在SQLAlchemy的辅助下，可以将<code>DataFrame</code>对象直接写入table。</p>
<h5 id="读取：pd-read-sql"><a href="#读取：pd-read-sql" class="headerlink" title="读取：pd.read_sql()"></a>读取：pd.read_sql()</h5><p><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html" target="_blank" rel="external">API</a>：</p>
<pre><code class="python">pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None)
</code></pre>
<p>其中的<code>con</code>参数，可以传入SQLAlchemy建立的<code>engine</code>对象，也可以是pyODBC或者pymssql建立的<code>DBAPI2 connection</code>对象。</p>
<h5 id="写入-pd-DataFrame-to-sql"><a href="#写入-pd-DataFrame-to-sql" class="headerlink" title="写入:pd.DataFrame.to_sql()"></a>写入:pd.DataFrame.to_sql()</h5><p><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html" target="_blank" rel="external">API</a>:</p>
<pre><code class="python">DataFrame.to_sql(name, con, flavor=None, schema=None, if_exists=&#39;fail&#39;, index=True, index_label=None, chunksize=None, dtype=None)
</code></pre>
<p>这里的<code>con</code>参数，只支持sqlite3的<code>DBAPI2 connection</code>对象，支持所有的<code>SQLAlchemy engine</code>对象。<br><code>name</code>参数传入表名，用<code>if_exists</code>参数控制表存在时的动作：</p>
<ul>
<li><code>‘fail’</code>: 啥也不干。</li>
<li><code>’replace‘</code>: 将原有表删除，新建表，插入数据。</li>
<li><code>’append&#39;</code>: 在表中插入数据。表不存在时新建表。</li>
</ul>
<h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p>利用<code>Sqlcmd</code>命令，也可以在命令行下执行SQL文件，用法如下：</p>
<pre><code class="bash">sqlcmd -U user -P password -S server -d DBName -i /path/to/myScript.sql
</code></pre>
<p>这样可以有如下思路，将数据写入.SQL文件，再生成.bat文件（批量）写入上述命令，之后完成执行。</p>
<h3 id="DSN"><a href="#DSN" class="headerlink" title="DSN"></a>DSN</h3><p>Windows下可以配置DSN(Data Source Names)预先存储数据库连接的信息，在<em>Control Panel</em> -&gt; <em>Administrative Tools</em> -&gt; <em>ODBC Data Source</em> 下添加即可。</p>
<p>配置好DSN后，pyODBC的连接过程可以简化为：</p>
<pre><code class="python">conn = pyodbc.connect(r&#39;DSN=DSNname;UID=user;PWD=password&#39;) #UID和PWD也可以在DSN中配置
</code></pre>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>Python与文件的IO、SQL数据库的读写时有中文字符可能会有编码问题。一种方案是在中文字符串前添加N，如<code>N&#39;python大法好&#39;</code>；另一种方案是传入<code>encoding</code>参数，常用的中文编码有<code>GB2123</code>，<code>GB18030</code>，推荐的还是统一用<code>UTF-8</code>编码、解码。</p>
<p>利用如下命令，可以在SQLAlchemy中指定编码：</p>
<pre><code class="python">engine = create_engine(&#39;mssql+pymssql://user:password@HostName\DBname&#39;, connect_args = {&#39;charset&#39;:&#39;utf-8&#39;})
</code></pre>
<p>其他自定义<code>DBAPI connect()</code>参数的方法参见<a href="http://docs.sqlalchemy.org/en/latest/core/engines.html#custom-dbapi-connect-arguments" target="_blank" rel="external">这里</a>。</p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> SQL </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[再次折腾我的WNDR4300：OpenWrt文件共享]]></title>
      <url>https://blog.ddlee.cn/2017/03/12/%E5%86%8D%E6%AC%A1%E6%8A%98%E8%85%BE%E6%88%91%E7%9A%84WNDR4300/</url>
      <content type="html"><![CDATA[<blockquote>
<p>生命不惜，折腾不止。</p>
</blockquote>
<h3 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h3><p>再次成为IOS用户后，访问Google和文件共享成了两大需求。问题出现了，就要解决，于是有此文记录的活动。</p>
<h3 id="重新安装OpenWrt"><a href="#重新安装OpenWrt" class="headerlink" title="重新安装OpenWrt"></a>重新安装OpenWrt</h3><p>OpenWrt已经到了<code>15.05</code>版本，版本代号是<code>Chaos Calmer</code>。重装需要的<code>-factory.img</code>，可以在<a href="https://downloads.openwrt.org/chaos_calmer/15.05/" target="_blank" rel="external">这里</a>下载。</p>
<p>我的WNDR4300平台是<code>ar71xx</code>，可以从OpenWrt对应的<a href="https://wiki.openwrt.org/toh/hwdata/netgear/netgear_wndr4300_v1" target="_blank" rel="external">硬件主页</a>找到固件镜像文件。</p>
<h4 id="TFTP重装"><a href="#TFTP重装" class="headerlink" title="TFTP重装"></a>TFTP重装</h4><p>如果你的路由器还是出厂系统的话，可以通过登入后台在线上传镜像文件进行刷机，而我的已经是OpenWrt系统，只能通过网页端升级，故选用了TFTP方式刷机。</p>
<p>刷机步骤摘自<a href="https://wiki.openwrt.org/toh/netgear/wndr4300" target="_blank" rel="external">OpenWrt wiki</a></p>
<p>&gt;</p>
<blockquote>
<ol>
<li>set a static IP on your computer, i.e 192.168.1.35, and connect the ethernet cable to the router</li>
<li>power on the router</li>
<li>press and hold the RESET button as soon as the switch LEDs light up.</li>
<li>keep holding RESET until the power LED begins to flash orange and then green.</li>
<li>once the power LED is flashing green, release RESET</li>
<li>start the TFTP transfer to router at 192.168.1.1. In your computer execute:<br><code>tftp 192.168.1.1 -m binary -c put factory.img</code></li>
</ol>
</blockquote>
<p>总体来说是分为三步：</p>
<ol>
<li>将电脑与路由器设置在同一内网中</li>
<li>令路由器进入恢复模式</li>
<li>利用TFTP将刷机包推入路由器</li>
</ol>
<h3 id="U盘挂载，文件共享"><a href="#U盘挂载，文件共享" class="headerlink" title="U盘挂载，文件共享"></a>U盘挂载，文件共享</h3><p>安装好OpenWrt后，就可以从网页端访问路由器，设置PPPoE拨号，设置WIFI等等。</p>
<h4 id="U盘挂载"><a href="#U盘挂载" class="headerlink" title="U盘挂载"></a>U盘挂载</h4><p>U盘挂载部分主要参考了<a href="https://my.oschina.net/umu618/blog/282984" target="_blank" rel="external">跟 UMU 一起玩 OpenWRT（入门篇6）：挂接 U 盘</a>。</p>
<p>首先是安装相应的包：</p>
<pre><code class="bash">opkg update

# 核心包
opkg install kmod-usb-storage
opkg install kmod-scsi-generic

# 文件系统
opkg install kmod-fs-ext4

# 辅助工具
opkg install usbutils fdisk e2fsprogs
</code></pre>
<p>利用<code>lsusb</code>命令查看U盘是否已经被路由器识别。</p>
<p>这时可以选择用fdisk进行重新分区，不需要分区的话，可以用命令<code>ls /dev | grep sd</code>查看/dev分区中是否已经出现U盘。</p>
<p>在OpenWrt上使用U盘，建议用ext4格式，可以用下面的命令进行格式化：</p>
<pre><code># sda1为上一命令得到的结果
mkfs.ext4 /dev/sda1
</code></pre><p>接下来就可以用<code>mount</code>命令进行挂载了：</p>
<pre><code># 路径/mnt/usb/即为挂载目标点
mkdir /mnt/usb
touch /mnt/usb/USB_DISK_NOT_PRESENT
chmod 555 /mnt/usb
chmod 444 /mnt/usb/USB_DISK_NOT_PRESENT
mount /dev/sda1 /mnt/usb
</code></pre><p>这时可以测试一下，如果U盘里面存储了文件，可以通过<code>/mnt/usb</code>访问的到。</p>
<p>下面是开机自动挂载U盘的命令。</p>
<pre><code class="bash"># block-mount blkid用于查看U盘的UUID
opkg install block-mount blkid

# 实际上要操作的是fstab的配置文件/etc/config/fstab，要将enabled值改成1
block detect &gt; /etc/config/fstab
uci set fstab.@mount[-1].target=&#39;/mnt/usb&#39; u
ci set fstab.@mount[-1].enabled=1
uci commit fstab
</code></pre>
<p>更详细的信息可以参见<a href="http://wiki.openwrt.org/doc/uci/fstab" target="_blank" rel="external">这里</a></p>
<h4 id="文件共享"><a href="#文件共享" class="headerlink" title="文件共享"></a>文件共享</h4><p>文件共享可以通过FTP和SAMBA，推荐的方式是SAMBA。</p>
<h5 id="SAMBA"><a href="#SAMBA" class="headerlink" title="SAMBA"></a>SAMBA</h5><p>安转SAMBA：</p>
<pre><code class="bash">opkg update
opkg install samba36-server

# luci程序，可选
opkg install luci-app-samba
</code></pre>
<p>安装好SAMBA后，主要配置两个参数，一是共享文件夹的路径，如<code>/mnt/usb/sambashare</code>，可以通过更改配置文件<code>/etc/samba/smb.conf</code>实现，也可以通过luci实现。</p>
<p>示例：</p>
<pre><code>[sambashare]
path = /mnt/usb/sambashare
valid users = root
read only = no
guest ok = yes
create mask = 0750
directory mask = 0750
</code></pre><p>第二个参数是访问账户，可以通过命令<code>sambpasswd -a</code>将你的当前用户加入到SAMBA的组中，需要设置一个密码。另外，可能需要将配置文件<code>/etc/samba/smb.conf</code>的[global]中的<code>invalid users = root</code>注释掉。</p>
<p>最后，设置SAMBA服务启动和开机自启</p>
<pre><code class="bash">/etc/init.d/samba start
/etc/init.d/samba enable
</code></pre>
<h5 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h5><p>FTP可以用<code>vsftpd</code>包来设置，大致过程与SAMBA类似：设置路径、添加用户、设置自启。</p>
<p>SAMBA服务可以在Windows文件资源管理器中自动检测的到，Linux下可以通过<code>smb://Host/sharepath</code>访问，在IOS系统中，类似Documents的应用也支持添加SAMBA的功能。</p>
<p>这里强推一下Documents这个应用，结合PDF EXPERT，已经成为了我的文档中心。</p>
<h3 id="访问Google"><a href="#访问Google" class="headerlink" title="访问Google"></a>访问Google</h3><p>这部分操作相当复杂，主要参考<a href="https://cokebar.info/archives/664" target="_blank" rel="external">这里</a>，感谢博主。</p>
<h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>这天的活动，本来只有我和上帝知道，再过一个月，就只有上帝知道了。遂作笔记。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Digital Life </tag>
            
            <tag> OpenWrt </tag>
            
            <tag> 路由器 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谁会不厌其烦地安慰那无知的少年（三）]]></title>
      <url>https://blog.ddlee.cn/2017/03/03/%E8%B0%81%E4%BC%9A%E4%B8%8D%E5%8E%8C%E5%85%B6%E7%83%A6%E5%9C%B0%E5%AE%89%E6%85%B0%E9%82%A3%E6%97%A0%E7%9F%A5%E7%9A%84%E5%B0%91%E5%B9%B4-3/</url>
      <content type="html"><![CDATA[<blockquote>
<p><em>年轻的时候可以随随便便喜欢一个人，可千万别真动情。那样的话你的余生就剩两种状态了，一种叫做想她，另一种是为克制自己想她而努力。——丁丁前舍友</em></p>
</blockquote>
<p>丁丁的舍友告诉他，那年不懂事，一直陷于人生的错觉之中。</p>
<p>他觉得那女生好像喜欢她，做什么事儿都是像在针对他，总是跑来问问题呀，不懂的时候卖个萌啊，连谢谢的话都是奶声奶气的。</p>
<p>可他怎么能被这个给连累了呢。</p>
<p>他可是老师眼中最有希望的学生，早熟的他也明白合适的平台对自己的发展是多么重要。他觉得在人生的一段时间内能单纯地为一个目标而奋斗是一件幸福的事儿，任何分心的想法都是罪恶。</p>
<p>他在开始之前，就故作冷漠，就像结束了之后想要挽回那样。</p>
<p>丁丁插着话问到底什么开始什么结束的啊？</p>
<p>舍友答，年少的初恋啊我的旁友！</p>
<p>舍友顿了顿，眼里含着惋惜。</p>
<p>讲真，我是那种动情就会倾其所有的人，我真真觉得一生就只够爱一个人。但让我从没想到的是，我的故作冷漠才是动情的开始啊。</p>
<p>那时候我千方百计地回避她。</p>
<p>我特别跟组里的同学换了座位，这样就能离她远一点。</p>
<p>问题的时候我也爱搭不理的，不是把她推给别人就是拖着藏着。</p>
<p>她也算知趣，渐渐的就不来烦我了。</p>
<p>就这样吧，高考完了以后，我们去了不同的学校，离得八十万杆子都打不着。</p>
<p>但我逐渐的发现，这颗种子，已经在我的心底长成了参天大树，不管我给它什么样的脸色，它还是生长起来了。</p>
<p>我再也不能回避它了，我再也不能隐藏它了。</p>
<p>我以前听人家说暗恋一个人的时候，把她的动态错过都会有罪恶感。</p>
<p>我细细的品味她的日志、说说里流露出来的情感，挖空心思复原她写下这些文字时的心情，然后小心翼翼地写下我的评论，斟酌一下，再发表。然后就是每隔几个小时就刷一下，看看她回复了没有。</p>
<p>我也找她聊天，谈心，新的生活还适应没，高数有哪些不懂的跟我说说。</p>
<p>我也跟她讲我的近况，我在听什么歌，我在读哪些书。</p>
<p>可我从来都不敢表露我真实的心意，我也从来不敢提高中时候我的那段冷漠的时光。</p>
<p>可是，你知道吗，就跟吃巧克力一样，她吃到了苦的，我却吃了块甜的，德芙，带榛果颗粒的。</p>
<p>我终于等到了一个机会——她生病住院了。</p>
<p>急性胃炎，但她没跟我说，她的闺蜜告诉我的。</p>
<p>我买了票，赶到她所在的城市，在一个下着小雨的傍晚。</p>
<p>行人匆匆，从四面赶往八方。风催着云，一来一回地玩弄着月亮，雨打在肩上，我才知道我还没有方向。</p>
<p>我给她打电话，说我来看你来了，你在哪家医院。</p>
<p>她说你怎么来了，她已经快好了，明天就要出院，那你过来吧，在江东北路的那家人民医院，8号楼，324。</p>
<p>我说没事儿，马上就到。</p>
<p>不过地铁并不方便，只能在珠江路那里下，我就打算骑ofo过去。</p>
<p>然而我还是太年轻了，南方的冬天下着雨，可没那么好欺负，找路，问路，手冻僵，衣服也淋湿了，我想着张士超华师大的姑娘真的那么可爱吗。</p>
<p>等我赶到时，已经是需要照顾的人了，一副洋葱模样，就剩一层一层剥开了。</p>
<p>狼狈的我跑到厕所里，等个没人的空档，用烘干机吹了吹头发，把外套脱下来搭在胳膊上，这才往病房赶去。</p>
<p>丁丁的舍友推了下眼镜，接着说。</p>
<p>你可知道什么叫近乡情更怯呀，就跟查高考成绩一样啊，你再往前一步，就把那些想象过的所有美好的可能性全破除了，木已成舟，一切皆不可挽回，尽管，尽管你不往前一步，一切也早就注定了呀。</p>
<p>我在病房门前愣住了，万一里面还有人怎么办，她的同学在晚上应该会陪她吧，她不会有男朋友了吧？</p>
<p>我跑到离门远一点的地方，又给她打了个电话，我说我快到了，你有什么想吃的我给你带点。</p>
<p>她说不用了，你过来就好，她也想赶快见到我。</p>
<p>我说好的，这么突然出现，没赶上不方便的时候吧。</p>
<p>她说没事儿，你直接过来吧，哪有什么方便不方便的。</p>
<p>挂了之后，我在楼里瞎逛了几圈，顺手把紧急逃生的路线考察了一下，发现还是很科学的，指引也做的很到位。估摸时间差不多了，我就敲门进去了。</p>
<p>她留起了长发，比高中的时候成熟不少，但终归有病在身，脸色有些发白，不过酒窝还是那样可爱。</p>
<p>我们聊起来，从病情开始，一直聊到那些在网易云音乐的歌曲下面刷评论的考研党们到底考上了没。</p>
<p>她似乎很开心，我也很开心。</p>
<p>她说上了大学就没跟别人聊这么久过，还是以前的同学好呀。</p>
<p>我说那当然了，以后有什么事你第一个告诉我。</p>
<p>要走的时候，她说谢谢我这么大老远地跑过来，不过病差不多要好了，明天亲自到车站送送你。</p>
<p>我说不用了，我自己走就行，你好好养着身体吧，注意一下饮食。</p>
<p>离开医院 ，我随便找了家旅馆住下来。心底里无限的舒适与满足。但很快，紧张与自责将我包裹起来。</p>
<p>太懦弱了我真是，聊那些没什么用的干啥，我该直接跟她说我喜欢你三年了我们在一起吧。</p>
<p>可又转念一想，这也有点趁人之危吧，还是等等再说？</p>
<p>这一等就是一夜，我慢慢睡着，天刚刚破晓。</p>
<p>第二天，她还是来送我了，下地铁后，她用手机看了下时间，说还不晚不用着急。</p>
<p>她竟然用的Xperia。我心想我喜欢的女孩子就是有格调。</p>
<p>然后我就看到了手机桌面上男孩子的傻笑。</p>
<p>那个男孩子似乎不是我，我笑的时候不傻，眼睛眯成一条缝。</p>
<p>我说这也不早了你赶紧回去吧。</p>
<p>她说你开玩笑呢这才几点啊。</p>
<p>我说不对，我不是这个意思，我是说你不用跟我一起等了，我自己等，我自己能行。</p>
<p>她告诉我她当然相信我能行，不然怎么能自己跑过来看她呢。</p>
<p>我说也是哈，我这么催你干哈。</p>
<p>后面的事情我自己也记不清了。</p>
<p>回来的时候，出站换乘，转角碰见一家鲜花店，就进去买了一束满天星，捧着它回到寝室，摆在桌上。</p>
<p>我是眼睁睁地看着那一束花慢慢枯萎的。</p>
<p>不插在水中的话，只用了三天不到。</p>
<p>舍友说我那三天跟个傻逼一样。</p>
<p>后来她说我是她最好的朋友，跟高中的那个我完全不一样了。</p>
<p>原来她从来就没喜欢过我，而我也从来没承认过我那么心动，但你知道吗？这的的确确发生了。</p>
<p>舍友觉得可以做结了，便说出了这句丁丁永生难忘的话。</p>
<p>年轻的时候可以随随便便喜欢一个人，可千万别真动情。那样的话你的余生就剩一种状态了，那就是想她。</p>
<p>丁丁说没事儿你还有机会，天下没有不散的筵席，他们迟早会分的。</p>
<p>舍友说丁丁是傻逼。</p>
<p>————————————————全文完—————————————————</p>
<p>我不再强说上面的故事是瞎编的了。它们是丁丁亲口告诉我的，在一次卧谈会上。</p>
<p>丁丁说在刚好记得的时候讲出来，其实是自私的。</p>
<p>他说他从小到大失去了很多人，从每天早到学校开门的劳动课老师到害了白血病的不幸前桌，从打架斗殴满嘴义气话的小魔王到奔走他乡借读名校的竞争对手，当好友列表里的灰色头像终于不再跳动的时候，我就不再是完整的了，他们把我的一部分带走了，而且永远找也找不回来了。这个永远是真的。</p>
<p>我跟丁丁说你错了，你不知道更可怕的事情。你有没有想过，即使是陪你一起长大的人，也有很多东西找不回来了。像你的父母，你的淘气和无知，早就淹没在他们眼角的层层皱纹里了。而且，是你亲手把它们埋葬进去的。你看，谁都没有失去谁，谁也失去了谁。</p>
<p>丁丁说是啊，我们都变了，变得都有些记不起从前的样子了。人们总是到失去了才懂得珍惜，这真是瞎话，我们就从来没有拥有过。</p>
<p>我记起很久以前的一个秋天，我打开了一册我昔日嗜爱的书读了下去，突然回复到十四岁时那样温柔而多感，我在那里面找到了一节写在发黄的纸上的以这样两行开始的短诗：
　　　 　</p>
<blockquote>
<p>在你眼睛里我找到了童年的梦，<br>如在秋天的园子里找到了迟暮的花……</p>
</blockquote>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2017年3月</p>
]]></content>
      
        <categories>
            
            <category> Writing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谁会不厌其烦地安慰那无知的少年（二）]]></title>
      <url>https://blog.ddlee.cn/2017/02/15/%E8%B0%81%E4%BC%9A%E4%B8%8D%E5%8E%8C%E5%85%B6%E7%83%A6%E5%9C%B0%E5%AE%89%E6%85%B0%E9%82%A3%E6%97%A0%E7%9F%A5%E7%9A%84%E5%B0%91%E5%B9%B4-2/</url>
      <content type="html"><![CDATA[<blockquote>
<p><em>不要做父母手中的烤鸭，要做一只自由的小小种马。——刘星，《（假的）家有儿女》</em></p>
</blockquote>
<p>丁丁再回到这条老街时，又是一年的光景。</p>
<p>这一年，家乡添了几处新房和俏媳妇，添了几家麻将交流中心，添了几座坟头。</p>
<p>村后的河今年却冻住了——往年不上冻的，因为里面东西太多。</p>
<p>村前公路两旁的树全砍掉了。主人缺钱，不缺树。</p>
<p>目力所及，坑洼的油漆路向北延伸到省道上，两旁田地里丛丛的麦子依偎而息，灰蒙蒙的天，树林间掩映着冬日里小姑娘红扑扑的脸蛋，那是北方的夕阳。哎呦，还蒙了层雾气。</p>
<p>这次回老家，丁丁照例去拜访过道尽头被奶奶称为”二嫂“的老太太。</p>
<p>二嫂是帝都过来的知青，这些年没入我们的乡音，跟谁也是一口侉侉的北京话。</p>
<p>她最著名的话是，“我主的了疼，也主的了管”。</p>
<p>这是跟人家解释为什么老打孙子。一时成为村里溺爱孙子老传统中的一股清流。</p>
<p>可老人家现在状态不好：去年初四，脑出血，救回来之后半边失去了控制，歪了嘴，动不了腿。</p>
<p>我进了门，走到轮椅边。老人眼睛亮了起来，一只手撑着扶手，要站起来。</p>
<p>我大声说奶奶您不用起来，多累啊。</p>
<p>二嫂摇着头坐下，攥着我的手，晃来晃去。又赶紧把暖手袋扯过来，叫我捧着。</p>
<p>就像小时候那样。</p>
<p>二嫂是看着我长大的。奶奶经常带着我到二嫂家里串门，二嫂家里有糖吃，有奶喝。</p>
<p>那时候我最喜欢翻彻二嫂厚厚的影集，上面有好多我没见过的东西。</p>
<p>奶奶你耳朵边别着的是什么花呀，那时候你几岁。</p>
<p>二嫂说那年她十六，别着的花叫白玉兰。</p>
<p>今年她七十六。照片上的小姑娘带一点自信，含一丝羞赧，就像每个十六七岁的女孩子那样。</p>
<p>这让我想起妈妈。妈妈年轻的时候追邓丽君、小虎队，最喜欢的是粉红色的回忆。家里有一张她结婚时的照片，大红毛衣，傻傻的杵在那里，另一头爸爸给二叔骑在背上，向妈妈鞠躬，胸前歪着一朵大红花。</p>
<p>我没见过作为年轻姑娘的二嫂和妈妈是什么样子的，跟我相关的，只有她们逐渐老去的岁月。</p>
<p>二嫂晃动着身子，她打算站起身来。</p>
<p>我扶着她，走一步，拖一步，不违背，不阻挡。</p>
<p>五六米的距离，老人已经气喘吁吁。我也不说话，我单单陪着她。</p>
<p>院里的枣树上落了一只麻雀，不知为何她没回南方的家。隔壁的二层小楼开始掌起灯火，夜色也正吞下了半边天。</p>
<p>二嫂接着往外拖着步子，这时媳妇却迎着面从小卖部回来。</p>
<p>哎呀，涛你怎么让你奶奶出屋里来了？外面冷，娘咱回屋里吧。</p>
<p>二嫂不肯，但她做不了主。</p>
<p>这已不是她做主的日子了。</p>
<p>爷爷大二嫂好多岁，早就没了精神。多少年大大小小，一直是二嫂操持着。</p>
<p>去年的时候，我坐在炕头边，绕着问她年轻时候的故事。</p>
<p>她说她的一生就分为两部分，给大伙种地和给自己种地。前半段三十年，后半段三十年。</p>
<p>明明从北京赶过来，她却说这里更冷一些。村支书被打得藏在柜子底下，三千斤麦子换来的推车充了公，大雨下到把房子冲塌，夜不闭户，好冷。</p>
<p>二嫂说后来却是倒春寒。家乡的新媳妇，都凑不出一件体面衣裳。地里什么东西也不长。饿死的人排着队。</p>
<p>我问再后来呢？自己种总好些了吧？</p>
<p>二嫂说自己种也要上交粮食给国家的。那年她推着小车，走了二十几里的土路，把麦子送到乡上。三十年了。</p>
<p>二嫂说这么多年看上去一直是我在做主支撑着这个家，但实际上我从来都没做过主，我对自己也做不了主，我对谁也做不了主。</p>
<p>我说还是我们这一代人幸福啊，赶上了好的时候。</p>
<p>二嫂说那只是看起来，长大了你就明白了。</p>
<p>然而我从来都长不大，二嫂却变老了。</p>
<p>二嫂老了，但从没老糊涂，也没装过糊涂，直到突然的疾病将糊涂的能力赐予给她。</p>
<p>回到屋里，二嫂就又安静地坐下来。电视里恰巧是场晚会，在希望的田野上。</p>
<p>夜幕已全然降临。猎户座的三星嵌在南面的而天空，月亮瘦成眉毛，挑在树枝上，除此之外，一片看不透的灰色将视野罩的密不透风。</p>
<p>我瞪着窗外，正出神，二嫂那边却哼了起来，摇起我的手。</p>
<p>呜呜声。奶奶又回到了回不去的小时候。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2017年2月</p>
]]></content>
      
        <categories>
            
            <category> Writing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谁会不厌其烦地安慰那无知的少年（一）]]></title>
      <url>https://blog.ddlee.cn/2017/01/27/%E8%B0%81%E4%BC%9A%E4%B8%8D%E5%8E%8C%E5%85%B6%E7%83%A6%E5%9C%B0%E5%AE%89%E6%85%B0%E9%82%A3%E6%97%A0%E7%9F%A5%E7%9A%84%E5%B0%91%E5%B9%B4-1/</url>
      <content type="html"><![CDATA[<blockquote>
<p><em>一个男孩要下过多少电影，才能称得上是一个男人？一只海鸥要飞过多少海洋，才能在柔柔的沙滩上安息？——鲍勃·迪伦，《答案在风中飘荡》</em></p>
</blockquote>
<p>星星眨着眼，银河却不见。万家灯火散落在不遥远的远方，贪婪的夜色吞噬着视野，列车不紧不慢地刺破雾气的深不可测，卧铺床头的小台灯透过车窗温暖出朦胧一片，笼住返乡人的放松与期盼。</p>
<p>其实丁丁差点没赶上火车。亏得遇到老司机，路上没怎么堵。过检票口的时候，广播刚刚喊着“你所乘坐的班次已停止检票”。</p>
<p>火车终于安稳地行着。丁丁的心情也慢慢舒畅起来。</p>
<p>丁丁趴在铺上，翻看相册，回想这又一个人生七年。</p>
<p>小学到中学就是一趟火车，有起点也有终点，不慌不忙。大学是脱了轨的同一趟火车，东栽西撞，没有诗也到不了远方。</p>
<p>想到这里，丁丁下了铺，留意了一下安全锤的位置，然而，在回来时，他还是不可避免地被旁边的大叔注意到了。</p>
<p>嘿，小伙子，你也用Lumia 啊。</p>
<p>丁丁尴尬地讲，没，只是备用机，主力还是安卓。心想着竟然还被看出来了，不过正好，用Lumia不装逼，那跟咸鱼有什么区别。</p>
<p>大叔你做什么工作的呀。</p>
<p>大叔讲他是个半个码农，三倍的房奴，两个孩子的爹地，一个老婆坚实的依靠。</p>
<p>丁丁说自己是三个舍友的爸爸，五门课的开课赞助商，七个女生的备胎，九个社团的划水副总监。</p>
<p>大叔说你这就是我的Pro版啊，深交吗小伙子？</p>
<p>丁丁说，好。</p>
<p>可这一开口，大叔就是从诗词歌赋到人生哲学。只不过，没有雪也没有月亮，我不是紫薇他也不叫尔康。</p>
<p>大叔并不大，现在在南京，江北一套房，鼓楼一套学区。两个儿子，大的刚上一年级，小的还不会撒谎。</p>
<p>自己公司年底出了状况，没能跟家人坐同一趟车回家。</p>
<p>大叔说自己本科数学，毕了业才发现自己卵没什么用。女朋友学计算机，早就找好了工作，自己只好考了研，后来拿了个硕士，主攻信号转发与缓存。</p>
<p>丁丁说我也数学。</p>
<p>大叔抿了抿嘴，嗯，有意思有意思。</p>
<p>大叔说那我给你介绍介绍考研经验吧。</p>
<p>丁丁说好啊好啊。</p>
<p>那年考研的形势很严峻，因为减招。</p>
<p>为了考研，大三那年寒假，我初五就从家里跑出来了。赶巧的是，那年跟今年一样，过年赶得好晚，我统共在家不到十天。</p>
<p>临走那天晚上，爸爸到单位值班，去之前又塞给我几百块钱，说穷家富路，但这种行为被我义正辞严地拒绝了。可爸爸走后，我泪湿眼底。</p>
<p>因为这一离开就又是半年。</p>
<p>考上大学第一年回家，奶奶跟我说你走后你爸来我这儿的时候哭了，说你跟小鸟一样飞走了。我说也是啊，我长大了，爸爸的一个时代也结束了呀，就在我报完到送他回去的那一刻。</p>
<p>那天在楼下值班室那里领钥匙，爸爸在一边摸着头笑，见我回头，他跟遇到喜欢的女生那样不好意思，红着脸。</p>
<p>爸爸的一个时代结束了呀。</p>
<p>还记得，我上小学那会儿，连午休都要家长签字确认的，还有作业也是，爸爸兢兢业业地把题都重新算一遍，马虎的地方狠狠批我一顿，这才用方方正正的钢笔给我签上“家长已检查”，现在我才知道，这叫“背书”。</p>
<p>那时候妈妈在一边儿踩着缝纫机，看点播台的我被爸爸叫过去，扭扭捏捏地摸着后脑勺，阳台上水仙开着，香味儿就飘到屋里来。</p>
<p>其实那时候的我才最懂事儿。那时我最大的梦想就是娶了班上最文静的女生，让妈妈少操点儿心。而她当时就是我的同桌，放学我们还一起走到灵石路的尽头，走过小酒馆的门口。后来四五年级，起了流言，我们就分了。</p>
<p>后来在外面求学，跟父母在一起的时间就越来越少了。那时候我最喜欢的时候是坐在大巴士高高的最后一排，靠窗，看路边的杨树一棵一棵闪过，我觉得我的人生康庄大道就在脚下一点一点伸展开来。</p>
<p>爸爸给我的支持也越来越少了。他不懂遗传平衡定律，找不到辅助线，也人脸识别不了虚拟语气。我的小小心思就像宇宙那般，无边地膨胀起来了。</p>
<p>高考就是碰到气球的那根针。我感觉自己是被发配到了南方，而且还被冻成了狗。</p>
<p>丁丁顺着说，南方确实冷的不行，尤其下雨天。</p>
<p>大叔说，你看，这些小事，我不说，就要一点一点埋葬在潺潺流去的岁月里了。</p>
<p>可我考研那年不懂事。我哭的时候，却觉得自己分分钟像个大人了，我早回去正是在做着那些英雄们不得不做的事儿。天将降大任于斯人矣。</p>
<p>为了呵护这个家，却要离开它。</p>
<p>浊酒一杯，家万里。</p>
<p>我觉得这就是我的燃情岁月。</p>
<p>后来研考上了，女朋友等了我三年，然后就媳妇也有了。后来我才知道燃情岁月才算刚刚开始。</p>
<p>丁丁蛮懂事，道，汪、汪、汪。</p>
<p>再后来，有了一室一厅，吉利帝豪，郊区的三室两厅，又因为堵车把车给卖了，再后来有了一个儿子，鼓楼的学区加户口房，又添了个儿子，就把爸妈接过来了。</p>
<p>这几年没有我特别想做的事儿。只有我需要做好的事儿。</p>
<p>两个小魔王，说实话我不觉得爸妈老年生活有多幸福。</p>
<p>不过多亏通了地铁，我每天八点半能到家，磕个瓜子，跟我爸聊聊我儿子和他儿子。</p>
<p>可是，小伙子，你知道吗？我考研那年，就是个愣头青。</p>
<p>那时候我对私人的时间有着近乎偏执的吝啬。我觉得自己独处的时间才是上天赐予的礼物。回家过年又烦又累，措不及防的应酬是对我神圣的私有时间的侵犯。所以，其实我早早就狠下心来，一定要早早的回学校。</p>
<p>我上车那天风声呼啸，暗云疾行，干燥的北风中赫赫抬起的，是我打车的一只大手。路两边白杨赤条条的，行人裹着衣，绷着脸。</p>
<p>风萧萧兮易水寒，众人向北我向南。</p>
<p>可是，小伙子，你知道吗？</p>
<p>让男孩成为男人的，不是事业，是家业啊。</p>
<p>大叔突然不说了。他翻了个身，晚安。</p>
<p>丁丁也回过头，抹了眼睛，退了返程。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2017年1月</p>
]]></content>
      
        <categories>
            
            <category> Writing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[小米3变身记]]></title>
      <url>https://blog.ddlee.cn/2016/09/23/%E5%B0%8F%E7%B1%B33%E5%8F%98%E8%BA%AB%E8%AE%B0/</url>
      <content type="html"><![CDATA[<blockquote>
<p>Across the Great Wall we can reach every corner in the world.”（越过长城，走向世界）</p>
</blockquote>
<h1 id="1-缘起——我、小米、安卓和Android"><a href="#1-缘起——我、小米、安卓和Android" class="headerlink" title="1.缘起——我、小米、安卓和Android"></a>1.缘起——我、小米、安卓和Android</h1><p>知乎上有个抖机灵的回答，问题是“Nexus 5 如果不使用 VPN，会有什么影响”，回答是“android 体验变成安卓体验”。感谢无所不能的墙，让Android也有了中国特色。</p>
<p>各家应用市场层出不穷，应用推广不择手段，申请权限多多益善，后台活动精彩不断。更不能忍的是，小米移除了Google的服务框架，无法从Play商店推动应用。为了用Sleep Cycle alarm clock, 我得用在线工具从Play商店获取链接，同步到云盘里，再从手机里打开.apk文件来安装。因为，百度搜索出的那个结果，应用中内置了烦人的广告。</p>
<p>终于，我投奔了IOS阵营，手中服役的小米3也就闲置。在一个并没有那么蛋疼的午后，我拿出数据线，对它说，It’s time.</p>
<p>注意：本系列不作为通用教程，只做经历分享。请移步相关论坛获取教程信息。</p>
<h1 id="2-基础——是什么，为什么，怎么办"><a href="#2-基础——是什么，为什么，怎么办" class="headerlink" title="2.基础——是什么，为什么，怎么办"></a>2.基础——是什么，为什么，怎么办</h1><h2 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h2><p>你一定听说过，有种技术叫刷机。你也一定听说过，还有种技术叫越狱。你更一定听说过，还有种技术叫FanQiang。</p>
<p>这三者有什么关系呢？</p>
<p>在我看来，它们都关乎我们作为用户常常忽略的两个字——权限。</p>
<p>换言之，我们常常关注可以用手机干什么、可以上网浏览什么，却常常不去注意，我们本来有更多事可以做，有更多信息可以获取。</p>
<p>刷机意味着给手机重装系统，你获得的是选择硬件所运行系统的权利；越狱获得的是掌控某一操作系统的权利；而FanQiang，获得的则是“越过长城，走向世界”的权利。</p>
<blockquote>
<p>中国第一封电子邮件的内容是：Across the Great Wall we can reach every corner in the world.”（越过长城，走向世界）。这是1987年9月14日从北京向海外发出的中国第一封电子邮件，揭开了中国人使用互联网的序幕。</p>
<p>来源：<a href="https://www.zhihu.com/question/32239520/answer/61354926" target="_blank" rel="external">知乎</a></p>
</blockquote>
<h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><p>因为无聊，因为好奇，因为喜欢，因为不满足，因为我们可以。</p>
<h2 id="怎么办"><a href="#怎么办" class="headerlink" title="怎么办"></a>怎么办</h2><p>如果把刷机比作建造楼房，你所需要准备的就是知识（图纸）、刷机包（水泥、混凝土）、调试环境（吊塔）。</p>
<h3 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h3><p>真正重要的知识，是关于知识的知识。拿到图纸不重要，重要的是学会如何看懂图纸。以我的经历，最耗费精力的部分不是学习教程，而是TROUBLE SHOOTING, 是如何解决出现的问题。</p>
<p>因此，绝对不要使用某些工具的“一键刷机功能”，它们不会告诉你问题出在哪。<br><img src="https://static.ddlee.cn/static/img/Mi3/something-happened.jpg" alt="Something Happened"></p>
<p>请保证你对整个过程的绝对控制，保证你清楚到底在哪一步无法继续进行。</p>
<p>而为了看懂图纸，你需要准备好你的Google.它会是你最可靠的伙伴。</p>
<p>下面是图纸中可能涉及的内容，请搜索并结合某些通用刷机教程理解它们发挥的作用。</p>
<ul>
<li>卡刷、线刷：两种刷机的操作方式（体位）</li>
<li>Root：获取Android系统管理员的过程</li>
<li>OTA：On The Air， 一种系统更新方式</li>
<li>ROM包： 刷入手机ROM的系统软件包</li>
<li>Recovery Mode： Android系统的一种模式，常在此模式下进行刷机操作</li>
<li>Fastboot Mode： Android系统的一种模式，可在此模式下刷入自定义recovery</li>
<li>ADB： Android Debug Bridge，用PC对Android系统进行USB调教所需的环境</li>
<li>CM： 一家著名的ROM制作方，现已改名Lineage OS</li>
<li>Android M： Android系统的一个版本，现在是N（Nougat，7.0）</li>
<li>GApps： Google服务全家桶，需要刷入系统分区，包括Play和GMS等服务</li>
</ul>
<h1 id="3-刷机——大致的步骤，常见的坑"><a href="#3-刷机——大致的步骤，常见的坑" class="headerlink" title="3.刷机——大致的步骤，常见的坑"></a>3.刷机——大致的步骤，常见的坑</h1><h3 id="一个负责任的教程，大概会告诉你如下几个步骤"><a href="#一个负责任的教程，大概会告诉你如下几个步骤" class="headerlink" title="一个负责任的教程，大概会告诉你如下几个步骤"></a>一个负责任的教程，大概会告诉你如下几个步骤</h3><ul>
<li>风险警示</li>
<li>备份数据</li>
<li>如何搭建ADB环境-PC</li>
<li>如何进入fastboot模式-手机</li>
<li>如何在ADB环境下，fastboot模式中刷入自定义recovery</li>
<li>如何利用recovery模式清除数据，刷入ROM包（和Gapps）</li>
<li>如何ROOT</li>
</ul>
<h3 id="一次蛋疼的刷机经历，常常会遇到这些坑"><a href="#一次蛋疼的刷机经历，常常会遇到这些坑" class="headerlink" title="一次蛋疼的刷机经历，常常会遇到这些坑"></a>一次蛋疼的刷机经历，常常会遇到这些坑</h3><ul>
<li>找到正确的Recovery和ROM包：一定要仔细比对型号，尽量选用开源机构制作的包</li>
<li>搭建ADB环境：要用到命令行（请慎重选用一件脚本，即.bat文件）</li>
<li>连接电脑与手机：Windows系统下需要硬件驱动（请注意型号）</li>
<li>……</li>
</ul>
<h1 id="4-资源——与你同行"><a href="#4-资源——与你同行" class="headerlink" title="4.资源——与你同行"></a>4.资源——与你同行</h1><h3 id="论坛与搜索引擎"><a href="#论坛与搜索引擎" class="headerlink" title="论坛与搜索引擎"></a>论坛与搜索引擎</h3><p>你会发现，手机厂商的官方论坛和XDA等论坛会很有帮助。但真正与你同行的，还是Google。</p>
<h3 id="常用网址"><a href="#常用网址" class="headerlink" title="常用网址"></a>常用网址</h3><ul>
<li><a href="https://twrp.me/" target="_blank" rel="external">TWRP，著名的自定义recovery</a></li>
<li><a href="http://opengapps.org/" target="_blank" rel="external">GApps，Google全家桶</a></li>
<li><a href="http://repo.xposed.info/module/de.robv.android.xposed.installer" target="_blank" rel="external">Xposed，著名开源框架</a></li>
<li><a href="https://developer.android.com/studio/command-line/adb.html" target="_blank" rel="external">ADB Guide</a></li>
<li><a href="http://www.supersu.com/download" target="_blank" rel="external">SuperSU，用于手机Root</a></li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Android </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Digital Life </tag>
            
            <tag> Android </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据分析在线学习资源(Personal Archive)]]></title>
      <url>https://blog.ddlee.cn/2016/08/07/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%B5%84%E6%BA%90/</url>
      <content type="html"><![CDATA[<p>数据分析方向的在线资源收集。</p>
<h2 id="1-Some-wonderful-Tutorials"><a href="#1-Some-wonderful-Tutorials" class="headerlink" title="1.Some wonderful Tutorials"></a>1.Some wonderful Tutorials</h2><ol>
<li><a href="https://www.springboard.com/learning-paths/data-analysis/learn/?" target="_blank" rel="external">Data Analysis Learning Path from Springboard</a></li>
<li><a href="http://datasciencemasters.org/" target="_blank" rel="external">The Open Source Data Science Masters</a></li>
</ol>
<h2 id="2-Basic"><a href="#2-Basic" class="headerlink" title="2. Basic"></a>2. Basic</h2><h3 id="2-1-Database"><a href="#2-1-Database" class="headerlink" title="2.1 Database"></a>2.1 Database</h3><p>Stanford’s Database <a href="https://lagunita.stanford.edu/courses/Engineering/db/2014_1/info" target="_blank" rel="external">course</a></p>
<h3 id="2-2-Agrolthms"><a href="#2-2-Agrolthms" class="headerlink" title="2.2 Agrolthms"></a>2.2 Agrolthms</h3><p>Algrothms from Stanford via <a href="https://class.coursera.org/algs4partI-010" target="_blank" rel="external">Coursera</a>(using Java)<br>Booksite <a href="http://algs4.cs.princeton.edu/home/" target="_blank" rel="external">here</a><br>Algorithm with Python in <a href="https://github.com/qiwsir/algorithm" target="_blank" rel="external">GItHub</a></p>
<h3 id="2-3-Algebra"><a href="#2-3-Algebra" class="headerlink" title="2.3 Algebra"></a>2.3 Algebra</h3><p>Harvard’s Massive Parralle Algebra Course on iTunes U</p>
<h3 id="2-4-Statistics"><a href="#2-4-Statistics" class="headerlink" title="2.4 Statistics"></a>2.4 Statistics</h3><p>Princeton’s Statistics One</p>
<h3 id="2-5-Books"><a href="#2-5-Books" class="headerlink" title="2.5 Books"></a>2.5 Books</h3><ul>
<li><em>Pattern Recognition and Machine Learning</em> by Bishop</li>
<li><em>The Elements of Statistical Learning</em></li>
</ul>
<h2 id="3-Python"><a href="#3-Python" class="headerlink" title="3. Python"></a>3. Python</h2><h3 id="3-1-Scipy-amp-Pandas-amp-sklearn"><a href="#3-1-Scipy-amp-Pandas-amp-sklearn" class="headerlink" title="3.1 Scipy &amp; Pandas &amp; sklearn"></a>3.1 Scipy &amp; Pandas &amp; sklearn</h3><ul>
<li>Scipy Lecture Notes</li>
<li>Pandas Doc</li>
<li>Pandas <a href="https://github.com/jvns/pandas-cookbook" target="_blank" rel="external">Cookbook</a></li>
<li>sklearn Doc</li>
</ul>
<h3 id="3-2-Python-MOOCs"><a href="#3-2-Python-MOOCs" class="headerlink" title="3.2 Python MOOCs"></a>3.2 Python MOOCs</h3><h5 id="edX-course"><a href="#edX-course" class="headerlink" title="edX course"></a>edX course</h5><p>MITx: 6.00.2x Introduction to Computational Thinking and Data Science via <a href="https://courses.edx.org/courses/course-v1:MITx+6.00.2x_5+1T2016/info" target="_blank" rel="external">edX</a></p>
<h5 id="Udacity-Course"><a href="#Udacity-Course" class="headerlink" title="Udacity Course"></a>Udacity Course</h5><ul>
<li>Design of Computer Programs with Peter Novig</li>
<li>Intro to Machine Learning (project oriented)</li>
<li>Machine Learning: Unsupervised Learning</li>
</ul>
<h3 id="3-3-Books"><a href="#3-3-Books" class="headerlink" title="3.3 Books"></a>3.3 Books</h3><ul>
<li><em>Python for Data Analysis</em> by Wes McKinney</li>
<li><em>Programming Collective Intelligence</em> by Toby Segaran</li>
</ul>
<h2 id="4-R"><a href="#4-R" class="headerlink" title="4. R"></a>4. R</h2><h3 id="4-1-R-MOOCs"><a href="#4-1-R-MOOCs" class="headerlink" title="4.1 R MOOCs"></a>4.1 R MOOCs</h3><h5 id="edX-course-1"><a href="#edX-course-1" class="headerlink" title="edX course"></a>edX course</h5><p>MIT’s The Analytics Edge</p>
<h5 id="JH-Data-Science-Specilization-via-Coursera"><a href="#JH-Data-Science-Specilization-via-Coursera" class="headerlink" title="JH Data Science Specilization via Coursera"></a>JH Data Science Specilization via Coursera</h5><ul>
<li>Statistical Inference</li>
<li>Regression Model</li>
<li>Practical Machine Learning</li>
<li>Develop Data Science Product</li>
</ul>
<h5 id="Stanford’s-Statistical-Learning"><a href="#Stanford’s-Statistical-Learning" class="headerlink" title="Stanford’s Statistical Learning"></a>Stanford’s Statistical Learning</h5><p><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/info" target="_blank" rel="external">here</a><br>and its text book <em>An Introduction to Statistical Learning</em> <a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank" rel="external">ISLAR</a></p>
<h3 id="4-2-R-Books"><a href="#4-2-R-Books" class="headerlink" title="4.2 R Books"></a>4.2 R Books</h3><ul>
<li><em>R Graphics Cookbook</em> by Winston Chang</li>
<li><em>ggplot2</em> by Hadley Wickham</li>
<li><em>R in Action</em> by Robert I. Kabacoff</li>
</ul>
<h2 id="5-Big-Data"><a href="#5-Big-Data" class="headerlink" title="5. Big Data"></a>5. Big Data</h2><h3 id="5-1-“Big”-MOOCs"><a href="#5-1-“Big”-MOOCs" class="headerlink" title="5.1 “Big” MOOCs"></a>5.1 “Big” MOOCs</h3><h5 id="Udacity-Course-1"><a href="#Udacity-Course-1" class="headerlink" title="Udacity Course"></a>Udacity Course</h5><p>Intro to Hadoop and MapReduce from clourdera</p>
<h5 id="Coursera-Course"><a href="#Coursera-Course" class="headerlink" title="Coursera Course"></a>Coursera Course</h5><p>Mining Massive Datasets</p>
<h5 id="edX-Course"><a href="#edX-Course" class="headerlink" title="edX Course"></a>edX Course</h5><p>Xserise on Spark from BerkleyX</p>
<h2 id="6-Capstone-Project"><a href="#6-Capstone-Project" class="headerlink" title="6. Capstone Project"></a>6. Capstone Project</h2><ul>
<li>SITP Project</li>
<li>Health Twitter Analysis via <a href="https://www.coursolve.org/need/229" target="_blank" rel="external">Coursolve</a></li>
</ul>
<h2 id="7-Additional-Resource"><a href="#7-Additional-Resource" class="headerlink" title="7. Additional Resource"></a>7. Additional Resource</h2><ul>
<li>Harvard’s <a href="http://cs109.github.io/2015/" target="_blank" rel="external">CS109</a> Course: Data Science</li>
<li>Berkley’s <a href="http://cs61a.org/" target="_blank" rel="external">CS61</a>:The Structure and Interpretation of Computer Programs</li>
<li>Probabilistic Graphical Models via <a href="https://www.coursera.org/course/pgm" target="_blank" rel="external">Coursera</a></li>
<li>Berkeley’s <a href="http://data8.org/datascience/" target="_blank" rel="external">Datascience’s Documentation</a></li>
<li><a href="https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks" target="_blank" rel="external">A Gallery of IPython Notebooks</a></li>
<li>A collection of Data Science Learning materials in the form of <a href="https://github.com/nborwankar/LearnDataScience" target="_blank" rel="external">IPython Notebooks</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="external">Unsupervised Feature Learing and Deep Learning</a></li>
</ul>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Data Science </tag>
            
            <tag> Data </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[个性化你的Ubuntu-3：主题，插件以及桌面小工具]]></title>
      <url>https://blog.ddlee.cn/2016/06/11/%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BD%A0%E7%9A%84Ubuntu-3%EF%BC%9A%E4%B8%BB%E9%A2%98%EF%BC%8C%E6%8F%92%E4%BB%B6%E4%BB%A5%E5%8F%8A%E6%A1%8C%E9%9D%A2%E5%B0%8F%E5%B7%A5%E5%85%B7/</url>
      <content type="html"><![CDATA[<h3 id="个性主题"><a href="#个性主题" class="headerlink" title="个性主题"></a>个性主题</h3><p>依赖于扩展<code>User themes</code>，分为GTK主题，shell主题和icon主题。</p>
<ol>
<li><p>从<a href="https://www.gnome-look.org/browse/cat/134/ord/latest/" target="_blank" rel="external">gnome-look.org</a>下载喜欢的主题（压缩文件）。</p>
</li>
<li><p>将下载的主题文件复制到用户文件夹</p>
<pre><code> cd ~
 mkdir .themes
 cp file_path_to_download_file ~/.themes
</code></pre><p> 并使用<code>unzip</code>或<code>tar xvzf</code>命令解压，或者：</p>
<pre><code> sudo cp file_path_to_download_file /usr/local/themes/
</code></pre></li>
<li><p>在<code>gnome-tweak-tool</code>的扩展<code>User themes</code>中选择主题。</p>
</li>
</ol>
<h5 id="推荐主题"><a href="#推荐主题" class="headerlink" title="推荐主题"></a>推荐主题</h5><p>我使用的是Numix系列的主题<a href="https://numixproject.org/" target="_blank" rel="external">（官网）</a></p>
<ul>
<li><a href="http://satya164.deviantart.com/art/Numix-GTK3-theme-360223962" target="_blank" rel="external">Numix-GTK3 theme</a></li>
<li><a href="http://gnome-look.org/content/show.php/Numix-like+GNOME+Shell+3.16+theme?content=174129" target="_blank" rel="external">Numix-like GNOME Shell theme</a></li>
<li><a href="http://me4oslav.deviantart.com/art/Numix-Circle-Linux-Desktop-Icon-Theme-414741466" target="_blank" rel="external">Numix-Circle Icons</a></li>
</ul>
<p>Numix开发者之一Satyajit Sahoo发布的GNOME shell theme:<br><a href="http://satya164.deviantart.com/art/Gnome-Shell-Elegance-Colors-305966388" target="_blank" rel="external">Gnome Shell - Elegance Colors</a></p>
<p>通过PPA安装</p>
<pre><code>sudo apt-add-repository ppa:numix/ppa
sudo apt-get update
sudo apt-get install numix-gtk-theme
sudo apt-get install numix-icon-theme-circle
</code></pre><pre><code>sudo add-apt-repository ppa:satyajit-happy/themes
sudo apt-get update &amp;&amp; sudo apt-get install gnome-shell-theme-elegance-colors
</code></pre><h2 id="扩展插件"><a href="#扩展插件" class="headerlink" title="扩展插件"></a>扩展插件</h2><p>我当前使用的插件：</p>
<ol>
<li>hide dash：隐藏侧边的favorite栏</li>
<li>Pomotodo：番茄时钟<br><img src="https://static.ddlee.cn/static/img/Ubuntu-3/Pomodoro1.jpg" alt="Pomotodo1"><br><img src="https://static.ddlee.cn/static/img/Ubuntu-3/Pomodoro2.jpg" alt="Pomotodo2"></li>
<li>（荐）Clipboard indicator：剪贴板切换<br><img src="https://static.ddlee.cn/static/img/Ubuntu-3/Selection_018.jpg" alt="Clipboard indicator"></li>
<li>ToDo.txt：待办事项整理<br><img src="https://static.ddlee.cn/static/img/Ubuntu-3/ToDo_txt.jpg" alt="ToDo"></li>
<li>Places indicator：文件浏览器的快捷方式</li>
<li>Activities configurator: 当前活动程序管理</li>
<li>Alternatetab: alt-tab桌面切换</li>
<li>Applications menu：类似Windows下开始菜单<br><img src="https://static.ddlee.cn/static/img/Ubuntu-3/app.png" alt="Applications"></li>
<li>（荐）Drop down terminal：快捷启动终端<br><img src="https://static.ddlee.cn/static/img/Ubuntu-3/drop.png" alt="Drop"></li>
<li>Netspeed：网速监控</li>
<li>Openweather：状态栏天气预报</li>
<li>Removable drive menu：弹出U盘等可移除硬件</li>
<li>（荐）Dynamic top bar：根据窗口颜色变换顶栏颜色</li>
</ol>
<h2 id="其他桌面工具"><a href="#其他桌面工具" class="headerlink" title="其他桌面工具"></a>其他桌面工具</h2><h4 id="DOCK"><a href="#DOCK" class="headerlink" title="DOCK"></a>DOCK</h4><p>推荐<code>Cairo-Dock</code>，效果如图，扩展性很高，自定义程度也很好。</p>
<p><img src="https://static.ddlee.cn/static/img/Ubuntu-3/Dock1.jpg" alt="Cairo-Dock"></p>
<h4 id="CONKY：桌面监测工具"><a href="#CONKY：桌面监测工具" class="headerlink" title="CONKY：桌面监测工具"></a>CONKY：桌面监测工具</h4><p>推荐<code>Conky</code>，皮肤也有很多，效果如图。</p>
<p><img src="https://static.ddlee.cn/static/img/Ubuntu-3/Conky1.jpg" alt="Conky"></p>
<p>本系列至此完结。欢迎入坑。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2016年6月</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Digital Life </tag>
            
            <tag> Linux </tag>
            
            <tag> Gnome </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[个性化你的Ubuntu-2：GNOME安装与工具]]></title>
      <url>https://blog.ddlee.cn/2016/06/02/%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BD%A0%E7%9A%84Ubuntu-2%EF%BC%9AGNOME%E5%AE%89%E8%A3%85%E4%B8%8E%E5%B7%A5%E5%85%B7/</url>
      <content type="html"><![CDATA[<h3 id="GNOME安装"><a href="#GNOME安装" class="headerlink" title="GNOME安装"></a>GNOME安装</h3><p>从上一篇文章，大家可以看到，GNOME是一系列软件的集合，安装时可以有不同的取舍。对于Ubuntu用户来说，可以有以下两类体验GNOME的方式。（参考：<a href="https://wiki.ubuntuusers.de/GNOME_Installation/" target="_blank" rel="external">GNOME installation</a>）</p>
<h4 id="1-Ubuntu-GNOME（系统）"><a href="#1-Ubuntu-GNOME（系统）" class="headerlink" title="1.Ubuntu GNOME（系统）"></a>1.Ubuntu GNOME（系统）</h4><p>Ubuntu GNOME是Ubuntu的一个发行版本（也称Ubuntu variants），就像Ubuntu和Fedora等都是GNU/Linux的发行版那样。Ubuntu GNOME不仅包含了Ubuntu的核心部分、GNOME的核心部分，还有一系列的标准应用。</p>
<h5 id="Install-from-DVD"><a href="#Install-from-DVD" class="headerlink" title="Install from DVD"></a>Install from DVD</h5><p>如果可以接受重新安装系统，请到这里<a href="http://ubuntugnome.org/download/" target="_blank" rel="external">下载</a>Ubuntu GNOME。</p>
<h5 id="Install-with-current-system"><a href="#Install-with-current-system" class="headerlink" title="Install with current system"></a>Install with current system</h5><p>你也可以通过安装metapackage，这样在安装GNOME桌面环境时，你的系统中未安装的标准应用也会被同时安装。</p>
<p><code>sudo apt-get install ubuntu-gnome-desktop</code></p>
<h4 id="2-GNOME（仅桌面环境）"><a href="#2-GNOME（仅桌面环境）" class="headerlink" title="2.GNOME（仅桌面环境）"></a>2.GNOME（仅桌面环境）</h4><h5 id="The-“real”-GNOME"><a href="#The-“real”-GNOME" class="headerlink" title="The “real” GNOME"></a>The “real” GNOME</h5><p>标准的GNOME桌面环境，没有Ubuntu的特性（尽管我区分不出哪些是Ubuntu提供的），也不安装附加的标准应用：</p>
<p><code>sudo apt-get install gnome</code></p>
<h5 id="The-minimux-GNOME"><a href="#The-minimux-GNOME" class="headerlink" title="The minimux GNOME"></a>The minimux GNOME</h5><p>GNOME的核心部分，不安装附加的标准应用：</p>
<p><code>sudo apt-get install gnome-core</code></p>
<h5 id="GNOME-shell"><a href="#GNOME-shell" class="headerlink" title="GNOME shell"></a>GNOME shell</h5><p>仅安装GNOME的图形界面：<br><code>sudo apt-get install gnome-shell</code></p>
<p>你还需要：<br><code>sudo apt-get install gnome-session</code></p>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>在同一系统上安装不同的桌面环境可能会造成一些意料不到的问题（如锁屏界面丢失），最推荐的方案还是重新安装Ubuntu GNOME，其次，可以安装<code>ubuntu-gnome-desktop</code>。</p>
<h4 id="使用新的桌面环境"><a href="#使用新的桌面环境" class="headerlink" title="使用新的桌面环境"></a>使用新的桌面环境</h4><p>安装完毕后，重启，可在登录界面选择桌面环境。</p>
<p><img src="https://static.ddlee.cn/static/img/Ubuntu-2/gnome-login1.png" alt="login1"></p>
<p><img src="https://static.ddlee.cn/static/img/Ubuntu-2/gnome-login2.png" alt="login2"></p>
<h3 id="GNOME配置工具：gnome-tweak-tool"><a href="#GNOME配置工具：gnome-tweak-tool" class="headerlink" title="GNOME配置工具：gnome-tweak-tool"></a>GNOME配置工具：gnome-tweak-tool</h3><p>想要充分个性化GNOME桌面环境，扩展GNOME的功能，你还需要安装GNOME的配置工具：gnome tweak tool</p>
<p><code>sudo apt-get install gnome-tweak-tool</code></p>
<p><img src="https://static.ddlee.cn/static/img/Ubuntu-2/gnome-tweak-tool_004.jpg" alt="图片：gnome tweak tool提供的功能"></p>
<p>利用gnome tweak tool，你可以管理桌面主题、调整窗口特性、调整显示字体、加载GNOME扩展、管理开机自启程序等等。</p>
<h3 id="扩展插件"><a href="#扩展插件" class="headerlink" title="扩展插件"></a>扩展插件</h3><p>在Ubuntu上，要调整桌面主题，可没有Windows上鼠标右击一下那么简单。<br>你要先安装上面的tweak tool，然后有人告诉你需要User theme扩展插件，而你跑到<code>extensions.gnome.org</code>，遇到的却是这个：<br><img src="https://static.ddlee.cn/static/img/Ubuntu-2/Selection_019.jpg" alt="错误"></p>
<p>我明明装了GNOME的啊！</p>
<p>这是因为，<code>extensions.gnome.org</code>需要与浏览器通信，调用click-to-play的功能，我们需要安装GNOMNE shell intergration这个插件。</p>
<h4 id="Chrome用户"><a href="#Chrome用户" class="headerlink" title="Chrome用户"></a>Chrome用户</h4><h5 id="利用PPA"><a href="#利用PPA" class="headerlink" title="利用PPA"></a>利用PPA</h5><pre><code>sudo add-apt-repository ppa:ne0sight/chrome-gnome-shell
sudo apt-get update
sudo apt-get install chrome-gnome-shell
</code></pre><h5 id="通过Chrome-Web-Store-GNOME-Shell-integration"><a href="#通过Chrome-Web-Store-GNOME-Shell-integration" class="headerlink" title="通过Chrome Web Store:GNOME Shell integration"></a>通过Chrome Web Store:<a href="https://chrome.google.com/webstore/detail/gnome-shell-integration/gphhapmejobijbbhgpjhcjognlahblep" target="_blank" rel="external">GNOME Shell integration</a></h5><p>可能需要通过CMake安装native connector,请参考这一<a href="https://wiki.gnome.org/Projects/GnomeShellIntegrationForChrome/Installation" target="_blank" rel="external">页面</a>。</p>
<h4 id="FireFox用户"><a href="#FireFox用户" class="headerlink" title="FireFox用户"></a>FireFox用户</h4><p>使用FireFox访问<code>extensions.gnome.org</code>时会有运行GNOME shell integration的通知，允许运行后刷新即可。</p>
<p>更多信息，请参考这一<a href="https://extensions.gnome.org/about/#no-detection" target="_blank" rel="external">页面</a></p>
<p>安装好<code>tweak-tool</code>后，祝贺你已经打开了新世界的大门。下篇文章是关于扩展插件的推荐，欢迎继续阅读。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Digital Life </tag>
            
            <tag> Linux </tag>
            
            <tag> Gnome </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[个性化你的Ubuntu-1：GNOME桌面环境]]></title>
      <url>https://blog.ddlee.cn/2016/05/30/%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BD%A0%E7%9A%84Ubuntu-1%EF%BC%9AGNOME%E6%A1%8C%E9%9D%A2%E7%8E%AF%E5%A2%83/</url>
      <content type="html"><![CDATA[<h2 id="我与Ubuntu"><a href="#我与Ubuntu" class="headerlink" title="我与Ubuntu"></a>我与Ubuntu</h2><p>我最初是Windows98用户，再到Windows2003,Windows XP,Windows 7,上了大学后用Windows 8.1,Windows 10（想不到竟然能列这么长；我从没用过Windows Vista,不知道那是什么东西），我很喜欢8.1和10的开始屏幕和动态磁贴。非常偶然的机会，我在CS50的课程中接触了GNU/Linux，才知道，原来在MS　Windows和Mac OSＸ之外，还有一个GNU/Linux。换完SSD，学会了装操作系统，我便踏上了折腾GNU/Linux的不归路。</p>
<p>曾经被一个软院的同学安利Red Hat系的Fedora（尽管他现在已经投入了MacBook的怀抱）,普及各种内核之类的知识。然而，我只想安静的用它上上网，进行科学计算，并没有深入到考虑系统底层的需求层次。我还是安心地用Ubuntu吧。我也推荐第一次尝试GNU/Linux系统的小白从Ubuntu开始，相信我,askubuntu.com和stackoverflow.com会解决你的大部分问题的。</p>
<h2 id="个性化你的Ubuntu（一）：GNOME桌面环境"><a href="#个性化你的Ubuntu（一）：GNOME桌面环境" class="headerlink" title="个性化你的Ubuntu（一）：GNOME桌面环境"></a>个性化你的Ubuntu（一）：GNOME桌面环境</h2><p>相信不少读者都是从Microsoft Windows转到GNU/Linux阵营的,早就习惯了用户图形界面。但是，配合桌面环境、主题和一些插件和软件，Ubuntu照样可以很酷炫。</p>
<h3 id="什么是GNOME"><a href="#什么是GNOME" class="headerlink" title="什么是GNOME"></a>什么是GNOME</h3><p><img src="https://static.ddlee.cn/static/img/Ubuntu-1/Gnomelogo.png" alt="GNOME Logo"><br>（大脚丫为什么这么大。。。）</p>
<p>GNOME(pronounced /ɡˈnoʊm/ or /ˈnoʊm/) 最初是GNU Network Object Model Environment的缩写，但这一缩写已不再沿用（更多历史情况请参见<a href="https://mail.gnome.org/archives/marketing-list/2010-April/msg00050.html" target="_blank" rel="external">这里</a>）。</p>
<p>我们所说的GNOME，通常指的是由<a href="https://www.gnome.org/about/" target="_blank" rel="external">The GNOME Project</a>开发的运行于Linux之上的桌面环境。</p>
<p>我们每天面对的，并不是全部的Microft Windows/OS X/Linux系统，而是系统提供给我们的人机接口，而桌面环境，则是统一在同一图形用户接口（GUI）之下的一揽子软件（X Window Manager, File manager, Terminal emulator, Text editor, Image viewer, E-mail client等）。</p>
<p><img src="https://static.ddlee.cn/static/img/Ubuntu-1/OS&amp;GUI.jpg" alt="操作系统提供用户图形界面给用户作为人机接口"><br><a href="http://www.slideshare.net/sherif_mosa/operating-systems-basics-26277922" target="_blank" rel="external">来源</a></p>
<p>Ubuntu自带的桌面环境是<a href="https://unity.ubuntu.com/" target="_blank" rel="external">Unity</a>（图形外壳）,其他流行的桌面环境还有<a href="http://www.kde.org/" target="_blank" rel="external">KDE</a>,<a href="http://www.xfce.org/" target="_blank" rel="external">Xfce</a>。但我们要谈的是GNOME。</p>
<h3 id="什么是X-window-system"><a href="#什么是X-window-system" class="headerlink" title="什么是X window system"></a>什么是X window system</h3><p>要谈Unix-like系统上的图形界面，就不得不提X Window System。那么，什么是X?</p>
<blockquote>
<p>The X Window System, commonly referred to merely as X, is a highly configurable, cross-platform, complete and free client-server system for managing graphical user interfaces (GUIs) on single computers and on networks of computers.</p>
<p>(X窗口系统，通常简称为X，是用于管理在单个计算机和计算机网络上运行的图形用户界面（GUI）一个高度可配置的，跨平台，完整的，自由的客户端-服务器系统。）</p>
<p>来源：<a href="http://www.linfo.org/x.html" target="_blank" rel="external">LINFO</a></p>
</blockquote>
<p>我们试着通过X能够干什么来理解一下这句话。</p>
<p>X是一组规则、一套方法。它提供了从硬件（键鼠）接受用户输入、创建图形窗口、画出直线、位图等基本的图形功能（图形引擎）。</p>
<p>X实现了客户端-服务器的机制。通过划分Server和Client，X既能在本地计算机上运行，也能在计算计算机网络中运行。</p>
<p>X与操作系统独立。X可以理解为运行在操作系统之上的一套软件。如果不需要GUI，完全可以不用安装X。而在Microsoft Windows和OS X中，图形引擎是操作系统的一部分。</p>
<p>X Window System的结构如图。<br><img src="https://static.ddlee.cn/static/img/Ubuntu-1/X-window-system.png" alt="X Window System"></p>
<h3 id="GNOME-amp-X"><a href="#GNOME-amp-X" class="headerlink" title="GNOME &amp; X"></a>GNOME &amp; X</h3><p>GNOME和X Window System是什么关系？<br>桌面环境可以理解为一系列X client的集合，其中最重要的组件是X Window Manager。由于X Window System的client-server机制，各client之间是相对独立的，这时，需要一个特殊的client管理其他client，将他们统一在一个框架之下，这就是X Window Manager。<br><img src="https://static.ddlee.cn/static/img/Ubuntu-1/Window_Manager.png" alt="Window Manager"><br><a href="http://www.slideshare.net/RBandes/x-window-system" target="_blank" rel="external">来源</a></p>
<p>而GNOME另一个重要的组成部分是GNOME shell，它是一个图形外壳程序，也就是我们要面对的接口。</p>
<p>跟GNOME相关的其他组件、库、概念</p>
<ul>
<li>GTK+：GIMP Widget toolkits，GNOME基于的<a href="https://en.wikipedia.org/wiki/Widget_toolkit" target="_blank" rel="external">GUI工具箱</a>。KDE则基于Qt。</li>
<li>Display Manager:图形用户登陆管理器，为用户提供登陆界面，与session manager通信，开启新的session。GNOME使用的是GDM。</li>
<li>Metacity：GNOME 2使用的window manager，GNOME 3使用的是Mutter。KDE使用的是KWin。</li>
<li>Wayland:与X Window System对应，也是一种窗口系统</li>
</ul>
<h3 id="我现在的桌面"><a href="#我现在的桌面" class="headerlink" title="我现在的桌面"></a>我现在的桌面</h3><p><img src="https://static.ddlee.cn/static/img/Ubuntu-1/Gnome1.jpg" alt="我的桌面"></p>
<p>我不喜欢双击桌面图标来启动程序，更多用的是Dock和全局搜索，所以，桌面上“什么都没有”。</p>
<p>桌面的壁纸是电影 <em>飞屋环游记</em> 的海报，使用了Numix系列的主题和图标。</p>
<p>下方Dock使用的程序是Cairo-Dock，桌面右方运行的程序是Conky，用来监测系统运行情况和提供天气信息，上方的Topbar里添加了许多GNOME的扩展应用。</p>
<p>接下来的两篇文章将介绍Gnome的安装与扩展推荐，欢迎继续阅读，撒花。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Digital Life </tag>
            
            <tag> Linux </tag>
            
            <tag> Gnome </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
