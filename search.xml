<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[[源码笔记]keras源码分析之Model]]></title>
      <url>http://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/</url>
      <content type="html"><![CDATA[<p>本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是<a href="https://github.com/fchollet/keras/blob/master/keras/engine/training.py" target="_blank" rel="external">/keras/engine/training.py</a>和<a href="https://github.com/fchollet/keras/blob/master/keras/models.py" target="_blank" rel="external">/keras/model.py</a>，要观察的类是<code>Model</code>和<code>Sequential</code>。</p>
<p>本系列第一篇：<a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">【源码笔记】keras源码分析之Tensor, Node和Layer</a><br>第二篇：<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">【源码笔记】keras源码分析之Container</a></p>
<h3 id="Model：添加了训练信息的Container"><a href="#Model：添加了训练信息的Container" class="headerlink" title="Model：添加了训练信息的Container"></a><code>Model</code>：添加了训练信息的<code>Container</code></h3><p><code>Model.compile()</code>主要完成了配置<code>optimizer</code>, <code>loss</code>, <code>metrics</code>等操作，而要执行的<code>fit</code>, <code>evaluate</code>等则不在<code>compile</code>过程中配置。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compile</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> loss_weights<span class="token operator">=</span>None<span class="token punctuation">,</span>
            sample_weight_mode<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss <span class="token operator">or</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> optimizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>sample_weight_mode <span class="token operator">=</span> sample_weight_mode
    self<span class="token punctuation">.</span>loss <span class="token operator">=</span> loss
    self<span class="token punctuation">.</span>loss_weights <span class="token operator">=</span> loss_weights

    loss_function <span class="token operator">=</span> losses<span class="token punctuation">.</span>get<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    loss_functions <span class="token operator">=</span> <span class="token punctuation">[</span>loss_function <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>loss_functions <span class="token operator">=</span> loss_functions

    <span class="token comment" spellcheck="true"># Prepare targets of model.</span>
    self<span class="token punctuation">.</span>targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>_feed_targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        shape <span class="token operator">=</span> self<span class="token punctuation">.</span>internal_output_shapes<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        name <span class="token operator">=</span> self<span class="token punctuation">.</span>output_names<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        target <span class="token operator">=</span> K<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>ndim<span class="token operator">=</span>len<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span>
                               name<span class="token operator">=</span>name <span class="token operator">+</span> <span class="token string">'_target'</span><span class="token punctuation">,</span>
                               sparse<span class="token operator">=</span>K<span class="token punctuation">.</span>is_sparse<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>K<span class="token punctuation">.</span>dtype<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>targets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_feed_targets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># Prepare metrics.</span>
    self<span class="token punctuation">.</span>metrics <span class="token operator">=</span> metrics
    self<span class="token punctuation">.</span>metrics_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>metrics_tensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># Compute total loss.</span>
    total_loss <span class="token operator">=</span> None
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        y_true <span class="token operator">=</span> self<span class="token punctuation">.</span>targets<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        y_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        loss_weight <span class="token operator">=</span> loss_weights_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">if</span> total_loss <span class="token keyword">is</span> None<span class="token punctuation">:</span>
            total_loss <span class="token operator">=</span> loss_weight <span class="token operator">*</span> output_loss
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            total_loss <span class="token operator">+=</span> loss_weight <span class="token operator">*</span> output_loss

    <span class="token keyword">for</span> loss_tensor <span class="token keyword">in</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">:</span>
        total_loss <span class="token operator">+=</span> loss_tensor

    self<span class="token punctuation">.</span>total_loss <span class="token operator">=</span> total_loss
    self<span class="token punctuation">.</span>sample_weights <span class="token operator">=</span> sample_weights
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Model</code>对象的<code>fit()</code>方法封装了<code>_fit_loop()</code>内部方法，而<code>_fit_loop()</code>方法的关键步骤由<code>_make_train_function()</code>方法完成，返回<code>history</code>对象，用于回调函数的处理。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token operator">=</span>None<span class="token punctuation">,</span> y<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：
      self<span class="token punctuation">.</span>_make_train_function<span class="token punctuation">(</span><span class="token punctuation">)</span>
      f <span class="token operator">=</span> self<span class="token punctuation">.</span>train_function
      <span class="token keyword">return</span> self<span class="token punctuation">.</span>_fit_loop<span class="token punctuation">(</span>f<span class="token punctuation">,</span> ins<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在<code>_fit_loop()</code>方法中，回调函数完成了对训练过程的监控记录等任务，<code>train_function</code>也被应用于传入的数据：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_fit_loop</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> f<span class="token punctuation">,</span> ins<span class="token punctuation">,</span> out_labels<span class="token operator">=</span>None<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
              epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span>
              val_f<span class="token operator">=</span>None<span class="token punctuation">,</span> val_ins<span class="token operator">=</span>None<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              callback_metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> initial_epoch<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>history <span class="token operator">=</span> cbks<span class="token punctuation">.</span>History<span class="token punctuation">(</span><span class="token punctuation">)</span>
    callbacks <span class="token operator">=</span> <span class="token punctuation">[</span>cbks<span class="token punctuation">.</span>BaseLogger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>callbacks <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>history<span class="token punctuation">]</span>
    callbacks <span class="token operator">=</span> cbks<span class="token punctuation">.</span>CallbackList<span class="token punctuation">(</span>callbacks<span class="token punctuation">)</span>
    out_labels <span class="token operator">=</span> out_labels <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    callbacks<span class="token punctuation">.</span>set_model<span class="token punctuation">(</span>callback_model<span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>set_params<span class="token punctuation">(</span><span class="token punctuation">{</span>
        <span class="token string">'batch_size'</span><span class="token punctuation">:</span> batch_size<span class="token punctuation">,</span>
        <span class="token string">'epochs'</span><span class="token punctuation">:</span> epochs<span class="token punctuation">,</span>
        <span class="token string">'samples'</span><span class="token punctuation">:</span> num_train_samples<span class="token punctuation">,</span>
        <span class="token string">'verbose'</span><span class="token punctuation">:</span> verbose<span class="token punctuation">,</span>
        <span class="token string">'do_validation'</span><span class="token punctuation">:</span> do_validation<span class="token punctuation">,</span>
        <span class="token string">'metrics'</span><span class="token punctuation">:</span> callback_metrics <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>on_train_begin<span class="token punctuation">(</span><span class="token punctuation">)</span>
    callback_model<span class="token punctuation">.</span>stop_training <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>initial_epoch<span class="token punctuation">,</span> epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        callbacks<span class="token punctuation">.</span>on_epoch_begin<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
        batches <span class="token operator">=</span> _make_batches<span class="token punctuation">(</span>num_train_samples<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        epoch_logs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        <span class="token keyword">for</span> batch_index<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_start<span class="token punctuation">,</span> batch_end<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
            batch_ids <span class="token operator">=</span> index_array<span class="token punctuation">[</span>batch_start<span class="token punctuation">:</span>batch_end<span class="token punctuation">]</span>
            batch_logs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            batch_logs<span class="token punctuation">[</span><span class="token string">'batch'</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch_index
            batch_logs<span class="token punctuation">[</span><span class="token string">'size'</span><span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>batch_ids<span class="token punctuation">)</span>
            callbacks<span class="token punctuation">.</span>on_batch_begin<span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> batch_logs<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># 应用传入的train_function</span>
            outs <span class="token operator">=</span> f<span class="token punctuation">(</span>ins_batch<span class="token punctuation">)</span>
            callbacks<span class="token punctuation">.</span>on_batch_end<span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> batch_logs<span class="token punctuation">)</span>
        callbacks<span class="token punctuation">.</span>on_epoch_end<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> epoch_logs<span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>history
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>_make_train_function()</code>方法从<code>optimizer</code>获取要更新的参数信息，并传入来自<code>backend</code>的<code>function</code>对象：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_make_train_function</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>train_function <span class="token keyword">is</span> None<span class="token punctuation">:</span>
        inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>_feed_inputs <span class="token operator">+</span> self<span class="token punctuation">.</span>_feed_targets <span class="token operator">+</span> self<span class="token punctuation">.</span>_feed_sample_weights
        training_updates <span class="token operator">=</span> self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>get_updates<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>_collected_trainable_weights<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>constraints<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>total_loss<span class="token punctuation">)</span>
        updates <span class="token operator">=</span> self<span class="token punctuation">.</span>updates <span class="token operator">+</span> training_updates
        <span class="token comment" spellcheck="true"># Gets loss and metrics. Updates weights at each call.</span>
        self<span class="token punctuation">.</span>train_function <span class="token operator">=</span> K<span class="token punctuation">.</span>function<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>
                                         <span class="token punctuation">[</span>self<span class="token punctuation">.</span>total_loss<span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>metrics_tensors<span class="token punctuation">,</span>
                                         updates<span class="token operator">=</span>updates<span class="token punctuation">,</span>
                                         name<span class="token operator">=</span><span class="token string">'train_function'</span><span class="token punctuation">,</span>
                                         <span class="token operator">**</span>self<span class="token punctuation">.</span>_function_kwargs<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Model</code>的其他方法<code>evaluate()</code>等，与<code>fit()</code>的结构类似。</p>
<h3 id="Sequential-构建模型的外层接口"><a href="#Sequential-构建模型的外层接口" class="headerlink" title="Sequential:构建模型的外层接口"></a><code>Sequential</code>:构建模型的外层接口</h3><p><code>Sequential</code>对象是<code>Model</code>对象的进一步封装，也是用户直接面对的接口，其<code>compile()</code>, <code>fit()</code>, <code>predict()</code>等方法与<code>Model</code>几乎一致，所不同的是添加了<code>add()</code>方法，也是我们用于构建网络的最基本操作。</p>
<p><code>Sequential.add()</code>方法的源码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 第一层必须是InputLayer对象</span>
    <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token operator">not</span> layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">:</span>
            x <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span>layer<span class="token punctuation">.</span>batch_input_shape<span class="token punctuation">,</span>
                      dtype<span class="token operator">=</span>layer<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> name<span class="token operator">=</span>layer<span class="token punctuation">.</span>name <span class="token operator">+</span> <span class="token string">'_input'</span><span class="token punctuation">)</span>
            layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>inputs <span class="token operator">=</span> topology<span class="token punctuation">.</span>get_source_inputs<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        topology<span class="token punctuation">.</span>Node<span class="token punctuation">(</span>outbound_layer<span class="token operator">=</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        output_tensor <span class="token operator">=</span> layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> <span class="token punctuation">[</span>output_tensor<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensors <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs

    self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到，<code>add()</code>方法总是确保网络的第一层为<code>InputLayer</code>对象，并将新加入的层应用于<code>outputs</code>，使之更新。因此，从本质上讲，在<code>Model</code>中添加新层还是在更新模型的<code>outputs</code>。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Programming </tag>
            
            <tag> Keras </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[源码笔记]keras源码分析之Container]]></title>
      <url>http://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/</url>
      <content type="html"><![CDATA[<p>本篇继续讨论keras的源码结构。</p>
<p><a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">第一篇源码笔记</a>中我们观察了<code>Layer</code>, <code>Tensor</code>和<code>Node</code>是如何耦合在一起的，而本篇的重点是观察多层网络构成的有向无环图（DAG）。主要涉及的文件为<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">keras/engine/topology.py</a>， 要观察的类是<code>Container</code>。</p>
<h3 id="Container对象：DAG的拓扑原型"><a href="#Container对象：DAG的拓扑原型" class="headerlink" title="Container对象：DAG的拓扑原型"></a><code>Container</code>对象：DAG的拓扑原型</h3><p>在第一篇中我们提到，Keras Tensor中增强的<code>\_keras_history</code>属性使得我们仅通过输入和输出的Tensor，就可以构建出整张计算图。而<code>Container</code>对象正是实现了这样的过程。</p>
<h4 id="计算图的构建"><a href="#计算图的构建" class="headerlink" title="计算图的构建"></a>计算图的构建</h4><p>DAG计算图的构建在<code>Container</code>对象实例化时完成，主要包括如下几个操作：</p>
<ol>
<li><p>记录<code>Container</code>的首尾连接信息</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
   layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
   self<span class="token punctuation">.</span>output_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>output_layers_node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>output_layers_tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>

<span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span>
   layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
   self<span class="token punctuation">.</span>input_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>input_layers_node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>input_layers_tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>从<code>output_tensors</code>开始反向递归构建计算图，采用广度优先的准则，本步的关键是构建<code>nodes_in_decreasing_depth</code>这一队列，这些<code>Node</code>包含的连接信息和深度信息将是后续正向传播和反向训练计算执行顺序的依据。</p>
</li>
</ol>
<pre class="line-numbers language-python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">build_map_of_graph</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">)</span><span class="token punctuation">:</span>
      layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> tensor<span class="token punctuation">.</span>_keras_history
      node <span class="token operator">=</span> layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>node_index<span class="token punctuation">]</span>
      nodes_in_progress<span class="token punctuation">.</span>add<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

      <span class="token comment" spellcheck="true"># 广度优先搜索</span>
      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          x <span class="token operator">=</span> node<span class="token punctuation">.</span>input_tensors<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          layer <span class="token operator">=</span> node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          node_index <span class="token operator">=</span> node<span class="token punctuation">.</span>node_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          tensor_index <span class="token operator">=</span> node<span class="token punctuation">.</span>tensor_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          <span class="token comment" spellcheck="true"># 递归调用</span>
          build_map_of_graph<span class="token punctuation">(</span>x<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">,</span>
                             layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index<span class="token punctuation">)</span>

      <span class="token comment" spellcheck="true"># 维护两个队列</span>
      finished_nodes<span class="token punctuation">.</span>add<span class="token punctuation">(</span>node<span class="token punctuation">)</span>
      nodes_in_progress<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>node<span class="token punctuation">)</span>
      nodes_in_decreasing_depth<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 反向构建DAG</span>
  <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
      build_map_of_graph<span class="token punctuation">(</span>x<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li>计算各节点的深度并按深度标定节点在DAG中的位置</li>
</ol>
<pre class="line-numbers language-python"><code class="language-python">  <span class="token comment" spellcheck="true"># 根据队列标定各节点的深度</span>
  <span class="token keyword">for</span> node <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>nodes_in_decreasing_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>
      depth <span class="token operator">=</span> nodes_depths<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span>node<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
      previous_depth <span class="token operator">=</span> layers_depths<span class="token punctuation">.</span>get<span class="token punctuation">(</span>node<span class="token punctuation">.</span>outbound_layer<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
      depth <span class="token operator">=</span> max<span class="token punctuation">(</span>depth<span class="token punctuation">,</span> previous_depth<span class="token punctuation">)</span>
      layers_depths<span class="token punctuation">[</span>node<span class="token punctuation">.</span>outbound_layer<span class="token punctuation">]</span> <span class="token operator">=</span> depth
      nodes_depths<span class="token punctuation">[</span>node<span class="token punctuation">]</span> <span class="token operator">=</span> depth

      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          inbound_layer <span class="token operator">=</span> node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          node_index <span class="token operator">=</span> node<span class="token punctuation">.</span>node_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          inbound_node <span class="token operator">=</span> inbound_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>node_index<span class="token punctuation">]</span>
          previous_depth <span class="token operator">=</span> nodes_depths<span class="token punctuation">.</span>get<span class="token punctuation">(</span>inbound_node<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
          nodes_depths<span class="token punctuation">[</span>inbound_node<span class="token punctuation">]</span> <span class="token operator">=</span> max<span class="token punctuation">(</span>depth <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> previous_depth<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 按深度标定各节点的位置</span>
  nodes_by_depth <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> node<span class="token punctuation">,</span> depth <span class="token keyword">in</span> nodes_depths<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> depth <span class="token operator">not</span> <span class="token keyword">in</span> nodes_by_depth<span class="token punctuation">:</span>
          nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 按深度标定各层的位置</span>
  layers_by_depth <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> layer<span class="token punctuation">,</span> depth <span class="token keyword">in</span> layers_depths<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> depth <span class="token operator">not</span> <span class="token keyword">in</span> layers_by_depth<span class="token punctuation">:</span>
          layers_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      layers_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>

  self<span class="token punctuation">.</span>layers_by_depth <span class="token operator">=</span> layers_by_depth
  self<span class="token punctuation">.</span>nodes_by_depth <span class="token operator">=</span> nodes_by_depth
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li>将整个<code>Container</code>并入<code>Node</code>以保持兼容性<pre class="line-numbers language-python"><code class="language-python">self<span class="token punctuation">.</span>outbound_nodes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
self<span class="token punctuation">.</span>inbound_nodes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
Node<span class="token punctuation">(</span>outbound_layer<span class="token operator">=</span>self<span class="token punctuation">,</span>
    inbound_layers<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    node_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    tensor_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    input_tensors<span class="token operator">=</span>self<span class="token punctuation">.</span>inputs<span class="token punctuation">,</span>
    output_tensors<span class="token operator">=</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ol>
<h3 id="计算图中的计算"><a href="#计算图中的计算" class="headerlink" title="计算图中的计算"></a>计算图中的计算</h3><p>计算在<code>Container</code>对象的<code>call()</code>方法完成，其实现又依靠内部方法<code>run_internal_graph()</code>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_internal_graph</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> masks<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
       depth_keys <span class="token operator">=</span> list<span class="token punctuation">(</span>self<span class="token punctuation">.</span>nodes_by_depth<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
       depth_keys<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
       <span class="token comment" spellcheck="true"># 依据深度</span>
       <span class="token keyword">for</span> depth <span class="token keyword">in</span> depth_keys<span class="token punctuation">:</span>
           nodes <span class="token operator">=</span> self<span class="token punctuation">.</span>nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span>
           <span class="token comment" spellcheck="true"># 对同一深度上的Node进行计算</span>
           <span class="token keyword">for</span> node <span class="token keyword">in</span> nodes<span class="token punctuation">:</span>
               layer <span class="token operator">=</span> node<span class="token punctuation">.</span>outbound_layer <span class="token comment" spellcheck="true"># Node对应的layer</span>
               reference_input_tensors <span class="token operator">=</span> node<span class="token punctuation">.</span>input_tensors
               reference_output_tensors <span class="token operator">=</span> node<span class="token punctuation">.</span>output_tensors
               computed_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
               <span class="token keyword">if</span> len<span class="token punctuation">(</span>computed_data<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>reference_input_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>
                   <span class="token comment" spellcheck="true"># 在Layer中进行计算</span>
                   <span class="token keyword">with</span> K<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
                       <span class="token keyword">if</span> len<span class="token punctuation">(</span>computed_data<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                           computed_tensor<span class="token punctuation">,</span> computed_mask <span class="token operator">=</span> computed_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                           output_tensors <span class="token operator">=</span> _to_list<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>call<span class="token punctuation">(</span>computed_tensor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
                           computed_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>computed_tensor<span class="token punctuation">]</span>
                       <span class="token keyword">else</span><span class="token punctuation">:</span>
                           computed_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> computed_data<span class="token punctuation">]</span>
                           output_tensors <span class="token operator">=</span> _to_list<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>call<span class="token punctuation">(</span>computed_tensors<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
       output_tensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
       output_masks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
       <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
           tensor<span class="token punctuation">,</span> mask <span class="token operator">=</span> tensor_map<span class="token punctuation">[</span>str<span class="token punctuation">(</span>id<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
           output_tensors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
           output_masks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
       <span class="token keyword">return</span> output_tensors<span class="token punctuation">,</span> output_masks
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>从上面的代码可以看到计算是依据深度进行的，并通过更新<code>computed_data</code>和<code>output_tensor</code>等变量完成整张图的遍历计算。</p>
<p>继续阅读系列第三篇：<a href="https://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/">【源码笔记】keras源码分析之Model</a></p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Programming </tag>
            
            <tag> Keras </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习中的权重衰减]]></title>
      <url>http://blog.ddlee.cn/2017/07/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</url>
      <content type="html"><![CDATA[<p>权重衰减（weight dacay），即$$L^2$$范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自<a href="https://deeplearningbook.org" target="_blank" rel="external">The Deep Learning Book</a>。</p>
<h3 id="为什么要引入权重衰减"><a href="#为什么要引入权重衰减" class="headerlink" title="为什么要引入权重衰减"></a>为什么要引入权重衰减</h3><p>机器学习的逻辑与我们最初解决问题的思维方式恰恰相反：要解决问题，一种经典的思路是把它拆成小问题，考虑之间的依赖，然后分而治之。而机器学习的哲学是“<em>trail-error-correct</em>”：先假设一堆可能的方案，根据结果去选择/调整这些方案，直到满意。换句话说，机器学习在假设空间中搜索最符合数据的模型：以果推因，即为最大似然的想法。随着数据量的增大，我们越来越需要表达能力更强的模型，而深度学习的优势正符合这一需要：通过分布式表示带来的指数增益，深度学习模型的扩展能力几乎是无限的（详见<a href="https://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/">深度学习和分布式表示</a>）。</p>
<p>有了模型（备选模型集），有了数据，就不得不面对机器学习领域的核心问题：如何保证模型能够描述数据（拟合）和生成数据（泛化）。</p>
<p>粗略来看，有以下三种情况：</p>
<ul>
<li>我们假定的模型族不包含真实数据的生成过程：欠拟合/高偏差</li>
<li>匹配真实的数据生成过程</li>
<li>除了包含真实的生成过程，还包含了其他信息：过拟合/高方差</li>
</ul>
<p>高偏差意味着我们的模型不够准确（模型族不足以描述数据），高方差意味着我们建模了不必要的信息（训练数据的随机性带来的）。前者通过提高模型的表述能力来解决（更深的网络），后者则需要合理的正则化技术来控制。这即是著名的trade-off。</p>
<h3 id="深度学习模型的参数"><a href="#深度学习模型的参数" class="headerlink" title="深度学习模型的参数"></a>深度学习模型的参数</h3><p>对数据建模，其实是从数据中提取我们能够理解的信息。建立的模型，是从数据分布的空间到目标变量所在空间的映射。从这个角度看，我们通过模型带来的变换获得了数据的一种表示，我们认为能够理解和操作的表示。</p>
<p>为了表述这一变换，深度模型的套路是线性层施加变换，非线性层固定信息（不能平移），然后将这样的结构堆叠起来，分层提取数据特征。</p>
<p>这让我想起实变中证明定理的套路：先证明简单函数的情形，再推广到连续函数，再到勒贝格可积的函数。</p>
<p>常规的套路（MLP）在拟合普通的函数任务上能够胜任，但面对更复杂的图像等数据，就需要更灵活的网络结构。</p>
<p>非常出色的CNN, LSTM, Inception块, ResNet, DenseNet等结构，就是加入了人类的先验知识，使之更有效的提取图像/音频数据分布空间的特征。（所以Manning有次在课堂上说，机器学习事实上还是人类在学习：机器只是在求导数、做乘法，最好的模型都是人们学习出来的。）</p>
<p>人们确实设计了很多巧妙的结构来解决不同的问题，但落实到网络的层和单元上，仍是最基本的矩阵乘法、加法运算。决定模型表述能力的，也正是这些普通的乘法运算中涉及的矩阵和向量了。</p>
<h3 id="权重衰减如何起作用"><a href="#权重衰减如何起作用" class="headerlink" title="权重衰减如何起作用"></a>权重衰减如何起作用</h3><p>下面我们通过观察加入权重衰减后目标函数的梯度变化来讨论权重衰减是如何起作用的。可以跳过公式部分直接看最后一段。</p>
<p>——————————————————————————————推导部分————————————————————————————————————————————</p>
<p>简单起见，令偏置为0，模型的目标函数：</p>
<p>$$J_{1}(w; X,y)=\frac{\alpha}{2} w^T w+J(w; X,y)$$</p>
<p>对应的梯度为：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w; X,y) = \alpha w + {\nabla}_{w} J(w; X,y)$$</p>
<p>进行梯度下降，参数的更新规则为：</p>
<p>$$w = w - \epsilon (\alpha w + {\nabla}_{w} J(w; X,y)) $$</p>
<p>也就是：</p>
<p>$$w = (1 - \epsilon \alpha )w - \epsilon {\nabla}_{w} J(w; X,y)$$</p>
<p>从上式可以发现，加入权重衰减后，先对参数进行伸缩，再沿梯度下降。下面令$$x^{(1)}$$为使目标函数达到最优的参数值，在其附近考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>其中$$H$$为近似目标函数在的Hessian矩阵。当近似目标函数最小时，其梯度为$$0$$，即：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>该式也向我们说明了基于梯度的优化算法主要的信息来自Hessian矩阵。添加入权重衰减项之后，上式变为（记此时的最优点为$$w^{(2)}$$）：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w) \approx \alpha w^{(2)} + H(w^{(2)} - w^{(1)}) = 0$$</p>
<p>所以</p>
<p>$$w^{(2)} = (H + \alpha I)^{-1} H w^{(1)} $$</p>
<p>该式表明了了加入正则化对参数最优质点的影响，由Hessian矩阵和正则化系数$$\alpha$$共同决定。</p>
<p>进一步将Hessian矩阵分解，可以得到：</p>
<p>$$w^{(2)} = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^{(1)}$$</p>
<p>其中，$$Q$$为正交矩阵，$$\Lambda$$为对角矩阵。这样可以看到，<em>权重衰减的效果是沿着由$$H$$的特征向量所定义的轴缩放$$w$$</em>， 具体的伸缩因子为$$\frac{ {\lambda}_{i} }{ {\lambda}<em>i + \alpha }$$，其中$${\lambda}</em>{i}$$表示第$$i$$个特征向量对应的特征值。</p>
<p>当特征值$$\lambda$$很大（相比$$\alpha$$）时，缩放因子对权重影响较小，因而更新过程中产生的变化也不大；而当特征值较小时，$$\alpha$$的缩放作用就显现出来，将这个方向的权重衰减到0。</p>
<p>这种效果也可以由下图表示：</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习中的权重衰减/transform.png" alt="transform"></p>
<p>——————————————————————————推导部分结束————————————————————————————————————————————————————————————————————</p>
<p><em>总结来说，目标函数的Hessian矩阵（显式、隐式或者近似的）是现有优化算法进行寻优的主要依据。通过控制权重衰减的$$\alpha$$参数，我们实际上控制的是在Hessian矩阵的特征方向上以多大的幅度缩放权重，相对重要（能够显著减小目标函数）的方向上权重保留比较完好，而无助于目标函数减小的方向上权重在训练过程中逐渐地衰减掉了。而这也就是权重衰减的意义。</em></p>
<p>从宏观上来看，对目标函数来说，特征值较大的方向包含更多有关数据的信息，较小的方向则有随机性的噪声，权重衰减正是通过忽略较少信息方向的变化来对抗过拟合的。</p>
<h3 id="L-1-范数正则化"><a href="#L-1-范数正则化" class="headerlink" title="$$L^1$$范数正则化"></a>$$L^1$$范数正则化</h3><p>通过类似的推导，可以得到加入了$$L^1$$范数惩罚项对参数最优解的影响如下：</p>
<p>$$w^{(2)}<em>{i} = sign(w^{(1)}</em>{i}) max \big{|w^{(1)}<em>{i}| - \frac{\alpha}{H</em>{i,i}}, 0 \big}$$</p>
<p>相比$$L^2$$范数的影响，这是一个离散的结果，因而$$L^1$$范数惩罚会将参数推向更加稀疏的解。这种稀疏性质常被用作特征选择。</p>
<h3 id="权重衰减的贝叶斯解释"><a href="#权重衰减的贝叶斯解释" class="headerlink" title="权重衰减的贝叶斯解释"></a>权重衰减的贝叶斯解释</h3><p>在贝叶斯统计的框架下，常用的推断策略是最大后验点估计(Maximum A Posteriori, MAP)。有如下的推断公式（由贝叶斯定律导出）：</p>
<p>$${\theta}_{MAP} = argmax p(\theta | x) = argmax (log p( x | \theta) + log p(\theta))$$</p>
<p>上式右边第一项是标准的对数似然项，而第二项对应着先验分布。</p>
<p>在这样的视角下，我们只进行最大似然估计是不够的，还要考虑先验$$p(\theta)$$的分布。而当假定参数为正态分布$$N(w; 0, \frac{1}{\lambda}I^2)$$时，带入上式（$$\theta$$为参数），即可发现第二项的结果正比于权重衰减惩罚项$$\lambda w^T w$$，加上一个不依赖于$$w$$也不影响学习过程的项。于是，具有高斯先验权重的MAP贝叶斯推断对应着权重衰减。</p>
<h3 id="权重衰减与提前终止"><a href="#权重衰减与提前终止" class="headerlink" title="权重衰减与提前终止"></a>权重衰减与提前终止</h3><p>提前终止也是一种正则化技术，其想法简单粗暴：每个epoch之后在验证集上评估结果，当验证集误差不再下降的时候，我们认为模型已经尽它所能了，于是终止训练过程。</p>
<p>提前终止以牺牲一部分训练数据来作为验证数据来的代价来对抗过拟合，其逻辑是实证主义的。</p>
<p>然而，在二次近似和简单梯度下降的情形下，可以观察到提前终止可以有相当于权重衰减的效果。</p>
<p>我们仍考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>记最优参数点为$$w^{(1)}$$，其梯度为：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>不加入正则化项，其梯度下降的更新策略（从第$$\tau-1$$步到$$\tau$$步）为：</p>
<p>$$ w^{(\tau)} = w^{\tau - 1)} - \epsilon H (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>累加得到</p>
<p>$$ w^{(\tau)} - w^{(1)} = (I - \epsilon H) (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>将Hessian矩阵分解，得到如下形式</p>
<p>$$ w^{(\tau)} = Q[I - (I - \epsilon \Lambda) ^ {\tau}] Q^T w^{(1)} $$</p>
<p>将加入正则化项的权重影响改写为</p>
<p>$$ w^{(2)} = Q[I - (\Lambda + \alpha I) ^ {-1} \alpha] Q^T w^{(1)} $$</p>
<p>对比可以得到，如果超参数$$\epsilon, \alpha, \tau$$满足</p>
<p>$$ (I - \epsilon \Lambda) ^ {\tau} = (\Lambda + \alpha I) ^ {-1} \alpha $$</p>
<p>则提前终止将与权重衰减有相当的效果。具体的，即第$$\tau$$步结束的训练过程将到达超参数为$$\alpha$$的$$L^2$$正则化得到的最优点。</p>
<p>但提前终止带来的好处是，我们不再需要去找合适的超参数$$\alpha$$，而只需要制定合理的终止策略（如3个epoch均不带来验证集误差的减小即终止训练），在训练成本的节约上，还是很值得的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[源码笔记]keras源码分析之Layer、Tensor和Node]]></title>
      <url>http://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/</url>
      <content type="html"><![CDATA[<p>Keras架构的主要逻辑实现在<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">/keras/engine/topology.py</a>中，主要有两个基类<code>Node()</code>和<code>Layer()</code>，一个重要函数<code>Input()</code>。具体地，</p>
<ul>
<li><code>Layer()</code>是一个计算层的抽象，完成网络中对Tensor的计算过程；</li>
<li><code>Node()</code>描述两个层之间连接关系的抽象，配合<code>Layer()</code>构建DAG；</li>
<li><code>Input()</code>实例化一个特殊的<code>Layer</code>(<code>InputLayer</code>)，将<code>backend</code>（TensorFlow或Theano）建立的Tensor对象转化为Keras Tensor对象。</li>
</ul>
<h3 id="Keras-Tensor：-增强版Tensor"><a href="#Keras-Tensor：-增强版Tensor" class="headerlink" title="Keras Tensor： 增强版Tensor"></a>Keras Tensor： 增强版Tensor</h3><p>相比原始的TensorFlow或者Theano的张量对象，Keras Tensor加入了如下两个属性，以使Tensor中包含了自己的来源和规模信息：</p>
<ul>
<li>_Keras_history: 保存了最近一个应用于这个Tensor的Layer</li>
<li>_keras_shape: 标准化的Keras shape接口</li>
</ul>
<p>当使用Keras建立深度网络时，传入的数据首先要经过<code>Input()</code>函数。在<code>Input()</code>函数中，实例化一个<code>InputLayer()</code>对象，并将此<code>Layer()</code>对象作为第一个应用于传入张量的Layer，置于<code>_keras_history</code>属性中。此外，<code>InputLayer()</code>和<code>Input()</code>还会对传入的数据进行规模检查和变换等，使之符合后续操作的要求。</p>
<p>代码上实现如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Input</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  input_layer <span class="token operator">=</span> InputLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
  outputs <span class="token operator">=</span> InputLayer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensor
  <span class="token keyword">return</span> outputs

<span class="token keyword">class</span> <span class="token class-name">InputLayer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_tensor<span class="token punctuation">.</span>_keras_history <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    Node<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在下面我们将看到，加入的<code>_keras_history</code>属性在计算图的构建上所起的作用是关键的。仅通过输入和输出的Tensor，我们可以构建出整张计算图。但这样的代价是Tensor对象太重了，包含了Layer的信息。</p>
<h3 id="Node对象：层与层之间链接的抽象"><a href="#Node对象：层与层之间链接的抽象" class="headerlink" title="Node对象：层与层之间链接的抽象"></a><code>Node</code>对象：层与层之间链接的抽象</h3><p>若考虑<code>Layer</code>对象抽象的是完成计算的神经元胞体，则<code>Node</code>对象是对神经元树突结构的抽象。其内聚的主要信息是：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Node</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outbound_layer<span class="token punctuation">,</span>
              inbound_layers<span class="token punctuation">,</span> node_indices<span class="token punctuation">,</span> tensor_indices<span class="token punctuation">,</span>
              input_tensors<span class="token punctuation">,</span> output_tensors<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>其中<code>outbound_layer</code>是施加计算（使<code>input_tensors</code>变为<code>output_tensors</code>）的层，<code>inbound_layers</code>对应了<code>input_tensors</code>来源的层，而<code>node_indices</code>和<code>tensor_indices</code>则记录了<code>Node</code>和<code>Layer</code>之间的标定信息。</p>
<p><code>Node</code>对象总在<code>outbound_layer</code>被执行时创建，并加入<code>outbound_layer</code>的<code>inbound_nodes</code>属性中。在<code>Node</code>对象的表述下，A和B两个层产生连接关系时，<code>Node</code>对象被建立，并被加入<code>A.outbound_nodes</code>和<code>B.inbound_nodes</code>。</p>
<h3 id="Layer对象：计算层的抽象"><a href="#Layer对象：计算层的抽象" class="headerlink" title="Layer对象：计算层的抽象"></a><code>Layer</code>对象：计算层的抽象</h3><p><code>Layer</code>对象是对网络中神经元计算层的抽象，实例化需要如下参数：</p>
<pre class="line-numbers language-python"><code class="language-python">allowed_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'input_shape'</span><span class="token punctuation">,</span>
                  <span class="token string">'batch_input_shape'</span><span class="token punctuation">,</span>
                  <span class="token string">'batch_size'</span><span class="token punctuation">,</span>
                  <span class="token string">'dtype'</span><span class="token punctuation">,</span>
                  <span class="token string">'name'</span><span class="token punctuation">,</span>
                  <span class="token string">'trainable'</span><span class="token punctuation">,</span>
                  <span class="token string">'weights'</span><span class="token punctuation">,</span>
                  <span class="token string">'input_dtype'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># legacy</span>
                  <span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>大部分与传入数据的类型和规模相关，<code>trainable</code>表征该层是否需要更新权重。此外，还有<code>inbound_nodes</code>和<code>outbound_nodes</code>属性来标定与<code>Node</code>对象的链接。</p>
<p><code>Layer</code>对象最重要的方法是<code>__call__()</code>，主要完成如下三件事情：</p>
<ol>
<li><p>验证传入数据的合法性，通过调用内部方法实现：<code>self.assert_input_compatibility(inputs)</code></p>
</li>
<li><p>进行计算<code>outputs = self.call(inputs, ...)</code>，被其子类具体实现，如<code>Linear</code>, <code>Dropout</code>等</p>
</li>
<li><p>更新Tensor中的<code>_keras_history</code>属性，记录该次计算操作，通过内部方法<code>_add_inbound_nodes()</code>实现</p>
</li>
</ol>
<p>方法<code>_add_inbound_nodes()</code>对Tensor的更新是构建<code>Layer</code>之间关系的关键操作，其主要代码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> x <span class="token keyword">in</span> input_tensors<span class="token punctuation">:</span>
    <span class="token keyword">if</span> hasattr<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'_keras_history'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        inbound_layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
        inbound_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>inbound_layer<span class="token punctuation">)</span>
        node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
        tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Node对象的建立过程中将更新self的inbound_nodes属性</span>
Node<span class="token punctuation">(</span>self<span class="token punctuation">,</span>
    inbound_layers<span class="token operator">=</span>inbound_layers<span class="token punctuation">,</span>
    node_indices<span class="token operator">=</span>node_indices<span class="token punctuation">,</span>
    tensor_indices<span class="token operator">=</span>tensor_indices<span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>output_tensors<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     output_tensors<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>_keras_history <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上段代码取出<code>input_tensor</code>的<code>_keras_history</code>属性，建立新的<code>Node</code>，并将当前<code>Layer</code>的信息更新到计算得到的<code>output_tensor</code>中。</p>
<h3 id="实例：Node-Tensor和Layer间连接关系的表征"><a href="#实例：Node-Tensor和Layer间连接关系的表征" class="headerlink" title="实例：Node,Tensor和Layer间连接关系的表征"></a>实例：<code>Node</code>,<code>Tensor</code>和<code>Layer</code>间连接关系的表征</h3><p>下面通过代码来说明三者之间的关系，来自于测试代码：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 建立新的keras Tensor</span>
a <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'input_a'</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'input_b'</span><span class="token punctuation">)</span>

a_layer<span class="token punctuation">,</span> a_node_index<span class="token punctuation">,</span> a_tensor_index <span class="token operator">=</span> a<span class="token punctuation">.</span>_keras_history
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>a_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>
<span class="token keyword">assert</span> a_tensor_index <span class="token keyword">is</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># node和layer之间的关系</span>
node <span class="token operator">=</span> a_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>a_node_index<span class="token punctuation">]</span>
<span class="token keyword">assert</span> node<span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> a_layer

<span class="token comment" spellcheck="true"># 建立连接层，将Tensor传入</span>
dense <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'dense_1'</span><span class="token punctuation">)</span>
a_2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
b_2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>b<span class="token punctuation">)</span>

<span class="token keyword">assert</span> len<span class="token punctuation">(</span>dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>dense<span class="token punctuation">.</span>outbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># 与张量a关联的Node</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>inbound_layers <span class="token operator">==</span> <span class="token punctuation">[</span>a_layer<span class="token punctuation">]</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> dense
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>input_tensors <span class="token operator">==</span> <span class="token punctuation">[</span>a<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 与张量b关联的Node</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>inbound_layers <span class="token operator">==</span> <span class="token punctuation">[</span>b_layer<span class="token punctuation">]</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> dense
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>input_tensors <span class="token operator">==</span> <span class="token punctuation">[</span>b<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>keras利用<code>Node</code>对象描述<code>Layer</code>之间的连接关系，并在<code>Tensor</code>中记录其来源信息。在<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">下篇</a>中，我们将看到keras如何利用这些抽象和增强属性构建DAG，并实现前向传播和反向训练的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Programming </tag>
            
            <tag> Keras </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tensorflow最佳实践：试验管理]]></title>
      <url>http://blog.ddlee.cn/2017/07/11/Tensorflow%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E8%AF%95%E9%AA%8C%E7%AE%A1%E7%90%86/</url>
      <content type="html"><![CDATA[<p>本文主要记录使用TensorFlow训练模型中与试验管理相关的最佳实践，主要包括模型训练的大致代码框架、模型的保存与恢复、训练过程的监测、随机性的控制等。主要材料来自<a href="https://web.stanford.edu/class/cs20si/index.html" target="_blank" rel="external">CS 20SI: Tensorflow for Deep Learning Research</a>。</p>
<h3 id="TensorFlow代码框架"><a href="#TensorFlow代码框架" class="headerlink" title="TensorFlow代码框架"></a>TensorFlow代码框架</h3><p>使用TensorFlow构建深度网络模型大致包括数据预处理、图的构建、模型训练、模型推断与评估等部分，大致的代码框架如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment" spellcheck="true"># Data</span>
X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_input<span class="token punctuation">]</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_output<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Parameters</span>
W <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>n_input<span class="token punctuation">,</span> n_output<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>n_output<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Define model</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span> b
y_pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
cost <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax_cross_entropy_with_logits<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y_true<span class="token punctuation">)</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>GradientDescentOptimizer<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Training</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    tf<span class="token punctuation">.</span>initialize_all_variables<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> x_data<span class="token punctuation">,</span> Y<span class="token punctuation">:</span> y_data<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Prediction</span>
y_test <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型的保存与恢复"><a href="#模型的保存与恢复" class="headerlink" title="模型的保存与恢复"></a>模型的保存与恢复</h3><p>一个很深的网络训练成本是比较高的，因而将模型定期保存（写入硬盘）则有必要。这里的模型，实际上是有组织的一批数据，包括图的结构描述、参数当前值等。因而我们要保存的不仅是模型，还有模型当前的运行状态，实际上每一次保存可以作为一个还原点。</p>
<h4 id="tf-train-Saver类"><a href="#tf-train-Saver类" class="headerlink" title="tf.train.Saver类"></a>tf.train.Saver类</h4><p>使用<code>tf.train.Saver</code>类需传入以下参数：<code>tf.train.Saver.save(sess, save_path, global_step=step)</code>。</p>
<p>首先定义步数变量：<code>self​.​global_step ​=​ tf​.​Variable​(​0​,​ dtype​=​tf​.​int32​,​ trainable​=​False​,name​=​&#39;global_step&#39;)</code></p>
<p>在模型训练的过程中插入还原点的保存：</p>
<pre class="line-numbers language-python"><code class="language-python">self​<span class="token punctuation">.</span>​optimizer ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​train​<span class="token punctuation">.</span>​GradientDescentOptimizer​<span class="token punctuation">(</span>​self​<span class="token punctuation">.</span>​lr​<span class="token punctuation">)</span><span class="token punctuation">.</span>​minimize​<span class="token punctuation">(</span>​self​<span class="token punctuation">.</span>​loss​<span class="token punctuation">,</span>global_step​<span class="token operator">=</span>​self​<span class="token punctuation">.</span>​global_step<span class="token punctuation">)</span>
saver ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​train​<span class="token punctuation">.</span>​Saver​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​
​<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
 sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​tf​<span class="token punctuation">.</span>​global_variables_initializer​<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
 average_loss ​<span class="token operator">=</span>​​<span class="token number">0.0</span>
 <span class="token keyword">for</span>​ index ​<span class="token keyword">in</span>​ range​<span class="token punctuation">(</span>​num_train_steps​<span class="token punctuation">)</span><span class="token punctuation">:</span>            
    batch ​<span class="token operator">=</span>​ batch_gen​<span class="token punctuation">.</span>​next​<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_batch​<span class="token punctuation">,</span>​ _ ​<span class="token operator">=</span>​ sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span><span class="token punctuation">[</span>​model​<span class="token punctuation">.</span>​loss​<span class="token punctuation">,</span>​ model​<span class="token punctuation">.</span>​optimizer​<span class="token punctuation">]</span><span class="token punctuation">,</span> feed_dict​<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
    average_loss ​<span class="token operator">+=</span>​ loss_batch
    <span class="token comment" spellcheck="true"># Save model every 1000 steps</span>
    ​<span class="token keyword">if</span>​​<span class="token punctuation">(</span>​index ​<span class="token operator">+</span>​​<span class="token number">1</span>​<span class="token punctuation">)</span>​​<span class="token operator">%</span>​​<span class="token number">1000</span>​​<span class="token operator">==</span>​​<span class="token number">0</span><span class="token punctuation">:</span>
      saver​<span class="token punctuation">.</span>​save​<span class="token punctuation">(</span>​sess​<span class="token punctuation">,</span>​​<span class="token string">'checkpoints/model'</span>​<span class="token punctuation">,</span>​ global_step​<span class="token operator">=</span>​model​<span class="token punctuation">.</span>​global_step<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在训练过程中，在<code>checkpoints</code>路径下会存储一系列的还原点文件，要恢复session到某个还原点，可使用如下代码：<code>saver.restore(sess, &#39;checkpoints/name_of_the_checkpoint&#39;)</code>。</p>
<h4 id="Keras封装：keras-callbacks-ModelCheckpoint"><a href="#Keras封装：keras-callbacks-ModelCheckpoint" class="headerlink" title="Keras封装：keras.callbacks.ModelCheckpoint()"></a>Keras封装：<code>keras.callbacks.ModelCheckpoint()</code></h4><p>Keras对TensorFlow进行了高层的封装，使用一系列回调函数<code>keras.callbacks.Callback()</code>来进行试验管理。</p>
<p>模型保存<code>ModelCheckpoint()</code>需要传入的参数：<br><code>keras.callbacks.ModelCheckpoint(filepath, monitor=&#39;val_loss&#39;, verbose=0, save_best_only=False, save_weights_only=False, mode=&#39;auto&#39;, period=1)</code></p>
<p>实际的使用中，将上述回调函数类传入<code>model.fit()</code>过程即可：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> ModelCheckpoint

model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'uniform'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Activation<span class="token punctuation">(</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'rmsprop'</span><span class="token punctuation">)</span>
checkpointer <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>filepath<span class="token operator">=</span><span class="token string">'/checkpoints/weights.hdf5'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> Y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>checkpointer<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型训练过程的监测"><a href="#模型训练过程的监测" class="headerlink" title="模型训练过程的监测"></a>模型训练过程的监测</h3><p>训练过程中，我们常常需要提取阶段性的信息来评估模型是否符合预期效果。</p>
<h4 id="tf-summary"><a href="#tf-summary" class="headerlink" title="tf.summary"></a><code>tf.summary</code></h4><p>首先创建想要观察指标的<code>tf.summary</code>对象：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"summaries"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>scalar<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span>
  tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>scalar<span class="token punctuation">(</span><span class="token string">"accuracy"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>accuracy<span class="token punctuation">)</span>
  tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>histogram<span class="token punctuation">(</span><span class="token string">"histogram loss"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span>
  <span class="token comment" spellcheck="true"># merge them all</span>
  self<span class="token punctuation">.</span>summary_op <span class="token operator">=</span> tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>merge_all<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>tf.summary</code>是一种operation，因而可以随训练过程一同运行：<br><code>loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)</code></p>
<p>最后，将summary加入writer以写入文件：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
  writer ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​summary​<span class="token punctuation">.</span>​FileWriter​<span class="token punctuation">(</span>​<span class="token string">'./summary'</span>​<span class="token punctuation">,</span>​ sess​<span class="token punctuation">.</span>​graph<span class="token punctuation">)</span>
  <span class="token keyword">for</span>​ index ​<span class="token keyword">in</span>​ range​<span class="token punctuation">(</span>​num_train_steps​<span class="token punctuation">)</span><span class="token punctuation">:</span>
    writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary<span class="token punctuation">,</span> global_step<span class="token operator">=</span>step<span class="token punctuation">)</span>
  writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这样，就可以用TensorBoard监测我们关心的指标在训练过程中的变化情况。</p>
<h4 id="Keras封装：keras-callbacks-TensorBoard"><a href="#Keras封装：keras-callbacks-TensorBoard" class="headerlink" title="Keras封装：keras.callbacks.TensorBoard()"></a>Keras封装：<code>keras.callbacks.TensorBoard()</code></h4><p>Keras同样将TensorBoard封装成回调函数的形式，在模型训练时进行调用即可：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> TensorBoard

tensorboard <span class="token operator">=</span> TensorBoard<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token string">"./logs"</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> Y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>tensorboard<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="随机性的控制"><a href="#随机性的控制" class="headerlink" title="随机性的控制"></a>随机性的控制</h3><p>TensorFlow中随机性的控制分为operation和graph两个层面。</p>
<h4 id="Operation层面"><a href="#Operation层面" class="headerlink" title="Operation层面"></a>Operation层面</h4><p>在Operation层面中，建立随机seed之后，新建立的Session每一次调用<code>sess.run()</code>都会遵循同一随机状态：</p>
<pre class="line-numbers language-python"><code class="language-python">c ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span>​<span class="token punctuation">,</span>​ seed​<span class="token operator">=</span>​<span class="token number">2</span><span class="token punctuation">)</span>

<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  <span class="token keyword">print</span>​ sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># >> 3.57493</span>

<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  <span class="token keyword">print</span>​ sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># >> 3.57493</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>而且，不同的operation可以保存自己的seed:</p>
<pre class="line-numbers language-python"><code class="language-python">c ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span>​<span class="token punctuation">,</span>​ seed​<span class="token operator">=</span>​<span class="token number">1</span><span class="token punctuation">)</span>
d ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span>​<span class="token punctuation">,</span>​ seed​<span class="token operator">=</span>​<span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span> ​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​d<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Graph层面"><a href="#Graph层面" class="headerlink" title="Graph层面"></a>Graph层面</h4><p>在Graph层面，整张图公用一个随机状态，多次运行同一图模型的计算，其随机状态保持一致。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span>​ tensorflow ​<span class="token keyword">as</span>​ tf

tf​<span class="token punctuation">.</span>​set_random_seed​<span class="token punctuation">(</span>​<span class="token number">2</span><span class="token punctuation">)</span>

c ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span><span class="token punctuation">)</span>
d ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​ <span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​d<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> best practice </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python最佳实践：遍历列表]]></title>
      <url>http://blog.ddlee.cn/2017/06/29/Python%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E9%81%8D%E5%8E%86%E5%88%97%E8%A1%A8/</url>
      <content type="html"><![CDATA[<h3 id="enumerate-遍历索引和值"><a href="#enumerate-遍历索引和值" class="headerlink" title="enumerate(): 遍历索引和值"></a>enumerate(): 遍历索引和值</h3><p>对列表进行遍历操作时，常也要用到当前遍历项的索引值：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>flavor_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  flavor <span class="token operator">=</span> flaver<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%d: %s'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> flavor<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>这种写法既要取出列表长度，又要根据索引取列表值。但要改用<code>enumerate()</code>函数，则可同时取出索引值和遍历项值：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> flavor <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>flavor_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%d: %s'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> flavor<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="zip-并行遍历多个列表"><a href="#zip-并行遍历多个列表" class="headerlink" title="zip(): 并行遍历多个列表"></a>zip(): 并行遍历多个列表</h3><p>我们有多个长度相同的列表，需要在同一索引下对遍历项值进行操作：</p>
<pre class="line-numbers language-python"><code class="language-python">names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Cecilia'</span><span class="token punctuation">,</span> <span class="token string">'Lise'</span><span class="token punctuation">,</span> <span class="token string">'Marie'</span><span class="token punctuation">]</span>
letters <span class="token operator">=</span> <span class="token punctuation">[</span>len<span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> names<span class="token punctuation">]</span>

longest_name <span class="token operator">=</span> None
max_letters <span class="token operator">=</span> <span class="token number">0</span>

<span class="token keyword">for</span> i<span class="token punctuation">,</span> name <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>names<span class="token punctuation">)</span><span class="token punctuation">:</span>
  count <span class="token operator">=</span> letters<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
  <span class="token keyword">if</span> count <span class="token operator">></span> max_letters<span class="token punctuation">:</span>
    longest_name <span class="token operator">=</span> name
    max_letters <span class="token operator">=</span> count
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>由上可见，name和letter通过索引值关联起来，而使用<code>zip()</code>函数，可免去根据索引取值的过程：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> count <span class="token keyword">in</span> zip<span class="token punctuation">(</span>names<span class="token punctuation">,</span> letters<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">if</span> count <span class="token operator">></span> max_letters<span class="token punctuation">:</span>
    longest_name <span class="token operator">=</span> name
    max_letters <span class="token operator">=</span> count
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>要注意的是，当多个列表长度不一时，到达最短列表的末尾时，遍历停止。</p>
<p>而且，<code>enumerate()</code>和<code>zip()</code>返回的对象都是lazy generator，相对来说更加高效。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Programming </tag>
            
            <tag> best practice </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Accurate, Large Minibatch SGD: Training ImageNet in One Hour]]></title>
      <url>http://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/</url>
      <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="external">Accurate, Large Minibatch SGD: Training ImageNet in One Hour</a></p>
<p>这篇文章在各处都有很广泛的讨论，作为实验经验并不多的小白，将文中tricks只做些记录。</p>
<h3 id="Linear-Scaling-Rule"><a href="#Linear-Scaling-Rule" class="headerlink" title="Linear Scaling Rule"></a>Linear Scaling Rule</h3><p>进行大批量的Minibatch SGD时会有批量越大，误差越大的问题。本文提出的Linear Scaling Rule正是试图解决这一问题。</p>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>设想两个情景：一是在一次参数更新中使用kn个样本梯度，二是分为k次更新，每次取n个样本梯度。</p>
<p>第一种情景的参数更新公式：<br>
$$w_t+1^{(1)} = w_t^{(1)} - \mu^{(1)} \frac{1}{kn} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t)$$
</p>
<p>第二种情景的参数更新公式：<br>
$$w_t+k^{(2)} = w_t^{(2)} - \mu^{(2)} \frac{1}{n} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t+j)$$
</p>
<p>由上面可以看出，主要的区别是梯度平均时批量的大小不同，前者为kn，后者为每次n，更新k次。</p>
<p>再假设双重求和号内项变化不大时，为使情景二更新k次（即使用同样数量的样本）之后参数与情景一类似，我们自然要将学习速率$\mu$线性提升。</p>
<h3 id="Gradual-Warmup"><a href="#Gradual-Warmup" class="headerlink" title="Gradual Warmup"></a>Gradual Warmup</h3><p>上面提到的Linear Scaling Rule使用的假设是梯度变化不大。但在训练初期，参数随机初始化，梯度变化很大，因而Linear Scaling Rule不再适用。在实践中，可以使学习速率在初始时较小，在经过几个epoch训练后再升至与kn批量相应的大小。</p>
<h3 id="BN-statistics"><a href="#BN-statistics" class="headerlink" title="BN statistics"></a>BN statistics</h3><p>在分布式训练的系统中，对于BN中要估计的均值和方差，文中给出的建议是对所有worker上的样本计算均值和方差，而不是每个worker单独计算。</p>
<h3 id="Weight-Decay"><a href="#Weight-Decay" class="headerlink" title="Weight Decay"></a>Weight Decay</h3><p>由于weight decay的存在，Linear Scaling Rule最好用于学习速率，而非用于Loss Function</p>
<h3 id="Momentum-Correction"><a href="#Momentum-Correction" class="headerlink" title="Momentum Correction"></a>Momentum Correction</h3><p>加入Linear Scaling Rule之后，适用动量加速的SGD需要进行动量更正。</p>
<h3 id="Data-Shuffling"><a href="#Data-Shuffling" class="headerlink" title="Data Shuffling"></a>Data Shuffling</h3><p>在分布式的系统中，先进行Data Shuffling，再分配数据到每个worker上。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[自用LaTeX中英文简历模板]]></title>
      <url>http://blog.ddlee.cn/2017/06/14/%E8%87%AA%E7%94%A8LaTeX%E4%B8%AD%E8%8B%B1%E6%96%87%E5%BB%BA%E7%AB%8B%E6%A8%A1%E6%9D%BF/</url>
      <content type="html"><![CDATA[<p>分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在<a href="https://www.sharelatex.com/templates/cv-or-resume/professional-cv" target="_blank" rel="external">ShareLaTeX</a>上分享的模板。</p>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li>Github仓库：<a href="https://github.com/ddlee96/latex_cv_template" target="_blank" rel="external">ddlee96/latex_cv_template</a></li>
<li>编译引擎： XeLaTeX</li>
<li>下载地址： <a href="https://github.com/ddlee96/latex_cv_template/releases/tag/0.1" target="_blank" rel="external">v0.1</a></li>
<li>压缩包内包含.tex文件和所用字体文件，解压后修改.tex文件再编译即可。</li>
<li>在Ubuntu 16.04, Texlive 2016环境下测试通过。</li>
<li>英文字体: Fontin，中文字体：方正兰亭黑</li>
</ul>
<h4 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h4><ul>
<li>.tex代码：Apache 2.0</li>
<li>字体： 仅供个人使用</li>
</ul>
<h4 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h4><h5 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h5><p><img src="http://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv-1.png" alt="en"></p>
<h5 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h5><p><img src="http://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv_zh-1.png" alt="zh"></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Individual Development </tag>
            
            <tag> LaTeX </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]On the Effects and Weight Normalization in GAN]]></title>
      <url>http://blog.ddlee.cn/2017/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-Effects-and-Weight-Normalization-in-GAN/</url>
      <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1704.03971" target="_blank" rel="external">On the Effects and Weight Normalization in GAN</a></p>
<p>本文探索了参数标准化(Weight Normalization)这一技术在GAN中的应用。BN在mini-batch的层级上计算均值和方差，容易引入噪声，并不适用于GAN这种生成模型，而WN对参数进行重写，引入噪声更少。</p>
<p>我觉得本文的亮点有二：</p>
<h3 id="1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变"><a href="#1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变" class="headerlink" title="1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变"></a>1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变</h3><p>朴素的参数标准化层有如下的形式：<br>
$$y=\frac{{w}^{T}x}{\|w\|}$$
<br>文中称这样形式的层为“strict weight-normalized layer”。若将线性层换为这样的层，网络的表达能力会下降，因而需要添加如下的affine transformation:</p>

$$y=\frac{{w}^{T}x}{\|w\|} \gamma + \beta$$

<p>用于恢复网络的表达能力。</p>
<p>将上述变换带入ReLU，简化后可以得到如下T-ReLu:<br>$$TReLU_\alpha (x) = ReLU(x-\alpha) + \alpha$$</p>
<p>文章的一个重要结论是，在网络的最后一层加入affine transformation层之后，堆叠的“线性层+ReLU”与“strict weight-normalized layer + T-ReLU”表达能力相同（在附录中给出证明）。</p>
<p>下面L表示线性层，R表示ReLU，TR表示TReLU，A表示affine transformation，S表示上述的strict weight-normalized layer。</p>
<p>证明的大致思路是，在ReLU与线性层之间加入affine transformation层，由于线性层的存在，affine transformation带来的效果会被吸收（相当于多个线性层叠在一起还是线性层），网络表达能力不变。而”L+R+A”的结构可以等价于”S+TR+A”。如此递归下去，即可得到结论。个人认为相当于把线性层中的bias转嫁成了TReLU中的threshold（即$\alpha$）。</p>
<h3 id="2-提出对生成图形的评估指标"><a href="#2-提出对生成图形的评估指标" class="headerlink" title="2. 提出对生成图形的评估指标"></a>2. 提出对生成图形的评估指标</h3><p>生成式模型的生成效果常常难以评价。DcGAN给出的结果也是生成图片的对比。本文中提出一个评价生成效果的指标，且与人的主观评价一致。</p>
<p>评价的具体指标是生成图片与测试集图片的欧氏距离，评价的对象是生成器是Generator。有如下形式：</p>

$$\frac{1}{m} \sum_{i=1}^{m} min_z {\|G(z)-x^{(i)}\|}^2$$

<p>其中的$min$指使用梯度下降方法等使生成图片的效果最好。但事实上这样做开销很高。</p>
<h3 id="PyTorch实现"><a href="#PyTorch实现" class="headerlink" title="PyTorch实现"></a>PyTorch实现</h3><p>作者将他们的实现代码公布在了<a href="https://github.com/stormraiser/GAN-weight-norm" target="_blank" rel="external">GitHub</a>上。</p>
<p>下面是利用PyTorch对T-ReLU的实现：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TPReLU</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_parameters<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> init<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>num_parameters <span class="token operator">=</span> num_parameters
        super<span class="token punctuation">(</span>TPReLU<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>num_parameters<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>init<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_parameters<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        bias_resize <span class="token operator">=</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_parameters<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>input<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>prelu<span class="token punctuation">(</span>input <span class="token operator">-</span> bias_resize<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> bias_resize
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对 Weigh-normalized layer 的实现：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">WeightNormalizedLinear</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> init_factor<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> init_scale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>WeightNormalizedLinear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> bias<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'bias'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>
        <span class="token keyword">if</span> scale<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>scale <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>init_scale<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'scale'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span>init_factor<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> factor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        stdv <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">*</span> factor <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>stdv<span class="token punctuation">,</span> stdv<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>stdv<span class="token punctuation">,</span> stdv<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">weight_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">norm_scale_bias</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> input<span class="token punctuation">.</span>div<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>scale <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            output <span class="token operator">=</span> output<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>scale<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            output <span class="token operator">=</span> output<span class="token punctuation">.</span>add<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm_scale_bias<span class="token punctuation">(</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>观察上面的forward函数可以发现，TReLU添加bias这一习得参数，而weight-normalized layer中则对传入的weight进行了标准化。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
            <tag> GAN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Large-Scale Evolution of Image Classifiers]]></title>
      <url>http://blog.ddlee.cn/2017/06/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Large-Scale-Evolution-of-Image-Classifiers/</url>
      <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">Large-Scale Evolution of Image, Classifiers</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>深层网络在图片分类问题上表现优异，但网络结构的设计上并没有统一的指导。进化是构建深度网络架构的一种方式。利用本文的自动化方法得出的深度网络结构，已经能在CIFAR-10上取得可以跟人工设计的网络相媲美的结果</p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Evolution-Algorithm"><a href="#Evolution-Algorithm" class="headerlink" title="Evolution Algorithm"></a>Evolution Algorithm</h3><p>整个算法的核心是如下的tournament selection:</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/tournament.jpg" alt="tournament"></p>
<ul>
<li>population: 供筛选的群体</li>
<li>individual: 个体，带有指标fitness，特别地，指在CV集上的损失</li>
<li>worker: 筛选者，上帝</li>
</ul>
<ol>
<li><em>population</em> 中的 <em>individual</em> 均已在训练集上训练完毕，带有指标 <em>fitness</em></li>
<li><em>worker</em> 随机选择一对 <em>individual</em>，比较 <em>fitness</em>，较差的 <em>individual</em> 被舍弃</li>
<li>表现较好的 <em>individual</em> 成为parent，对其施加 <em>mutation</em> (变异)，得到 <em>child</em></li>
<li>训练 <em>child</em> 并在CV集上得到其 <em>fitness</em>，归还到 <em>population</em> 中</li>
</ol>
<h3 id="Encoding-and-Mutation"><a href="#Encoding-and-Mutation" class="headerlink" title="Encoding and Mutation"></a>Encoding and Mutation</h3><p>个体的网络结构和部分参数被编码为DNA。</p>
<p>能够施加的变异有：</p>
<ul>
<li>改变学习率</li>
<li>恒等（不变）</li>
<li>重设参数</li>
<li>加入卷积层</li>
<li>移除卷积层</li>
<li>更改卷积层的stride参数</li>
<li>更改卷积层的Channel参数</li>
<li>更改卷积核大小</li>
<li>加入skip连接（类似ResNet)</li>
<li>移除skip连接</li>
</ul>
<h3 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h3><p>计算方面采用了并行、异步、无锁的策略。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/comp.jpg" alt="comp"></p>
<p>建立约为 <em>population</em> 数1/4的 <em>worker</em>，分别运行于不同的机器上，之间独立异步。<em>population</em> 共享，若两个 <em>worker</em> 在一个 <em>individual</em> 上产生冲突，则后一个 <em>worker</em> 停止并等待再次尝试。</p>
<h3 id="Weight-Inheritance"><a href="#Weight-Inheritance" class="headerlink" title="Weight Inheritance"></a>Weight Inheritance</h3><p>除了架构之外，子模型还会继承父母模型未经变异影响的隐藏层参数（不仅是DNA中的），这样使子模型的训练时间大幅减小。</p>
<h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><p>文章的主要结果如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/results.png" alt="results"></p>
<p>最右边的结构是在CIFAR-10上发现的最好（CV集准确度最高）的结构，左边两个是它的祖先。其中白色块相当于简单的线性层，彩色块则带有非线性激活，可以看到，不同于人工设计的网络，某一线性层之后可能包含多个非线性层。</p>
<p>另外，利用本文的模型，也在CIFAR-100上做了实验，可以达到76.3%的准确率，一定程度上说明了算法的扩展性。</p>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/popu.png" alt="popu"></p>
<p>上图说明随着 <em>population</em> 规模和训练步数的增加，模型的整体水平在变好。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/mutation.png" alt="mutation"></p>
<p>在模型陷入局部最优值时，提高变异率和重设参数会使群体继续进化。这是由于变异中包含恒等映射等不改变模型架构的变异类型，再加上weight Inheritance，一些子模型只是训练次数比其他模型多很多的“活化石”。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Google I/O时就提到了自动筛选最优网络结构，但没有公布论文。但将网络结构自动化，必定是未来的方向。个人认为，ResNet就相当于自动化网络深度（一些层实际上被跳过了），而Inception单元似乎包含了太多的先验，而且也没有逻辑上的证据说明这样的结构更有效。网络结构本身就是先验信息，而要达到通用的人工智能，这些先验也必须由模型自行发觉。</p>
<p>强化学习本身也是一个进化过程，应该也有相关的工作将强化学习的框架应用于网络结构的学习上。</p>
<p>更进一步地，若数据是一阶信息，深度网络的隐藏层学到的表示是二阶信息，深度网络的结构则是三阶信息，从一阶到二阶的框架是不是都可以移植到二阶到三阶上来？关键之处在于我们还没有描述好深度网络的结构空间，但就现在的发展看，深度网络的一些基本结构(conv, BN)等，已经被作为基本单元（离散的）来进行构建和筛选了，也就是说，所有深度网络构成的空间之性质如何，还有大量的工作可以做。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
            <tag> autoML </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]An Analysis of Deep Neural Network Models for Practical Applications]]></title>
      <url>http://blog.ddlee.cn/2017/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/</url>
      <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1605.07678" target="_blank" rel="external">An Analysis of Deep Neural Network Models for Practical Applications</a></p>
<p>本文是对现有（论文发表于2016年5月）深度网络的比较，从以下方面入手：</p>
<ul>
<li>accuracy</li>
<li>memory footprint</li>
<li>parameters</li>
<li>operations count</li>
<li>inference time</li>
<li>power consumption</li>
</ul>
<p>以下图片各模型的着色是统一的：蓝色是Inception系，绿色是VGG系，粉色是ResNet系，黄色为AlexNet系。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/top1.png" alt="top1"></p>
<p>上图是Top1准确率与模型参数数、操作数的关系。可以看到Inception系列网络以较少的参数取得相对高的准确率，而VGG系则在这一点上表现很差。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/infer-batch.png" alt="infer-batch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/power-batch.png" alt="power-batch"></p>
<p>上面两图分别是推断耗时和电量消耗与批量大小的关系。可以看到，两者均与批量大小无明显的相关关系。但电量消耗在不同的模型之间也非常类似，而推断时间与模型结构关系很大（VGG再次尴尬）。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/mem-batch.png" alt="mem-batch"></p>
<p>上图展示了模型占用内存大小与批量大小的关系，大部分网络都有相对固定的内存占用，随后随批量大小的上扬而上涨。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-infer.png" alt="infer-ops"></p>
<p>从上图可以发现推断耗时和模型的操作数大体上呈现线性关系。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-power.png" alt="ops-power"></p>
<p>电量消耗与模型的参数数、操作数并没有明显的相关性。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/accuracy-infer.png" alt="accuracy-infer"></p>
<p>注意，上图中点的大小代表模型操作数，横轴代表推断效率，纵轴表示准确率。灰色区域表示模型获得了额外的推断效率或准确率，而白色区域代表非最优。</p>
<p>操作数越多的模型推断效率越低，大部分模型都落在相对平衡的边界上，VGG和小批量情形下的AlexNet落在了非最优区域。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从这篇论文的比较中可以看到，在特定的任务中对网络特定结构的设计（如Inception单元），即加入更强的先验知识，比堆叠网络层数更有效。深度网络还是需要人类的指导才能发挥更大的作用。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Papers </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习和分布式表示]]></title>
      <url>http://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/</url>
      <content type="html"><![CDATA[<p>本文的两个主要参考资料：</p>
<ol>
<li>Yoshua Bengio在2016年九月<a href="https://www.bayareadlschool.org/" target="_blank" rel="external">Deep Learning School</a>的演讲Foundations and Challenges of Deep Learning。<a href="https://www.youtube.com/watch?v=11rsu_WwZTc" target="_blank" rel="external">YouTube</a></li>
<li><a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning</a>, Goodfellow et al, Section 15.4</li>
</ol>
<h3 id="从机器学习到人工智能"><a href="#从机器学习到人工智能" class="headerlink" title="从机器学习到人工智能"></a>从机器学习到人工智能</h3><p>在演讲中，Bengio提到从机器学习到人工智能有五个关键的飞跃：</p>
<ol>
<li>Lots of data</li>
<li>Very flexible models</li>
<li>Enough computing power</li>
<li>Powerful priors that can defeat the curse of dimensionality</li>
<li>Computationally efficient inference</li>
</ol>
<p>第一点已经发生，到处都提大数据，到处都在招数据分析师。<br>我在读高中时，就曾预感数据将是新时代的石油和煤炭，因为数据正是人类社会经验的总结，数据带来的知识和见解将在驱动社会进步中发挥越来越重要的作用，而自己要立志成为新时代的矿工。</p>
<p>第二点在我看来有两个例子，一是核技巧，通过核函数对分布空间的转换，赋予了模型更强大的表述能力；二是深度神经网络，多层的框架和非线性的引入使得模型理论上可以拟合任意函数。</p>
<p>第三点，借云计算的浪潮，计算力不再是一项资产而是一项可供消费的服务，我们学生也可以廉价地接触到根本负担不起的计算力资源。而GPU等芯片技术的进步也为AI的浩浩征程添砖加瓦。</p>
<p>第五点，近期发布的Tensorflow Lite和Caffe2等工具也有助于越来越多地将计算任务分配在终端上进行，而非作为一个发送与接收器。</p>
<p>最后第四点，也是这篇文章的中心话题：借助分布式表示的强大能力，深度学习正尝试解决维度带来的灾难。</p>
<h3 id="没有免费的午餐"><a href="#没有免费的午餐" class="headerlink" title="没有免费的午餐"></a>没有免费的午餐</h3><p>简单说，没有免费的午餐定理指出找不到一个在任何问题上都表现最优的模型/算法。不同的模型都有其擅长的问题，这由该模型建立时引入的先验知识决定。</p>
<p>那么，深度学习加入的先验知识是什么？</p>
<p>Bengio用的词是Compositionality，即复合性，<em>某一概念之意义由其组成部分的意义以及组合规则决定</em>。复合性的原则可以用于高效地描述我们的世界，而深度学习模型中隐藏的层正是去学习其组成部分，网络的结构则代表了组合规则。这正是深度学习模型潜在的信念。</p>
<h3 id="分布式表示带来的指数增益"><a href="#分布式表示带来的指数增益" class="headerlink" title="分布式表示带来的指数增益"></a>分布式表示带来的指数增益</h3><p>分布式表示(Distributed Representation)是连接主义的核心概念，与复合性的原理相合。整体由组成它的个体及其组合来表示。请看下面的例子：</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/distributed.webp" alt="Distributed"></p>
<p>描述一个形状，我们将其分解为不同的特征来表述。分布式表示是一种解耦，它试图复杂的概念分离成独立的部分。而这也引出了分布式表示带来的缺点：隐藏层学到的分解特征难以得到显式的解释。</p>
<p>传统的机器学习算法，如K-Means聚类、决策树等，大多使用的是非分布式表示，即用特定的参数去描述特定的区域。如K-Means聚类，我们要划分多少区域，就需要有多少个中心点。因而，这类算法的特点是，随着参数个数的提升，其能描述的概念线性增长。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/non-dist.png" alt="non-dist"><br>使用分布式表示的深度网络，则可以享受到指数级的增益，即，随着参数个数的提升，其表述能力是指数级的增长。具有$k$个值的$n$个特征，可以描述${k}^{n}$个不同的概念。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/dist.png" alt="dist"></p>
<h3 id="分布式表示在泛化上的优势"><a href="#分布式表示在泛化上的优势" class="headerlink" title="分布式表示在泛化上的优势"></a>分布式表示在泛化上的优势</h3><p>分布式的想法还可以得到额外的泛化优势。通过重新组合在原有数据中抽离出来的特征，可以表示得到原有数据中不存在的实例。在Radford et al.的工作中，生成模型区习得了性别，并能从“戴眼镜的男人”-“男人”+“女人”=“戴眼镜的女人”这样的抽象概念表达式中生成实例。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/generative.png" alt="generative"></p>
<h3 id="分布式表示与巻积神经网络"><a href="#分布式表示与巻积神经网络" class="headerlink" title="分布式表示与巻积神经网络"></a>分布式表示与巻积神经网络</h3><p>巻积神经网络不同的滤波器习得的特征可以为分布式表示的概念分解这一特性提供一些例子。下图是VGG16不同滤波器得到结果的可视化表示，<br>出自Francois Chollet的博文<a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" target="_blank" rel="external">How convolutional neural networks see the world</a></p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/filters.jpg" alt="filters"></p>
<p>可以看到，浅层的滤波器学到的是简单的颜色、线条走向等特征，较深的滤波器学到复杂的纹理。</p>
<h3 id="量子计算机与分布式表示"><a href="#量子计算机与分布式表示" class="headerlink" title="量子计算机与分布式表示"></a>量子计算机与分布式表示</h3><p>在我看来，量子计算机的激动人心之处也在于其表示能力。一个量子态可以表示原先两个静态表示的信息，原先需要8个单位静态存储表示的信息只需要3个量子态单位即可表示，这也是指数级的增益。在这一点上，计算模型和概念模型已然殊途同归。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从经验中总结原则，用原则生成套路，正是我们自己处理和解决新问题的途径。通过解耦得到的信息来消除未知和不确定性，是我们智能的一部分。我们眼中的世界，只是适合我们的一种表示而已。也许，真正的人工智能到来那一刻，会是我们创造的机器“理解”了自己的表示系统之时——我们所关注的可解释性，也就无关紧要了。</p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]On-the-fly Operation Batching in Dynamic Computation Graphs]]></title>
      <url>http://blog.ddlee.cn/2017/05/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/</url>
      <content type="html"><![CDATA[<p>论文：<a href="http://arxiv.org/abs/1705.07860" target="_blank" rel="external">On-the-fly Operation Batching in Dynamic Computaion Graphs</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>基于动态图的深度学习框架如<code>Pytorch</code>,<code>DyNet</code>提供了更为灵活的结构和数据维度的选择，但要求开发者自行将数据批量化，才能最大限度地发挥框架的并行计算优势。</p>
<h2 id="当前的状况：灵活的结构与高效计算"><a href="#当前的状况：灵活的结构与高效计算" class="headerlink" title="当前的状况：灵活的结构与高效计算"></a>当前的状况：灵活的结构与高效计算</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/comparison.png" alt="左图为循环结构，右图将序列补齐，批量化"><br>左图为循环结构，右图将序列补齐，批量化</p>
<ol>
<li>灵活的结构和数据输入维度，采用朴素的循环结构实现，但不高效，因为尽管维度不同，在循环内数据接受的是同样的操作。</li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/loop.png" alt="loop"></p>
<ol>
<li>对数据做“Padding”，即用傀儡数据将输入维度对齐，进而实现向量化，但这种操作对开发者并不友好，会使开发者浪费掉很多本该投入到结构设计等方面的精力。</li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/padding.png" alt="padding"></p>
<h2 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h2><h3 id="三个部分"><a href="#三个部分" class="headerlink" title="三个部分"></a>三个部分</h3><ol>
<li>Graph Definition</li>
<li>Operation Batching</li>
<li>Computation</li>
</ol>
<p>第一步和第三步在当前已被大部分深度学习框架较好地实现。主要特点是，构建计算图与计算的分离，即”Lazy Evaluation”。比如在<code>Tensorflow</code>中，一个抽象层负责解析计算图各节点之间的依赖，决定执行计算的顺序，而另一个抽象层则负责分配计算资源。</p>
<h3 id="Operation-Batching"><a href="#Operation-Batching" class="headerlink" title="Operation Batching"></a>Operation Batching</h3><h4 id="Computing-compatibility-groups"><a href="#Computing-compatibility-groups" class="headerlink" title="Computing compatibility groups"></a>Computing compatibility groups</h4><p>这一步是建立可以批量化计算的节点组。具体做法是，给每一个计算节点建立 <em>signature</em>，用于描述节点计算的特性，文中举出了如下几个例子:</p>
<ol>
<li>Component-wise operations: 直接施加在每个张量元素上的计算，跟张量的维度无关，如$tanh$,$log$</li>
<li>Dimension-sensitive operations: 基于维度的计算，如线性传递$Wh+b$，要求$W$和$h$维度相符，<em>signature</em> 中要包含维度信息</li>
<li>Operations with shared elements: 包含共享元素的计算，如共享的权值$W$</li>
<li>Unbatchable operations: 其他</li>
</ol>
<h4 id="Determining-execution-order"><a href="#Determining-execution-order" class="headerlink" title="Determining execution order"></a>Determining execution order</h4><p>执行顺序要满足两个目标：</p>
<ol>
<li>每一节点的计算要在其依赖之后</li>
<li>带有同样 <em>signature</em> 且没有依赖关系的节点放在同一批量执行</li>
</ol>
<p>但在一般情况下找到最大化批量规模的执行顺序是个NP问题。有如下两种策略：</p>
<ol>
<li>Depth-based Batching: 库<code>Tensorflow Fold</code>中使用的方法。某一节点的深度定义为其子节点到其本身的最大长度，同一深度的节点进行批量计算。但由于输入序列长度不一，可能会错失一些批量化的机会。</li>
<li>Agenda-based Batching: 本文的方法，核心的想法是维护一个 <em>agenda</em> 序列，所有依赖已经被解析的节点入列，每次迭代时从 <em>agenda</em> 序列中按 <em>signature</em> 相同的原则取出节点进行批量计算。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章选取了四个模型：BiLSTM, BiLSTM w/char, Tree-structured LSTMs, Transition-based Dependency Parsing。</p>
<p>实验结果：（单位为Sentences/second）<br><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/result.png" alt="result"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本来读到题目还是蛮惊喜的，期待的是从模型构建的角度解决序列长度不一带来的计算上的不便。但通读下来发现是在计算图的计算这一层面进行的优化，有些失望但也感激，作者使用<code>DyNet</code>框架实现了他们的方法，希望自己也可以为<code>Pytorch</code>等框架该算法的实现出一份力。</p>
<p>感谢这些开源的框架，正一步步拉近人类构建模型和机器高效计算之间的距离。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Paper </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[LSTM:Pytorch实现]]></title>
      <url>http://blog.ddlee.cn/2017/05/29/LSTM-Pytorch%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p>本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM是一种循环神经网络，适用于对序列化的输入建模。Chris Olah的这篇<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">文章</a>细致地解释了一个LSTM单元的运作方式，建议阅读。</p>
<h3 id="两个想法"><a href="#两个想法" class="headerlink" title="两个想法"></a>两个想法</h3><h4 id="Gate：信息流动的闸门"><a href="#Gate：信息流动的闸门" class="headerlink" title="Gate：信息流动的闸门"></a>Gate：信息流动的闸门</h4><p>$$i<em>t = sigmoid(W</em>{xi} x<em>t  +  W</em>{hi}h_{t-1} + b_i)$$<br>$$f<em>t = sigmoid(W</em>{xf} x<em>t  +  W</em>{hf}h_{t-1} + b_f)$$<br>$$o<em>t = sigmoid(W</em>{xo} x<em>t  +  W</em>{ho}h_{t-1} + b_o)$$<br>$x$ 表示输入，$h$表示隐藏状态，用$sigmoid$函数将输入二者的传递结果映射到$（0,1)$上，分别赋予输入门、遗忘门、输出门的含义，来控制不同神经单元（同一神经元不同时间点的状态）之间信息流动。</p>
<h4 id="Cell：记忆池"><a href="#Cell：记忆池" class="headerlink" title="Cell：记忆池"></a>Cell：记忆池</h4><p>$$c_t = f<em>t \odot c</em>{t - 1} + i<em>t \odot tanh(W</em>{xc} x<em>t  +  W</em>{hc}h_{t-1} + b_c)\<br>h_t = o_t \odot tanh(c_t)$$<br>$h$表示隐藏状态，$C$表示记忆池，通过Gate，上一单元（状态）的信息有控制地遗忘，当前的输入有控制地流入，记忆池中的信息有控制地流入隐藏状态。</p>
<h3 id="与普通RNN的对比"><a href="#与普通RNN的对比" class="headerlink" title="与普通RNN的对比"></a>与普通RNN的对比</h3><p><img src="http://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM3-SimpleRNN.png" alt="RNN"><br>普通RNN只有一个自更新的隐藏状态单元。</p>
<p><img src="http://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM.jpg" alt="LSTM"><br>LSTM增加了记忆池Cell，并通过几个Gate将信息有控制地更新在记忆池中，并通过记忆池中的信息来决定隐藏状态。</p>
<h2 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h2><p>下面是手动实现LSTM的代码，继承了基类<code>nn.Module</code>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable

<span class="token keyword">class</span> <span class="token class-name">LSTM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> cell_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>LSTM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>cell_size <span class="token operator">=</span> cell_size
        self<span class="token punctuation">.</span>gate <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size <span class="token operator">+</span> hidden_size<span class="token punctuation">,</span> cell_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh <span class="token operator">=</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span><span class="token punctuation">:</span>
        combined <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        f_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        i_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        o_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        f_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>f_gate<span class="token punctuation">)</span>
        i_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>i_gate<span class="token punctuation">)</span>
        o_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>o_gate<span class="token punctuation">)</span>
        cell_helper <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        cell_helper <span class="token operator">=</span> self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cell_helper<span class="token punctuation">)</span>
        cell <span class="token operator">=</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> f_gate<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>cell_helper<span class="token punctuation">,</span> i_gate<span class="token punctuation">)</span><span class="token punctuation">)</span>
        hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">,</span> o_gate<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> cell

    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">initCell</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>cell_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>几个关键点：</p>
<ol>
<li>Tensor的大小</li>
<li>信息的传递顺序</li>
</ol>
<h2 id="Pytorch-Module"><a href="#Pytorch-Module" class="headerlink" title="Pytorch Module"></a>Pytorch Module</h2><p>Pytorch库本身对LSTM的实现封装了更多功能，类和函数的组织也非常有借鉴意义。我对其实现的理解基于以下两点展开：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p>下面开始按图索骥，源码见<a href="https://github.com/pytorch/pytorch/tree/master/torch" target="_blank" rel="external">GitHub</a>。</p>
<h4 id="LSTM类"><a href="#LSTM类" class="headerlink" title="LSTM类"></a>LSTM类</h4><p>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" target="_blank" rel="external">nn/modules/rnn.py</a></p>
<pre class="line-numbers language-Python"><code class="language-Python"># nn/modules/rnn.py
class RNNBase(Module):
  def __init__(self, mode, input_size, output_size):
      pass
  def forward(self, input, hx=None):
      if hx is None:
          hx = torch.autograd.Variable()
      if self.mode == 'LSTM':
          hx = (hx, hx)
      func = self._backend.RNN() #!!!
      output, hidden = func(input, self.all_weights, hx) #!!!
      return output, hidden

class LSTM(RNNBase):
    def __init__(self, *args, **kwargs):
        super(LSTM, self).__init__('LSTM', *args, **kwargs)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>LSTM</code>类只是<code>RNNBase</code>类的一个装饰器。</li>
<li>在基类<code>nn.Module</code>中，把<code>__call__()</code>定义为调用<code>forward()</code>方法，因而真正的功能实现在<code>_backend.RNN()</code>中</li>
</ol>
<h4 id="AutogradRNN函数"><a href="#AutogradRNN函数" class="headerlink" title="AutogradRNN函数"></a>AutogradRNN函数</h4><p>下面寻找<code>_backend.RNN</code>。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/backends/thnn.py" target="_blank" rel="external">nn/backends/thnn.py</a></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn/backends/thnn.py</span>
<span class="token keyword">def</span> <span class="token function">_initialize_backend</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>_functions<span class="token punctuation">.</span>rnn <span class="token keyword">import</span> RNN<span class="token punctuation">,</span> LSTMCell
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>原来，<code>_backend</code>也是索引。</p>
<p>终于找到<code>RNN()</code>函数。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py" target="_blank" rel="external">nn/_functions/rnn.py</a></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn/_functions/rnn.py</span>
<span class="token keyword">def</span> <span class="token function">RNN</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> <span class="token operator">*</span>fargs<span class="token punctuation">,</span> <span class="token operator">**</span>fkwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        func <span class="token operator">=</span> AutogradRNN<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> func<span class="token punctuation">(</span>input<span class="token punctuation">,</span> <span class="token operator">*</span>fargs<span class="token punctuation">,</span> <span class="token operator">**</span>fkwargs<span class="token punctuation">)</span>
    <span class="token keyword">return</span> forward

<span class="token keyword">def</span> <span class="token function">AutogradRNN</span><span class="token punctuation">(</span>mode<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cell <span class="token operator">=</span> LSTMCell
    rec_factory <span class="token operator">=</span> Recurrent
    layer <span class="token operator">=</span> <span class="token punctuation">(</span>rec_factory<span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    func <span class="token operator">=</span> StackedRNN<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> num_layers<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nexth<span class="token punctuation">,</span> output <span class="token operator">=</span> func<span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> weight<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> nexth
    <span class="token keyword">return</span> forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>RNN()</code>是一个装饰器，根据是否有<code>cudnn</code>库决定调用<code>AutogradRNN()</code>还是<code>CudnnRNN()</code>，这里仅观察<code>AutogradRNN()</code></li>
<li><code>AutogradRNN()</code>选用了<code>LSTMCell</code>，用<code>Recurrent()</code>函数处理了<code>Cell</code>构成<code>Layer</code>，再将<code>Layer</code>传入<code>StackedRNN()</code>函数</li>
<li><code>RNN()</code>和<code>AutogradRNN()</code>返回的都是其<code>forward()</code>函数句柄</li>
</ol>
<p>下面是<code>Recurrent()</code>函数：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Recurrent</span><span class="token punctuation">(</span>inner<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> weight<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        steps <span class="token operator">=</span> range<span class="token punctuation">(</span>input<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> steps<span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> inner<span class="token punctuation">(</span>input<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> <span class="token operator">*</span>weight<span class="token punctuation">)</span>
            output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> hidden<span class="token punctuation">,</span> output
    <span class="token keyword">return</span> forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>Recurrent()</code>函数实现了“递归”的结构，根据输入的大小组合<code>Cell</code>，完成了隐藏状态和参数的迭代。</li>
<li><code>Recurrent()</code>函数将<code>Cell(inner)</code>组合为<code>Layer</code>。</li>
</ol>
<h4 id="StackedRNN-函数"><a href="#StackedRNN-函数" class="headerlink" title="StackedRNN()函数"></a>StackedRNN()函数</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">StackedRNN</span><span class="token punctuation">(</span>inners<span class="token punctuation">,</span> num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_directions <span class="token operator">=</span> len<span class="token punctuation">(</span>inners<span class="token punctuation">)</span>
    total_layers <span class="token operator">=</span> num_layers <span class="token operator">*</span> num_directions
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> weight<span class="token punctuation">)</span><span class="token punctuation">:</span>
        next_hidden <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        hidden <span class="token operator">=</span> list<span class="token punctuation">(</span>zip<span class="token punctuation">(</span><span class="token operator">*</span>hidden<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
          all_output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
          <span class="token keyword">for</span> j<span class="token punctuation">,</span> inner <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>inners<span class="token punctuation">)</span><span class="token punctuation">:</span>
              hy<span class="token punctuation">,</span> output <span class="token operator">=</span> inner<span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> weight<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">)</span>
              next_hidden<span class="token punctuation">.</span>append<span class="token punctuation">(</span>hy<span class="token punctuation">)</span>
              all_output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
          input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>all_output<span class="token punctuation">,</span> input<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
        next_h<span class="token punctuation">,</span> next_c <span class="token operator">=</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>next_hidden<span class="token punctuation">)</span>
        next_hidden <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>next_h<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>total_layers<span class="token punctuation">,</span> <span class="token operator">*</span>next_h<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>next_c<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>total_layers<span class="token punctuation">,</span> <span class="token operator">*</span>next_c<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> next_hidden<span class="token punctuation">,</span> input
    <span class="token keyword">return</span> forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>StackedRNN()</code>函数将<code>Layer(inner)</code>组合为栈</li>
</ol>
<p>最后的最后，一个基本的LSTM单元内的计算由<code>LSTMCell()</code>函数实现。</p>
<h4 id="LSTMCell-函数"><a href="#LSTMCell-函数" class="headerlink" title="LSTMCell()函数"></a>LSTMCell()函数</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">LSTMCell</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> w_ih<span class="token punctuation">,</span> w_hh<span class="token punctuation">,</span> b_ih<span class="token operator">=</span>None<span class="token punctuation">,</span> b_hh<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> input<span class="token punctuation">.</span>is_cuda<span class="token punctuation">:</span>
        igates <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> w_ih<span class="token punctuation">)</span>
        hgates <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w_hh<span class="token punctuation">)</span>
        state <span class="token operator">=</span> fusedBackend<span class="token punctuation">.</span>LSTMFused<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> state<span class="token punctuation">(</span>igates<span class="token punctuation">,</span> hgates<span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span> b_ih <span class="token keyword">is</span> None <span class="token keyword">else</span> state<span class="token punctuation">(</span>igates<span class="token punctuation">,</span> hgates<span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> b_ih<span class="token punctuation">,</span> b_hh<span class="token punctuation">)</span>

    hx<span class="token punctuation">,</span> cx <span class="token operator">=</span> hidden
    gates <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> w_ih<span class="token punctuation">,</span> b_ih<span class="token punctuation">)</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hx<span class="token punctuation">,</span> w_hh<span class="token punctuation">,</span> b_hh<span class="token punctuation">)</span>

    ingate<span class="token punctuation">,</span> forgetgate<span class="token punctuation">,</span> cellgate<span class="token punctuation">,</span> outgate <span class="token operator">=</span> gates<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    ingate <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>ingate<span class="token punctuation">)</span>
    forgetgate <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>forgetgate<span class="token punctuation">)</span>
    cellgate <span class="token operator">=</span> F<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cellgate<span class="token punctuation">)</span>
    outgate <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>outgate<span class="token punctuation">)</span>

    cy <span class="token operator">=</span> <span class="token punctuation">(</span>forgetgate <span class="token operator">*</span> cx<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>ingate <span class="token operator">*</span> cellgate<span class="token punctuation">)</span>
    hy <span class="token operator">=</span> outgate <span class="token operator">*</span> F<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cy<span class="token punctuation">)</span>

    <span class="token keyword">return</span> hy<span class="token punctuation">,</span> cy
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>观察上面的代码，即是LSTM的基本信息传递公式。至此，我们的旅程完成。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><blockquote>
<p>没有什么是增加一层抽象不能解决的，如果不能，那就再加一层。</p>
</blockquote>
<p>重复一下我对上述代码的理解：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/LSTM-Pytorch实现/str.jpg" alt="str"></p>
<p>如洋葱一般，我们剥到最后，发现处理的信息正是输入、隐藏状态和LSTM单元几个控制门的参数。在一层一层的抽象之中，Pytorch在不同的层面处理了不同的参数，保证了扩展性和抽象层之间的解耦。</p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Pandas速度优化]]></title>
      <url>http://blog.ddlee.cn/2017/05/28/Pandas%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<p>本文主要内容取自Sofia Heisler在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=HN5d490_KKk" target="_blank" rel="external">No More Sad Pandas Optimizing Pandas Code for Speed and Efficiency</a>，讲稿代码和幻灯片见<a href="https://github.com/sversh/pycon2017-optimizing-pandas" target="_blank" rel="external">GitHub</a>。</p>
<h2 id="Set-Up"><a href="#Set-Up" class="headerlink" title="Set Up"></a>Set Up</h2><h4 id="示例数据"><a href="#示例数据" class="headerlink" title="示例数据"></a>示例数据</h4><table>
<thead>
<tr>
<th></th>
<th>ean_hotel_id</th>
<th>name</th>
<th>address1</th>
<th>city</th>
<th>state_province</th>
<th>postal_code</th>
<th>latitude</th>
<th>longitude</th>
<th>star_rating</th>
<th>high_rate</th>
<th>low_rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>269955</td>
<td>Hilton Garden Inn Albany/SUNY Area</td>
<td>1389 Washington Ave</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68751</td>
<td>-73.81643</td>
<td>3.0</td>
<td>154.0272</td>
<td>124.0216</td>
</tr>
<tr>
<td>1</td>
<td>113431</td>
<td>Courtyard by Marriott Albany Thruway</td>
<td>1455 Washington Avenue</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68971</td>
<td>-73.82021</td>
<td>3.0</td>
<td>179.0100</td>
<td>134.0000</td>
</tr>
<tr>
<td>2</td>
<td>108151</td>
<td>Radisson Hotel Albany</td>
<td>205 Wolf Rd</td>
<td>Albany</td>
<td>NY</td>
<td>12205</td>
<td>42.72410</td>
<td>-73.79822</td>
<td>3.0</td>
<td>134.1700</td>
<td>84.1600</td>
</tr>
</tbody>
</table>
<h4 id="示例函数：Haversine-Distance"><a href="#示例函数：Haversine-Distance" class="headerlink" title="示例函数：Haversine Distance"></a>示例函数：Haversine Distance</h4><pre class="line-numbers language-Python"><code class="language-Python">def haversine(lat1, lon1, lat2, lon2):
    miles_constant = 3959
    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    mi = miles_constant * c
    return mi
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="优化它之前，先测量它"><a href="#优化它之前，先测量它" class="headerlink" title="优化它之前，先测量它"></a>优化它之前，先测量它</h2><h4 id="IPython-Notebook的Magic-Command-timeit"><a href="#IPython-Notebook的Magic-Command-timeit" class="headerlink" title="IPython Notebook的Magic Command: %timeit"></a>IPython Notebook的Magic Command: <code>%timeit</code></h4><p>既可以测量某一行代码的执行时间，又可以测量整个单元格里代码快的执行时间。</p>
<h4 id="Package-line-profiler"><a href="#Package-line-profiler" class="headerlink" title="Package: line_profiler"></a>Package: line_profiler</h4><p>记录每行代码的执行次数和执行时间。</p>
<p>在IPython Notebook中使用时，先运行<code>%load_ext line_profiler</code>， 之后可以用<code>%lprun -f [function name]</code>命令记录指定函数的执行情况。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h4 id="对行做循环-Baseline"><a href="#对行做循环-Baseline" class="headerlink" title="对行做循环(Baseline)"></a>对行做循环(Baseline)</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span><span class="token operator">%</span>timeit
haversine_series <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> index<span class="token punctuation">,</span> row <span class="token keyword">in</span> df<span class="token punctuation">.</span>iterrows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    haversine_series<span class="token punctuation">.</span>append<span class="token punctuation">(</span>haversine<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                                      row<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
df<span class="token punctuation">[</span><span class="token string">'distance'</span><span class="token punctuation">]</span> <span class="token operator">=</span> haversine_series
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>197 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><h4 id="pd-DataFrame-apply-方法"><a href="#pd-DataFrame-apply-方法" class="headerlink" title="pd.DataFrame.apply()方法"></a>pd.DataFrame.apply()方法</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>lprun <span class="token operator">-</span>f haversine \
df<span class="token punctuation">.</span>apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> row<span class="token punctuation">:</span> haversine<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                               row<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>90.6 ms ± 7.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.049982 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2      1631         1535      0.9      3.1      miles_constant = 3959
     3      1631        16602     10.2     33.2      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4      1631         2019      1.2      4.0      dlat = lat2 - lat1
     5      1631         1143      0.7      2.3      dlon = lon2 - lon1
     6      1631        18128     11.1     36.3      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7      1631         7857      4.8     15.7      c = 2 * np.arcsin(np.sqrt(a))
     8      1631         1708      1.0      3.4      mi = miles_constant * c
     9      1631          990      0.6      2.0      return mi
</code></pre><p>观察Hits这一列可以看到，<code>apply()</code>方法还是将函数一行行地应用于每行。</p>
<h4 id="向量化：将pd-Series传入函数"><a href="#向量化：将pd-Series传入函数" class="headerlink" title="向量化：将pd.Series传入函数"></a>向量化：将pd.Series传入函数</h4><pre><code>%lprun -f haversine haversine(40.671, -73.985,\
                              df[&#39;latitude&#39;], df[&#39;longitude&#39;])
</code></pre><p>Output:</p>
<pre><code>2.21 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.008601 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.0      miles_constant = 3959
     3         1          838    838.0      9.7      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1          597    597.0      6.9      dlat = lat2 - lat1
     5         1          572    572.0      6.7      dlon = lon2 - lon1
     6         1         5033   5033.0     58.5      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1         1060   1060.0     12.3      c = 2 * np.arcsin(np.sqrt(a))
     8         1          496    496.0      5.8      mi = miles_constant * c
     9         1            2      2.0      0.0      return mi
</code></pre><p>向量化之后，函数内的每行操作只被访问一次，达到了行结构上的并行。</p>
<h3 id="向量化：将np-array传入函数"><a href="#向量化：将np-array传入函数" class="headerlink" title="向量化：将np.array传入函数"></a>向量化：将np.array传入函数</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>lprun <span class="token operator">-</span>f haversine df<span class="token punctuation">[</span><span class="token string">'distance'</span><span class="token punctuation">]</span> <span class="token operator">=</span> haversine<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                        df<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>Output：</p>
<pre><code>370 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.001382 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.2      miles_constant = 3959
     3         1          292    292.0     21.1      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1           40     40.0      2.9      dlat = lat2 - lat1
     5         1           29     29.0      2.1      dlon = lon2 - lon1
     6         1          815    815.0     59.0      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1          183    183.0     13.2      c = 2 * np.arcsin(np.sqrt(a))
     8         1           18     18.0      1.3      mi = miles_constant * c
     9         1            2      2.0      0.1      return mi
</code></pre><p>相比<code>pd.Series</code>，<code>np.array</code>不含索引等额外信息，因而更加高效。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><table>
<thead>
<tr>
<th>Methodology</th>
<th>Avg.    single    run    time</th>
<th>Marginal    performance    improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Looping    with    iterrows</td>
<td>184.00</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Looping    with    apply</td>
<td>78.10</td>
<td>2.4x</td>
</tr>
<tr>
<td>Vectorization    with    Pandas    series</td>
<td>1.79</td>
<td>43.6x</td>
</tr>
<tr>
<td>Vectorization    with    NumPy    arrays</td>
<td>0.37</td>
<td>4.8x</td>
</tr>
</tbody>
</table>
<p>通过上面的对比，我们比最初的baseline快了近500倍。最大的提升来自于向量化。因而，实现的函数能够很方便地向量化是高效处理的关键。</p>
<h2 id="用Cython优化"><a href="#用Cython优化" class="headerlink" title="用Cython优化"></a>用<code>Cython</code>优化</h2><p><code>Cython</code>可以将<code>python</code>代码转化为<code>C</code>代码来执行，可以进行如下优化（静态化变量类型，调用C函数库）</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>load_ext cython

<span class="token operator">%</span><span class="token operator">%</span>cython <span class="token operator">-</span>a
<span class="token comment" spellcheck="true"># Haversine cythonized</span>
<span class="token keyword">from</span> libc<span class="token punctuation">.</span>math cimport sin<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> acos<span class="token punctuation">,</span> asin<span class="token punctuation">,</span> sqrt

cdef deg2rad_cy<span class="token punctuation">(</span>float deg<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cdef float rad
    rad <span class="token operator">=</span> <span class="token number">0.01745329252</span><span class="token operator">*</span>deg
    <span class="token keyword">return</span> rad

cpdef haversine_cy_dtyped<span class="token punctuation">(</span>float lat1<span class="token punctuation">,</span> float lon1<span class="token punctuation">,</span> float lat2<span class="token punctuation">,</span> float lon2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cdef<span class="token punctuation">:</span>
        float dlon
        float dlat
        float a
        float c
        float mi

    lat1<span class="token punctuation">,</span> lon1<span class="token punctuation">,</span> lat2<span class="token punctuation">,</span> lon2 <span class="token operator">=</span> map<span class="token punctuation">(</span>deg2rad_cy<span class="token punctuation">,</span> <span class="token punctuation">[</span>lat1<span class="token punctuation">,</span> lon1<span class="token punctuation">,</span> lat2<span class="token punctuation">,</span> lon2<span class="token punctuation">]</span><span class="token punctuation">)</span>
    dlat <span class="token operator">=</span> lat2 <span class="token operator">-</span> lat1
    dlon <span class="token operator">=</span> lon2 <span class="token operator">-</span> lon1
    a <span class="token operator">=</span> sin<span class="token punctuation">(</span>dlat<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> cos<span class="token punctuation">(</span>lat1<span class="token punctuation">)</span> <span class="token operator">*</span> cos<span class="token punctuation">(</span>lat2<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">(</span>dlon<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>
    c <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> asin<span class="token punctuation">(</span>sqrt<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
    mi <span class="token operator">=</span> <span class="token number">3959</span> <span class="token operator">*</span> c
    <span class="token keyword">return</span> mi
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>嵌套于循坏中：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>timeit df<span class="token punctuation">[</span><span class="token string">'distance'</span><span class="token punctuation">]</span> <span class="token operator">=</span>\
df<span class="token punctuation">.</span>apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> row<span class="token punctuation">:</span> haversine_cy_dtyped<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                              row<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>10 loops, best of 3: 68.4 ms per loop
</code></pre><p>可以看到，<code>Cython</code>确实带来速度上的提升，但效果不及向量化（并行化）。</p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Data Science </tag>
            
            <tag> Programming </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python可视化工具指引]]></title>
      <url>http://blog.ddlee.cn/2017/05/28/Python%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E6%8C%87%E5%BC%95/</url>
      <content type="html"><![CDATA[<p>本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=FytuB8nFHPQ" target="_blank" rel="external">Python’s Visualization Landscape</a></p>
<p>Python真是越来越火了。活跃的开源社区为Python这门语言贡献着长青的活力。</p>
<p>子曾经曰过：轮子多了，车就稳了。</p>
<p>本文帮助你选好轮子，也祝愿可视化的车开得越来越稳。</p>
<h2 id="The-Landscape"><a href="#The-Landscape" class="headerlink" title="The Landscape"></a>The Landscape</h2><p><img src="http://static.ddlee.cn/static/img/Python可视化工具指引/landscape.png" alt="landscape"></p>
<p>如图。</p>
<p>VanderPlas在展示完这张全景图后给大家贴了这张图：</p>
<p><img src="http://static.ddlee.cn/static/img/Python可视化工具指引/chan.png" alt="chan"></p>
<p>我差点笑喷。我们的表情包可能要在人民币之前走向国际化了。</p>
<p>回到正题，可视化工具有两个主要阵营，一是基于matplotlib，二是基于JavaScript。还有的接入了JS下著名的D3.js库。</p>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>numpy, pandas, matplotlib可以说是python数据科学的三驾马车。凡以python为教学语言的数据科学相关课程必提这三个库。而matplotlib又有什么特点呢？</p>
<p>先说优点：</p>
<ol>
<li>像MATLAB的语法，对MATLAB用户好上手</li>
<li>稳定，久经考验</li>
<li>渲染后端丰富，跨平台（GTK, Qt5, svg, pdf等）</li>
</ol>
<p>缺点也有很多：</p>
<ol>
<li>API过于繁琐</li>
<li>默认配色太丑</li>
<li>对web支持差，交互性差</li>
<li>对大数据集处理较慢</li>
</ol>
<p>于是就有了很多基于matplotlib的扩展，提供了更丰富、更人性化的API。</p>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/matplotlib.png" alt="matplotlib"></p>
<p>下面是几个比较受欢迎的包：</p>
<h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p>pandas的DataFrame对象是有plot()方法的，如：<br><code>iris.plot.scatter(&#39;petalLength&#39;, &#39;petalWidth&#39;)</code>生成二维散点图，只需指明两个轴取自哪一列数据即可。</p>
<h3 id="seaborn"><a href="#seaborn" class="headerlink" title="seaborn"></a>seaborn</h3><p>seaborn(<a href="http://seaborn.pydata.org/examples/" target="_blank" rel="external">gallery</a>)专注于统计数据可视化，默认配色也还可以。语法示例：</p>
<pre class="line-numbers language-Python"><code class="language-Python">import seaborn as sns
sns.lmplot('petalLength', 'sepalWidth', iris, hue='species', fit_reg=False)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="类ggplot"><a href="#类ggplot" class="headerlink" title="类ggplot"></a>类ggplot</h3><p>对于R用户，最熟悉的可视化包可能是ggplot2，在python中可以考虑ggpy(<a href="https://github.com/yhat/ggpy)和近期上了Github" target="_blank" rel="external">https://github.com/yhat/ggpy)和近期上了Github</a> Trends的plotnie(<a href="https://github.com/has2k1/plotnine)。" target="_blank" rel="external">https://github.com/has2k1/plotnine)。</a></p>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>基于JS的包常常具有非常好的交互性，其共同点是将图形格式化为json文件，再由JS完成渲染。</p>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/js.png" alt="js"></p>
<h3 id="Bokeh"><a href="#Bokeh" class="headerlink" title="Bokeh"></a>Bokeh</h3><p>Bokeh(<a href="http://bokeh.pydata.org/en/latest/docs/gallery.html" target="_blank" rel="external">Gallery</a>)定位于绘制用于浏览器展示的交互式图形。其优点是交互性、能够处理大量数据和流数据。语法示例：</p>
<pre class="line-numbers language-python"><code class="language-python">p <span class="token operator">=</span> figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
p<span class="token punctuation">.</span>circle<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>petalLength<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>sepalWidth<span class="token punctuation">)</span>
show<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="Plotly"><a href="#Plotly" class="headerlink" title="Plotly"></a>Plotly</h3><p>Plotly(<a href="https://plot.ly/python/" target="_blank" rel="external">Gallery</a>)跟Bokeh类似。但其提供了多种语言接口(JS, R, Python, MATLAB)，并且支持3D和动画效果，缺点是有些功能需要付费。<br>语法示例：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> plotly<span class="token punctuation">.</span>graph_objs <span class="token keyword">import</span> Scatter
<span class="token keyword">from</span> plotly<span class="token punctuation">.</span>offline <span class="token keyword">import</span> iplot
p <span class="token operator">=</span> Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>iris<span class="token punctuation">.</span>petalLength<span class="token punctuation">,</span>
            y<span class="token operator">=</span>iris<span class="token punctuation">.</span>sepalWidth<span class="token punctuation">,</span>
            mode<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">)</span>
iplot<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="处理大型数据集"><a href="#处理大型数据集" class="headerlink" title="处理大型数据集"></a>处理大型数据集</h2><p>对于大型数据集，可以考虑的包包括datashader, Vaex, 基于OpenGL的Vispy和Glumpy，GlueViz等。这里介绍datashader。</p>
<h3 id="datashader"><a href="#datashader" class="headerlink" title="datashader"></a>datashader</h3><p><a href="https://github.com/bokeh/datashader" target="_blank" rel="external">datashader</a>是Bokeh的子项目，为处理大型数据集而生。</p>
<p>示例语法：</p>
<pre class="line-numbers language-Python"><code class="language-Python">from colorcet import fire
export(tf.shade(agg, cmap=cm(fire, 0.2), how='eq_hist'), 'census_ds_fier_eq_hist')
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/fire.jpg" alt="fire"></p>
<h2 id="最终的建议"><a href="#最终的建议" class="headerlink" title="最终的建议"></a>最终的建议</h2><p>上车忠告：</p>
<ol>
<li>matplotlib必会</li>
<li>R用户：ggpy/plotnine</li>
<li>交互式：plotly(与R接口统一)/bokeh(免费)</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Visualization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Deep Learning]]></title>
      <url>http://blog.ddlee.cn/2017/05/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Learning/</url>
      <content type="html"><![CDATA[<p>论文：<a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" target="_blank" rel="external">Deep Learning</a></p>
<p>这篇文章是三位大牛15年发表在Nature上有关深度学习的综述，尽管这两年深度学习又有更多的模型和成果出现，文章显得有些过时，但来自三位领军人物对深度学习的深度阐述还是值得反复回味。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>摘要的第一句话可以说给深度学习下了定义。有一些观点认为深度学习就是堆叠了很多层的神经网络，因计算力的提升而迎来第二春。但请看三位是怎么说的：</p>
<blockquote>
<p>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.</p>
</blockquote>
<p>也就是说，深度学习是允许由 <em>多个处理层构成的计算模型</em> 用多个层次的 <em>抽象</em> 来习得 <em>数据表示</em> 的技术。我的解读如下：</p>
<ol>
<li>深度学习不限于神经网络模型，其关键之处在于多层的表示</li>
<li>深度学习属于表示学习，目的是习得数据的某种表示，而这种表示由多个层次的抽象完成</li>
</ol>
<p>在第一段的导言中，文章总结了深度学习技术取得突破性成果的各个领域，也再次指出了深度学习与传统学习算法的不同之处：</p>
<ul>
<li>传统学习模型需要特征工程和领域知识来从数据构建较好的特征</li>
<li>深度学习中，多层的特征由通用的学习过程得到，而不需要人类工程师的参与</li>
</ul>
<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><p>这一段概述了监督学习的一般框架、优化策略，并指出浅层学习需要Feature Extractor来提取对最适合目标问题的特征。</p>
<h2 id="Backpropagation-to-train-multilayer-architectures"><a href="#Backpropagation-to-train-multilayer-architectures" class="headerlink" title="Backpropagation to train multilayer architectures"></a>Backpropagation to train multilayer architectures</h2><p>这一段指出BP算法的关键在于目标函数关于某一子模块输入的导数可以反向通过目标函数关于该子模块输出的导数得出，而这一过程是可迭代的。BP算法曾因容易陷于局部最优解而被冷落，但对于大型网络，在实践中，理论和经验都表明尽管落于局部最优解，但这个解的效果却和全局最优解相差无几，而且几乎所有的局部最优解都可以取得类似的效果。</p>
<h2 id="Convolutional-neural-networks"><a href="#Convolutional-neural-networks" class="headerlink" title="Convolutional neural networks"></a>Convolutional neural networks</h2><p>巻积网络背后有四个关键想法：</p>
<ul>
<li>local connections</li>
<li>shared weights</li>
<li>pooling</li>
<li>the use of many layers</li>
</ul>
<p>巻积网络常由巻积层、池化层和激活层构成，巻积层用于提取局部特征，池化层用于整合相似的特征，激活层用于加入非线性。这样的结构有两点理由：</p>
<ol>
<li>张量性数据的局部数值常常高度相关，局部特征容易发现</li>
<li>局部特征跟位置无关（平移不变性）</li>
</ol>
<p>文章也提到了这种巻积结构的仿生学证据。</p>
<h2 id="Image-understanding-with-deep-convolutional-networks"><a href="#Image-understanding-with-deep-convolutional-networks" class="headerlink" title="Image understanding with deep convolutional networks"></a>Image understanding with deep convolutional networks</h2><p>这一段总结了巻积网路在图像方面取得的成就。</p>
<h2 id="Distributed-representations-and-language-processing"><a href="#Distributed-representations-and-language-processing" class="headerlink" title="Distributed representations and language processing"></a>Distributed representations and language processing</h2><p>分布式表示在两点上可以取得指数级增益：</p>
<ol>
<li>习得特征的不同组合可以泛化出训练数据中不存在的类型</li>
<li>特征组合的个数的增加关于层数是指数级的</li>
</ol>
<p>文章还比较了分布式表示相比传统的词频统计在表述人类语言方面的优势。</p>
<h2 id="Recurrent-neural-networks"><a href="#Recurrent-neural-networks" class="headerlink" title="Recurrent neural networks"></a>Recurrent neural networks</h2><p>这一段概述了循环神经网络的动态特性和LSTM等结构上的改进。</p>
<h2 id="The-future-of-deep-learning"><a href="#The-future-of-deep-learning" class="headerlink" title="The future of deep learning"></a>The future of deep learning</h2><p>作者认为在长期看来，无监督学习会更为重要，人工智能领域的重大飞跃将由组合了表示学习和复杂推理的系统取得。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Papers </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Dropout-Pytorch实现]]></title>
      <url>http://blog.ddlee.cn/2017/05/17/Dropout-Pytorch%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p>Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。</p>
<h3 id="Dropout的洞察"><a href="#Dropout的洞察" class="headerlink" title="Dropout的洞察"></a>Dropout的洞察</h3><p>关于Dropout，一个流行的解释是，通过随机行为训练网络，并平均多个随机决定的结果，实现了参数共享的Bagging。如下图，通过随机地冻结/抛弃某些隐藏单元，我们得到了新的子网络，而参数共享是说，与Bagging中子模型相互独立的参数不同，深度网络中Dropout生成的子网络是串行的，后一个子模型继承了前一个子模型的某些参数。</p>
<p><img src="http://static.ddlee.cn/static/img/Dropout-Pytorch实现/dropout.jpg" alt="dropout"></p>
<p>Dropout是模型自我破坏的一种形式，这种破坏使得存活下来的部分更加鲁棒。例如，某一隐藏单元学得了脸部鼻子的特征，而在Dropout中遭到破坏，则在之后的迭代中，要么该隐藏单元重新学习到鼻子的特征，要么学到别的特征，后者则说明，鼻子特征对该任务来说是冗余的，因而，通过Dropout，保留下来的特征更加稳定和富有信息。</p>
<p>Hinton曾用生物学的观点解释这一点。神经网络的训练过程可以看做是生物种群逐渐适应环境的过程，在迭代中传递的模型参数可以看做种群的基因，Dropout以随机信号的方式给环境随机的干扰，使得传递的基因不得不适应更多的情况才能存活。</p>
<p>另一个需要指出的地方是，Dropout给隐藏单元加入的噪声是乘性的，不像Bias那样加在隐藏单元上，这样在进行反向传播时，Dropout引入的噪声仍能够起作用。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面看在实践中，Dropout层是如何实现的。简单来说，就是生成一系列随机数作为mask，然后再用mask点乘原有的输入，达到引入噪声的效果。</p>
<h4 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># forward pass</span>
<span class="token keyword">def</span> <span class="token function">dropout_forward</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> dropout_param<span class="token punctuation">)</span><span class="token punctuation">:</span>
  p<span class="token punctuation">,</span> mode <span class="token operator">=</span> dropout_param<span class="token punctuation">[</span><span class="token string">'p'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dropout_param<span class="token punctuation">[</span><span class="token string">'mode'</span><span class="token punctuation">]</span>
  <span class="token comment" spellcheck="true"># p: dropout rate; mode: train or test</span>
  <span class="token keyword">if</span> <span class="token string">'seed'</span> <span class="token keyword">in</span> dropout_param<span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random_seed<span class="token punctuation">(</span>dropout_param<span class="token punctuation">[</span><span class="token string">'seed'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token comment" spellcheck="true"># seed: random seed</span>
  mask <span class="token operator">=</span> None
  out <span class="token operator">=</span> None
  <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>
    mask <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token operator">*</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">>=</span> p<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>p<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 1-p as normalization multiplier: to keep the size of input</span>
    out <span class="token operator">=</span> x <span class="token operator">*</span> mask
  <span class="token keyword">elif</span> mode <span class="token operator">==</span> <span class="token string">'test'</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># do nothing when perform inference</span>
    out <span class="token operator">=</span> x
  cache <span class="token operator">=</span> <span class="token punctuation">(</span>dropout_param<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
  out <span class="token operator">=</span> out<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> copy<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> out<span class="token punctuation">,</span> cache

<span class="token comment" spellcheck="true"># backward pass</span>
<span class="token keyword">def</span> <span class="token function">dropout_backward</span><span class="token punctuation">(</span>dout<span class="token punctuation">,</span> cache<span class="token punctuation">)</span><span class="token punctuation">:</span>
  dropout_param<span class="token punctuation">,</span> mask <span class="token operator">=</span> cache
  mode <span class="token operator">=</span> dropout_param<span class="token punctuation">[</span><span class="token string">'mode'</span><span class="token punctuation">]</span>

  dx <span class="token operator">=</span> None
  <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>
    dx <span class="token operator">=</span> dout <span class="token operator">*</span> mask
  <span class="token keyword">elif</span> mode <span class="token operator">==</span> <span class="token string">'test'</span><span class="token punctuation">:</span>
    dx <span class="token operator">=</span> dout
  <span class="token keyword">return</span> dx
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><p>file: <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/dropout.py" target="_blank" rel="external">/torch/nn/_functions/dropout.py</a></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Dropout</span><span class="token punctuation">(</span>InplaceFunction<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Dropout<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> p <span class="token operator">&lt;</span> <span class="token number">0</span> <span class="token operator">or</span> p <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"dropout probability has to be between 0 and 1, "</span>
                             <span class="token string">"but got {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> p
        self<span class="token punctuation">.</span>train <span class="token operator">=</span> train
        self<span class="token punctuation">.</span>inplace <span class="token operator">=</span> inplace

    <span class="token keyword">def</span> <span class="token function">_make_noise</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># generate random signal</span>
        <span class="token keyword">return</span> input<span class="token punctuation">.</span>new<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize_as_<span class="token punctuation">(</span>input<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>inplace<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>mark_dirty<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
            output <span class="token operator">=</span> input
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            output <span class="token operator">=</span> input<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>p <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>train<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>noise <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_noise<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># multiply mask to input</span>
            self<span class="token punctuation">.</span>noise<span class="token punctuation">.</span>bernoulli_<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>p<span class="token punctuation">)</span><span class="token punctuation">.</span>div_<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>p<span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>p <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>noise<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>noise <span class="token operator">=</span> self<span class="token punctuation">.</span>noise<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
            output<span class="token punctuation">.</span>mul_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>noise<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>p <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>train<span class="token punctuation">:</span>
            <span class="token keyword">return</span> grad_output<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>noise<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> grad_output
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Visualizing and Understanding Recurrent Networks]]></title>
      <url>http://blog.ddlee.cn/2017/05/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Visualizing-and-Understanding-Recurrent-Networks/</url>
      <content type="html"><![CDATA[<p>论文： <a href="http://arxiv.org/abs/1506.02078" target="_blank" rel="external">Visualizing and Understanding Recurrent Networks</a></p>
<h2 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h2><p>字母级的循环神经网络，用Torch实现，代码见<a href="http://github.com/karpathy/char-rnn" target="_blank" rel="external">GitHub</a>。字母嵌入成One-hot向量。优化方面，采用了RMSProp算法，加入了学习速率的decay和early stopping。</p>
<p>数据集采用了托尔斯泰的《战争与和平》和Linux核心的代码。</p>
<h2 id="可解释性激活的例子"><a href="#可解释性激活的例子" class="headerlink" title="可解释性激活的例子"></a>可解释性激活的例子</h2><p>$tanh$函数激活的例子，$-1$为红色，$+1$为蓝色。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane1.png" alt="pane1"></p>
<p>上图分别是记录了行位置、引文和if语句特征的例子和失败的例子。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane2.png" alt="pane2"></p>
<p>上图分别是记录代码中注释、代码嵌套深度和行末标记特征的例子。</p>
<h2 id="Gates数值的统计"><a href="#Gates数值的统计" class="headerlink" title="Gates数值的统计"></a>Gates数值的统计</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/gates.png" alt="gates"></p>
<p>此图信息量很大。</p>
<ol>
<li>left-saturated和right-saturated表示各个Gates激活函数（$sigmoid$）小于0.1和大于0.9，即总是阻止信息流过和总是允许信息流过。</li>
<li>横轴和纵轴表示该Gate处于这两种状态的时间比例，即有多少时间是阻塞状态，有多少时间是畅通状态。</li>
<li>三种颜色表示不同的层。</li>
</ol>
<p>有以下几个观察：</p>
<ol>
<li>第一层的门总是比较中庸，既不阻塞，也不畅通</li>
<li>第二三层的门在这两种状态间比较分散，经常处于畅通状态的门可能记录了长期的依赖信息，而经常处于阻塞状态的门则负责了短期信息的控制。</li>
</ol>
<h2 id="错误来源分析"><a href="#错误来源分析" class="headerlink" title="错误来源分析"></a>错误来源分析</h2><p>在这一节，作者用了“剥洋葱”的方法，建立了不同的模型将错误进行分解。此处错误指LSTM预测下一个字母产生的错误，数据集为托尔斯泰的《战争与和平》。</p>
<ol>
<li>n-gram</li>
<li>Dynamic n-long memory，即对已经出现过得单词的复现。如句子”Jon yelled at<br>Mary but Mary couldn’t hear him.”中的Mary。</li>
<li>Rare words，不常见单词</li>
<li>Word model，单词首字母、新行、空格之后出现的错误</li>
<li>Punctuation，标点之后</li>
<li>Boost，其他错误</li>
</ol>
<p>根据作者的实验，错误的来源有如下分解：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/error.png" alt="error"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇文章是打开LSTM黑箱的尝试，提供了序列维度上共享权值的合理性证据，对Gates状态的可视化也非常值得关注，最后对误差的分解可能对新的网络结构有所启发（比如，如何将单词级别和字母级别的LSTM嵌套起来，解决首字母预测的问题？）。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AI </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Papers </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Deep Residual Learning for Image Recognition]]></title>
      <url>http://blog.ddlee.cn/2017/04/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/</url>
      <content type="html"><![CDATA[<p>论文：<a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>网络在堆叠到越来越深之后，由于BP算法所依赖的链式法则的连乘形式，会出现梯度消失和梯度下降的问题。初始标准化和中间标准化参数在一定程度上缓解了这一问题，但仍然存在更深的网络比浅层网络具有更大的训练误差的问题。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/error.png" alt="error"></p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><p>多层的网络结构能够任意接近地拟合目标映射$H(x)$，那么也能任意接近地拟合其关于恒等映射的残差函数$H(x)-x$。记$F(x)=H(x)-x$，则原来的目标映射表为$F(x)+x$。由此，可以设计如下结构。</p>
<h3 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/block.jpg" alt="Residual Learning: a building block"></p>
<p>残差单元包含一条恒等映射的捷径，不会给原有的网络结构增添新的参数。</p>
<h2 id="动机-启发"><a href="#动机-启发" class="headerlink" title="动机/启发"></a>动机/启发</h2><p>层数的加深会导致更大的训练误差，但只增加恒等映射层则一定不会使训练误差增加，而若多层网络块要拟合的映射与恒等映射十分类似时，加入的捷径便可方便的发挥作用。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章中列举了大量在ImagNet和CIFAR-10上的分类表现，效果很好，在此不表。</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><h4 id="Deeper-Bottleneck-Architectures"><a href="#Deeper-Bottleneck-Architectures" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/Bottleneck.png" alt="Bottleneck"></p>
<p>两头的1 * 1巻积核先降维再升维，中间的3 * 3巻积核成为“瓶颈”，用于提取重要的特征。这样的结构跟恒等映射捷径配合，在ImageNet上有很好的分类效果。</p>
<h4 id="Standard-deviations-of-layer-responses"><a href="#Standard-deviations-of-layer-responses" class="headerlink" title="Standard deviations of layer responses"></a>Standard deviations of layer responses</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/std.png" alt="std"><br>上图是在CIFAR-10数据集上训练的网络各层的相应方差（Batch-Normalization之后，激活之前）。可以看到，残差网络相对普通网络有更小的方差。这一结果支持了残差函数比非残差函数更接近于0的想法（即更接近恒等映射）。此外，还显示出网络越深，越倾向于保留流过的信息。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>深度残差网络在当年的比赛中几乎是满贯。<br>下面是我的一些（未经实验证实的）理解：</p>
<p>首先，其”跳级”的网络结构对深度网络的设计是一种启发，通过“跳级”，可以把之前网络的信息相对完整的跟后层网络结合起来，即低层次解耦得到的特征和高层次解耦得到的特征再组合。<br>再者，这种分叉的结构可以看作网络结构层面的”Dropout”: 如果被跳过的网络块不能习得更有用的信息，就被恒等映射跳过了。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Papers </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[[论文笔记]Tensorflow White Paper]]></title>
      <url>http://blog.ddlee.cn/2017/04/20/Tensorflow-White-Paper/</url>
      <content type="html"><![CDATA[<p>论文：<a href="https://www.tensorflow.org/about/bib" target="_blank" rel="external">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</a></p>
<h2 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h2><h3 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h3><p>整张图如同管道结构，数据流就是其中的水流。<em>Control Dependency</em> 描述了管道的有向结构，而反向传播可以通过增加新的管道节点来实现。</p>
<h3 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h3><p>即计算操作的抽象，相当于映射、函数。</p>
<h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><p>执行计算的单元，CPU或GPU</p>
<h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>Client-Server结构，进行计算或者调整图结构则视为一次会话</p>
<h3 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h3><p>特殊的Operation，返回一个句柄，指向持久化的张量，这些张量在整张图的计算中不会被释放。</p>
<h3 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h3><p>对Kernel的封装，包含类型属性，实行注册机制维护可供使用的Device列表。</p>
<h2 id="多机实现"><a href="#多机实现" class="headerlink" title="多机实现"></a>多机实现</h2><p>要考虑两个问题：</p>
<ol>
<li>计算节点在Device间的分配问题</li>
<li>Devices之间的通信</li>
</ol>
<p>针对这两个问题，分别建立了两个抽象层。</p>
<h3 id="计算节点分配的C-S机制"><a href="#计算节点分配的C-S机制" class="headerlink" title="计算节点分配的C/S机制"></a>计算节点分配的C/S机制</h3><p><img src="http://static.ddlee.cn/static/img/Tensorflow-White-Paper/master.png" alt="master"><br>client提出计算请求，master负责切割计算图为子图，分配子图到Devices。分配时，会模拟执行子图，并采取贪心的策略分配。</p>
<h3 id="不同Device之间的发送和接收节点"><a href="#不同Device之间的发送和接收节点" class="headerlink" title="不同Device之间的发送和接收节点"></a>不同Device之间的发送和接收节点</h3><p><img src="http://static.ddlee.cn/static/img/Tensorflow-White-Paper/rev-send.png" alt="rec-send"></p>
<p>在每个Device上建立Receive和Send节点，负责与其他Device通信。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="数据化并行"><a href="#数据化并行" class="headerlink" title="数据化并行"></a>数据化并行</h3><p><img src="http://static.ddlee.cn/static/img/Tensorflow-White-Paper/Parallelize.png" alt="Parallelize"></p>
<p>上：单线程，同步数据并行<br>下：多线程，异步更新</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><p>文章中很多内容并没涉及到（看不懂）。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>TensorFlow是个庞大的计算框架，不仅仅定位于深度网络。其对计算图的抽象和数据、计算资源的分配的处理是值得关注的。</p>
]]></content>
      
        <categories>
            
            <category> Papers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Papers </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[编程方法论(一):重构]]></title>
      <url>http://blog.ddlee.cn/2017/04/08/%E7%BC%96%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA-%E9%87%8D%E6%9E%84/</url>
      <content type="html"><![CDATA[<p>本文内容主要整理自lynda.com课程<a href="https://www.lynda.com/Developer-Programming-Foundations-tutorials/Foundations-Programming-Refactoring-Code/122457-2.html" target="_blank" rel="external">Programming Foudations: Refactoring Code</a>和<em>Martin Fowler</em>的<a href="https://martinfowler.com/books/refactoring.html" target="_blank" rel="external">重构</a>。全部例子来源于<a href="https://refactoring.com" target="_blank" rel="external">refactoring.com</a>。</p>
<p>内容大纲：<br><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring.png" alt="structure"></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-1.png" alt="intro-method"></p>
<h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>重构是在不影响软件功能的情况下，重新组织代码，使之更清晰、更容易理解的过程。</p>
<ul>
<li>前提：功能不变</li>
<li>行为：改写代码</li>
<li>目的：提高可理解性</li>
</ul>
<p>大白话讲，重构就是改写，造福以后需要理解这段代码的人们。</p>
<h5 id="重构不是什么"><a href="#重构不是什么" class="headerlink" title="重构不是什么"></a>重构不是什么</h5><p>给一件事物下定义，有时候从反方面更好讲些。比如你难给正义下一个定义，但很容易举出什么是非正义的例子。</p>
<ul>
<li>重构不是Debug，代码已经运行良好</li>
<li>重构不是优化</li>
<li>重构不是添加新功能</li>
</ul>
<p>也就是说，重构对使用代码的人没有任何好处，对使用者来讲，代码是黑箱。重构是准备给要打开黑箱的人，而那个人常常是你自己。</p>
<h5 id="玄学：Code-Smells"><a href="#玄学：Code-Smells" class="headerlink" title="玄学：Code Smells"></a>玄学：Code Smells</h5><p>玄学二字是我自己加的。Martin Fowler当然没有这样说。我只是表达一下对无法精确描述的定义的敬意。</p>
<p>我的理解是Code Smells是best practice和code style的总和，直接和根本来源是自己的代码经验。所谓语感、文笔、血淋淋的人生道理。</p>
<h5 id="准备工作：自动化测试"><a href="#准备工作：自动化测试" class="headerlink" title="准备工作：自动化测试"></a>准备工作：自动化测试</h5><p>重构当然不是breaking the code，写好测试，保证代码仍能正常运行。</p>
<h3 id="重构范例：方法层面"><a href="#重构范例：方法层面" class="headerlink" title="重构范例：方法层面"></a>重构范例：方法层面</h3><p>首先是一句良言：哪里加了注视，哪里可能就需要重构。</p>
<p>这一点的潜在信念是，好的代码是self-explained的，通过合理的命名、清晰的组织，代码应该像皇帝的新装那样一目了然。</p>
<h4 id="可以用于重构的工具"><a href="#可以用于重构的工具" class="headerlink" title="可以用于重构的工具"></a>可以用于重构的工具</h4><p>常见的IDE会有重构的功能，如重命名变量。另外，一个严厉的Linter加上像我这样的强迫癌患者会将风格问题扼杀在摇篮之中。</p>
<h4 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h4><p>举例均以Code smell和重构建议两部分构成，较抽象(wo kan bu dong)的给出代码。</p>
<h5 id="Extract-Method"><a href="#Extract-Method" class="headerlink" title="Extract Method"></a><em>Extract Method</em></h5><p>Code smell: 太长的方法，带注释的代码块</p>
<p>重构： 提取，新建，用评论命名</p>
<h5 id="Remove-temps"><a href="#Remove-temps" class="headerlink" title="Remove temps"></a>Remove temps</h5><p>Code smell: 冗余的临时变量（本地）</p>
<p>重构：</p>
<ul>
<li><em>Replace with Query</em>: 把表达式提取为方法（规模较大）</li>
<li><em>Inline temps</em>: 直接用表达式代替这个变量（规模较小）</li>
</ul>
<h5 id="Add-temps"><a href="#Add-temps" class="headerlink" title="Add temps"></a>Add temps</h5><p>Code smell: 同样的变量有多重含义</p>
<p>重构：</p>
<ul>
<li><em>Split temporary variable</em>: 同一个临时变量在上下文赋予了不同含义（复用），拆</li>
<li><em>Remove assignment to parameters</em>: 对参数默认值的设定，在函数内新建变量，初始化这个新变量</li>
</ul>
<p>Remove assignment to parameters的例子：</p>
<pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//Before</span>
<span class="token keyword">int</span> <span class="token function">discount</span> <span class="token punctuation">(</span><span class="token keyword">int</span> inputVal<span class="token punctuation">,</span> <span class="token keyword">int</span> quantity<span class="token punctuation">,</span> <span class="token keyword">int</span> yearToDate<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>inputVal <span class="token operator">></span> <span class="token number">50</span><span class="token punctuation">)</span> inputVal <span class="token operator">-=</span> <span class="token number">2</span><span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">//After Refactoring</span>
<span class="token keyword">int</span> <span class="token function">discount</span> <span class="token punctuation">(</span><span class="token keyword">int</span> inputVal<span class="token punctuation">,</span> <span class="token keyword">int</span> quantity<span class="token punctuation">,</span> <span class="token keyword">int</span> yearToDate<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">int</span> result <span class="token operator">=</span> inputVal<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>inputVal <span class="token operator">></span> <span class="token number">50</span><span class="token punctuation">)</span> result <span class="token operator">-=</span> <span class="token number">2</span><span class="token punctuation">;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这一点基于的信念是，参数只能代表被传进来的变量，不应该在本地再赋予别的含义。</p>
<h3 id="重构范例：类与方法"><a href="#重构范例：类与方法" class="headerlink" title="重构范例：类与方法"></a>重构范例：类与方法</h3><p><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-2.png" alt="class and method"></p>
<h4 id="Move-Method"><a href="#Move-Method" class="headerlink" title="Move Method"></a><em>Move Method</em></h4><p>Code smell: feature envy（依恋情结）</p>
<p>用中文来说，是指某个方法操作/使用（依恋）某一个类多过自己所处的类，我们用“出轨”这个词来表示这种现象。但这是违反婚姻法的，因而，我们的重构手段就是，把这个方法移动到它依恋的类中，圆满一段木石良缘。*</p>
<p>重构： 圆满木石良缘。</p>
<h4 id="Extract-Class"><a href="#Extract-Class" class="headerlink" title="Extract Class"></a><em>Extract Class</em></h4><p>Code smell: 规模太大的类</p>
<p>重构： 把部分移出，自立门户</p>
<h4 id="Inline-Class"><a href="#Inline-Class" class="headerlink" title="Inline Class"></a><em>Inline Class</em></h4><p>Code smell: 冗余的类</p>
<p>重构： 像我这中请天假组内运转几乎不受影响的人，应该清除掉（这是瞎话）</p>
<h4 id="Condition-Focused（条件语句相关）"><a href="#Condition-Focused（条件语句相关）" class="headerlink" title="Condition Focused（条件语句相关）"></a>Condition Focused（条件语句相关）</h4><p>Code smell: 写完判断条件自己都看不懂/看着难受</p>
<p>重构：</p>
<ul>
<li><em>Decompose conditional</em>: 分解</li>
<li><em>Consolidate conditional expression</em>: 多项条件指向同一段后续操作，提取这些条件为方法</li>
<li><em>Consolidate duplicate conditional fragments</em>: 不同条件的后续操作中含有共同的部分，将共有部分提取出来（不管哪个条件总要执行）</li>
<li><em>Replace condition with polymorphism</em>: 针对有判断分支的方法，替换成多态方法</li>
<li><em>Replace type code with subclass*</em>: 针对有判断分支的类，替换为子类</li>
</ul>
<p>Consolidate conditional expression的例子：</p>
<pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//Before</span>
<span class="token keyword">double</span> <span class="token function">disabilityAmount</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>_seniority <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>_monthsDisabled <span class="token operator">></span> <span class="token number">12</span><span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>_isPartTime<span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
  <span class="token comment" spellcheck="true">// compute the disability amount</span>
<span class="token comment" spellcheck="true">//After Refactoring</span>
<span class="token keyword">double</span> <span class="token function">disabilityAmount</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">isNotEligableForDisability</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
  <span class="token comment" spellcheck="true">// compute the disability amount</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="重构范例：数据相关"><a href="#重构范例：数据相关" class="headerlink" title="重构范例：数据相关"></a>重构范例：数据相关</h3><p><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-3.png" alt="class and method"></p>
<h4 id="Move-field"><a href="#Move-field" class="headerlink" title="Move field"></a><em>Move field</em></h4><p>code smell: inverse feature envy（自造）</p>
<p>某一个类使用某一数据比该数据所属的类还多。</p>
<p>重构：送给你了还不行吗！？</p>
<h4 id="Data-Clumps（数据团）"><a href="#Data-Clumps（数据团）" class="headerlink" title="Data Clumps（数据团）"></a>Data Clumps（数据团）</h4><p>code smell: 某些数据总是抱团出现</p>
<p>重构：</p>
<ul>
<li><em>Preserve whole object</em>: 在一个方法中反复提取某个类的一些属性，将整个对象传入</li>
<li><em>Introducing parameter object</em>: 把这些参数合并为一个类，把新建的类传入</li>
</ul>
<h4 id="Similifying"><a href="#Similifying" class="headerlink" title="Similifying"></a>Similifying</h4><p>重构：</p>
<ul>
<li><em>Renaming</em>: 顾名思义</li>
<li><em>Add or remove parameters</em>: 顾名思义</li>
<li><em>Replace parameter with explicit Method</em>: 根据不同参数值新建专属的方法</li>
<li><em>Parameterize Method</em>: 与上者相反，把不同方法合并，传入参数</li>
<li><em>Separate queries form modifiers</em>: 将找到数据和更该数据两个操作拆成两个方法</li>
</ul>
<p>Replace parameter with explicit Method的例子：</p>
<pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//Before</span>
<span class="token keyword">void</span> <span class="token function">setValue</span> <span class="token punctuation">(</span>String name<span class="token punctuation">,</span> <span class="token keyword">int</span> value<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>name<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"height"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    _height <span class="token operator">=</span> value<span class="token punctuation">;</span>
    <span class="token keyword">return</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>name<span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"width"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    _width <span class="token operator">=</span> value<span class="token punctuation">;</span>
    <span class="token keyword">return</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
  Assert<span class="token punctuation">.</span><span class="token function">shouldNeverReachHere</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token comment" spellcheck="true">//After Refactoring</span>
<span class="token keyword">void</span> <span class="token function">setHeight</span><span class="token punctuation">(</span><span class="token keyword">int</span> arg<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  _height <span class="token operator">=</span> arg<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">void</span> <span class="token function">setWidth</span> <span class="token punctuation">(</span><span class="token keyword">int</span> arg<span class="token punctuation">)</span> <span class="token punctuation">{</span>
  _width <span class="token operator">=</span> arg<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Pulling-and-pushing-升级与降级"><a href="#Pulling-and-pushing-升级与降级" class="headerlink" title="Pulling and pushing(升级与降级)"></a>Pulling and pushing(升级与降级)</h4><ul>
<li><em>Pull up method</em> and <em>pull up field</em></li>
<li><em>Push down method</em> and <em>push down field</em></li>
</ul>
<p>解决方法、数据归属不合理的问题。</p>
<h3 id="高阶重构（大坑，大坑）"><a href="#高阶重构（大坑，大坑）" class="headerlink" title="高阶重构（大坑，大坑）"></a>高阶重构（大坑，大坑）</h3><h4 id="Convert-procedural-design-to-objects"><a href="#Convert-procedural-design-to-objects" class="headerlink" title="Convert procedural design to objects"></a><em>Convert procedural design to objects</em></h4><p>化函数式变成为面向对象，祝好运。</p>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>写代码和改代码是一个不断被自己坑和被别人坑的旅程。且行且珍惜。</p>
<p>Cheers, have a good one.</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Programming </tag>
            
            <tag> Refactoring </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ddlee约书计划（第二弹）]]></title>
      <url>http://blog.ddlee.cn/2017/04/05/ddlee%E7%BA%A6%E4%B9%A6%E8%AE%A1%E5%88%92%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%BC%B9%EF%BC%89/</url>
      <content type="html"><![CDATA[<h4 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h4><p>读书这种事情，每几个月都会有那么几天。</p>
<p>浑身难受。不干点什么，眼睛闲得团团转，双手也不知道往哪搁。</p>
<p>大概是愧疚吧。要立个FLAG把这压抑着的自卑和热情释放一下。</p>
<p>花大概十分钟的时间，下载/买下十个月都读不完的书。</p>
<h4 id="书单"><a href="#书单" class="headerlink" title="书单"></a>书单</h4><p>选书原则：</p>
<ol>
<li>我感兴趣</li>
<li>拒绝大部头</li>
<li>均为论述类，有的聊</li>
<li>我能提供电子版</li>
</ol>
<p>书单：</p>
<ul>
<li><a href="https://book.douban.com/subject/26943161/" target="_blank" rel="external">《未来简史》尤瓦尔·赫拉利 </a></li>
<li><a href="https://book.douban.com/subject/26279954/" target="_blank" rel="external">《知识的边界》戴维·温伯格 </a></li>
<li><a href="https://book.douban.com/subject/1089508/" target="_blank" rel="external">《数字化生存》尼葛洛庞帝 </a></li>
<li><a href="https://book.douban.com/subject/25846075/" target="_blank" rel="external">《技术的本质》布莱恩•阿瑟</a></li>
<li><a href="https://book.douban.com/subject/6965746/" target="_blank" rel="external">《科技想要什么》凯文·凯利 </a></li>
<li><a href="https://book.douban.com/subject/1813841/" target="_blank" rel="external">《枪炮、病菌与钢铁》贾雷德·戴蒙德 </a></li>
<li><a href="https://book.douban.com/subject/4850629/" target="_blank" rel="external">《言论的边界》安东尼·刘易斯 </a></li>
<li><a href="https://book.douban.com/subject/1087547/" target="_blank" rel="external">《理解媒介》马歇尔·麦克卢汉 </a></li>
<li><a href="https://book.douban.com/subject/1292405/" target="_blank" rel="external">《自私的基因》里查德.道金斯 </a></li>
<li><a href="https://book.douban.com/subject/1003692/" target="_blank" rel="external">《真实世界的脉络》戴维·多伊奇 </a></li>
</ul>
<p>豆列<a href="https://www.douban.com/doulist/45917752/?start=0&amp;sort=" target="_blank" rel="external">在此</a></p>
<p>剩下一些，我读过，但意犹未尽，也可以聊。</p>
<ul>
<li>《中国近代史》徐中约</li>
<li>《人类简史》尤瓦尔·赫拉利</li>
<li>《娱乐至死》尼尔·波兹曼</li>
<li>《浅薄：互联网如何毒害了我们的大脑》尼古拉斯·卡尔</li>
</ul>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>有条件的，我们用Google Docs共享想法。没条件的，用Evernote。</p>
<p>如果你不习惯写下来，我们可以线下聊（聊到风花雪月人生哲学就不保证了，所以最好还是写下来）。</p>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><ol>
<li>精力有限，同时运行三个线程，多了就溢出了。</li>
<li>其他书也可以推荐，如果长得足够好看的话，我会同意的。</li>
<li>谁也是诸事缠身，有事情不能坚持的，随时退出，我太能理解了；能坚持读完的，我陪你到最后。</li>
<li>这种计划似乎跟熟人约过一次，没成，向我骚扰过的人抱歉。</li>
<li>如果你感兴趣，私信/微信/邮箱联系我。</li>
</ol>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Reading </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Reading </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Coroutine,Generator,Async与Await]]></title>
      <url>http://blog.ddlee.cn/2017/04/03/Coroutine-Generator-Async%E4%B8%8EAwait/</url>
      <content type="html"><![CDATA[<h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>Generator能保存自己的状态，进入一种“Paused”状态，再次调用时会继续执行。</p>
<p>Generator的好处之一是节省了存储空间开销，带一些”流处理”的思想。</p>
<p>其实，我们也可以对Generator进行传入数据的操作：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">coro</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    hello <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token string">"Hello"</span>
    <span class="token keyword">yield</span> hello

c <span class="token operator">=</span> coro<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>next<span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">.</span>send<span class="token punctuation">(</span><span class="token string">"World"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Coroutine"><a href="#Coroutine" class="headerlink" title="Coroutine"></a>Coroutine</h4><p>coroutine可以认为是generator思想的泛化：</p>
<ul>
<li>generator一个一个地吐出数据（返回值）</li>
<li>coroutine一个一个地吃掉数据（传入参数）并返回结果，即可控地执行函数</li>
</ul>
<p>关键点在于，generator与coroutine都能保存自己的状态，而这种特点正可以用于任务切换。yield可以看做是操作系统在进行进程管理时的traps:</p>
<p><img src="http://static.ddlee.cn/static/img/coroutine/os.png" alt="traps"></p>
<p>实际上，coroutine可以看做”用户自定义”的进程，状态、启用和暂停都可控，David Beazley就利用这一点用coroutine实现了Python上的操作系统（参见Reference)。</p>
<h4 id="Conroutine与Concurrent-Programming"><a href="#Conroutine与Concurrent-Programming" class="headerlink" title="Conroutine与Concurrent Programming"></a>Conroutine与Concurrent Programming</h4><p>Concurrent Programming中有Task的概念，有如下特点：</p>
<ul>
<li>独立的控制流</li>
<li>内部状态变量</li>
<li>支持计划任务（暂停、恢复执行）</li>
<li>与其他Task通信</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python">@coroutine
<span class="token keyword">def</span> <span class="token function">grep</span><span class="token punctuation">(</span>pattern<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true">#正则匹配</span>
    <span class="token keyword">print</span> <span class="token string">"Looking for %s"</span> <span class="token operator">%</span> pattern    
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        line <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">yield</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> pattern <span class="token keyword">in</span> line<span class="token punctuation">:</span>
            <span class="token keyword">print</span> line<span class="token punctuation">,</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>conroutine有自己的控制流（while/if），有局部变量（pattern, line），能暂停和恢复（yield()/send()），能相互通信（send()）</p>
<p>====》coroutine就是一种Task！</p>
<p>Python Docs中提供了一个例子：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> asyncio

<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Compute %s + %s ..."</span> <span class="token operator">%</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x <span class="token operator">+</span> y

<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">print_sum</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    result <span class="token operator">=</span> <span class="token keyword">await</span> compute<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"%s + %s = %s"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> result<span class="token punctuation">)</span><span class="token punctuation">)</span>

loop <span class="token operator">=</span> asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>
loop<span class="token punctuation">.</span>run_until_complete<span class="token punctuation">(</span>print_sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
loop<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>执行方式如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/coroutine/tulip_coro.png" alt="Chaining coroutines"></p>
<p>利用coroutine，可以在一个线程(Task)上实现异步。</p>
<h4 id="Impletation"><a href="#Impletation" class="headerlink" title="Impletation"></a>Impletation</h4><p>coroutine有两种实现方式，基于generator和原生async, awati关键字。</p>
<h5 id="generator-based-coroutine"><a href="#generator-based-coroutine" class="headerlink" title="generator based coroutine"></a>generator based coroutine</h5><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> asyncio
<span class="token keyword">import</span> datetime
<span class="token keyword">import</span> random

@asyncio<span class="token punctuation">.</span>coroutine
<span class="token keyword">def</span> <span class="token function">display_date</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> loop<span class="token punctuation">)</span><span class="token punctuation">:</span>
    end_time <span class="token operator">=</span> loop<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">50.0</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Loop: {} Time: {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>num<span class="token punctuation">,</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>loop<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token operator">>=</span> end_time<span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        <span class="token keyword">yield</span> <span class="token keyword">from</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

loop <span class="token operator">=</span> asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span>

asyncio<span class="token punctuation">.</span>ensure_future<span class="token punctuation">(</span>display_date<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> loop<span class="token punctuation">)</span><span class="token punctuation">)</span>
asyncio<span class="token punctuation">.</span>ensure_future<span class="token punctuation">(</span>display_date<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> loop<span class="token punctuation">)</span><span class="token punctuation">)</span>

loop<span class="token punctuation">.</span>run_forever<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上面的程序实现了在同一个线程里交互执行两个函数（sleep），而又能保持各自的状态</p>
<h5 id="Native-support-python-3-5"><a href="#Native-support-python-3-5" class="headerlink" title="Native support(python 3.5+)"></a>Native support(python 3.5+)</h5><p>只需要修改函数定义头和<code>yield from</code>为关键字<code>await</code>即可。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">display_date</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> loop<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">:</span>
    end_time <span class="token operator">=</span> loop<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">50.0</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Loop: {} Time: {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>num<span class="token punctuation">,</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>loop<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token operator">>=</span> end_time<span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><p>Coroutine常翻译成“协程”。</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h4><ul>
<li><a href="http://www.dabeaz.com/coroutines/Coroutines.pdf" target="_blank" rel="external">David Beazley @ PyCon2009 Slides</a></li>
<li><a href="http://masnun.com/2015/11/13/python-generators-coroutines-native-coroutines-and-async-await.html" target="_blank" rel="external">PYTHON: GENERATORS, COROUTINES, NATIVE COROUTINES AND ASYNC/AWAIT</a></li>
<li><a href="https://docs.python.org/3/library/asyncio-task.html" target="_blank" rel="external">Python 3.6 Docs: Taks and coroutines</a></li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 异步 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[500lines项目Crawler源码阅读笔记]]></title>
      <url>http://blog.ddlee.cn/2017/04/03/500lines%E9%A1%B9%E7%9B%AECrawler%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>源码来自GitHub上著名的Repo: <a href="https://github.com/aosabook/500lines" target="_blank" rel="external">500lines or less</a>。</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p><img src="http://static.ddlee.cn/static/img/500lines_crawler/500lines_crawler.png" alt="structure"></p>
<p>代码结构由crawling, crawl, reporting三大部分组成。</p>
<ul>
<li>crawl: 驱动，解析传入的参数，管理loop，调用crawler，生成report</li>
<li>Crawling: 实现crawler类及一系列辅助函数</li>
<li>reporting： 生成记录</li>
</ul>
<h3 id="Crawler类"><a href="#Crawler类" class="headerlink" title="Crawler类"></a>Crawler类</h3><p>Crawler类实现了解析网址，抓取内容等基本功能，利用<code>asyncio</code>库构建<code>coroutine</code>（parse_lings(), fetch(), work()）。</p>
<p><img src="http://static.ddlee.cn/static/img/500lines_crawler/500lines_crawler_class.png" alt="Class Crawler"></p>
<p>核心之处是组织管理异步的抓取任务，代码块结构如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Crawler</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> roots<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> loop<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>q <span class="token operator">=</span> Queue<span class="token punctuation">(</span>loop<span class="token operator">=</span>self<span class="token punctuation">.</span>loop<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 建立队列</span>

  @asyncio<span class="token punctuation">.</span>coroutine
  <span class="token keyword">def</span> <span class="token function">parse_links</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''从返回内容中解析出要抓取的链接'''</span>
    body <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token keyword">from</span> response<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> response<span class="token punctuation">.</span>status <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> content_type<span class="token punctuation">:</span>
        text <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token keyword">from</span> response<span class="token punctuation">.</span>text<span class="token punctuation">(</span><span class="token punctuation">)</span>
        urls <span class="token operator">=</span> set<span class="token punctuation">(</span>re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> url <span class="token keyword">in</span> urls<span class="token punctuation">:</span>
          <span class="token keyword">if</span> self<span class="token punctuation">.</span>url_allowed<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            links<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token punctuation">)</span>
  @asynico<span class="token punctuation">.</span>coroutine
  <span class="token keyword">def</span> <span class="token function">fetch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> url<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''访问链接，抓取返回结果'''</span>
    <span class="token keyword">while</span> tries<span class="token punctuation">:</span>
      <span class="token keyword">try</span><span class="token punctuation">:</span>
        response <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token keyword">from</span> self<span class="token punctuation">.</span>session<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> is_redirect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        links <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token keyword">from</span> self<span class="token punctuation">.</span>parse_links<span class="token punctuation">(</span>response<span class="token punctuation">)</span>
    <span class="token keyword">finally</span><span class="token punctuation">:</span>
      <span class="token keyword">yield</span> <span class="token keyword">from</span> response<span class="token punctuation">.</span>releas<span class="token punctuation">(</span><span class="token punctuation">)</span>
  @asyncio<span class="token punctuation">.</span>coroutine
  <span class="token keyword">def</span> <span class="token function">work</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''封装抓取过程，与队列交互'''</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
      <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        url <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token keyword">from</span> self<span class="token punctuation">.</span>q<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> url <span class="token keyword">in</span> self<span class="token punctuation">.</span>seen_urls
        <span class="token keyword">yield</span> <span class="token keyword">from</span> self<span class="token punctuation">.</span>fetch<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>q<span class="token punctuation">.</span>task_done<span class="token punctuation">(</span><span class="token punctuation">)</span>
  @asyncio<span class="token punctuation">.</span>coroutine
  <span class="token keyword">def</span> <span class="token function">crawl</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''建立Tasks，启动Task'''</span>
    workers <span class="token operator">=</span> <span class="token punctuation">[</span>asyncio<span class="token punctuation">.</span>Task<span class="token punctuation">(</span>self<span class="token punctuation">.</span>work<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loop<span class="token operator">=</span>self<span class="token punctuation">.</span>loop<span class="token punctuation">)</span>
                <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_tasks<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">yield</span> <span class="token keyword">from</span> self<span class="token punctuation">.</span>q<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>下图是我对上述代码结构的理解：</p>
<p><img src="http://static.ddlee.cn/static/img/500lines_crawler/coroutine.JPG" alt="coroutines"></p>
<p>对coroutine的进一步介绍，参见<a href="/2017/04/03/Coroutine-Generator-Async与Await/">Coroutine-Generator-Async与Await</a>。</p>
<h3 id="A-Web-Crawler-With-asyncio-Coroutines导读"><a href="#A-Web-Crawler-With-asyncio-Coroutines导读" class="headerlink" title="A Web Crawler With asyncio Coroutines导读"></a><a href="http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html" target="_blank" rel="external">A Web Crawler With asyncio Coroutines</a>导读</h3><p>文章整体结构：</p>
<ul>
<li>分析爬虫任务</li>
<li><ul>
<li>传统方式：抢锁</li>
</ul>
</li>
<li><ul>
<li>异步方式的特点：无锁；单线程上同时运行多操作</li>
</ul>
</li>
<li>回调函数：fetch(),connecte(),read_response()的实现</li>
<li>Coroutine</li>
<li><ul>
<li>Generator的工作原理</li>
</ul>
</li>
<li><ul>
<li>用Generator实现Coroutine</li>
</ul>
</li>
<li>Asyncio库中的Coroutine</li>
<li><ul>
<li>crawl()</li>
</ul>
</li>
<li><ul>
<li>work()</li>
</ul>
</li>
<li><ul>
<li>fetch(), handle redirections</li>
</ul>
</li>
<li><ul>
<li>Queue()</li>
</ul>
</li>
<li><ul>
<li>EventLoop()</li>
</ul>
</li>
<li><ul>
<li>Task()</li>
</ul>
</li>
<li>Conclusion</li>
</ul>
<p>文章最后，作者点明了主题思想：</p>
<blockquote>
<p>Increasingly often, modern programs are I/O-bound instead of CPU-bound. For such programs, Python threads are the worst of both worlds: the global interpreter lock prevents them from actually executing computations in parallel, and preemptive switching makes them prone to races. Async is often the right pattern. But as callback-based async code grows, it tends to become a dishevelled mess. Coroutines are a tidy alternative. They factor naturally into subroutines, with sane exception handling and stack traces.</p>
</blockquote>
<p>大意是说，对I/O密集型的程序，Python多线程在两方面令人失望：全局锁的设定使之不能真正并行；抢占式多任务处理机制又让多个线程间形成竞争关系。异步通常是正确的选择。但持续增长的回调函数会使代码丧失可读性，Coroutine便是一种保持整洁性的替代方案。</p>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><p>这样说，Python的多线程效率带来的提高只是Python程序抢占了系统中非Python进程的资源（参考召集一波狐朋狗友帮你抢选修课），多个线程提高了Python作为一个整体在系统资源调配中的竞争力。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Programming </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Feedly+Reeder3+FeedMe:信息获取与处理]]></title>
      <url>http://blog.ddlee.cn/2017/04/03/Feedly-Reeder3-FeedMe-%E4%BF%A1%E6%81%AF%E8%8E%B7%E5%8F%96%E4%B8%8E%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<p>我蛮早就意识到自己被信息淹没了。于是关了票圈，屏蔽了空间，不装任何新闻APP，失去效力的微信群一概不留，知乎上的关注也缩减了很多很多。</p>
<blockquote>
<p>世界终于清静了。</p>
</blockquote>
<p>但仍然需要有关注的动向。我又捡起了RSS这个老朋友，建立起的信息获取跟处理流如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/Feedly_Reeder/Feedly+Reeder.png" alt="Feedly+Reeder3"></p>
<h3 id="服务与APP"><a href="#服务与APP" class="headerlink" title="服务与APP"></a>服务与APP</h3><p>主要涉及的服务：Feedly（免费，有高级版）</p>
<p>IOS APP：</p>
<ul>
<li>Reeder3（￥30）</li>
<li>Pocket（免费）</li>
<li>Pushbullet（免费）</li>
<li>Evernote（免费版限制客户端个数）</li>
</ul>
<p>Android APP:</p>
<ul>
<li>FeedMe（免费）</li>
<li>Pocket</li>
<li>Inbox</li>
<li>Evernote</li>
<li>Google Keep</li>
</ul>
<h3 id="获取：Feedly整合"><a href="#获取：Feedly整合" class="headerlink" title="获取：Feedly整合"></a>获取：Feedly整合</h3><p><a href="https://feedly.com/i/welcome" target="_blank" rel="external">Feedly</a>是著名的信息聚合服务，能从媒体RSS、博客、YouTube Chanel等拉取文章/动态，还提供Google关键词动态提醒服务。</p>
<p>这里先推荐两个Chrome插件，可以更方便地将网页端想要订阅的信息整合到Feedly中。</p>
<ul>
<li><a href="https://chrome.google.com/webstore/detail/save-to-feedly-board/hdhblphcdjcicefneapkhmleapfaocih?hl=en-US" target="_blank" rel="external">Follow Feed</a>: 识别跟当前网页内容相关的信息源，添加到Feedly订阅中。</li>
<li><a href="https://chrome.google.com/webstore/detail/save-to-feedly-board/hdhblphcdjcicefneapkhmleapfaocih?hl=en-US" target="_blank" rel="external">Save to Feedly Board</a>: 将当前网页添加到Feedly Board中，可以标记后分享给团队，实时更新。</li>
</ul>
<h4 id="信息源"><a href="#信息源" class="headerlink" title="信息源"></a>信息源</h4><h5 id="微信公众号"><a href="#微信公众号" class="headerlink" title="微信公众号"></a>微信公众号</h5><p>先直接在Feedly中搜索公众号，若找不到订阅源，则可通过<a href="http://www.iwgc.cn/" target="_blank" rel="external">微广场</a>等服务转成RSS。</p>
<h5 id="媒体-博客RSS源"><a href="#媒体-博客RSS源" class="headerlink" title="媒体/博客RSS源"></a>媒体/博客RSS源</h5><p>很多在线媒体会在主页提供RSS地址，也可直接在Feedly中搜索媒体名。</p>
<h5 id="知乎专栏"><a href="#知乎专栏" class="headerlink" title="知乎专栏"></a>知乎专栏</h5><p>有些<a href="https://rss.lilydjwg.me/" target="_blank" rel="external">工具</a>可以将知乎专栏转成RSS订阅源。</p>
<h5 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h5><p>Feedly支持设置关键词动态提醒。</p>
<h3 id="处理：Reeder3-FeedMe"><a href="#处理：Reeder3-FeedMe" class="headerlink" title="处理：Reeder3 + FeedMe"></a>处理：Reeder3 + FeedMe</h3><p>支持Feedly的APP实在太多，<a href="https://feedly.com/apps.html" target="_blank" rel="external">这里</a>是官方给出的列表，可看脸挑选。</p>
<p>我平常同时用Android和IOS处理订阅的信息，大屏精读，小屏浏览。</p>
<h4 id="IOS-Reeder3"><a href="#IOS-Reeder3" class="headerlink" title="IOS: Reeder3"></a>IOS: Reeder3</h4><p>不幸的是Reeder3是要付费的， 30块，几乎没有降过价。</p>
<p>替代品可以考虑自家的Feedly，Ziner等，可以参考<a href="http://www.makeuseof.com/tag/5-best-ipad-rss-readers/" target="_blank" rel="external">这篇文章</a>对比的结果选择。</p>
<h4 id="Android-FeedMe"><a href="#Android-FeedMe" class="headerlink" title="Android: FeedMe"></a>Android: FeedMe</h4><p>这里强推FeedMe(<a href="https://play.google.com/store/apps/details?id=com.seazon.feedme&amp;hl=en" target="_blank" rel="external">Google Play</a>)，抓取、缓存迅速，界面简洁，还有“中国大陆”模式。</p>
<h3 id="消化：Pocket-Inbox-Evernote-Keep"><a href="#消化：Pocket-Inbox-Evernote-Keep" class="headerlink" title="消化：Pocket, Inbox, Evernote/Keep"></a>消化：Pocket, Inbox, Evernote/Keep</h3><p>我个人将信息处理的结果分为三类：</p>
<ul>
<li>Read Later: 没消化</li>
<li>Links to save: 还想接着吃</li>
<li>Favorite:　想学着做</li>
</ul>
<p>稍后再读用Pocket，接口丰富，功能专一（尽管也有了“发现”模块）。</p>
<p>文章中挂的一些外链，移动端不好处理，要发往PC，手机端存在Inbox中，当临时的标签栏，iPad端用Pushbullet发给Chrome，下次打开Chrome时浏览处理。</p>
<p>收藏的文章存到Evernote，打好tags，长篇干货/可反复参考的转到OneNote。</p>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>其他情境下遇到的好文章、信息等尽量文字存到Google Keep，链接存到Inbox，或者给自己写封邮件。</p>
<p>微信的Favorite尽量不用，收藏的目的就在于情景分离，在不同的上下文中，我门信息获取的效率和质量区别实在太大了。详情参考拉微信群异地参加美赛的战友们。</p>
<p>最后推荐几个不错的订阅源(右击复制链接)：</p>
<ul>
<li><a href="http://www.sbnation.com/authors/mike-prada/rss" target="_blank" rel="external">SBNation上Mike Prada的文章</a>: 对NBA比赛、球队战术的分析</li>
<li><a href="http://rsarxiv.github.io/atom.xml" target="_blank" rel="external">Paper Weekly</a>: 机器学习方面的论文解读</li>
<li><a href="http://cos.name/feed/" target="_blank" rel="external">统计之都</a>: 统计学及应用、R语言方面的优秀内容</li>
<li><a href="http://github-trends.ryotarai.info/rss/github_trends_all_weekly.rss" target="_blank" rel="external">GitHub Trends</a></li>
<li><a href="https://blog.ddlee.cn/atom.xml">blog.ddlee.cn</a>: 大言不惭 -_-！</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Individual Development </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Individual Management </tag>
            
            <tag> RSS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[网站迁移小记：腾讯云+Debian+Vestacp]]></title>
      <url>http://blog.ddlee.cn/2017/04/02/%E7%BD%91%E7%AB%99%E8%BF%81%E7%A7%BB%E5%B0%8F%E8%AE%B0%EF%BC%9A%E8%85%BE%E8%AE%AF%E4%BA%91-Debian-Vestacp/</url>
      <content type="html"><![CDATA[<p>先贴一张文章大纲。</p>
<p><img src="http://static.ddlee.cn/static/img/Website-Migeration/structure.png" alt="structure"></p>
<p>这是一个樱花开得正好但我很蛋疼的下午。</p>
<p>中午抢到了腾讯的校园优惠，便打算把网站<code>ddlee.cn</code>迁到国内的服务器上来。</p>
<h3 id="密码管理"><a href="#密码管理" class="headerlink" title="密码管理"></a>密码管理</h3><p>先谈密码管理。</p>
<p>建站会涉及设置很多密码，之前明文保存在云笔记里的方案总觉得又土又笨，何况很多密码最好要随机生成，密码管理服务还是必要的。</p>
<p>搜索之后，我选择的是<a href="https://www.lastpass.com/2" target="_blank" rel="external">lastPass</a>。主要考虑了免费和跨平台的特性。有更高要求的建议选择付费的<a href="https://1password.com/" target="_blank" rel="external">1Password</a>。</p>
<p>需要安装Chrome插件和Ubuntu下的deb包，添加Secure Note的功能深得我心，也支持自定义模板。</p>
<h3 id="主机"><a href="#主机" class="headerlink" title="主机"></a>主机</h3><p>腾讯云的校园优惠力度很大。阿里云是9.9块/月，腾讯用完券1块/月。</p>
<p>这里多讲一句，学生真是幸福得不得了。GitHub Education Pack中既有有Digital Ocean的优惠，AWS也有150刀的礼品卡，Jetbeans大部分产品免费……这还不提学校里买的License。</p>
<p>腾讯的主机1核CPU，2G内存，20G系统盘（Linux），挂个网站还算够用。</p>
<h4 id="SSH-Key-配置"><a href="#SSH-Key-配置" class="headerlink" title="SSH Key 配置"></a>SSH Key 配置</h4><p>建议在配置主机前创建一个SSH Key，这样访问起来安全又省心。</p>
<p>Linux系统下，在<code>~/.ssh/</code>下新建<code>config</code>，写入如下类似内容：</p>
<pre><code>Host Name
  HostName Host_IP
  User root
  IdentityFile path/to/ssh_private_key
</code></pre><p>这样就可以通过命令<code>ssh Name</code>直接访问主机。</p>
<h4 id="系统选择"><a href="#系统选择" class="headerlink" title="系统选择"></a>系统选择</h4><p>建议选择Linux主机。具体哪一系可自行选择，我的选择是Debian，CentOS也是个不错的选择。</p>
<h4 id="安全组设置"><a href="#安全组设置" class="headerlink" title="安全组设置"></a>安全组设置</h4><p>建议先只开启用于SSH的22端口，之后再开HTTP访问的80端口，FTP的20,21端口和主机面板所用端口。</p>
<p>如果个人有代理服务器的话，也可以限制一下来源IP，这样可以通过登入代理服务器，在代理服务器上通过SSH登入WEB主机，需要迁移下SSH Private Key，可以通过命令<code>scp usr1@host1:/path1 usr2@host2:/path2</code>实现。</p>
<h4 id="网络环境"><a href="#网络环境" class="headerlink" title="网络环境"></a>网络环境</h4><p>LNMP和LAMP是两种流行的结构。可以分别安装，再配置相应的<code>config</code>，也可以搜索得到很多一键安装脚本。另一种方案是用Docker部署。</p>
<p>我懒而笨，选择的是用主机面板一键安装。</p>
<h4 id="主机面板"><a href="#主机面板" class="headerlink" title="主机面板"></a>主机面板</h4><p>在此之前，一直用的是AMH的免费4.2版本，简洁轻巧，功能也够用。付费版推出后，免费版遭到冷落，几乎没有更新，这如何能忍。</p>
<p>说起主机面板，我的启蒙是WDCP，其远古风格的UI仍历历在目，后来听说爆出漏洞，但那时我已转战AMH。</p>
<p>一番艰苦卓绝的搜索之后（其实就是检索了’best host control pannel’），我选择了<a href="https://vestacp.com/" target="_blank" rel="external">Vestacp</a>。</p>
<p>UI漂亮，功能不缺（建站，MAIL，备份），GitHub还算活跃，就决定是你了。</p>
<p>缺点是文件管理器收费，不能通过WEB管理文件。安装过程持续蛮久（半个小时，当然也包括了新主机系统包更新的时间）。</p>
<p>安装时注意Hostname填写IP或者已经配置好DNS解析的域名（如<code>admin.ddlee.cn</code>）。8083是管理面板的端口，记得在主机提供商的安全组里开放一下。</p>
<h5 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h5><p>Vestacp支持多人管理，User身份由Package定义，安装过程会自动新建admin，拥有最高权限。</p>
<p>在User的设置里，可以配置用户的Package，而Package的设置里，可以配置每一用户身份的建站模板，资源上限等。如图。</p>
<p><img src="http://static.ddlee.cn/static/img/Website-Migeration/Package.png" alt="Package"></p>
<p>建站相当容易，注意在高级选项里添加FTP账户，用于之后上传HTML文件。</p>
<h5 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h5><p>建站完成后，记得配置好DNS解析，开放20和21端口，就可以用FileZilla测试链接。</p>
<p>注意，在高级选项里配置好Default local directory，设置Default remote directory为<code>/public_html</code>，并启用synchronized browsing和directory comparison，以后的FTP生活会很幸福。</p>
<h5 id="Mail"><a href="#Mail" class="headerlink" title="Mail"></a>Mail</h5><p>若在建站时勾选了Mail support，可以建立个性化的邮箱名，可以设置自动回复/转发，也可以用Gmail托管。以后留邮箱的时候可以短短的了呢。</p>
<h5 id="配置SSL"><a href="#配置SSL" class="headerlink" title="配置SSL"></a>配置SSL</h5><p>这是无意发现的技能。</p>
<p>本来在我的印象里，SSL证书都是要收费的。但留心的朋友可能注意到，建站时SSL support下有Lets Encrypt Support。这一服务可以用上免费的SSL。</p>
<p>官网：<a href="https://letsencrypt.org/" target="_blank" rel="external">Let’s Encrypt</a></p>
<p>要利用这项服务，需要证明自己对网站的至高无上不可侵犯的神圣权利，方法之一是运行支持[ACME protocol]的Client，官网推荐了<a href="https://certbot.eff.org/" target="_blank" rel="external">Cerbot</a>。</p>
<p>在Cerbot主页可以选择自己的操作系统，会有详细的步骤，在此不表。</p>
<p>下面谈两个问题，一是强制重定向至HTTPS，二是取消管理端口的HTTPS。</p>
<h6 id="强制HTTPS"><a href="#强制HTTPS" class="headerlink" title="强制HTTPS"></a>强制HTTPS</h6><p>Vestacp的架构是用nginx做proxy，Apache2做HTTP Server，首先下载nginx template（proxy template）：</p>
<pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cd</span> /usr/local/vesta/data/templates/web
<span class="token function">wget</span> http://c.vestacp.com/0.9.8/rhel/force-https/nginx.tar.gz
<span class="token function">tar</span> -xzvf nginx.tar.gz
<span class="token function">rm</span> -f nginx.tar.gz
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>之后在Package配置里，将proxy template配置为force-https，这样，身份由相应Package定义的用户建站时，proxy template就是用的强制HTTPS版本了。</p>
<h6 id="取消管理端口的SSL"><a href="#取消管理端口的SSL" class="headerlink" title="取消管理端口的SSL"></a>取消管理端口的SSL</h6><p>用chrome访问管理页面时，会有Unsecure的警告，这里的SSL在<code>/usr/local/vesta/nginx/conf/nginx.conf</code>中配置。找到</p>
<pre><code># Vhost
server {
    listen          8083;
    server_name     _;
    root            /usr/local/vesta/web;
    charset         utf-8;

    # Fix error &quot;The plain HTTP request was sent to HTTPS port&quot;
    error_page      497 https://$host:$server_port$request_uri;

    # ssl                  on;
    # ssl_certificate      /usr/local/vesta/ssl/certificate.crt;
    # ssl_certificate_key  /usr/local/vesta/ssl/certificate.key;
    # ssl_session_cache    shared:SSL:10m;
    # ssl_session_timeout  10m;
</code></pre><p>将配置SSL的几行注释掉即可。顺便，管理页面的端口也可以在这里更改。之后运行<code>service vesta restart</code>重启服务。</p>
<h3 id="域名与DNS"><a href="#域名与DNS" class="headerlink" title="域名与DNS"></a>域名与DNS</h3><p>最后简单提一下域名注册跟DNS。要注意的几个点：</p>
<ul>
<li>国内域名注册要备案，很烦，但cn域名好便宜。</li>
<li>在域名注册商那里配置DNS解析服务器（万网、DNSPod都好，不一定用自建网站的DNS）</li>
<li>在DNS服务商那里添加解析记录，顺便开启监控</li>
</ul>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>域名备案的时候，需要签一张备案单。方案是在纸上签字后调背景为透明，用Adobe PDF Reader的签字功能签好PDF，再转成JPG。</p>
<p>几项操作都可以通过在线工具完成，低碳生活，人人有责。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>建站过程本身就其乐无穷，教程一抓一大把，难的在TROUBLE SHOOTING，所以Google是最好的伴侣。</p>
</li>
<li><p>命令行、vi编辑、必要的WEB知识等是基础，在此感谢我们的墙两年前就教给我这些东西。</p>
</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 云主机 </tag>
            
            <tag> Vestacp </tag>
            
            <tag> 博客迁移 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[The Devtools Way: devtools+RStudio快速R程序包开发]]></title>
      <url>http://blog.ddlee.cn/2017/03/31/The-Devtools-Way-devtools-RStudio%E5%BF%AB%E9%80%9FR%E7%A8%8B%E5%BA%8F%E5%8C%85%E5%BC%80%E5%8F%91/</url>
      <content type="html"><![CDATA[<p>本文记录我的首个R程序包<a href="https://ddlee.cn/projects/mcmi.html" target="_blank" rel="external">MCMI</a>的开发过程。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol>
<li>两本Hadley Wickham写的书：<a href="http://adv-r.had.co.nz/" target="_blank" rel="external">Advanced R</a>和<a href="http://r-pkgs.had.co.nz/" target="_blank" rel="external">R Packages</a>。</li>
<li>Coursera上的课程<a href="https://www.coursera.org/specializations/r" target="_blank" rel="external">Mastering Software Development in R Specialization</a>和配套教材<a href="https://bookdown.org/rdpeng/RProgDA/" target="_blank" rel="external">Mastering Software Development in R</a>。</li>
</ol>
<p>感谢开源社区，以上的资料都可以免费获取得到（Coursera课程可以申请补助）。</p>
<h3 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h3><p>开发R包还需要系统中存在编译工具，编译文档需要LaTeX支持。</p>
<ul>
<li>Linux用户：<code>sudo apt-get install r-base-dev texlive-full</code></li>
<li>Windows用户：1.<a href="https://cran.rstudio.com/bin/windows/Rtools/" target="_blank" rel="external">RTools</a>;2.<a href="http://miktex.org/download" target="_blank" rel="external">MikTeX</a></li>
</ul>
<p>另外，请确保以下两个包已安装于系统中：<code>devtools</code>和<code>roxygen2</code>。推荐使用RStudio。</p>
<h3 id="Get-Started"><a href="#Get-Started" class="headerlink" title="Get Started"></a>Get Started</h3><p>在RStudio中新建项目，选择R程序包类型即可。建议同时建立Git路径以监控开发流程。</p>
<p>在编写代码之前，先要修改DESCRIPTION文件，要注意的几个地方：</p>
<ol>
<li>Package的命名要容易记忆和查询</li>
<li>Depends指你所用开发环境的R版本</li>
<li>慎重选择License</li>
<li>Imports指你所要调用的其他包，但在代码中，也要明确指出函数所处的包，如<code>ggplot2::qplot()</code></li>
</ol>
<h3 id="Git-Workflow"><a href="#Git-Workflow" class="headerlink" title="Git Workflow"></a>Git Workflow</h3><p>建议在GitHub上为本机申请SSH密钥，并在RStudo-&gt;Tools-&gt;Global Options-&gt;Git/SVN配置好路径，这样在执行<code>git push</code>时不用再次输入凭据。下面是有关Git的工作流：</p>
<ol>
<li>修改代码/文档</li>
<li>编译，测试</li>
<li>git commit</li>
<li>git push</li>
<li>重复以上循环</li>
</ol>
<p>RStudio对Git的集成很好，以上三四步操作均可在Git的操作面板里完成。</p>
<h3 id="Code-Workflow"><a href="#Code-Workflow" class="headerlink" title="Code Workflow"></a>Code Workflow</h3><ol>
<li>修改代码</li>
<li>Build&amp;Reload</li>
<li>用命令行测试功能</li>
<li>重复以上循环</li>
</ol>
<p>上述第二步可以在命令行中<code>devtools::load_all()</code>完成，也可以使用快捷键”Ctrl + Shift + L”，也可以在RStudio的开发面板中执行”Build&amp;Reload”命令。之后，便可在命令行中调用编写好的函数，验证其功能。</p>
<h3 id="Documentation-Workflow"><a href="#Documentation-Workflow" class="headerlink" title="Documentation Workflow"></a>Documentation Workflow</h3><ol>
<li>在.R文件中添加roxygen注释</li>
<li>Document</li>
<li>使用help面板或?命令预览文档</li>
<li>重复以上循环</li>
</ol>
<p>同样地，第二步有三种实现方式：1.<code>devtools::document()</code>;2.”Ctrl + Shift + D”；3. RStudio开发面板中的”Document”命令。</p>
<h3 id="Test-Workflow"><a href="#Test-Workflow" class="headerlink" title="Test Workflow"></a>Test Workflow</h3><p>测试方面，除了上述Coding Workflow中提到的在命令行中调用函数进行测试之外，还可以利用<code>testthat</code>包来使测试自动化。</p>
<p>首先要安装<code>testthat</code>包，再使用<code>devtools::ust_testthat()</code>命令建立<code>testthat</code>路径。</p>
<p>下面是自动化测试的工作流：</p>
<ol>
<li>修改代码</li>
<li>在<code>testthat</code>路径下编写相应的测试语句</li>
<li>Build&amp;Reload</li>
<li>Test</li>
<li>排除Bug，重复上述过程</li>
</ol>
<p>以上流程第四步同样有三种实现方式：1.<code>devtools::test()</code>;2.”Ctrl + Shift + T”;3.RStudio中开发面板的“Test”命令。</p>
<h3 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h3><p>按照上述过程开发的R程序包，每一次<code>git push</code>事实上都是一次发布。使用<code>devtools::install_github(&quot;git_repo_goest_here&quot;)</code>命令，可以很方便地安装R程序包。</p>
<h3 id="Next-Step"><a href="#Next-Step" class="headerlink" title="Next Step"></a>Next Step</h3><p>使用<code>devtools</code>配合RStudio和Git，开发R包的过程已经非常亲民和流水线化。但要开发高质量的R包，需要对R的数据结构和S3等对象系统有更深的理解，而Advanced R则是你通往这一方向的最好伴侣。</p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Data Science </tag>
            
            <tag> Programming </tag>
            
            <tag> R </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HADOOP学习速记]]></title>
      <url>http://blog.ddlee.cn/2017/03/30/HADOOP%E5%AD%A6%E4%B9%A0%E9%80%9F%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h4 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS: 分布式文件系统"></a>HDFS: 分布式文件系统</h4><p>NameNode, DataNode: a MetaData-Data Model<br>Strategy: Block split, multi-copy, distribution</p>
<p>NameNode: High Availability<br>solution 1: backup using NFS<br>solution 2: Two NameNodes(Active and Standby)</p>
<h4 id="MapReduce-计算框架"><a href="#MapReduce-计算框架" class="headerlink" title="MapReduce: 计算框架"></a>MapReduce: 计算框架</h4><p>split -&gt; Process -&gt; aggregate</p>
<p>Deamon: Job Tracker &amp; Task Tracker</p>
<h4 id="Design-Pattern"><a href="#Design-Pattern" class="headerlink" title="Design Pattern"></a>Design Pattern</h4><h3 id="Course-Project"><a href="#Course-Project" class="headerlink" title="Course Project"></a>Course Project</h3><p>（未完待续）<br>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> AI </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分布式计算 </tag>
            
            <tag> Data Sicence </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle比赛中EDA：流程、做法与目的]]></title>
      <url>http://blog.ddlee.cn/2017/03/26/Kaggle%E6%AF%94%E8%B5%9B%E4%B8%ADEDA%EF%BC%9A%E6%B5%81%E7%A8%8B%E3%80%81%E5%81%9A%E6%B3%95%E4%B8%8E%E7%9B%AE%E7%9A%84/</url>
      <content type="html"><![CDATA[<h4 id="数据集大小、字段及相应的数据类型"><a href="#数据集大小、字段及相应的数据类型" class="headerlink" title="数据集大小、字段及相应的数据类型"></a>数据集大小、字段及相应的数据类型</h4><ul>
<li>大小：占用内存估计</li>
<li>字段数：维度估计，是否需要降维</li>
<li>数据类型：numerical, factor, string, etc. 是否需要归一化，二元化等等</li>
</ul>
<h4 id="了解数据的缺失值情况及分布"><a href="#了解数据的缺失值情况及分布" class="headerlink" title="了解数据的缺失值情况及分布"></a>了解数据的缺失值情况及分布</h4><h4 id="了解数据分布情况，可用众多图形完成"><a href="#了解数据分布情况，可用众多图形完成" class="headerlink" title="了解数据分布情况，可用众多图形完成"></a>了解数据分布情况，可用众多图形完成</h4><ul>
<li>bar plot</li>
<li>histogram</li>
<li>violin plot</li>
<li>box plot</li>
<li>scatter plot</li>
</ul>
<h5 id="主要目的："><a href="#主要目的：" class="headerlink" title="主要目的："></a>主要目的：</h5><ol>
<li>了解整体状况，是否具有野点</li>
<li>结合目标变量，考察特征与目标变量间的相关性</li>
</ol>
<h5 id="文本数据常用的探索："><a href="#文本数据常用的探索：" class="headerlink" title="文本数据常用的探索："></a>文本数据常用的探索：</h5><ul>
<li>词频统计（消除stopwords之后）</li>
<li>词云</li>
</ul>
<h4 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h4><p>本来文章是从几个经典的EDA notebook开始，试图总结出其共性之处，但写来写去，总觉得随便一本跟数据分析相关的书中，探索性数据分析的章节也大概都会涉及到这些内容，但在读书的情景之下又难留下深刻的印象，做分析的真正见地与经验，还是要从实践中来啊。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> EDA </tag>
            
            <tag> Data Sciencce </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Udacity课程： Intro to DevOps侧记]]></title>
      <url>http://blog.ddlee.cn/2017/03/22/DevOps%E4%BE%A7%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>文章主要内容来自Udacity的课程：<a href="https://www.udacity.com/course/intro-to-devops--ud611" target="_blank" rel="external">Intro to DevOps</a></p>
<h3 id="CAMS-The-DevOps-Lifecycle"><a href="#CAMS-The-DevOps-Lifecycle" class="headerlink" title="CAMS: The DevOps Lifecycle"></a>CAMS: The DevOps Lifecycle</h3><p>The Purpose of DevOps： 产品、开发、运维之间的协作问题</p>
<h4 id="Definition-from-wiki"><a href="#Definition-from-wiki" class="headerlink" title="Definition(from wiki)"></a>Definition(from wiki)</h4><blockquote>
<p>a set of practices that emphasize the collaboration and communication of both software developers and information technology (IT) professionals while automating the process of software delivery and infrastructure changes.</p>
</blockquote>
<p><img src="http://static.ddlee.cn/static/img/DevOps/Devops_venn.png" alt="DevOps_Venn"><br>(<a href="https://commons.wikimedia.org/wiki/File:Devops.png" target="_blank" rel="external">Source</a>)</p>
<p>对比： Agile Development（敏捷软件开发）</p>
<p>Plan-&gt;Code-&gt;Test-&gt;Release-&gt;Deploy-&gt;Operate</p>
<h4 id="Means-of-CAMS"><a href="#Means-of-CAMS" class="headerlink" title="Means of CAMS"></a>Means of CAMS</h4><ul>
<li>C: Culture</li>
<li>A: Automation</li>
<li>M: Measurement</li>
<li>S: Sharing</li>
</ul>
<h3 id="The-DevOps-Environment"><a href="#The-DevOps-Environment" class="headerlink" title="The DevOps Environment"></a>The DevOps Environment</h3><p>Solving the Environment Problem:</p>
<ul>
<li>Golden Image: apps-libs-OS</li>
<li>Configuration Management</li>
</ul>
<h4 id="Course-Project（使用Golden-Image方案，引入一批DevOps工具"><a href="#Course-Project（使用Golden-Image方案，引入一批DevOps工具" class="headerlink" title="Course Project（使用Golden Image方案，引入一批DevOps工具)"></a>Course Project（使用Golden Image方案，引入一批DevOps工具)</h4><p>dependencies-&gt;build scripts-&gt;tests-&gt;web apps</p>
<h5 id="Packer"><a href="#Packer" class="headerlink" title="Packer"></a>Packer</h5><p>Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.</p>
<ul>
<li><code>Artifacts</code> are the results of a single build, and are usually a set of IDs or files to represent a machine image.</li>
<li><code>Builds</code> are a single task that eventually produces an image for a single platform.</li>
<li><code>Builders</code> are components of Packer that are able to create a machine image for a single platform.</li>
<li><code>Commands</code> are sub-commands for the packer program that perform some job.</li>
<li><code>Post-processors</code> are components of Packer that take the result of a builder or another post-processor and process that to create a new artifact.</li>
<li><code>Provisioners</code> are components of Packer that install and configure software within a running machine prior to that machine being turned into a static image.</li>
<li><code>Templates</code> are JSON files which define one or more builds by configuring the various components of Packer.</li>
</ul>
<p><a href="https://github.com/ddlee96/devops-intro-project/blob/master/packer-templates/application-server.json" target="_blank" rel="external">Example JSON File</a></p>
<h5 id="Vagrant"><a href="#Vagrant" class="headerlink" title="Vagrant"></a>Vagrant</h5><p>Vagrant is a tool for building complete development environments. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases development/production parity, and makes the “works on my machine” excuse a relic of the past.</p>
<h5 id="Project-Workflow-Packer-gt-Vagrant-gt-Virtualbox-gt-Web-Application"><a href="#Project-Workflow-Packer-gt-Vagrant-gt-Virtualbox-gt-Web-Application" class="headerlink" title="Project Workflow: Packer-&gt;Vagrant-&gt;Virtualbox-&gt;Web Application"></a>Project Workflow: Packer-&gt;Vagrant-&gt;Virtualbox-&gt;Web Application</h5><h6 id="Part-I-Building-a-box-with-Packer"><a href="#Part-I-Building-a-box-with-Packer" class="headerlink" title="Part I: Building a box with Packer"></a>Part I: Building a box with Packer</h6><p>From the packer-templates directory on your local machine:</p>
<ul>
<li><p>Run <code>packer build -only=virtualbox-iso application-server.json</code></p>
<p>Troubleshooting: <em>Find the newest version number and checksum from the <a href="http://releases.ubuntu.com/trusty/" target="_blank" rel="external">Ubuntu website for this release</a></em><br><em>Edit <code>PACKER_BOX_NAME</code> and <code>iso_checksum</code> in the template files to match that version number and checksum.</em></p>
</li>
<li>Run <code>cd virtualbox</code></li>
<li>Run <code>vagrant box add ubuntu-14.04.4-server-amd64-appserver_virtualbox.box --name devops-appserver</code></li>
<li>Run <code>vagrant up</code></li>
<li>Run <code>vagrant ssh</code> to connect to the server</li>
</ul>
<h6 id="Part-II-Cloning-developing-and-running-the-web-application"><a href="#Part-II-Cloning-developing-and-running-the-web-application" class="headerlink" title="Part II: Cloning, developing, and running the web application"></a>Part II: Cloning, developing, and running the web application</h6><ul>
<li>On your local machine go to the root directory of the cloned repository</li>
<li>Run <code>git clone https://github.com/chef/devops-kungfu.git devops-kungfu</code></li>
<li>Open <a href="http://localhost:8080" target="_blank" rel="external">http://localhost:8080</a> from your local machine to see the app running.</li>
<li>In the VM, run <code>cd devops-kungfu</code></li>
<li>To install app specific node packages, run <code>sudo npm install</code>. You may see several errors; they can be ignored for now.</li>
<li>Now you can run tests with the command <code>grunt -v</code>. The tests will run, then quit with an error.</li>
</ul>
<h6 id="On-Cloud-platform"><a href="#On-Cloud-platform" class="headerlink" title="On Cloud platform"></a>On Cloud platform</h6><p>Similar commands using packer:</p>
<ul>
<li><code>packer build -only=amazon-ebs &lt;server-name&gt;.json</code></li>
<li><code>packer build -only=googlecompute application-server.json</code></li>
</ul>
<h3 id="Continuous-Integration（持续集成）"><a href="#Continuous-Integration（持续集成）" class="headerlink" title="Continuous Integration（持续集成）"></a>Continuous Integration（持续集成）</h3><p>CI System:</p>
<p><img src="http://static.ddlee.cn/static/img/DevOps/CI.png" alt="CI"></p>
<h4 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h4><p>Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks such as building, testing, and deploying software.</p>
<p>Using command:<br><code>packer build -only=&lt;cloud service target&gt; control-server.json</code></p>
<p><a href="https://github.com/ddlee96/devops-intro-project/blob/master/packer-templates/control-server.json" target="_blank" rel="external">Example control-server.json file</a></p>
<p>Jenkins was configured to be installed according to <code>Provisioners</code>.</p>
<p>After building and launching, access Jenkins via URL <code>/jenkins</code>.</p>
<h4 id="Testing（测试，QA）"><a href="#Testing（测试，QA）" class="headerlink" title="Testing（测试，QA）"></a>Testing（测试，QA）</h4><ul>
<li>Unit Testing</li>
<li>Regression testing</li>
<li>Smoke testing</li>
<li>System Integration testing</li>
<li>Automate acceptance testing</li>
<li>Manual QA testing</li>
</ul>
<p>Adding Manual QA step in Pipeline</p>
<p><img src="http://static.ddlee.cn/static/img/DevOps/pipline.png" alt="Pipeline"></p>
<h4 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h4><p>Monitoring process:</p>
<p><img src="http://static.ddlee.cn/static/img/DevOps/Monitoring.png" alt="Monitoring"></p>
<h3 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h3><p><a href="https://www.udacity.com/wiki/ud611#!#additional-resources" target="_blank" rel="external">Course Wiki</a></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>DevOps从开发和运维合作的角度审视软件开发过程，并提供了一套方法论，涉及开发、测试、部署、维护、监测各个方面。软件行业，不仅仅是写代码而已。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Internet </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> 技术 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python与SQL_Server的交互：pyODBC, pymssql, SQLAlchemy]]></title>
      <url>http://blog.ddlee.cn/2017/03/16/Python%E4%B8%8ESQL_Server%E7%9A%84%E4%BA%A4%E4%BA%92%EF%BC%9ApyODBC,%20pymssql,%20SQLAlchemy/</url>
      <content type="html"><![CDATA[<p>Windows平台下Python读取、写入SQL Server相关的函数库，文章结构如下：</p>
<p><img src="http://static.ddlee.cn/static/img/Python+SQLserver/Python+SQL_Server.png" alt="Python+SQLserver"></p>
<h3 id="Python-Drivers"><a href="#Python-Drivers" class="headerlink" title="Python Drivers"></a>Python Drivers</h3><h4 id="PyODBC"><a href="#PyODBC" class="headerlink" title="PyODBC"></a>PyODBC</h4><p>Annaconda下可以用<code>pip install pyodbc</code>安装，也可以到<a href="https://www.microsoft.com/en-us/download/details.aspx?id=50420" target="_blank" rel="external">这里</a>下载。</p>
<p>首先建立<code>connection</code>对象：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pyodbc
conn <span class="token operator">=</span> pyodbc<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>
    r<span class="token string">'DRIVER={ODBC Driver 11 for SQL Server};'</span>  <span class="token comment" spellcheck="true">#or {ODBC Driver 13 for SQL Server}</span>
    r<span class="token string">'SERVER=ServerHostName;'</span>
    r<span class="token string">'DATABASE=DBName;'</span>
    r<span class="token string">'UID=user;'</span>
    r<span class="token string">'PWD=password'</span>
    <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>添加游标（Cursor）对象并执行SQL查询语句：</p>
<pre class="line-numbers language-python"><code class="language-python">cursor <span class="token operator">=</span> conn<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>
cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'SQL Query Goes Here'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> row <span class="token keyword">in</span> cursor<span class="token punctuation">.</span>fetchall<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span>rows<span class="token punctuation">.</span><span class="token punctuation">[</span>column name<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>更多信息参见<a href="https://docs.microsoft.com/en-us/sql/connect/python/pyodbc/python-sql-driver-pyodbc" target="_blank" rel="external">MSDN DOCs</a>。</p>
<h4 id="pymssql"><a href="#pymssql" class="headerlink" title="pymssql"></a>pymssql</h4><p>同样可以用<code>pip install pymssql</code>安装，也可以到<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#pymssql" target="_blank" rel="external">这里</a>，然后用<code>pip</code>安装<code>wheel</code>文件。</p>
<p>pymssql目前还不支持Python3.6，这点要注意下。</p>
<p>pymssql的用法跟pyODBC很像，下面是官网给出的例子：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> os <span class="token keyword">import</span> getenv
<span class="token keyword">import</span> pymssql

server <span class="token operator">=</span> getenv<span class="token punctuation">(</span><span class="token string">"PYMSSQL_TEST_SERVER"</span><span class="token punctuation">)</span>
user <span class="token operator">=</span> getenv<span class="token punctuation">(</span><span class="token string">"PYMSSQL_TEST_USERNAME"</span><span class="token punctuation">)</span>
password <span class="token operator">=</span> getenv<span class="token punctuation">(</span><span class="token string">"PYMSSQL_TEST_PASSWORD"</span><span class="token punctuation">)</span>

conn <span class="token operator">=</span> pymssql<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>server<span class="token punctuation">,</span> user<span class="token punctuation">,</span> password<span class="token punctuation">,</span> <span class="token string">"tempdb"</span><span class="token punctuation">)</span>
cursor <span class="token operator">=</span> conn<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>
cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token triple-quoted-string string">"""
IF OBJECT_ID('persons', 'U') IS NOT NULL
    DROP TABLE persons
CREATE TABLE persons (
    id INT NOT NULL,
    name VARCHAR(100),
    salesrep VARCHAR(100),
    PRIMARY KEY(id)
)
"""</span><span class="token punctuation">)</span>
cursor<span class="token punctuation">.</span>executemany<span class="token punctuation">(</span>
    <span class="token string">"INSERT INTO persons VALUES (%d, %s, %s)"</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'John Smith'</span><span class="token punctuation">,</span> <span class="token string">'John Doe'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'Jane Doe'</span><span class="token punctuation">,</span> <span class="token string">'Joe Dog'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'Mike T.'</span><span class="token punctuation">,</span> <span class="token string">'Sarah H.'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># you must call commit() to persist your data if you don't set autocommit to True</span>
conn<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span>

cursor<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'SELECT * FROM persons WHERE salesrep=%s'</span><span class="token punctuation">,</span> <span class="token string">'John Doe'</span><span class="token punctuation">)</span>
row <span class="token operator">=</span> cursor<span class="token punctuation">.</span>fetchone<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">while</span> row<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"ID=%d, Name=%s"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>row<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    row <span class="token operator">=</span> cursor<span class="token punctuation">.</span>fetchone<span class="token punctuation">(</span><span class="token punctuation">)</span>

conn<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>详细用法参见<a href="http://www.pymssql.org/en/stable/index.html" target="_blank" rel="external">pymssql docs</a>和<a href="https://docs.microsoft.com/en-us/sql/connect/python/pymssql/python-sql-driver-pymssql" target="_blank" rel="external">MSDN DOCs</a></p>
<h4 id="SQLAlchemy-Python-SQL-Toolkit"><a href="#SQLAlchemy-Python-SQL-Toolkit" class="headerlink" title="SQLAlchemy(Python SQL Toolkit)"></a><a href="https://www.sqlalchemy.org/" target="_blank" rel="external">SQLAlchemy</a>(Python SQL Toolkit)</h4><p>SQLAlchemy提供了一系列丰富、完整、（我看不懂）的API用于数据库操作。这里只谈其<code>create_engine</code>方法。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sqlalchemy <span class="token keyword">import</span> create_engine
<span class="token comment" spellcheck="true"># pyodbc</span>
engine <span class="token operator">=</span> create_engine<span class="token punctuation">(</span><span class="token string">'mssql+pyodbc://user:password@DSNname'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#需要配置DSN，参见最后一节</span>

<span class="token comment" spellcheck="true"># pymssql</span>
engine <span class="token operator">=</span> create_engine<span class="token punctuation">(</span><span class="token string">'mssql+pymssql://user:password@Hostname:port/DBname'</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>利用创建好的<code>engine</code>，可以结合pandas库进行批量的读取、写入操作。</p>
<p>用SQLAlchemy与其他类型的数据库建立链接的方法参见<a href="http://docs.sqlalchemy.org/en/latest/core/engines.html" target="_blank" rel="external">这里</a>。</p>
<h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><p>利用pyODBC和pymssql拉取的对象需要进一步处理才能进行常见的数据清洗等工作，而Pandas也提供了SQL相关的方法，在SQLAlchemy的辅助下，可以将<code>DataFrame</code>对象直接写入table。</p>
<h5 id="读取：pd-read-sql"><a href="#读取：pd-read-sql" class="headerlink" title="读取：pd.read_sql()"></a>读取：pd.read_sql()</h5><p><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html" target="_blank" rel="external">API</a>：</p>
<pre class="line-numbers language-python"><code class="language-python">pandas<span class="token punctuation">.</span>read_sql<span class="token punctuation">(</span>sql<span class="token punctuation">,</span> con<span class="token punctuation">,</span> index_col<span class="token operator">=</span>None<span class="token punctuation">,</span> coerce_float<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> params<span class="token operator">=</span>None<span class="token punctuation">,</span> parse_dates<span class="token operator">=</span>None<span class="token punctuation">,</span> columns<span class="token operator">=</span>None<span class="token punctuation">,</span> chunksize<span class="token operator">=</span>None<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>其中的<code>con</code>参数，可以传入SQLAlchemy建立的<code>engine</code>对象，也可以是pyODBC或者pymssql建立的<code>DBAPI2 connection</code>对象。</p>
<h5 id="写入-pd-DataFrame-to-sql"><a href="#写入-pd-DataFrame-to-sql" class="headerlink" title="写入:pd.DataFrame.to_sql()"></a>写入:pd.DataFrame.to_sql()</h5><p><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html" target="_blank" rel="external">API</a>:</p>
<pre class="line-numbers language-python"><code class="language-python">DataFrame<span class="token punctuation">.</span>to_sql<span class="token punctuation">(</span>name<span class="token punctuation">,</span> con<span class="token punctuation">,</span> flavor<span class="token operator">=</span>None<span class="token punctuation">,</span> schema<span class="token operator">=</span>None<span class="token punctuation">,</span> if_exists<span class="token operator">=</span><span class="token string">'fail'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> index_label<span class="token operator">=</span>None<span class="token punctuation">,</span> chunksize<span class="token operator">=</span>None<span class="token punctuation">,</span> dtype<span class="token operator">=</span>None<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>这里的<code>con</code>参数，只支持sqlite3的<code>DBAPI2 connection</code>对象，支持所有的<code>SQLAlchemy engine</code>对象。<br><code>name</code>参数传入表名，用<code>if_exists</code>参数控制表存在时的动作：</p>
<ul>
<li><code>‘fail’</code>: 啥也不干。</li>
<li><code>’replace‘</code>: 将原有表删除，新建表，插入数据。</li>
<li><code>’append&#39;</code>: 在表中插入数据。表不存在时新建表。</li>
</ul>
<h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p>利用<code>Sqlcmd</code>命令，也可以在命令行下执行SQL文件，用法如下：</p>
<pre class="line-numbers language-bash"><code class="language-bash">sqlcmd -U user -P password -S server -d DBName -i /path/to/myScript.sql
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>这样可以有如下思路，将数据写入.SQL文件，再生成.bat文件（批量）写入上述命令，之后完成执行。</p>
<h3 id="DSN"><a href="#DSN" class="headerlink" title="DSN"></a>DSN</h3><p>Windows下可以配置DSN(Data Source Names)预先存储数据库连接的信息，在<em>Control Panel</em> -&gt; <em>Administrative Tools</em> -&gt; <em>ODBC Data Source</em> 下添加即可。</p>
<p>配置好DSN后，pyODBC的连接过程可以简化为：</p>
<pre class="line-numbers language-python"><code class="language-python">conn <span class="token operator">=</span> pyodbc<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>r<span class="token string">'DSN=DSNname;UID=user;PWD=password'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#UID和PWD也可以在DSN中配置</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>Python与文件的IO、SQL数据库的读写时有中文字符可能会有编码问题。一种方案是在中文字符串前添加N，如<code>N&#39;python大法好&#39;</code>；另一种方案是传入<code>encoding</code>参数，常用的中文编码有<code>GB2123</code>，<code>GB18030</code>，推荐的还是统一用<code>UTF-8</code>编码、解码。</p>
<p>利用如下命令，可以在SQLAlchemy中指定编码：</p>
<pre class="line-numbers language-python"><code class="language-python">engine <span class="token operator">=</span> create_engine<span class="token punctuation">(</span><span class="token string">'mssql+pymssql://user:password@HostName\DBname'</span><span class="token punctuation">,</span> connect_args <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'charset'</span><span class="token punctuation">:</span><span class="token string">'utf-8'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>其他自定义<code>DBAPI connect()</code>参数的方法参见<a href="http://docs.sqlalchemy.org/en/latest/core/engines.html#custom-dbapi-connect-arguments" target="_blank" rel="external">这里</a>。</p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> SQL </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[再次折腾我的WNDR4300：OpenWrt文件共享]]></title>
      <url>http://blog.ddlee.cn/2017/03/12/%E5%86%8D%E6%AC%A1%E6%8A%98%E8%85%BE%E6%88%91%E7%9A%84WNDR4300/</url>
      <content type="html"><![CDATA[<blockquote>
<p>生命不惜，折腾不止。</p>
</blockquote>
<h3 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h3><p>再次成为IOS用户后，访问Google和文件共享成了两大需求。问题出现了，就要解决，于是有此文记录的活动。</p>
<h3 id="重新安装OpenWrt"><a href="#重新安装OpenWrt" class="headerlink" title="重新安装OpenWrt"></a>重新安装OpenWrt</h3><p>OpenWrt已经到了<code>15.05</code>版本，版本代号是<code>Chaos Calmer</code>。重装需要的<code>-factory.img</code>，可以在<a href="https://downloads.openwrt.org/chaos_calmer/15.05/" target="_blank" rel="external">这里</a>下载。</p>
<p>我的WNDR4300平台是<code>ar71xx</code>，可以从OpenWrt对应的<a href="https://wiki.openwrt.org/toh/hwdata/netgear/netgear_wndr4300_v1" target="_blank" rel="external">硬件主页</a>找到固件镜像文件。</p>
<h4 id="TFTP重装"><a href="#TFTP重装" class="headerlink" title="TFTP重装"></a>TFTP重装</h4><p>如果你的路由器还是出厂系统的话，可以通过登入后台在线上传镜像文件进行刷机，而我的已经是OpenWrt系统，只能通过网页端升级，故选用了TFTP方式刷机。</p>
<p>刷机步骤摘自<a href="https://wiki.openwrt.org/toh/netgear/wndr4300" target="_blank" rel="external">OpenWrt wiki</a></p>
<p>&gt;</p>
<blockquote>
<ol>
<li>set a static IP on your computer, i.e 192.168.1.35, and connect the ethernet cable to the router</li>
<li>power on the router</li>
<li>press and hold the RESET button as soon as the switch LEDs light up.</li>
<li>keep holding RESET until the power LED begins to flash orange and then green.</li>
<li>once the power LED is flashing green, release RESET</li>
<li>start the TFTP transfer to router at 192.168.1.1. In your computer execute:<br><code>tftp 192.168.1.1 -m binary -c put factory.img</code></li>
</ol>
</blockquote>
<p>总体来说是分为三步：</p>
<ol>
<li>将电脑与路由器设置在同一内网中</li>
<li>令路由器进入恢复模式</li>
<li>利用TFTP将刷机包推入路由器</li>
</ol>
<h3 id="U盘挂载，文件共享"><a href="#U盘挂载，文件共享" class="headerlink" title="U盘挂载，文件共享"></a>U盘挂载，文件共享</h3><p>安装好OpenWrt后，就可以从网页端访问路由器，设置PPPoE拨号，设置WIFI等等。</p>
<h4 id="U盘挂载"><a href="#U盘挂载" class="headerlink" title="U盘挂载"></a>U盘挂载</h4><p>U盘挂载部分主要参考了<a href="https://my.oschina.net/umu618/blog/282984" target="_blank" rel="external">跟 UMU 一起玩 OpenWRT（入门篇6）：挂接 U 盘</a>。</p>
<p>首先是安装相应的包：</p>
<pre class="line-numbers language-bash"><code class="language-bash">opkg update

<span class="token comment" spellcheck="true"># 核心包</span>
opkg <span class="token function">install</span> kmod-usb-storage
opkg <span class="token function">install</span> kmod-scsi-generic

<span class="token comment" spellcheck="true"># 文件系统</span>
opkg <span class="token function">install</span> kmod-fs-ext4

<span class="token comment" spellcheck="true"># 辅助工具</span>
opkg <span class="token function">install</span> usbutils <span class="token function">fdisk</span> e2fsprogs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>利用<code>lsusb</code>命令查看U盘是否已经被路由器识别。</p>
<p>这时可以选择用fdisk进行重新分区，不需要分区的话，可以用命令<code>ls /dev | grep sd</code>查看/dev分区中是否已经出现U盘。</p>
<p>在OpenWrt上使用U盘，建议用ext4格式，可以用下面的命令进行格式化：</p>
<pre><code># sda1为上一命令得到的结果
mkfs.ext4 /dev/sda1
</code></pre><p>接下来就可以用<code>mount</code>命令进行挂载了：</p>
<pre><code># 路径/mnt/usb/即为挂载目标点
mkdir /mnt/usb
touch /mnt/usb/USB_DISK_NOT_PRESENT
chmod 555 /mnt/usb
chmod 444 /mnt/usb/USB_DISK_NOT_PRESENT
mount /dev/sda1 /mnt/usb
</code></pre><p>这时可以测试一下，如果U盘里面存储了文件，可以通过<code>/mnt/usb</code>访问的到。</p>
<p>下面是开机自动挂载U盘的命令。</p>
<pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># block-mount blkid用于查看U盘的UUID</span>
opkg <span class="token function">install</span> block-mount blkid

<span class="token comment" spellcheck="true"># 实际上要操作的是fstab的配置文件/etc/config/fstab，要将enabled值改成1</span>
block detect <span class="token operator">></span> /etc/config/fstab
uci <span class="token keyword">set</span> fstab.@mount<span class="token punctuation">[</span>-1<span class="token punctuation">]</span>.target<span class="token operator">=</span><span class="token string">'/mnt/usb'</span> u
ci <span class="token keyword">set</span> fstab.@mount<span class="token punctuation">[</span>-1<span class="token punctuation">]</span>.enabled<span class="token operator">=</span>1
uci commit fstab
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>更详细的信息可以参见<a href="http://wiki.openwrt.org/doc/uci/fstab" target="_blank" rel="external">这里</a></p>
<h4 id="文件共享"><a href="#文件共享" class="headerlink" title="文件共享"></a>文件共享</h4><p>文件共享可以通过FTP和SAMBA，推荐的方式是SAMBA。</p>
<h5 id="SAMBA"><a href="#SAMBA" class="headerlink" title="SAMBA"></a>SAMBA</h5><p>安转SAMBA：</p>
<pre class="line-numbers language-bash"><code class="language-bash">opkg update
opkg <span class="token function">install</span> samba36-server

<span class="token comment" spellcheck="true"># luci程序，可选</span>
opkg <span class="token function">install</span> luci-app-samba
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>安装好SAMBA后，主要配置两个参数，一是共享文件夹的路径，如<code>/mnt/usb/sambashare</code>，可以通过更改配置文件<code>/etc/samba/smb.conf</code>实现，也可以通过luci实现。</p>
<p>示例：</p>
<pre><code>[sambashare]
path = /mnt/usb/sambashare
valid users = root
read only = no
guest ok = yes
create mask = 0750
directory mask = 0750
</code></pre><p>第二个参数是访问账户，可以通过命令<code>sambpasswd -a</code>将你的当前用户加入到SAMBA的组中，需要设置一个密码。另外，可能需要将配置文件<code>/etc/samba/smb.conf</code>的[global]中的<code>invalid users = root</code>注释掉。</p>
<p>最后，设置SAMBA服务启动和开机自启</p>
<pre class="line-numbers language-bash"><code class="language-bash">/etc/init.d/samba start
/etc/init.d/samba <span class="token function">enable</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h5 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h5><p>FTP可以用<code>vsftpd</code>包来设置，大致过程与SAMBA类似：设置路径、添加用户、设置自启。</p>
<p>SAMBA服务可以在Windows文件资源管理器中自动检测的到，Linux下可以通过<code>smb://Host/sharepath</code>访问，在IOS系统中，类似Documents的应用也支持添加SAMBA的功能。</p>
<p>这里强推一下Documents这个应用，结合PDF EXPERT，已经成为了我的文档中心。</p>
<h3 id="访问Google"><a href="#访问Google" class="headerlink" title="访问Google"></a>访问Google</h3><p>这部分操作相当复杂，主要参考<a href="https://cokebar.info/archives/664" target="_blank" rel="external">这里</a>，感谢博主。</p>
<h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>这天的活动，本来只有我和上帝知道，再过一个月，就只有上帝知道了。遂作笔记。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> OpenWrt </tag>
            
            <tag> 路由器 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谁会不厌其烦地安慰那无知的少年（三）]]></title>
      <url>http://blog.ddlee.cn/2017/03/03/%E8%B0%81%E4%BC%9A%E4%B8%8D%E5%8E%8C%E5%85%B6%E7%83%A6%E5%9C%B0%E5%AE%89%E6%85%B0%E9%82%A3%E6%97%A0%E7%9F%A5%E7%9A%84%E5%B0%91%E5%B9%B4-3/</url>
      <content type="html"><![CDATA[<blockquote>
<p><em>年轻的时候可以随随便便喜欢一个人，可千万别真动情。那样的话你的余生就剩两种状态了，一种叫做想她，另一种是为克制自己想她而努力。——丁丁前舍友</em></p>
</blockquote>
<p>丁丁的舍友告诉他，那年不懂事，一直陷于人生的错觉之中。</p>
<p>他觉得那女生好像喜欢她，做什么事儿都是像在针对他，总是跑来问问题呀，不懂的时候卖个萌啊，连谢谢的话都是奶声奶气的。</p>
<p>可他怎么能被这个给连累了呢。</p>
<p>他可是老师眼中最有希望的学生，早熟的他也明白合适的平台对自己的发展是多么重要。他觉得在人生的一段时间内能单纯地为一个目标而奋斗是一件幸福的事儿，任何分心的想法都是罪恶。</p>
<p>他在开始之前，就故作冷漠，就像结束了之后想要挽回那样。</p>
<p>丁丁插着话问到底什么开始什么结束的啊？</p>
<p>舍友答，年少的初恋啊我的旁友！</p>
<p>舍友顿了顿，眼里含着惋惜。</p>
<p>讲真，我是那种动情就会倾其所有的人，我真真觉得一生就只够爱一个人。但让我从没想到的是，我的故作冷漠才是动情的开始啊。</p>
<p>那时候我千方百计地回避她。</p>
<p>我特别跟组里的同学换了座位，这样就能离她远一点。</p>
<p>问题的时候我也爱搭不理的，不是把她推给别人就是拖着藏着。</p>
<p>她也算知趣，渐渐的就不来烦我了。</p>
<p>就这样吧，高考完了以后，我们去了不同的学校，离得八十万杆子都打不着。</p>
<p>但我逐渐的发现，这颗种子，已经在我的心底长成了参天大树，不管我给它什么样的脸色，它还是生长起来了。</p>
<p>我再也不能回避它了，我再也不能隐藏它了。</p>
<p>我以前听人家说暗恋一个人的时候，把她的动态错过都会有罪恶感。</p>
<p>我细细的品味她的日志、说说里流露出来的情感，挖空心思复原她写下这些文字时的心情，然后小心翼翼地写下我的评论，斟酌一下，再发表。然后就是每隔几个小时就刷一下，看看她回复了没有。</p>
<p>我也找她聊天，谈心，新的生活还适应没，高数有哪些不懂的跟我说说。</p>
<p>我也跟她讲我的近况，我在听什么歌，我在读哪些书。</p>
<p>可我从来都不敢表露我真实的心意，我也从来不敢提高中时候我的那段冷漠的时光。</p>
<p>可是，你知道吗，就跟吃巧克力一样，她吃到了苦的，我却吃了块甜的，德芙，带榛果颗粒的。</p>
<p>我终于等到了一个机会——她生病住院了。</p>
<p>急性胃炎，但她没跟我说，她的闺蜜告诉我的。</p>
<p>我买了票，赶到她所在的城市，在一个下着小雨的傍晚。</p>
<p>行人匆匆，从四面赶往八方。风催着云，一来一回地玩弄着月亮，雨打在肩上，我才知道我还没有方向。</p>
<p>我给她打电话，说我来看你来了，你在哪家医院。</p>
<p>她说你怎么来了，她已经快好了，明天就要出院，那你过来吧，在江东北路的那家人民医院，8号楼，324。</p>
<p>我说没事儿，马上就到。</p>
<p>不过地铁并不方便，只能在珠江路那里下，我就打算骑ofo过去。</p>
<p>然而我还是太年轻了，南方的冬天下着雨，可没那么好欺负，找路，问路，手冻僵，衣服也淋湿了，我想着张士超华师大的姑娘真的那么可爱吗。</p>
<p>等我赶到时，已经是需要照顾的人了，一副洋葱模样，就剩一层一层剥开了。</p>
<p>狼狈的我跑到厕所里，等个没人的空档，用烘干机吹了吹头发，把外套脱下来搭在胳膊上，这才往病房赶去。</p>
<p>丁丁的舍友推了下眼镜，接着说。</p>
<p>你可知道什么叫近乡情更怯呀，就跟查高考成绩一样啊，你再往前一步，就把那些想象过的所有美好的可能性全破除了，木已成舟，一切皆不可挽回，尽管，尽管你不往前一步，一切也早就注定了呀。</p>
<p>我在病房门前愣住了，万一里面还有人怎么办，她的同学在晚上应该会陪她吧，她不会有男朋友了吧？</p>
<p>我跑到离门远一点的地方，又给她打了个电话，我说我快到了，你有什么想吃的我给你带点。</p>
<p>她说不用了，你过来就好，她也想赶快见到我。</p>
<p>我说好的，这么突然出现，没赶上不方便的时候吧。</p>
<p>她说没事儿，你直接过来吧，哪有什么方便不方便的。</p>
<p>挂了之后，我在楼里瞎逛了几圈，顺手把紧急逃生的路线考察了一下，发现还是很科学的，指引也做的很到位。估摸时间差不多了，我就敲门进去了。</p>
<p>她留起了长发，比高中的时候成熟不少，但终归有病在身，脸色有些发白，不过酒窝还是那样可爱。</p>
<p>我们聊起来，从病情开始，一直聊到那些在网易云音乐的歌曲下面刷评论的考研党们到底考上了没。</p>
<p>她似乎很开心，我也很开心。</p>
<p>她说上了大学就没跟别人聊这么久过，还是以前的同学好呀。</p>
<p>我说那当然了，以后有什么事你第一个告诉我。</p>
<p>要走的时候，她说谢谢我这么大老远地跑过来，不过病差不多要好了，明天亲自到车站送送你。</p>
<p>我说不用了，我自己走就行，你好好养着身体吧，注意一下饮食。</p>
<p>离开医院 ，我随便找了家旅馆住下来。心底里无限的舒适与满足。但很快，紧张与自责将我包裹起来。</p>
<p>太懦弱了我真是，聊那些没什么用的干啥，我该直接跟她说我喜欢你三年了我们在一起吧。</p>
<p>可又转念一想，这也有点趁人之危吧，还是等等再说？</p>
<p>这一等就是一夜，我慢慢睡着，天刚刚破晓。</p>
<p>第二天，她还是来送我了，下地铁后，她用手机看了下时间，说还不晚不用着急。</p>
<p>她竟然用的Xperia。我心想我喜欢的女孩子就是有格调。</p>
<p>然后我就看到了手机桌面上男孩子的傻笑。</p>
<p>那个男孩子似乎不是我，我笑的时候不傻，眼睛眯成一条缝。</p>
<p>我说这也不早了你赶紧回去吧。</p>
<p>她说你开玩笑呢这才几点啊。</p>
<p>我说不对，我不是这个意思，我是说你不用跟我一起等了，我自己等，我自己能行。</p>
<p>她告诉我她当然相信我能行，不然怎么能自己跑过来看她呢。</p>
<p>我说也是哈，我这么催你干哈。</p>
<p>后面的事情我自己也记不清了。</p>
<p>回来的时候，出站换乘，转角碰见一家鲜花店，就进去买了一束满天星，捧着它回到寝室，摆在桌上。</p>
<p>我是眼睁睁地看着那一束花慢慢枯萎的。</p>
<p>不插在水中的话，只用了三天不到。</p>
<p>舍友说我那三天跟个傻逼一样。</p>
<p>后来她说我是她最好的朋友，跟高中的那个我完全不一样了。</p>
<p>原来她从来就没喜欢过我，而我也从来没承认过我那么心动，但你知道吗？这的的确确发生了。</p>
<p>舍友觉得可以做结了，便说出了这句丁丁永生难忘的话。</p>
<p>年轻的时候可以随随便便喜欢一个人，可千万别真动情。那样的话你的余生就剩一种状态了，那就是想她。</p>
<p>丁丁说没事儿你还有机会，天下没有不散的筵席，他们迟早会分的。</p>
<p>舍友说丁丁是傻逼。</p>
<p>————————————————全文完—————————————————</p>
<p>我不再强说上面的故事是瞎编的了。它们是丁丁亲口告诉我的，在一次卧谈会上。</p>
<p>丁丁说在刚好记得的时候讲出来，其实是自私的。</p>
<p>他说他从小到大失去了很多人，从每天早到学校开门的劳动课老师到害了白血病的不幸前桌，从打架斗殴满嘴义气话的小魔王到奔走他乡借读名校的竞争对手，当好友列表里的灰色头像终于不再跳动的时候，我就不再是完整的了，他们把我的一部分带走了，而且永远找也找不回来了。这个永远是真的。</p>
<p>我跟丁丁说你错了，你不知道更可怕的事情。你有没有想过，即使是陪你一起长大的人，也有很多东西找不回来了。像你的父母，你的淘气和无知，早就淹没在他们眼角的层层皱纹里了。而且，是你亲手把它们埋葬进去的。你看，谁都没有失去谁，谁也失去了谁。</p>
<p>丁丁说是啊，我们都变了，变得都有些记不起从前的样子了。人们总是到失去了才懂得珍惜，这真是瞎话，我们就从来没有拥有过。</p>
<p>我记起很久以前的一个秋天，我打开了一册我昔日嗜爱的书读了下去，突然回复到十四岁时那样温柔而多感，我在那里面找到了一节写在发黄的纸上的以这样两行开始的短诗：
　　　 　</p>
<blockquote>
<p>在你眼睛里我找到了童年的梦，<br>如在秋天的园子里找到了迟暮的花……</p>
</blockquote>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2017年3月</p>
]]></content>
      
        <categories>
            
            <category> 随笔 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谁会不厌其烦地安慰那无知的少年（二）]]></title>
      <url>http://blog.ddlee.cn/2017/02/15/%E8%B0%81%E4%BC%9A%E4%B8%8D%E5%8E%8C%E5%85%B6%E7%83%A6%E5%9C%B0%E5%AE%89%E6%85%B0%E9%82%A3%E6%97%A0%E7%9F%A5%E7%9A%84%E5%B0%91%E5%B9%B4-2/</url>
      <content type="html"><![CDATA[<blockquote>
<p><em>不要做父母手中的烤鸭，要做一只自由的小小种马。——刘星，《（假的）家有儿女》</em></p>
</blockquote>
<p>丁丁再回到这条老街时，又是一年的光景。</p>
<p>这一年，家乡添了几处新房和俏媳妇，添了几家麻将交流中心，添了几座坟头。</p>
<p>村后的河今年却冻住了——往年不上冻的，因为里面东西太多。</p>
<p>村前公路两旁的树全砍掉了。主人缺钱，不缺树。</p>
<p>目力所及，坑洼的油漆路向北延伸到省道上，两旁田地里丛丛的麦子依偎而息，灰蒙蒙的天，树林间掩映着冬日里小姑娘红扑扑的脸蛋，那是北方的夕阳。哎呦，还蒙了层雾气。</p>
<p>这次回老家，丁丁照例去拜访过道尽头被奶奶称为”二嫂“的老太太。</p>
<p>二嫂是帝都过来的知青，这些年没入我们的乡音，跟谁也是一口侉侉的北京话。</p>
<p>她最著名的话是，“我主的了疼，也主的了管”。</p>
<p>这是跟人家解释为什么老打孙子。一时成为村里溺爱孙子老传统中的一股清流。</p>
<p>可老人家现在状态不好：去年初四，脑出血，救回来之后半边失去了控制，歪了嘴，动不了腿。</p>
<p>我进了门，走到轮椅边。老人眼睛亮了起来，一只手撑着扶手，要站起来。</p>
<p>我大声说奶奶您不用起来，多累啊。</p>
<p>二嫂摇着头坐下，攥着我的手，晃来晃去。又赶紧把暖手袋扯过来，叫我捧着。</p>
<p>就像小时候那样。</p>
<p>二嫂是看着我长大的。奶奶经常带着我到二嫂家里串门，二嫂家里有糖吃，有奶喝。</p>
<p>那时候我最喜欢翻彻二嫂厚厚的影集，上面有好多我没见过的东西。</p>
<p>奶奶你耳朵边别着的是什么花呀，那时候你几岁。</p>
<p>二嫂说那年她十六，别着的花叫白玉兰。</p>
<p>今年她七十六。照片上的小姑娘带一点自信，含一丝羞赧，就像每个十六七岁的女孩子那样。</p>
<p>这让我想起妈妈。妈妈年轻的时候追邓丽君、小虎队，最喜欢的是粉红色的回忆。家里有一张她结婚时的照片，大红毛衣，傻傻的杵在那里，另一头爸爸给二叔骑在背上，向妈妈鞠躬，胸前歪着一朵大红花。</p>
<p>我没见过作为年轻姑娘的二嫂和妈妈是什么样子的，跟我相关的，只有她们逐渐老去的岁月。</p>
<p>二嫂晃动着身子，她打算站起身来。</p>
<p>我扶着她，走一步，拖一步，不违背，不阻挡。</p>
<p>五六米的距离，老人已经气喘吁吁。我也不说话，我单单陪着她。</p>
<p>院里的枣树上落了一只麻雀，不知为何她没回南方的家。隔壁的二层小楼开始掌起灯火，夜色也正吞下了半边天。</p>
<p>二嫂接着往外拖着步子，这时媳妇却迎着面从小卖部回来。</p>
<p>哎呀，涛你怎么让你奶奶出屋里来了？外面冷，娘咱回屋里吧。</p>
<p>二嫂不肯，但她做不了主。</p>
<p>这已不是她做主的日子了。</p>
<p>爷爷大二嫂好多岁，早就没了精神。多少年大大小小，一直是二嫂操持着。</p>
<p>去年的时候，我坐在炕头边，绕着问她年轻时候的故事。</p>
<p>她说她的一生就分为两部分，给大伙种地和给自己种地。前半段三十年，后半段三十年。</p>
<p>明明从北京赶过来，她却说这里更冷一些。村支书被打得藏在柜子底下，三千斤麦子换来的推车充了公，大雨下到把房子冲塌，夜不闭户，好冷。</p>
<p>二嫂说后来却是倒春寒。家乡的新媳妇，都凑不出一件体面衣裳。地里什么东西也不长。饿死的人排着队。</p>
<p>我问再后来呢？自己种总好些了吧？</p>
<p>二嫂说自己种也要上交粮食给国家的。那年她推着小车，走了二十几里的土路，把麦子送到乡上。三十年了。</p>
<p>二嫂说这么多年看上去一直是我在做主支撑着这个家，但实际上我从来都没做过主，我对自己也做不了主，我对谁也做不了主。</p>
<p>我说还是我们这一代人幸福啊，赶上了好的时候。</p>
<p>二嫂说那只是看起来，长大了你就明白了。</p>
<p>然而我从来都长不大，二嫂却变老了。</p>
<p>二嫂老了，但从没老糊涂，也没装过糊涂，直到突然的疾病将糊涂的能力赐予给她。</p>
<p>回到屋里，二嫂就又安静地坐下来。电视里恰巧是场晚会，在希望的田野上。</p>
<p>夜幕已全然降临。猎户座的三星嵌在南面的而天空，月亮瘦成眉毛，挑在树枝上，除此之外，一片看不透的灰色将视野罩的密不透风。</p>
<p>我瞪着窗外，正出神，二嫂那边却哼了起来，摇起我的手。</p>
<p>呜呜声。奶奶又回到了回不去的小时候。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2017年2月</p>
]]></content>
      
        <categories>
            
            <category> 随笔 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[谁会不厌其烦地安慰那无知的少年（一）]]></title>
      <url>http://blog.ddlee.cn/2017/01/27/%E8%B0%81%E4%BC%9A%E4%B8%8D%E5%8E%8C%E5%85%B6%E7%83%A6%E5%9C%B0%E5%AE%89%E6%85%B0%E9%82%A3%E6%97%A0%E7%9F%A5%E7%9A%84%E5%B0%91%E5%B9%B4-1/</url>
      <content type="html"><![CDATA[<blockquote>
<p><em>一个男孩要下过多少电影，才能称得上是一个男人？一只海鸥要飞过多少海洋，才能在柔柔的沙滩上安息？——鲍勃·迪伦，《答案在风中飘荡》</em></p>
</blockquote>
<p>星星眨着眼，银河却不见。万家灯火散落在不遥远的远方，贪婪的夜色吞噬着视野，列车不紧不慢地刺破雾气的深不可测，卧铺床头的小台灯透过车窗温暖出朦胧一片，笼住返乡人的放松与期盼。</p>
<p>其实丁丁差点没赶上火车。亏得遇到老司机，路上没怎么堵。过检票口的时候，广播刚刚喊着“你所乘坐的班次已停止检票”。</p>
<p>火车终于安稳地行着。丁丁的心情也慢慢舒畅起来。</p>
<p>丁丁趴在铺上，翻看相册，回想这又一个人生七年。</p>
<p>小学到中学就是一趟火车，有起点也有终点，不慌不忙。大学是脱了轨的同一趟火车，东栽西撞，没有诗也到不了远方。</p>
<p>想到这里，丁丁下了铺，留意了一下安全锤的位置，然而，在回来时，他还是不可避免地被旁边的大叔注意到了。</p>
<p>嘿，小伙子，你也用Lumia 啊。</p>
<p>丁丁尴尬地讲，没，只是备用机，主力还是安卓。心想着竟然还被看出来了，不过正好，用Lumia不装逼，那跟咸鱼有什么区别。</p>
<p>大叔你做什么工作的呀。</p>
<p>大叔讲他是个半个码农，三倍的房奴，两个孩子的爹地，一个老婆坚实的依靠。</p>
<p>丁丁说自己是三个舍友的爸爸，五门课的开课赞助商，七个女生的备胎，九个社团的划水副总监。</p>
<p>大叔说你这就是我的Pro版啊，深交吗小伙子？</p>
<p>丁丁说，好。</p>
<p>可这一开口，大叔就是从诗词歌赋到人生哲学。只不过，没有雪也没有月亮，我不是紫薇他也不叫尔康。</p>
<p>大叔并不大，现在在南京，江北一套房，鼓楼一套学区。两个儿子，大的刚上一年级，小的还不会撒谎。</p>
<p>自己公司年底出了状况，没能跟家人坐同一趟车回家。</p>
<p>大叔说自己本科数学，毕了业才发现自己卵没什么用。女朋友学计算机，早就找好了工作，自己只好考了研，后来拿了个硕士，主攻信号转发与缓存。</p>
<p>丁丁说我也数学。</p>
<p>大叔抿了抿嘴，嗯，有意思有意思。</p>
<p>大叔说那我给你介绍介绍考研经验吧。</p>
<p>丁丁说好啊好啊。</p>
<p>那年考研的形势很严峻，因为减招。</p>
<p>为了考研，大三那年寒假，我初五就从家里跑出来了。赶巧的是，那年跟今年一样，过年赶得好晚，我统共在家不到十天。</p>
<p>临走那天晚上，爸爸到单位值班，去之前又塞给我几百块钱，说穷家富路，但这种行为被我义正辞严地拒绝了。可爸爸走后，我泪湿眼底。</p>
<p>因为这一离开就又是半年。</p>
<p>考上大学第一年回家，奶奶跟我说你走后你爸来我这儿的时候哭了，说你跟小鸟一样飞走了。我说也是啊，我长大了，爸爸的一个时代也结束了呀，就在我报完到送他回去的那一刻。</p>
<p>那天在楼下值班室那里领钥匙，爸爸在一边摸着头笑，见我回头，他跟遇到喜欢的女生那样不好意思，红着脸。</p>
<p>爸爸的一个时代结束了呀。</p>
<p>还记得，我上小学那会儿，连午休都要家长签字确认的，还有作业也是，爸爸兢兢业业地把题都重新算一遍，马虎的地方狠狠批我一顿，这才用方方正正的钢笔给我签上“家长已检查”，现在我才知道，这叫“背书”。</p>
<p>那时候妈妈在一边儿踩着缝纫机，看点播台的我被爸爸叫过去，扭扭捏捏地摸着后脑勺，阳台上水仙开着，香味儿就飘到屋里来。</p>
<p>其实那时候的我才最懂事儿。那时我最大的梦想就是娶了班上最文静的女生，让妈妈少操点儿心。而她当时就是我的同桌，放学我们还一起走到灵石路的尽头，走过小酒馆的门口。后来四五年级，起了流言，我们就分了。</p>
<p>后来在外面求学，跟父母在一起的时间就越来越少了。那时候我最喜欢的时候是坐在大巴士高高的最后一排，靠窗，看路边的杨树一棵一棵闪过，我觉得我的人生康庄大道就在脚下一点一点伸展开来。</p>
<p>爸爸给我的支持也越来越少了。他不懂遗传平衡定律，找不到辅助线，也人脸识别不了虚拟语气。我的小小心思就像宇宙那般，无边地膨胀起来了。</p>
<p>高考就是碰到气球的那根针。我感觉自己是被发配到了南方，而且还被冻成了狗。</p>
<p>丁丁顺着说，南方确实冷的不行，尤其下雨天。</p>
<p>大叔说，你看，这些小事，我不说，就要一点一点埋葬在潺潺流去的岁月里了。</p>
<p>可我考研那年不懂事。我哭的时候，却觉得自己分分钟像个大人了，我早回去正是在做着那些英雄们不得不做的事儿。天将降大任于斯人矣。</p>
<p>为了呵护这个家，却要离开它。</p>
<p>浊酒一杯，家万里。</p>
<p>我觉得这就是我的燃情岁月。</p>
<p>后来研考上了，女朋友等了我三年，然后就媳妇也有了。后来我才知道燃情岁月才算刚刚开始。</p>
<p>丁丁蛮懂事，道，汪、汪、汪。</p>
<p>再后来，有了一室一厅，吉利帝豪，郊区的三室两厅，又因为堵车把车给卖了，再后来有了一个儿子，鼓楼的学区加户口房，又添了个儿子，就把爸妈接过来了。</p>
<p>这几年没有我特别想做的事儿。只有我需要做好的事儿。</p>
<p>两个小魔王，说实话我不觉得爸妈老年生活有多幸福。</p>
<p>不过多亏通了地铁，我每天八点半能到家，磕个瓜子，跟我爸聊聊我儿子和他儿子。</p>
<p>可是，小伙子，你知道吗？我考研那年，就是个愣头青。</p>
<p>那时候我对私人的时间有着近乎偏执的吝啬。我觉得自己独处的时间才是上天赐予的礼物。回家过年又烦又累，措不及防的应酬是对我神圣的私有时间的侵犯。所以，其实我早早就狠下心来，一定要早早的回学校。</p>
<p>我上车那天风声呼啸，暗云疾行，干燥的北风中赫赫抬起的，是我打车的一只大手。路两边白杨赤条条的，行人裹着衣，绷着脸。</p>
<p>风萧萧兮易水寒，众人向北我向南。</p>
<p>可是，小伙子，你知道吗？</p>
<p>让男孩成为男人的，不是事业，是家业啊。</p>
<p>大叔突然不说了。他翻了个身，晚安。</p>
<p>丁丁也回过头，抹了眼睛，退了返程。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2017年1月</p>
]]></content>
      
        <categories>
            
            <category> 随笔 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[小米3变身记]]></title>
      <url>http://blog.ddlee.cn/2016/09/23/%E5%B0%8F%E7%B1%B33%E5%8F%98%E8%BA%AB%E8%AE%B0/</url>
      <content type="html"><![CDATA[<blockquote>
<p>Across the Great Wall we can reach every corner in the world.”（越过长城，走向世界）</p>
</blockquote>
<h1 id="1-缘起——我、小米、安卓和Android"><a href="#1-缘起——我、小米、安卓和Android" class="headerlink" title="1.缘起——我、小米、安卓和Android"></a>1.缘起——我、小米、安卓和Android</h1><p>知乎上有个抖机灵的回答，问题是“Nexus 5 如果不使用 VPN，会有什么影响”，回答是“android 体验变成安卓体验”。感谢无所不能的墙，让Android也有了中国特色。</p>
<p>各家应用市场层出不穷，应用推广不择手段，申请权限多多益善，后台活动精彩不断。更不能忍的是，小米移除了Google的服务框架，无法从Play商店推动应用。为了用Sleep Cycle alarm clock, 我得用在线工具从Play商店获取链接，同步到云盘里，再从手机里打开.apk文件来安装。因为，百度搜索出的那个结果，应用中内置了烦人的广告。</p>
<p>终于，我投奔了IOS阵营，手中服役的小米3也就闲置。在一个并没有那么蛋疼的午后，我拿出数据线，对它说，It’s time.</p>
<p>注意：本系列不作为通用教程，只做经历分享。请移步相关论坛获取教程信息。</p>
<h1 id="2-基础——是什么，为什么，怎么办"><a href="#2-基础——是什么，为什么，怎么办" class="headerlink" title="2.基础——是什么，为什么，怎么办"></a>2.基础——是什么，为什么，怎么办</h1><h2 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h2><p>你一定听说过，有种技术叫刷机。你也一定听说过，还有种技术叫越狱。你更一定听说过，还有种技术叫FanQiang。</p>
<p>这三者有什么关系呢？</p>
<p>在我看来，它们都关乎我们作为用户常常忽略的两个字——权限。</p>
<p>换言之，我们常常关注可以用手机干什么、可以上网浏览什么，却常常不去注意，我们本来有更多事可以做，有更多信息可以获取。</p>
<p>刷机意味着给手机重装系统，你获得的是选择硬件所运行系统的权利；越狱获得的是掌控某一操作系统的权利；而FanQiang，获得的则是“越过长城，走向世界”的权利。</p>
<blockquote>
<p>中国第一封电子邮件的内容是：Across the Great Wall we can reach every corner in the world.”（越过长城，走向世界）。这是1987年9月14日从北京向海外发出的中国第一封电子邮件，揭开了中国人使用互联网的序幕。</p>
<p>来源：<a href="https://www.zhihu.com/question/32239520/answer/61354926" target="_blank" rel="external">知乎</a></p>
</blockquote>
<h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><p>因为无聊，因为好奇，因为喜欢，因为不满足，因为我们可以。</p>
<h2 id="怎么办"><a href="#怎么办" class="headerlink" title="怎么办"></a>怎么办</h2><p>如果把刷机比作建造楼房，你所需要准备的就是知识（图纸）、刷机包（水泥、混凝土）、调试环境（吊塔）。</p>
<h3 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h3><p>真正重要的知识，是关于知识的知识。拿到图纸不重要，重要的是学会如何看懂图纸。以我的经历，最耗费精力的部分不是学习教程，而是TROUBLE SHOOTING, 是如何解决出现的问题。</p>
<p>因此，绝对不要使用某些工具的“一键刷机功能”，它们不会告诉你问题出在哪。<br><img src="http://static.ddlee.cn/static/img/Mi3/something-happened.jpg" alt="Something Happened"></p>
<p>请保证你对整个过程的绝对控制，保证你清楚到底在哪一步无法继续进行。</p>
<p>而为了看懂图纸，你需要准备好你的Google.它会是你最可靠的伙伴。</p>
<p>下面是图纸中可能涉及的内容，请搜索并结合某些通用刷机教程理解它们发挥的作用。</p>
<ul>
<li>卡刷、线刷：两种刷机的操作方式（体位）</li>
<li>Root：获取Android系统管理员的过程</li>
<li>OTA：On The Air， 一种系统更新方式</li>
<li>ROM包： 刷入手机ROM的系统软件包</li>
<li>Recovery Mode： Android系统的一种模式，常在此模式下进行刷机操作</li>
<li>Fastboot Mode： Android系统的一种模式，可在此模式下刷入自定义recovery</li>
<li>ADB： Android Debug Bridge，用PC对Android系统进行USB调教所需的环境</li>
<li>CM： 一家著名的ROM制作方，现已改名Lineage OS</li>
<li>Android M： Android系统的一个版本，现在是N（Nougat，7.0）</li>
<li>GApps： Google服务全家桶，需要刷入系统分区，包括Play和GMS等服务</li>
</ul>
<h1 id="3-刷机——大致的步骤，常见的坑"><a href="#3-刷机——大致的步骤，常见的坑" class="headerlink" title="3.刷机——大致的步骤，常见的坑"></a>3.刷机——大致的步骤，常见的坑</h1><h3 id="一个负责任的教程，大概会告诉你如下几个步骤"><a href="#一个负责任的教程，大概会告诉你如下几个步骤" class="headerlink" title="一个负责任的教程，大概会告诉你如下几个步骤"></a>一个负责任的教程，大概会告诉你如下几个步骤</h3><ul>
<li>风险警示</li>
<li>备份数据</li>
<li>如何搭建ADB环境-PC</li>
<li>如何进入fastboot模式-手机</li>
<li>如何在ADB环境下，fastboot模式中刷入自定义recovery</li>
<li>如何利用recovery模式清除数据，刷入ROM包（和Gapps）</li>
<li>如何ROOT</li>
</ul>
<h3 id="一次蛋疼的刷机经历，常常会遇到这些坑"><a href="#一次蛋疼的刷机经历，常常会遇到这些坑" class="headerlink" title="一次蛋疼的刷机经历，常常会遇到这些坑"></a>一次蛋疼的刷机经历，常常会遇到这些坑</h3><ul>
<li>找到正确的Recovery和ROM包：一定要仔细比对型号，尽量选用开源机构制作的包</li>
<li>搭建ADB环境：要用到命令行（请慎重选用一件脚本，即.bat文件）</li>
<li>连接电脑与手机：Windows系统下需要硬件驱动（请注意型号）</li>
<li>……</li>
</ul>
<h1 id="4-资源——与你同行"><a href="#4-资源——与你同行" class="headerlink" title="4.资源——与你同行"></a>4.资源——与你同行</h1><h3 id="论坛与搜索引擎"><a href="#论坛与搜索引擎" class="headerlink" title="论坛与搜索引擎"></a>论坛与搜索引擎</h3><p>你会发现，手机厂商的官方论坛和XDA等论坛会很有帮助。但真正与你同行的，还是Google。</p>
<h3 id="常用网址"><a href="#常用网址" class="headerlink" title="常用网址"></a>常用网址</h3><ul>
<li><a href="https://twrp.me/" target="_blank" rel="external">TWRP，著名的自定义recovery</a></li>
<li><a href="http://opengapps.org/" target="_blank" rel="external">GApps，Google全家桶</a></li>
<li><a href="http://repo.xposed.info/module/de.robv.android.xposed.installer" target="_blank" rel="external">Xposed，著名开源框架</a></li>
<li><a href="https://developer.android.com/studio/command-line/adb.html" target="_blank" rel="external">ADB Guide</a></li>
<li><a href="http://www.supersu.com/download" target="_blank" rel="external">SuperSU，用于手机Root</a></li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Android </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Android </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据分析在线学习资源(Personal Archive)]]></title>
      <url>http://blog.ddlee.cn/2016/08/07/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%B5%84%E6%BA%90/</url>
      <content type="html"><![CDATA[<p>数据分析方向的在线资源收集。</p>
<h2 id="1-Some-wonderful-Tutorials"><a href="#1-Some-wonderful-Tutorials" class="headerlink" title="1.Some wonderful Tutorials"></a>1.Some wonderful Tutorials</h2><ol>
<li><a href="https://www.springboard.com/learning-paths/data-analysis/learn/?" target="_blank" rel="external">Data Analysis Learning Path from Springboard</a></li>
<li><a href="http://datasciencemasters.org/" target="_blank" rel="external">The Open Source Data Science Masters</a></li>
</ol>
<h2 id="2-Basic"><a href="#2-Basic" class="headerlink" title="2. Basic"></a>2. Basic</h2><h3 id="2-1-Database"><a href="#2-1-Database" class="headerlink" title="2.1 Database"></a>2.1 Database</h3><p>Stanford’s Database <a href="https://lagunita.stanford.edu/courses/Engineering/db/2014_1/info" target="_blank" rel="external">course</a></p>
<h3 id="2-2-Agrolthms"><a href="#2-2-Agrolthms" class="headerlink" title="2.2 Agrolthms"></a>2.2 Agrolthms</h3><p>Algrothms from Stanford via <a href="https://class.coursera.org/algs4partI-010" target="_blank" rel="external">Coursera</a>(using Java)<br>Booksite <a href="http://algs4.cs.princeton.edu/home/" target="_blank" rel="external">here</a><br>Algorithm with Python in <a href="https://github.com/qiwsir/algorithm" target="_blank" rel="external">GItHub</a></p>
<h3 id="2-3-Algebra"><a href="#2-3-Algebra" class="headerlink" title="2.3 Algebra"></a>2.3 Algebra</h3><p>Harvard’s Massive Parralle Algebra Course on iTunes U</p>
<h3 id="2-4-Statistics"><a href="#2-4-Statistics" class="headerlink" title="2.4 Statistics"></a>2.4 Statistics</h3><p>Princeton’s Statistics One</p>
<h3 id="2-5-Books"><a href="#2-5-Books" class="headerlink" title="2.5 Books"></a>2.5 Books</h3><ul>
<li><em>Pattern Recognition and Machine Learning</em> by Bishop</li>
<li><em>The Elements of Statistical Learning</em></li>
</ul>
<h2 id="3-Python"><a href="#3-Python" class="headerlink" title="3. Python"></a>3. Python</h2><h3 id="3-1-Scipy-amp-Pandas-amp-sklearn"><a href="#3-1-Scipy-amp-Pandas-amp-sklearn" class="headerlink" title="3.1 Scipy &amp; Pandas &amp; sklearn"></a>3.1 Scipy &amp; Pandas &amp; sklearn</h3><ul>
<li>Scipy Lecture Notes</li>
<li>Pandas Doc</li>
<li>Pandas <a href="https://github.com/jvns/pandas-cookbook" target="_blank" rel="external">Cookbook</a></li>
<li>sklearn Doc</li>
</ul>
<h3 id="3-2-Python-MOOCs"><a href="#3-2-Python-MOOCs" class="headerlink" title="3.2 Python MOOCs"></a>3.2 Python MOOCs</h3><h5 id="edX-course"><a href="#edX-course" class="headerlink" title="edX course"></a>edX course</h5><p>MITx: 6.00.2x Introduction to Computational Thinking and Data Science via <a href="https://courses.edx.org/courses/course-v1:MITx+6.00.2x_5+1T2016/info" target="_blank" rel="external">edX</a></p>
<h5 id="Udacity-Course"><a href="#Udacity-Course" class="headerlink" title="Udacity Course"></a>Udacity Course</h5><ul>
<li>Design of Computer Programs with Peter Novig</li>
<li>Intro to Machine Learning (project oriented)</li>
<li>Machine Learning: Unsupervised Learning</li>
</ul>
<h3 id="3-3-Books"><a href="#3-3-Books" class="headerlink" title="3.3 Books"></a>3.3 Books</h3><ul>
<li><em>Python for Data Analysis</em> by Wes McKinney</li>
<li><em>Programming Collective Intelligence</em> by Toby Segaran</li>
</ul>
<h2 id="4-R"><a href="#4-R" class="headerlink" title="4. R"></a>4. R</h2><h3 id="4-1-R-MOOCs"><a href="#4-1-R-MOOCs" class="headerlink" title="4.1 R MOOCs"></a>4.1 R MOOCs</h3><h5 id="edX-course-1"><a href="#edX-course-1" class="headerlink" title="edX course"></a>edX course</h5><p>MIT’s The Analytics Edge</p>
<h5 id="JH-Data-Science-Specilization-via-Coursera"><a href="#JH-Data-Science-Specilization-via-Coursera" class="headerlink" title="JH Data Science Specilization via Coursera"></a>JH Data Science Specilization via Coursera</h5><ul>
<li>Statistical Inference</li>
<li>Regression Model</li>
<li>Practical Machine Learning</li>
<li>Develop Data Science Product</li>
</ul>
<h5 id="Stanford’s-Statistical-Learning"><a href="#Stanford’s-Statistical-Learning" class="headerlink" title="Stanford’s Statistical Learning"></a>Stanford’s Statistical Learning</h5><p><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/info" target="_blank" rel="external">here</a><br>and its text book <em>An Introduction to Statistical Learning</em> <a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank" rel="external">ISLAR</a></p>
<h3 id="4-2-R-Books"><a href="#4-2-R-Books" class="headerlink" title="4.2 R Books"></a>4.2 R Books</h3><ul>
<li><em>R Graphics Cookbook</em> by Winston Chang</li>
<li><em>ggplot2</em> by Hadley Wickham</li>
<li><em>R in Action</em> by Robert I. Kabacoff</li>
</ul>
<h2 id="5-Big-Data"><a href="#5-Big-Data" class="headerlink" title="5. Big Data"></a>5. Big Data</h2><h3 id="5-1-“Big”-MOOCs"><a href="#5-1-“Big”-MOOCs" class="headerlink" title="5.1 “Big” MOOCs"></a>5.1 “Big” MOOCs</h3><h5 id="Udacity-Course-1"><a href="#Udacity-Course-1" class="headerlink" title="Udacity Course"></a>Udacity Course</h5><p>Intro to Hadoop and MapReduce from clourdera</p>
<h5 id="Coursera-Course"><a href="#Coursera-Course" class="headerlink" title="Coursera Course"></a>Coursera Course</h5><p>Mining Massive Datasets</p>
<h5 id="edX-Course"><a href="#edX-Course" class="headerlink" title="edX Course"></a>edX Course</h5><p>Xserise on Spark from BerkleyX</p>
<h2 id="6-Capstone-Project"><a href="#6-Capstone-Project" class="headerlink" title="6. Capstone Project"></a>6. Capstone Project</h2><ul>
<li>SITP Project</li>
<li>Health Twitter Analysis via <a href="https://www.coursolve.org/need/229" target="_blank" rel="external">Coursolve</a></li>
</ul>
<h2 id="7-Additional-Resource"><a href="#7-Additional-Resource" class="headerlink" title="7. Additional Resource"></a>7. Additional Resource</h2><ul>
<li>Harvard’s <a href="http://cs109.github.io/2015/" target="_blank" rel="external">CS109</a> Course: Data Science</li>
<li>Berkley’s <a href="http://cs61a.org/" target="_blank" rel="external">CS61</a>:The Structure and Interpretation of Computer Programs</li>
<li>Probabilistic Graphical Models via <a href="https://www.coursera.org/course/pgm" target="_blank" rel="external">Coursera</a></li>
<li>Berkeley’s <a href="http://data8.org/datascience/" target="_blank" rel="external">Datascience’s Documentation</a></li>
<li><a href="https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks" target="_blank" rel="external">A Gallery of IPython Notebooks</a></li>
<li>A collection of Data Science Learning materials in the form of <a href="https://github.com/nborwankar/LearnDataScience" target="_blank" rel="external">IPython Notebooks</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="external">Unsupervised Feature Learing and Deep Learning</a></li>
</ul>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Data Science </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Data Science </tag>
            
            <tag> Data </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[个性化你的Ubuntu-3：主题，插件以及桌面小工具]]></title>
      <url>http://blog.ddlee.cn/2016/06/11/%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BD%A0%E7%9A%84Ubuntu-3%EF%BC%9A%E4%B8%BB%E9%A2%98%EF%BC%8C%E6%8F%92%E4%BB%B6%E4%BB%A5%E5%8F%8A%E6%A1%8C%E9%9D%A2%E5%B0%8F%E5%B7%A5%E5%85%B7/</url>
      <content type="html"><![CDATA[<h3 id="个性主题"><a href="#个性主题" class="headerlink" title="个性主题"></a>个性主题</h3><p>依赖于扩展<code>User themes</code>，分为GTK主题，shell主题和icon主题。</p>
<ol>
<li><p>从<a href="https://www.gnome-look.org/browse/cat/134/ord/latest/" target="_blank" rel="external">gnome-look.org</a>下载喜欢的主题（压缩文件）。</p>
</li>
<li><p>将下载的主题文件复制到用户文件夹</p>
<pre><code> cd ~
 mkdir .themes
 cp file_path_to_download_file ~/.themes
</code></pre><p> 并使用<code>unzip</code>或<code>tar xvzf</code>命令解压，或者：</p>
<pre><code> sudo cp file_path_to_download_file /usr/local/themes/
</code></pre></li>
<li><p>在<code>gnome-tweak-tool</code>的扩展<code>User themes</code>中选择主题。</p>
</li>
</ol>
<h5 id="推荐主题"><a href="#推荐主题" class="headerlink" title="推荐主题"></a>推荐主题</h5><p>我使用的是Numix系列的主题<a href="https://numixproject.org/" target="_blank" rel="external">（官网）</a></p>
<ul>
<li><a href="http://satya164.deviantart.com/art/Numix-GTK3-theme-360223962" target="_blank" rel="external">Numix-GTK3 theme</a></li>
<li><a href="http://gnome-look.org/content/show.php/Numix-like+GNOME+Shell+3.16+theme?content=174129" target="_blank" rel="external">Numix-like GNOME Shell theme</a></li>
<li><a href="http://me4oslav.deviantart.com/art/Numix-Circle-Linux-Desktop-Icon-Theme-414741466" target="_blank" rel="external">Numix-Circle Icons</a></li>
</ul>
<p>Numix开发者之一Satyajit Sahoo发布的GNOME shell theme:<br><a href="http://satya164.deviantart.com/art/Gnome-Shell-Elegance-Colors-305966388" target="_blank" rel="external">Gnome Shell - Elegance Colors</a></p>
<p>通过PPA安装</p>
<pre><code>sudo apt-add-repository ppa:numix/ppa
sudo apt-get update
sudo apt-get install numix-gtk-theme
sudo apt-get install numix-icon-theme-circle
</code></pre><pre><code>sudo add-apt-repository ppa:satyajit-happy/themes
sudo apt-get update &amp;&amp; sudo apt-get install gnome-shell-theme-elegance-colors
</code></pre><h2 id="扩展插件"><a href="#扩展插件" class="headerlink" title="扩展插件"></a>扩展插件</h2><p>我当前使用的插件：</p>
<ol>
<li>hide dash：隐藏侧边的favorite栏</li>
<li>Pomotodo：番茄时钟<br><img src="http://static.ddlee.cn/static/img/Ubuntu-3/Pomodoro1.jpg" alt="Pomotodo1"><br><img src="http://static.ddlee.cn/static/img/Ubuntu-3/Pomodoro2.jpg" alt="Pomotodo2"></li>
<li>（荐）Clipboard indicator：剪贴板切换<br><img src="http://static.ddlee.cn/static/img/Ubuntu-3/Selection_018.jpg" alt="Clipboard indicator"></li>
<li>ToDo.txt：待办事项整理<br><img src="http://static.ddlee.cn/static/img/Ubuntu-3/ToDo_txt.jpg" alt="ToDo"></li>
<li>Places indicator：文件浏览器的快捷方式</li>
<li>Activities configurator: 当前活动程序管理</li>
<li>Alternatetab: alt-tab桌面切换</li>
<li>Applications menu：类似Windows下开始菜单<br><img src="http://static.ddlee.cn/static/img/Ubuntu-3/app.png" alt="Applications"></li>
<li>（荐）Drop down terminal：快捷启动终端<br><img src="http://static.ddlee.cn/static/img/Ubuntu-3/drop.png" alt="Drop"></li>
<li>Netspeed：网速监控</li>
<li>Openweather：状态栏天气预报</li>
<li>Removable drive menu：弹出U盘等可移除硬件</li>
<li>（荐）Dynamic top bar：根据窗口颜色变换顶栏颜色</li>
</ol>
<h2 id="其他桌面工具"><a href="#其他桌面工具" class="headerlink" title="其他桌面工具"></a>其他桌面工具</h2><h4 id="DOCK"><a href="#DOCK" class="headerlink" title="DOCK"></a>DOCK</h4><p>推荐<code>Cairo-Dock</code>，效果如图，扩展性很高，自定义程度也很好。</p>
<p><img src="http://static.ddlee.cn/static/img/Ubuntu-3/Dock1.jpg" alt="Cairo-Dock"></p>
<h4 id="CONKY：桌面监测工具"><a href="#CONKY：桌面监测工具" class="headerlink" title="CONKY：桌面监测工具"></a>CONKY：桌面监测工具</h4><p>推荐<code>Conky</code>，皮肤也有很多，效果如图。</p>
<p><img src="http://static.ddlee.cn/static/img/Ubuntu-3/Conky1.jpg" alt="Conky"></p>
<p>本系列至此完结。欢迎入坑。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
<p>2016年6月</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Gnome </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[个性化你的Ubuntu-2：GNOME安装与工具]]></title>
      <url>http://blog.ddlee.cn/2016/06/02/%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BD%A0%E7%9A%84Ubuntu-2%EF%BC%9AGNOME%E5%AE%89%E8%A3%85%E4%B8%8E%E5%B7%A5%E5%85%B7/</url>
      <content type="html"><![CDATA[<h3 id="GNOME安装"><a href="#GNOME安装" class="headerlink" title="GNOME安装"></a>GNOME安装</h3><p>从上一篇文章，大家可以看到，GNOME是一系列软件的集合，安装时可以有不同的取舍。对于Ubuntu用户来说，可以有以下两类体验GNOME的方式。（参考：<a href="https://wiki.ubuntuusers.de/GNOME_Installation/" target="_blank" rel="external">GNOME installation</a>）</p>
<h4 id="1-Ubuntu-GNOME（系统）"><a href="#1-Ubuntu-GNOME（系统）" class="headerlink" title="1.Ubuntu GNOME（系统）"></a>1.Ubuntu GNOME（系统）</h4><p>Ubuntu GNOME是Ubuntu的一个发行版本（也称Ubuntu variants），就像Ubuntu和Fedora等都是GNU/Linux的发行版那样。Ubuntu GNOME不仅包含了Ubuntu的核心部分、GNOME的核心部分，还有一系列的标准应用。</p>
<h5 id="Install-from-DVD"><a href="#Install-from-DVD" class="headerlink" title="Install from DVD"></a>Install from DVD</h5><p>如果可以接受重新安装系统，请到这里<a href="http://ubuntugnome.org/download/" target="_blank" rel="external">下载</a>Ubuntu GNOME。</p>
<h5 id="Install-with-current-system"><a href="#Install-with-current-system" class="headerlink" title="Install with current system"></a>Install with current system</h5><p>你也可以通过安装metapackage，这样在安装GNOME桌面环境时，你的系统中未安装的标准应用也会被同时安装。</p>
<p><code>sudo apt-get install ubuntu-gnome-desktop</code></p>
<h4 id="2-GNOME（仅桌面环境）"><a href="#2-GNOME（仅桌面环境）" class="headerlink" title="2.GNOME（仅桌面环境）"></a>2.GNOME（仅桌面环境）</h4><h5 id="The-“real”-GNOME"><a href="#The-“real”-GNOME" class="headerlink" title="The “real” GNOME"></a>The “real” GNOME</h5><p>标准的GNOME桌面环境，没有Ubuntu的特性（尽管我区分不出哪些是Ubuntu提供的），也不安装附加的标准应用：</p>
<p><code>sudo apt-get install gnome</code></p>
<h5 id="The-minimux-GNOME"><a href="#The-minimux-GNOME" class="headerlink" title="The minimux GNOME"></a>The minimux GNOME</h5><p>GNOME的核心部分，不安装附加的标准应用：</p>
<p><code>sudo apt-get install gnome-core</code></p>
<h5 id="GNOME-shell"><a href="#GNOME-shell" class="headerlink" title="GNOME shell"></a>GNOME shell</h5><p>仅安装GNOME的图形界面：<br><code>sudo apt-get install gnome-shell</code></p>
<p>你还需要：<br><code>sudo apt-get install gnome-session</code></p>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>在同一系统上安装不同的桌面环境可能会造成一些意料不到的问题（如锁屏界面丢失），最推荐的方案还是重新安装Ubuntu GNOME，其次，可以安装<code>ubuntu-gnome-desktop</code>。</p>
<h4 id="使用新的桌面环境"><a href="#使用新的桌面环境" class="headerlink" title="使用新的桌面环境"></a>使用新的桌面环境</h4><p>安装完毕后，重启，可在登录界面选择桌面环境。</p>
<p><img src="http://static.ddlee.cn/static/img/Ubuntu-2/gnome-login1.png" alt="login1"></p>
<p><img src="http://static.ddlee.cn/static/img/Ubuntu-2/gnome-login2.png" alt="login2"></p>
<h3 id="GNOME配置工具：gnome-tweak-tool"><a href="#GNOME配置工具：gnome-tweak-tool" class="headerlink" title="GNOME配置工具：gnome-tweak-tool"></a>GNOME配置工具：gnome-tweak-tool</h3><p>想要充分个性化GNOME桌面环境，扩展GNOME的功能，你还需要安装GNOME的配置工具：gnome tweak tool</p>
<p><code>sudo apt-get install gnome-tweak-tool</code></p>
<p><img src="http://static.ddlee.cn/static/img/Ubuntu-2/gnome-tweak-tool_004.jpg" alt="图片：gnome tweak tool提供的功能"></p>
<p>利用gnome tweak tool，你可以管理桌面主题、调整窗口特性、调整显示字体、加载GNOME扩展、管理开机自启程序等等。</p>
<h3 id="扩展插件"><a href="#扩展插件" class="headerlink" title="扩展插件"></a>扩展插件</h3><p>在Ubuntu上，要调整桌面主题，可没有Windows上鼠标右击一下那么简单。<br>你要先安装上面的tweak tool，然后有人告诉你需要User theme扩展插件，而你跑到<code>extensions.gnome.org</code>，遇到的却是这个：<br><img src="http://static.ddlee.cn/static/img/Ubuntu-2/Selection_019.jpg" alt="错误"></p>
<p>我明明装了GNOME的啊！</p>
<p>这是因为，<code>extensions.gnome.org</code>需要与浏览器通信，调用click-to-play的功能，我们需要安装GNOMNE shell intergration这个插件。</p>
<h4 id="Chrome用户"><a href="#Chrome用户" class="headerlink" title="Chrome用户"></a>Chrome用户</h4><h5 id="利用PPA"><a href="#利用PPA" class="headerlink" title="利用PPA"></a>利用PPA</h5><pre><code>sudo add-apt-repository ppa:ne0sight/chrome-gnome-shell
sudo apt-get update
sudo apt-get install chrome-gnome-shell
</code></pre><h5 id="通过Chrome-Web-Store-GNOME-Shell-integration"><a href="#通过Chrome-Web-Store-GNOME-Shell-integration" class="headerlink" title="通过Chrome Web Store:GNOME Shell integration"></a>通过Chrome Web Store:<a href="https://chrome.google.com/webstore/detail/gnome-shell-integration/gphhapmejobijbbhgpjhcjognlahblep" target="_blank" rel="external">GNOME Shell integration</a></h5><p>可能需要通过CMake安装native connector,请参考这一<a href="https://wiki.gnome.org/Projects/GnomeShellIntegrationForChrome/Installation" target="_blank" rel="external">页面</a>。</p>
<h4 id="FireFox用户"><a href="#FireFox用户" class="headerlink" title="FireFox用户"></a>FireFox用户</h4><p>使用FireFox访问<code>extensions.gnome.org</code>时会有运行GNOME shell integration的通知，允许运行后刷新即可。</p>
<p>更多信息，请参考这一<a href="https://extensions.gnome.org/about/#no-detection" target="_blank" rel="external">页面</a></p>
<p>安装好<code>tweak-tool</code>后，祝贺你已经打开了新世界的大门。下篇文章是关于扩展插件的推荐，欢迎继续阅读。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Gnome </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[个性化你的Ubuntu-1：GNOME桌面环境]]></title>
      <url>http://blog.ddlee.cn/2016/05/30/%E4%B8%AA%E6%80%A7%E5%8C%96%E4%BD%A0%E7%9A%84Ubuntu-1%EF%BC%9AGNOME%E6%A1%8C%E9%9D%A2%E7%8E%AF%E5%A2%83/</url>
      <content type="html"><![CDATA[<h2 id="我与Ubuntu"><a href="#我与Ubuntu" class="headerlink" title="我与Ubuntu"></a>我与Ubuntu</h2><p>我最初是Windows98用户，再到Windows2003,Windows XP,Windows 7,上了大学后用Windows 8.1,Windows 10（想不到竟然能列这么长；我从没用过Windows Vista,不知道那是什么东西），我很喜欢8.1和10的开始屏幕和动态磁贴。非常偶然的机会，我在CS50的课程中接触了GNU/Linux，才知道，原来在MS　Windows和Mac OSＸ之外，还有一个GNU/Linux。换完SSD，学会了装操作系统，我便踏上了折腾GNU/Linux的不归路。</p>
<p>曾经被一个软院的同学安利Red Hat系的Fedora（尽管他现在已经投入了MacBook的怀抱）,普及各种内核之类的知识。然而，我只想安静的用它上上网，进行科学计算，并没有深入到考虑系统底层的需求层次。我还是安心地用Ubuntu吧。我也推荐第一次尝试GNU/Linux系统的小白从Ubuntu开始，相信我,askubuntu.com和stackoverflow.com会解决你的大部分问题的。</p>
<h2 id="个性化你的Ubuntu（一）：GNOME桌面环境"><a href="#个性化你的Ubuntu（一）：GNOME桌面环境" class="headerlink" title="个性化你的Ubuntu（一）：GNOME桌面环境"></a>个性化你的Ubuntu（一）：GNOME桌面环境</h2><p>相信不少读者都是从Microsoft Windows转到GNU/Linux阵营的,早就习惯了用户图形界面。但是，配合桌面环境、主题和一些插件和软件，Ubuntu照样可以很酷炫。</p>
<h3 id="什么是GNOME"><a href="#什么是GNOME" class="headerlink" title="什么是GNOME"></a>什么是GNOME</h3><p><img src="http://static.ddlee.cn/static/img/Ubuntu-1/Gnomelogo.png" alt="GNOME Logo"><br>（大脚丫为什么这么大。。。）</p>
<p>GNOME(pronounced /ɡˈnoʊm/ or /ˈnoʊm/) 最初是GNU Network Object Model Environment的缩写，但这一缩写已不再沿用（更多历史情况请参见<a href="https://mail.gnome.org/archives/marketing-list/2010-April/msg00050.html" target="_blank" rel="external">这里</a>）。</p>
<p>我们所说的GNOME，通常指的是由<a href="https://www.gnome.org/about/" target="_blank" rel="external">The GNOME Project</a>开发的运行于Linux之上的桌面环境。</p>
<p>我们每天面对的，并不是全部的Microft Windows/OS X/Linux系统，而是系统提供给我们的人机接口，而桌面环境，则是统一在同一图形用户接口（GUI）之下的一揽子软件（X Window Manager, File manager, Terminal emulator, Text editor, Image viewer, E-mail client等）。</p>
<p><img src="http://static.ddlee.cn/static/img/Ubuntu-1/OS&amp;GUI.jpg" alt="操作系统提供用户图形界面给用户作为人机接口"><br><a href="http://www.slideshare.net/sherif_mosa/operating-systems-basics-26277922" target="_blank" rel="external">来源</a></p>
<p>Ubuntu自带的桌面环境是<a href="https://unity.ubuntu.com/" target="_blank" rel="external">Unity</a>（图形外壳）,其他流行的桌面环境还有<a href="http://www.kde.org/" target="_blank" rel="external">KDE</a>,<a href="http://www.xfce.org/" target="_blank" rel="external">Xfce</a>。但我们要谈的是GNOME。</p>
<h3 id="什么是X-window-system"><a href="#什么是X-window-system" class="headerlink" title="什么是X window system"></a>什么是X window system</h3><p>要谈Unix-like系统上的图形界面，就不得不提X Window System。那么，什么是X?</p>
<blockquote>
<p>The X Window System, commonly referred to merely as X, is a highly configurable, cross-platform, complete and free client-server system for managing graphical user interfaces (GUIs) on single computers and on networks of computers.</p>
<p>(X窗口系统，通常简称为X，是用于管理在单个计算机和计算机网络上运行的图形用户界面（GUI）一个高度可配置的，跨平台，完整的，自由的客户端-服务器系统。）</p>
<p>来源：<a href="http://www.linfo.org/x.html" target="_blank" rel="external">LINFO</a></p>
</blockquote>
<p>我们试着通过X能够干什么来理解一下这句话。</p>
<p>X是一组规则、一套方法。它提供了从硬件（键鼠）接受用户输入、创建图形窗口、画出直线、位图等基本的图形功能（图形引擎）。</p>
<p>X实现了客户端-服务器的机制。通过划分Server和Client，X既能在本地计算机上运行，也能在计算计算机网络中运行。</p>
<p>X与操作系统独立。X可以理解为运行在操作系统之上的一套软件。如果不需要GUI，完全可以不用安装X。而在Microsoft Windows和OS X中，图形引擎是操作系统的一部分。</p>
<p>X Window System的结构如图。<br><img src="http://static.ddlee.cn/static/img/Ubuntu-1/X-window-system.png" alt="X Window System"></p>
<h3 id="GNOME-amp-X"><a href="#GNOME-amp-X" class="headerlink" title="GNOME &amp; X"></a>GNOME &amp; X</h3><p>GNOME和X Window System是什么关系？<br>桌面环境可以理解为一系列X client的集合，其中最重要的组件是X Window Manager。由于X Window System的client-server机制，各client之间是相对独立的，这时，需要一个特殊的client管理其他client，将他们统一在一个框架之下，这就是X Window Manager。<br><img src="http://static.ddlee.cn/static/img/Ubuntu-1/Window_Manager.png" alt="Window Manager"><br><a href="http://www.slideshare.net/RBandes/x-window-system" target="_blank" rel="external">来源</a></p>
<p>而GNOME另一个重要的组成部分是GNOME shell，它是一个图形外壳程序，也就是我们要面对的接口。</p>
<p>跟GNOME相关的其他组件、库、概念</p>
<ul>
<li>GTK+：GIMP Widget toolkits，GNOME基于的<a href="https://en.wikipedia.org/wiki/Widget_toolkit" target="_blank" rel="external">GUI工具箱</a>。KDE则基于Qt。</li>
<li>Display Manager:图形用户登陆管理器，为用户提供登陆界面，与session manager通信，开启新的session。GNOME使用的是GDM。</li>
<li>Metacity：GNOME 2使用的window manager，GNOME 3使用的是Mutter。KDE使用的是KWin。</li>
<li>Wayland:与X Window System对应，也是一种窗口系统</li>
</ul>
<h3 id="我现在的桌面"><a href="#我现在的桌面" class="headerlink" title="我现在的桌面"></a>我现在的桌面</h3><p><img src="http://static.ddlee.cn/static/img/Ubuntu-1/Gnome1.jpg" alt="我的桌面"></p>
<p>我不喜欢双击桌面图标来启动程序，更多用的是Dock和全局搜索，所以，桌面上“什么都没有”。</p>
<p>桌面的壁纸是电影 <em>飞屋环游记</em> 的海报，使用了Numix系列的主题和图标。</p>
<p>下方Dock使用的程序是Cairo-Dock，桌面右方运行的程序是Conky，用来监测系统运行情况和提供天气信息，上方的Topbar里添加了许多GNOME的扩展应用。</p>
<p>接下来的两篇文章将介绍Gnome的安装与扩展推荐，欢迎继续阅读，撒花。</p>
<p>@<a href="http://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Gnome </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
