<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>萧爽楼</title>
  <subtitle>李家丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.ddlee.cn/"/>
  <updated>2018-03-18T13:36:01.000Z</updated>
  <id>https://blog.ddlee.cn/</id>
  
  <author>
    <name>ddlee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>目标检测常用评测集：Pascal VOC, MS COCO, Cityscapes</title>
    <link href="https://blog.ddlee.cn/posts/7845fb62/"/>
    <id>https://blog.ddlee.cn/posts/7845fb62/</id>
    <published>2018-03-04T16:18:16.000Z</published>
    <updated>2018-03-18T13:36:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文节选自博主通过格灵深瞳机构号发表于知乎的文章：<a href="https://zhuanlan.zhihu.com/p/34179420" target="_blank" rel="external">目标检测入门（二）：模型的评测与训练技巧
</a>。</p>
<h2 id="检测模型的评测指标"><a href="#检测模型的评测指标" class="headerlink" title="检测模型的评测指标"></a>检测模型的评测指标</h2><p>目标检测模型本源上可以用统计推断的框架描述，我们关注其犯第一类错误和第二类错误的概率，通常用准确率和召回率来描述。准确率描述了模型有多准，即在预测为正例的结果中，有多少是真正例；召回率则描述了模型有多全，即在为真的样本中，有多少被我们的模型预测为正例。不同的任务，对两类错误有不同的偏好，常常在某一类错误不多于一定阈值的情况下，努力减少另一类错误。在检测中，mAP（mean Average Precision）作为一个统一的指标将这两种错误兼顾考虑。</p>
<p>具体地，对于每张图片，检测模型输出多个预测框（常常远超真实框的个数），我们使用IoU（Intersection Over Union，交并比）来标记预测框是否为预测正确。标记完成后，随着预测框的增多，召回率总会提升，在不同的召回率水平下对准确率做平均，即得到AP，最后再对所有类别按其所占比例做平均，即得到mAP。</p>
<p>在较早的Pascal VOC数据集上，常采用固定的一个IoU阈值（如0.5, 0.75）来计算mAP，现阶段较为权威的MS COCO数据集上，对不同的IoU阈值（0.5-0.95，0.05为步长）分别计算AP，再综合平均，并且给出了不同大小物体分别的AP表现，对定位准确的模型给予奖励并全面地展现不同大小物体上检测算法的性能，更为科学合理。</p>
<p>在实践中，我们不仅关注检测模型的精度，还关注其运行的速度，常常用FPS（Frame Per Second，每秒帧率）来表示检测模型能够在指定硬件上每秒处理图片的张数。通常来讲，在单块GPU上，两阶段方法的FPS一般在个位数，而单阶段方法可以达到数十。现在检测模型运行的平台并不统一，实践中也不能部署较为昂贵的GPU进行推断。事实上，很多文章并没有严谨讨论其提出模型的速度表现（加了较多的trick以使精度达到SOTA），另外，考虑到目前移动端专用芯片的发展速度和研究进展，速度方面的指标可能较难形成统一的参考标准，需要谨慎看待文章中汇报的测试结果。</p>
<h2 id="标准评测数据集"><a href="#标准评测数据集" class="headerlink" title="标准评测数据集"></a>标准评测数据集</h2><h3 id="Pascal-VOC（Pascal-Visual-Object-Classes）"><a href="#Pascal-VOC（Pascal-Visual-Object-Classes）" class="headerlink" title="Pascal VOC（Pascal Visual Object Classes）"></a>Pascal VOC（<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="external">Pascal Visual Object Classes</a>）</h3><p>自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的trainval2007和16k的trainval2012作为训练集（07+12），test2007作为测试集，用10k的trainval2007+test2007和和16k的trainval2012作为训练集（07++12），test2012作为测试集，分别汇报结果。</p>
<p>Pascal VOC对早期检测工作起到了重要的推动作用，目前提升的空间相对有限，权威评测集的交接棒也逐渐传给了下面要介绍的COCO。</p>
<h3 id="MS-COCO（Common-Objects-in-COntext）"><a href="#MS-COCO（Common-Objects-in-COntext）" class="headerlink" title="MS COCO（Common Objects in COntext）"></a>MS COCO（<a href="http://cocodataset.org" target="_blank" rel="external">Common Objects in COntext</a>）</h3><p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco.png" alt="MS COCO Roadmap，https://places-coco2017.github.io/"> <em>检测任务在COCO数据集上的进展</em></p>
<p>COCO数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/iconic-non-iconic.png" alt="iconic"> <em>iconic与non-iconic图片对比</em></p>
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p>
<p>COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco-stat.png" alt="dist"> <em>COCO数据集分布</em></p>
<p>在分布方面，COCO的每个类含有更多实例，分布也较为均衡（上图a），每张图片包含更多类和更多的实例（上图b和c，均为直方图，每张图片平均分别含3.3个类和7.7个实例），相比Pascal VOC，COCO还含有更多的小物体（下图，横轴是物体占图片的比例）。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco-size.png" alt="dist-2"> <em>COCO数据集物体大小分布</em></p>
<p>如本文第一节所述，COCO提供的评测标准更为精细化，提供的<a href="https://github.com/cocodataset/cocoapi" target="_blank" rel="external">API</a>不仅包含了可视化、评测数据的功能，还有对模型的错误来源分析脚本，能够更清晰地展现算法的不足之处。COCO所建立的这些标准也逐渐被学术界认可，成为通用的评测标准。您可以在<a href="http://cocodataset.org/#detections-leaderboard" target="_blank" rel="external">这里</a>找到目前检测任务的LeaderBoard。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/coco-error-analysis.jpg" alt="coco-error-breakdown"> <em>错误来源分解，详见<a href="http://cocodataset.org/#detections-eval" target="_blank" rel="external">http://cocodataset.org/#detections-eval</a></em></p>
<h3 id="Cityscapes"><a href="#Cityscapes" class="headerlink" title="Cityscapes"></a><a href="https://www.cityscapes-dataset.com/" target="_blank" rel="external">Cityscapes</a></h3><p><img src="https://static.ddlee.cn/static/img/目标检测常用评测集：Pascal-VOC-MS-COCO-Cityscapes/cityscapes-example.png" alt="cityscapes"> <em>Cityscapes数据示例</em></p>
<p>Cityscapes数据集专注于现代城市道路场景的理解，提供了30个类的像素级标注，是自动驾驶方向较为权威的评测集。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文节选自博主通过格灵深瞳机构号发表于知乎的文章：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34179420&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;目标检测入门（二）：模型的评测与训练技巧
&lt;/a&gt;。&lt;/p&gt;
&lt;h
    
    </summary>
    
      <category term="AI" scheme="https://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>目标检测任务表述与模型基本结构</title>
    <link href="https://blog.ddlee.cn/posts/13efca78/"/>
    <id>https://blog.ddlee.cn/posts/13efca78/</id>
    <published>2018-03-04T16:14:31.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文节选自博主通过格灵深瞳机构号发表在知乎上的文章：<a href="https://zhuanlan.zhihu.com/p/34142321" target="_blank" rel="external">干货 | 目标检测入门，看这篇就够了</a>。</p>
<h2 id="目标检测的任务表述"><a href="#目标检测的任务表述" class="headerlink" title="目标检测的任务表述"></a>目标检测的任务表述</h2><p>如何从图像中解析出可供计算机理解的信息，是机器视觉的中心问题。深度学习模型由于其强大的表示能力，加之数据量的积累和计算力的进步，成为机器视觉的热点研究方向。</p>
<p>那么，如何理解一张图片？根据后续任务的需要，有三个主要的层次。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测任务表述与模型基本结构/cv.jpg" alt="cv"> <em>图像理解的三个层次</em></p>
<p>一是分类（Classification），即是将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片。这一任务是最简单、最基础的图像理解任务，也是深度学习模型最先取得突破和实现大规模应用的任务。其中，ImageNet是最权威的评测集，每年的ILSVRC催生了大量的优秀深度网络结构，为其他任务提供了基础。在应用领域，人脸、场景的识别等都可以归为分类任务。</p>
<p>二是检测（Detection）。分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置），因而，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检出目标的类别和位置（常用矩形检测框的坐标表示）。</p>
<p>三是分割（Segmentation）。分割包括语义分割（semantic segmentation）和实例分割（instance segmentation），前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。</p>
<h2 id="检测模型基本特点"><a href="#检测模型基本特点" class="headerlink" title="检测模型基本特点"></a>检测模型基本特点</h2><p>深度学习方法主导下的检测模型，可以分为两阶段（two-stage）和单阶段（one-stage）。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测任务表述与模型基本结构/faster-rcnn-arch.png" alt="faster-rcnn-arch"> <em>两阶段检测模型Pipeline，<a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_blank" rel="external">来源</a></em></p>
<p>检测模型整体上由基础网络（Backbone Network）和检测头部（Detection Head）构成。前者作为特征提取器，给出图像不同大小、不同抽象层次的表示；后者则依据这些表示和监督信息学习类别和位置关联。检测头部负责的类别预测和位置回归两个任务常常是并行进行的，构成多任务的损失进行联合训练。</p>
<p><img src="https://static.ddlee.cn/static/img/目标检测任务表述与模型基本结构/faster-rcnn-head.png" alt="faster-rcnn-head"> <em>检测模型头部并行的分支，<a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_blank" rel="external">来源</a></em></p>
<p>相比单阶段，两阶段检测模型通常含有一个串行的头部结构，即完成前背景分类和回归后，把中间结果作为RCNN头部的输入再进行一次多分类和位置回归。这种设计带来了一些优点：</p>
<ul>
<li>对检测任务的解构，先进行前背景的分类，再进行物体的分类，这种解构使得监督信息在不同阶段对网络参数的学习进行指导</li>
<li>RPN网络为RCNN网络提供良好的先验，并有机会整理样本的比例，减轻RCNN网络的学习负担</li>
</ul>
<p>这种设计的缺点也很明显：中间结果常常带来空间开销，而串行的方式也使得推断速度无法跟单阶段相比；级联的位置回归则会导致RCNN部分的重复计算（如两个RoI有重叠）。</p>
<p>另一方面，单阶段模型只有一次类别预测和位置回归，卷积运算的共享程度更高，拥有更快的速度和更小的内存占用。读者将会在接下来的文章中看到，两种类型的模型也在互相吸收彼此的优点，这也使得两者的界限更为模糊。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文节选自博主通过格灵深瞳机构号发表在知乎上的文章：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34142321&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;干货 | 目标检测入门，看这篇就够了&lt;/a&gt;。&lt;/p&gt;
&lt;h2 
    
    </summary>
    
      <category term="AI" scheme="https://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="R-CNN" scheme="https://blog.ddlee.cn/tags/R-CNN/"/>
    
      <category term="YOLO" scheme="https://blog.ddlee.cn/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]YOLO9000: Better, Faster, Stronger</title>
    <link href="https://blog.ddlee.cn/posts/16826395/"/>
    <id>https://blog.ddlee.cn/posts/16826395/</id>
    <published>2018-03-02T15:55:48.000Z</published>
    <updated>2018-03-18T13:37:25.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/1612.08242" target="_blank" rel="external">YOLO9000: Better, Faster, Stronger</a></p>
<p>在这篇文章里，单阶段检测模型的先驱工作<a href="https://blog.ddlee.cn/posts/41036331/">YOLO</a>迎来了全面的更新：</p>
<ol>
<li>在卷积层添加BN，舍弃Dropout</li>
<li>更高尺寸的输入</li>
<li>使用Anchor Boxes，并在头部运用卷积替代全连接层</li>
<li>使用聚类方法得到更好的先验，用于生成Anchor Boxes</li>
<li>参考Fast R-CNN的方法对位置坐标进行log/exp变换使坐标回归的损失保持在合适的数量级</li>
<li>passthrough层：类似ResNet的skip-connection，将不同尺寸的feature map拼接到一起</li>
<li>多尺度训练</li>
<li>更高效的网络Darknet-19，类似VGG的网络，在ImageNet上以较少的参数量达到跟当前最佳相当的精度</li>
</ol>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-YOLO9000-Better-Faster-Stronger/yolov2.jpg" alt="yolov2"></p>
<p>此次改进后，YOLOv2吸收了很多工作的优点，达到跟SSD相当的精度和更快的推断速度。</p>
<p>作者还介绍了一种新的联合训练方式：同时训练分类任务和检测任务，使得检测模型能够泛化到检测训练集之外的目标类上。</p>
<p>YOLO9000使用了ImageNet和COCO数据集联合训练，在合并两者的标签时，根据WordNet的继承关系构建了了树状的类别预测图：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-YOLO9000-Better-Faster-Stronger/yolo9000_tree.jpg" alt="wordtree"></p>
<p>类似条件概率的方式计算每个子标签的概率值，超出一定的阈值时则选定该类作为输出，训练时也仅对其路径上的类别进行损失的计算和BP。</p>
<p>YOLO9000为我们提供了一种泛化检测模型的训练方式，文章的结果显示YOLO9000在没有COCO标注的类别上有约20的mAP表现，能够检测的物体类别超过9000种。当然，其泛化性能也受检测标注类别的制约，在有类别继承关系的类上表现不错，而在完全没有语义联系的类上表现很差。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/1612.08242&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;YOLO9000: Better, Faster, Stronger&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在这篇文章里，单阶段检测模型的先驱工作&lt;a
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="YOLO" scheme="https://blog.ddlee.cn/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记](OHEM)Training Region-based Object Detectors with Online Hard Example Mining</title>
    <link href="https://blog.ddlee.cn/posts/a1e87dc9/"/>
    <id>https://blog.ddlee.cn/posts/a1e87dc9/</id>
    <published>2018-02-21T16:00:41.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1604.03540" target="_blank" rel="external">Training Region-based Object Detectors with Online Hard Example Mining</a></p>
<h3 id="OHEM-Online-Hard-negative-Example-Mining，在线难例挖掘"><a href="#OHEM-Online-Hard-negative-Example-Mining，在线难例挖掘" class="headerlink" title="OHEM(Online Hard negative Example Mining，在线难例挖掘)"></a>OHEM(Online Hard negative Example Mining，在线难例挖掘)</h3><p>本文是Bootstrapping（自助采样）在深度网络中的应用。两阶段网络由于其多步的特性，在RCNN子网络的计算前会有对RoI的整理过程，早期工作中，Fast R-CNN利用随机上采样和下采样的方法来维持正负样本比例，而本文提出的方法则使得网络具有挑选“好的”正负样本的能力。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Training-Region-based-Object-Detectors-with-Online-Hard-Example-Mining/ohem.png" alt="ohem"></p>
<p>作者提出用R-CNN子网络对RoI Proposal预测的分数来决定每个batch选用的样本，这样，输入R-CNN子网络的RoI Proposal总为其表现不好的样本，提高了监督学习的效率。实际操作中，维护两个完全相同的R-CNN子网络，其中一个只进行前向传播来为RoI Proposal的选择提供指导，另一个则为正常的R-CNN，参与损失的计算并更新权重，并且将权重复制到前者以使两个分支权重同步。</p>
<p>OHEM以额外的R-CNN子网络的开销来改善RoI Proposal的质量，更有效地利用数据的监督信息，成为两阶段模型提升性能的常用部件之一。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1604.03540&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Training Region-based Object Detectors with Online Hard Example
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>PyCharm+PipEnv本地Python开发环境配置</title>
    <link href="https://blog.ddlee.cn/posts/2da92ac3/"/>
    <id>https://blog.ddlee.cn/posts/2da92ac3/</id>
    <published>2018-02-12T02:09:22.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录基于PyCharm的Python工作环境配置，最终实现的效果是本地修改，远程调试，运行环境可迁移。</p>
<h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>该环境的配置需要如下准备：</p>
<ul>
<li>开发用笔记本电脑和部署用服务器</li>
<li><p>PyCharm Pro版本，如果是学生，可以免费得到这一版本</p>
</li>
<li><p>依赖管理包Pipenv</p>
</li>
</ul>
<p>逻辑是在本地编写和调试代码，调用服务器的计算资源运行，并通过PyCharm和Pipenv保证本地开发环境和服务器端运行环境的一致。</p>
<p>关于Pipenv：<br>Pipenv是一个将pip和virtualenv功能整合在一起的依赖管理包，提倡一个项目一个环境，利用Pipfile存储依赖信息并提供可迁移性。Pipenv为每个项目建立一个virtualenv，并记录pip安装依赖包的版本信息。</p>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>1.（可选）在GitHub等版本控制服务建立repo。<br>2.在服务器上建立项目文件夹，并用命令<code>pipenv install</code>初始化项目环境。<br>3.本地使用PyCharm打开项目（从GitHub，或者新建在本地文件夹），并配置部署环境（Tools-&gt;Deployment-&gt;Configurations)，以SFTP方式连接，并在本地开发项目文件夹和服务器端项目文件夹建立映射。</p>
<p><img src="https://static.ddlee.cn/static/img/PyCharm-PipEnv本地Python开发环境配置/sftp.png" alt="sftp"></p>
<p>4.打开Files-&gt;settings-&gt;project settings-&gt;project interpreter，选择add remote，勾选Deployment configuration到上一步建立的配置，选择Move this server to IDE settings。</p>
<p><img src="https://static.ddlee.cn/static/img/PyCharm-PipEnv本地Python开发环境配置/interpreter.png" alt="interpreter"></p>
<p>最后指定好上一步pipenv建立的vitualenv（默认目录为~/.local/share/virtualenvs/）为Python解释器的路径。</p>
<p><img src="https://static.ddlee.cn/static/img/PyCharm-PipEnv本地Python开发环境配置/patch.png" alt="path"></p>
<p>5.在Tools-&gt;Deployment-&gt;Download from将服务器代码下载到本地（主要是Pipfile），之后右击项目，将项目文件上传到服务器（本地代码文件），并在Tools-&gt;Deployment勾选Automatic Upload，使本地代码跟服务器代码保持同步。</p>
<p>6.在Tools-&gt;Start SSH Session利用部署配置登录服务器，使用Pipenv来安装需要的依赖后，在本地新建脚本文件进行测试。</p>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p>1.本地修改代码，自动上传到服务器<br>2.利用SSH处理数据存取路径等问题<br>3.运行脚本（使用远程环境）并调试<br>4.解决报错问题<br>5.调试成功，将代码提交到版本控制服务<br>6.重复</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录基于PyCharm的Python工作环境配置，最终实现的效果是本地修改，远程调试，运行环境可迁移。&lt;/p&gt;
&lt;h3 id=&quot;准备&quot;&gt;&lt;a href=&quot;#准备&quot; class=&quot;headerlink&quot; title=&quot;准备&quot;&gt;&lt;/a&gt;准备&lt;/h3&gt;&lt;p&gt;该环境的配置需要如
    
    </summary>
    
      <category term="Programming" scheme="https://blog.ddlee.cn/categories/Programming/"/>
    
    
      <category term="Python" scheme="https://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Software" scheme="https://blog.ddlee.cn/tags/Software/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记](MobileNet V2)Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</title>
    <link href="https://blog.ddlee.cn/posts/c9816b0a/"/>
    <id>https://blog.ddlee.cn/posts/c9816b0a/</id>
    <published>2018-01-17T16:52:08.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是MobileNets的第二版。<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">第一版</a>中，MobileNets全面应用了Depth-wise Seperable Convolution并提出两个超参来控制网络容量，在保持移动端可接受的模型复杂性的基础上达到了相当的精度。而第二版中，MobileNets应用了新的单元：Inverted residual with linear bottleneck，主要的改动是添加了线性Bottleneck和将skip-connection转移到低维bottleneck层。</p>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>本篇比较丰富的地方是对网络中bottleneck结构的探讨。</p>
<p>在最早的Network in Network工作中，1x1卷积被作为一个降维的操作而引入，后来逐渐发展为Depth-wise Seperable Convolution（可分离卷积）并被广泛应用，堪称跟skip-connection同样具有影响力的网络部件。在<a href="https://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/">Inception单元</a>最初提出之时，具有较多channel的feature map被认为是可供压缩的，作者引入1x1卷积将它们映射到低维（较少channel数）空间上并添加多路径处理的范式。之后的<a href="https://blog.ddlee.cn/2018/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/">Xception</a>、MobileNets等工作则将可分离卷积应用到极致：前者指出可分离卷积背后的假设是跨channel相关性和跨spatial相关性的解耦，后者则利用可控的两个超参来获得在效率和精度上取得较好平衡的网络。</p>
<p>文中，经过激活层后的张量被称为兴趣流形，具有维HxWxD，其中D即为通常意义的channel数，部分文章也将其称为网络的宽度（width）。</p>
<p>根据之前的研究，兴趣流形可能仅分布在激活空间的一个低维子空间里，利用这一点很容易使用1x1卷积将张量降维（即MobileNet V1的工作），但由于ReLU的存在，这种降维实际上会损失较多的信息。下图是一个例子。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/collapse.png" alt="envolve"></p>
<p>上图中，利用MxN的矩阵B将张量（2D，即N=2）变换到M维的空间中，通过ReLUctant后（y=ReLU(Bx)），再用此矩阵之逆恢复原来的张量。可以看到，当M较小时，恢复后的张量坍缩严重，M较大时则恢复较好。</p>
<p>这意味着，在较低维度的张量表示（兴趣流形）上进行ReLU等线性变换会有很大的信息损耗。因而本文提出使用线性变换替代Bottleneck的激活层，而在需要激活的卷积层中，使用较大的M使张量在进行激活前先扩张，整个单元的输入输出是低维张量，而中间的层则用较高维的张量。文中所用单元的演化过程如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/envolve.png" alt="envolve"></p>
<p>。图a中普通卷积将channel和spatial的信息同时进行映射，参数量较大；图b为可分离卷积，解耦了channel和spatial，化乘法为加法，有一定比例的参数节省；图c中进行可分离卷积后又添加了bottleneck，映射到低维空间中；图d则是从低维空间开始，进行可分离卷积时扩张到较高的维度（前后维度之比被称为expansion factor，扩张系数），之后再通过1x1卷积降到原始维度。</p>
<p>实际上，图c和图d的结构在堆叠时是等价的，只是观察起点的不同。但基于兴趣流形应该分布在一个低维子空间上的假设，这引出了文章的第二个关键点：将skip-connection转移到低维表达间，即Inverted residual block。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/invert_residual.png" alt="inverted"></p>
<p>综合以上两点，文章中网络所用的基本单元如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/module.png" alt="envolve"></p>
<p>文章指出，这种设计将层输入、输出空间跟层变换分离，即网络容量（capacity）和表达力（expressIveness）的解耦。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/module2.png" alt="envolve"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>MobileNet V2的整体结构如下表：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/arch.png" alt="envolve"></p>
<p>上图中，t代表单元的扩张系数，c代表channel数，n为单元重复个数，s为stride数。可见，网络整体上遵循了重复相同单元和加深则变宽等设计范式。也不免有人工设计的成分（如28^2*64单元的stride，单元重复数等）。</p>
<h4 id="ImageNet-Classification"><a href="#ImageNet-Classification" class="headerlink" title="ImageNet Classification"></a>ImageNet Classification</h4><p>MoblieNets V2仍然集成了V1版本的两个超参数，在ImageNet上的实验结果如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/imagenet.png" alt="envolve"></p>
<p>可以看到相比V1版本优势明显，在精度方面跟NAS搜索出的结构有相当的表现。</p>
<h4 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h4><p>文章还提出SSDLite来更好适应移动端需求，改动是将head部分的普通卷积都替换为了可分离卷积。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/coco.png" alt="envolve"></p>
<p>上面是在COCO上的表现，可以看到精度方面跟YOLOv2和SSD300相当（尽管很低，相比SOTA差距还很大），但模型参数和运算复杂度都有一个数量级的减少。最后的CPU时间是在Pixel上测得，可以到5FPS，达不到真正移动实时的要求，但也是不小的推进了（并没有给出GPU上的推断时间，而Pixel+TF-Lite的benchmark又跟其他网络难以产生有效的比较）。</p>
<h4 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h4><p>下图是在VOC上分割的结果：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/seg.png" alt="envolve"></p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>文章还做了关于线性变换bottleneck替代ReLU和skip-connection位置的实验，进一步支撑之前的分析。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/ablation.png" alt="envolve"></p>
<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p>本篇文章的附录部分提供了紧的n维流形在经过升维线性变化加ReLU后被映射到子集的期望大小的界，这个界说明在扩张到足够高的维度后，升维线性变换加ReLu能以较高的概率可逆（保持信息）并加入非线性。</p>
<p>上面的结论是非常拗口的。自己的理解是，使用ReLU引入非线性的同时会导致信息损失（非线性指不会被卷积、全连接等线性映射吸收掉，信息损失则是指ReLU将&lt;0的输入置0，输入变得稀疏，而若所有输入的某一维度都被置0，则会使输入空间本身降维），我们要对抗这一可能的信息损失，需要将输入先扩张，即y=ReLU(Bx)，x为R^n空间上的输入，B为m×n矩阵，我们期望m足够大，以达到扩张的效果并在经过ReLU后保持y跟x的信息量同样多（文中的引理二，即是证此变换可逆性的一个条件，应该是借用了代数的概念，矩阵在经过可逆变换后不会降秩，秩成为衡量信息损失的指标）。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/distribution.png" alt="envolve"></p>
<p>在ReLU(Bx)算符可逆性的问题上，作者做了一些经验性实验，如上图。a和b分别为训练前后，每层正激活channel数（可逆性条件）和其占总channel数比例的分布。图a和图b的左图，随网络加深，channel数增多，即变宽；训练前后，方差增大，且有两层低于了可逆性条件阈值（图b左图中绿色线低于紫色阈值的部分）。右图是一个比例，由于随机初始化，均值在0.5附近，训练后同样方差增大，而可逆性条件阈值一直为1/6（即为MobileNet V2扩张系数的倒数）。</p>
<p>附录的Theorem 1则证明了ReLU(Bx)算符将输入x压缩后的空间维度(n-volume)的界，此界在扩张系数较大时可以跟原空间相当，即信息损失很小。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>能够看到附录里给出文中假定或观察的数学证明的论文还是开心的。太多论文只是终于实验的SOTA而避而不谈Insights，况且给出证明。尽管本篇附录中的证明仍有经验主义的部分，且并没有完全定义清楚问题和结论，其对后续工作的启发价值还是有的。</p>
<p>这也暴露了当前领域的通病，我们没有共通的一套语言来描述自己的网络，譬如，如何定义网络的容量、表达力，如何衡量信息的损失。没有通用的定义造成了论文表述常常有令经验少者难以理解的表达。去定义这样一套语言和标准来为网络设计提供参考，希望成为以后的研究热点，也是我自己的一个思考方向。</p>
<p>总体来看，本篇文章提供的两点改进都是有启发性的，但并不完整，需要更多工作来补充。另外源码没有给出，会尝试复现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是MobileNets的第二版。&lt;a href=&quot;https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neura
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
      <category term="Mobile" scheme="https://blog.ddlee.cn/tags/Mobile/"/>
    
  </entry>
  
  <entry>
    <title>Chrome扩展推荐</title>
    <link href="https://blog.ddlee.cn/posts/72ed0837/"/>
    <id>https://blog.ddlee.cn/posts/72ed0837/</id>
    <published>2018-01-16T12:59:51.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是一个Chrome扩展的推荐列表。所有扩展插件在<a href="https://chrome.google.com/webstore/" target="_blank" rel="external">Chrome Web Store</a>都可搜索得到。</p>
<ul>
<li>Adblock Plus，用于屏蔽广告，效果出众，必备</li>
<li>Chrono Download Manager，下载任务管理，必备</li>
<li>Draw.io Desktop，流程图编辑，轻量方便</li>
<li>Emoji Keyboard (2016) by EmojiOne，表情输入</li>
<li>Evernote Web Clipper，保存有价值的信息到Evernote</li>
<li>Fatkun Batch Download Image，批量下载图片的插件，配合Google搜图</li>
<li>Follow Feed(by Feedly)，在当前网页搜索RSS订阅源并订阅至Feedly，用于订阅浏览到的价值博客等</li>
<li>GNOME Shell Integration，GNOME插件集成，用于在extension.gnome.org给GNOME安装插件，GNOME用户必备（有关GNOME可参考<a href="https://blog.ddlee.cn/tags/Gnome/">这里</a>）</li>
<li>Google Dictionary (by Google)，字典，设置快捷键Ctrl+D，搜词很方便</li>
<li>Google Input Tools，Google输入工具，应急之用</li>
<li>Google Scholar Button，用于在当前网页识别论文并在Google Scholar上检索相关内容</li>
<li>Google Translate，选取网页内容，可进行方便翻译</li>
<li>Inbox by Gmail，用于保存连接等内容到邮箱，方便分享</li>
<li>LastPass，跨平台的免费密码管理</li>
<li>Mega，如名</li>
<li>Mendeley Importer，导入文章到Mendeley Library</li>
<li>Mercury Reader，渲染网页到阅读模式，清爽干净</li>
<li>Momentum，增强新标签页，显示时钟、天气、To Do List等</li>
<li>Octotree，显示Github Repo的目录结构，必备</li>
<li>One-Click Extensions Manager，管理扩展用，用于节省内存</li>
<li>PDF Viewer，用于阅读PDF（免于直接下载），记得勾选Allow access to file URLs</li>
<li>Proxy SwitchyOmega，代理，善用auto switch和备份等功能</li>
<li>Pushbullet，跨设备文字通信，精分（PC用Ubuntu，平板iOS，手机Android）推荐</li>
<li>Quick QRCode，利器，用于把文字、连接等转成二维码，方便分享</li>
<li>Save to Google，保存网页到Google</li>
<li>Save to Pocket，保存到Pocket，稍后再读工具</li>
<li>Secure Shell，网页版SSH</li>
<li>Tab Snooze，折叠暂时不必要的标签页，利器</li>
<li>Telegram，如名</li>
<li>Text，轻量文本编辑器</li>
<li>Turn Off the Lights，利器，关灯。YouTube和Bilibili可用，其他未测试</li>
<li>Vimium，利器，脱离鼠标的网页浏览体验</li>
<li>微软雅黑字体，强迫症推荐</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是一个Chrome扩展的推荐列表。所有扩展插件在&lt;a href=&quot;https://chrome.google.com/webstore/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Chrome Web Store&lt;/a&gt;都可搜索得到。&lt;/p&gt;
&lt;u
    
    </summary>
    
      <category term="Individual Development" scheme="https://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Chrome" scheme="https://blog.ddlee.cn/tags/Chrome/"/>
    
      <category term="Software" scheme="https://blog.ddlee.cn/tags/Software/"/>
    
      <category term="Digital Life" scheme="https://blog.ddlee.cn/tags/Digital-Life/"/>
    
      <category term="Recommendation" scheme="https://blog.ddlee.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>Laptop Reborn: 系统重装侧记</title>
    <link href="https://blog.ddlee.cn/posts/99ba96d1/"/>
    <id>https://blog.ddlee.cn/posts/99ba96d1/</id>
    <published>2018-01-14T18:06:16.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是个人向的系统重装记录，以便自己日后参考。</p>
<p>我使用的系统是Win10+Ubuntu16.04。</p>
<p>Ubuntu为主力，Windows在两种情况下发挥作用：</p>
<ol>
<li>用只支持win平台下解锁的移动硬盘存取文件</li>
<li>用iTunes备份iPad和传输文件</li>
</ol>
<h2 id="备份-Backup"><a href="#备份-Backup" class="headerlink" title="备份 Backup"></a>备份 Backup</h2><p>备份必然是第一步，几乎只需要备份用户文件夹就好。</p>
<p>Ubuntu:<br>/home/Documents/<br>/home/Pictures(Music, Video, etc.)<br>/home/Downloads(Maybe)</p>
<p>Windows<br>~/Documents/(Music, Picture, Video, Desktop)</p>
<h2 id="镜像准备-ISO-Preparation"><a href="#镜像准备-ISO-Preparation" class="headerlink" title="镜像准备 ISO Preparation"></a>镜像准备 ISO Preparation</h2><p>准备系统镜像。这次用的刻录软件分别是Ruff(Windows)和Etcher(Ubuntu)。</p>
<p>Ubuntu: Ubuntu Gnome 16.04(From TUNA Mirror)</p>
<p>Windows: Win10-multi-ver1709(msdn.itellyou.cn)</p>
<p>Burning Tool: etcher portable</p>
<h2 id="硬盘分区-Disk-Partition"><a href="#硬盘分区-Disk-Partition" class="headerlink" title="硬盘分区 Disk Partition"></a>硬盘分区 Disk Partition</h2><p>双系统共存的话，要先装Windows(反之Windows安装时会影响已存在的Ubuntu)，因此分区这一部分在安装Windows的时候完成。</p>
<p>我的SSD只有240G，因此选择50+50给Windows，剩下的给Ubuntu(包括swap)</p>
<p>Windows(sys 50G, data 50G)<br>Ubuntu(sys 120G)</p>
<h2 id="驱动准备-Driver-Preparation"><a href="#驱动准备-Driver-Preparation" class="headerlink" title="驱动准备 Driver Preparation"></a>驱动准备 Driver Preparation</h2><p>Ubuntu还好，全新的Windows可能没法驱动网卡，这样就没办法更新和后续操作。我之前的方案是用驱动人生或者驱动精灵的网卡版，用完之后，良犬烹。但这次洁癖发作，因此选择去联想官网下载网卡驱动。再用相对干净的Driver boost更新其他驱动。</p>
<p>netcard: <a href="https://pcsupport.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-t-series-laptops/thinkpad-t460p/downloads" target="_blank" rel="external">https://pcsupport.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-t-series-laptops/thinkpad-t460p/downloads</a></p>
<p>Driver update tool(Driver boost 3): <a href="http://download.cnet.com/Driver-Booster-2/3000-18513_4-75992725.html?part=dl-&amp;subj=dl&amp;tag=button" target="_blank" rel="external">http://download.cnet.com/Driver-Booster-2/3000-18513_4-75992725.html?part=dl-&amp;subj=dl&amp;tag=button</a></p>
<h2 id="软件安装-Software-Installation"><a href="#软件安装-Software-Installation" class="headerlink" title="软件安装 Software Installation"></a>软件安装 Software Installation</h2><p>这一部分只是一个list，主要给自己参考用。</p>
<p>Windows:</p>
<ul>
<li>Minimal-Net-Pack</li>
<li>Office 2016</li>
<li>iTunes</li>
<li>CCleaner</li>
<li>Picasa</li>
<li>PotPlayer</li>
<li>mini-Thunder</li>
<li>Launchy</li>
</ul>
<p>Ubuntu:</p>
<ul>
<li>Minimal-Net-Pack</li>
<li>Google Chrome</li>
<li>Sougou Pinyin, Nutstore, Stacer</li>
<li>Netease Music, Variety</li>
<li>Gnome themes, Telegram, Xmind, FileZilla</li>
</ul>
<h2 id="开发环境配置-Environment-Configuration"><a href="#开发环境配置-Environment-Configuration" class="headerlink" title="开发环境配置 Environment Configuration"></a>开发环境配置 Environment Configuration</h2><p>这一部分完成自己开发环境的配置。</p>
<h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h4><p>SSH key: ssh-keygen and ssh-add, git config name&amp;email</p>
<h4 id="Python-Annaconda"><a href="#Python-Annaconda" class="headerlink" title="Python(Annaconda)"></a>Python(Annaconda)</h4><p>(pypi: 修改 ~/.config/pip/pip.conf (Linux), %APPDATA%\pip\pip.ini (Windows 10) 或 $HOME/Library/Application Support/pip/pip.conf (macOS) (没有就创建一个)， 修改 index-url至tuna，例如<br>[global]<br>index-url = <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="external">https://pypi.tuna.tsinghua.edu.cn/simple</a> )<br>(conda TUNA 还提供了 Anaconda 仓库的镜像，运行以下命令:<br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a></p>
<p>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a></p>
<p>conda config –set show_channel_urls yes<br>即可添加 Anaconda Python 免费仓库。)</p>
<h4 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h4><p>install: <a href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/</a></p>
<p>To verify: sudo docker run hello-world</p>
<p>mirror: <a href="https://www.docker-cn.com/registry-mirror" target="_blank" rel="external">https://www.docker-cn.com/registry-mirror</a><br>chmod 755 /etc/docker<br>为了永久性保留更改，您可以修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。<br>{ “registry-mirrors”: [“<a href="https://registry.docker-cn.com" target="_blank" rel="external">https://registry.docker-cn.com</a>“] }<br>修改保存后重启 Docker 以使配置生效</p>
<p>ref: <a href="https://docker_practice.gitee.io" target="_blank" rel="external">https://docker_practice.gitee.io</a></p>
<h4 id="R"><a href="#R" class="headerlink" title="R"></a>R</h4><p>sudo apt install r-base r-base-dev</p>
<h4 id="Hexo-Node-js"><a href="#Hexo-Node-js" class="headerlink" title="Hexo(Node.js)"></a>Hexo(Node.js)</h4><p>Node.js:<br>curl -sL <a href="https://deb.nodesource.com/setup_8.x" target="_blank" rel="external">https://deb.nodesource.com/setup_8.x</a> | sudo -E bash -<br>sudo apt-get install -y nodejs<br>hexo:<br>sudo npm install -g hexo-cli<br>rebuild bind in hexo directory:<br>npm rebuild node-sass</p>
<h2 id="杂项-Minor-Changes"><a href="#杂项-Minor-Changes" class="headerlink" title="杂项 Minor Changes"></a>杂项 Minor Changes</h2><p>一些需要操作的小地方备忘。</p>
<p>1.install Nvidia Driver<br><a href="http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux" target="_blank" rel="external">http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux</a><br><a href="https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07" target="_blank" rel="external">https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07</a></p>
<p>sudo add-apt-repository ppa:graphics-drivers/ppa<br>sudo apt-get update<br>sudo apt-get install nvidia-384<br>(ref at <a href="http://www.nvidia.com/object/unix.html" target="_blank" rel="external">http://www.nvidia.com/object/unix.html</a> to determine driver version)<br>optional: pip install gpustat</p>
<p>Proxy for ppa.launchpad.net<br>修改/etc/apt/sources.list.d下面需要代理的仓库，将ppa.launchpad.net换成代理地址，执行sudo apt update更新即可。 可用代理地址：<a href="http://launchpad.proxy.ustclug.org/" target="_blank" rel="external">http://launchpad.proxy.ustclug.org/</a></p>
<p>2.Grub skip time<br><a href="https://askubuntu.com/questions/157925/how-do-i-skip-the-grub-menu-on-a-dual-boot-system" target="_blank" rel="external">https://askubuntu.com/questions/157925/how-do-i-skip-the-grub-menu-on-a-dual-boot-system</a></p>
<p>Edit /etc/default/grub to contain</p>
<p>if you prefer to see the menu for 1 second:<br>GRUB_HIDDEN_TIMEOUT=<br>GRUB_TIMEOUT=1</p>
<p>When you’re done, run sudo update-grub to save your changes.</p>
<p>3.cpufrequtils(set performance &amp; get temperature)<br><a href="http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Power-Management-with-cpufrequtils" target="_blank" rel="external">http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Power-Management-with-cpufrequtils</a><br><a href="https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature" target="_blank" rel="external">https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature</a></p>
<ul>
<li>Disable intel turbo boost technology: terminal:<br>sudo -i<br>echo “1” | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo<br>exit</li>
</ul>
<p>4.tracke-miner-fs bug<br><a href="https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup" target="_blank" rel="external">https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup</a></p>
<p>echo -e “\nHidden=true\n” | sudo tee –append /etc/xdg/autostart/tracker-extract.desktop /etc/xdg/autostart/tracker-miner-apps.desktop /etc/xdg/autostart/tracker-miner-fs.desktop /etc/xdg/autostart/tracker-miner-user-guides.desktop /etc/xdg/autostart/tracker-store.desktop &gt; /dev/null<br>gsettings set org.freedesktop.Tracker.Miner.Files crawling-interval -2<br>gsettings set org.freedesktop.Tracker.Miner.Files enable-monitors false<br>tracker reset –hard</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是个人向的系统重装记录，以便自己日后参考。&lt;/p&gt;
&lt;p&gt;我使用的系统是Win10+Ubuntu16.04。&lt;/p&gt;
&lt;p&gt;Ubuntu为主力，Windows在两种情况下发挥作用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用只支持win平台下解锁的移动硬盘存取文件&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://blog.ddlee.cn/categories/Linux/"/>
    
    
      <category term="Digital Life" scheme="https://blog.ddlee.cn/tags/Digital-Life/"/>
    
      <category term="Linux" scheme="https://blog.ddlee.cn/tags/Linux/"/>
    
      <category term="OS" scheme="https://blog.ddlee.cn/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>FlintOS轻体验</title>
    <link href="https://blog.ddlee.cn/posts/4213a4c1/"/>
    <id>https://blog.ddlee.cn/posts/4213a4c1/</id>
    <published>2018-01-09T17:36:06.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>呃，我在<a href="https://ddlee.cn/index_ver2016.html" target="_blank" rel="external">2016版的个人主页</a>里曾表达自己特别想拥有一台Chromebook，可终于还是屈服在了网络环境面前。</p>
<p>离了我的路由器，我的Chromebook几乎就是个废物了。</p>
<p>不过，FlintOS的出现让我重拾了这个想法，我可能还是需要一台轻便的上网本。</p>
<p>FlintOS是Chromium OS的中文本地化项目，而后者正是Chromebook的操作系统，在美国的低端笔记本市场和教育市场占有很大的份额。</p>
<p>只不过在大陆呵呵。</p>
<p>言归正传，FlintOS背后的公司是成立不久的燧炻科技，这里是他们的<a href="https://flintos.com" target="_blank" rel="external">官网</a>，可以在这个<a href="https://flintos.com/faq/" target="_blank" rel="external">页面</a>了解更多FlintOS的信息，本篇中的版本是在<a href="https://flintos.com/community/topic/flint-os-for-pc-dev-3-1-%e5%8f%91%e5%b8%83%e9%80%9a%e7%9f%a5/" target="_blank" rel="external">论坛</a>的DEV3.2中国版。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>DEV3.2中国版的最大亮点是支持本地账户。这意味着在大陆的网络环境下也有使用的可能性。而且，这个版本内置了可供科学上网的服务，测试可用。另外，FlintOS settings里面也包括了安装Flash的快捷方式。</p>
<p>目前建议是在U盘上体验，安装教程在<a href="https://flintos.com/instructions-pc" target="_blank" rel="external">这里</a>。</p>
<p>也可以选择安装双启动和硬盘独占安装，可以参考社区的<a href="https://flintos.com/community/forum/flint-os-for-pc/" target="_blank" rel="external">置顶帖</a>。</p>
<h2 id="界面UI"><a href="#界面UI" class="headerlink" title="界面UI"></a>界面UI</h2><p>桌面和右下角的通知栏，遵从了Material Design，看着舒服。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/desktop.png" alt="desktop"></p>
<p>APP Launcher，跟Gnome的风格很像。搜索栏可以直接Google搜索，也可以搜索文件和APP。实际上大部分APP可以当做交互逻辑级别更高的书签，打开即是新建相应网站的标签页（如YouTube）。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/apps.png" alt="desktop"></p>
<p>这个是Chrome Web Store，跟作为浏览器的Chrome完全一致，只不过这个平台上可没有homebrew也没有dpkg。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/webstore.png" alt="desktop"></p>
<p>文件管理应用，跟Google Drive深度集成。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/files.png" alt="desktop"></p>
<h2 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h2><p>我也尝试探索用于开发的可能性，由于ssh的存在，可操作性还是很高的。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/ssh.png" alt="desktop"></p>
<p>Texts是一个比较轻亮的文本编辑器。当然也有仿atom的付费APP。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/texts.png" alt="desktop"></p>
<p>crosh（通过Crtl+Shift+T打开）是内置的shell，功能上还有待探索，图为运行top的效果。</p>
<p><img src="https://static.ddlee.cn/static/img/FlintOS轻体验/crosh.png" alt="desktop"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Chromium OS的理念是web为王，浏览器即一切。这种理念配合Google相当完整的生态使得Chromebook在廉价本市场几乎是统治地位。去年的MS build上，微软也发布了对标的Windows版本，demo用的场景就是老师为每个学生配备PC。</p>
<p>相比之下，大陆的这个市场还是空白（或许不一定有，毕竟教育方面并不普及）。看到有把这种理念本地化的公司出现还是很惊艳，在此默默祝福他们。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;呃，我在&lt;a href=&quot;https://ddlee.cn/index_ver2016.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;2016版的个人主页&lt;/a&gt;里曾表达自己特别想拥有一台Chromebook，可终于还是屈服在了网络环境面前。&lt;/
    
    </summary>
    
      <category term="Linux" scheme="https://blog.ddlee.cn/categories/Linux/"/>
    
    
      <category term="Software" scheme="https://blog.ddlee.cn/tags/Software/"/>
    
      <category term="Chrome OS" scheme="https://blog.ddlee.cn/tags/Chrome-OS/"/>
    
      <category term="Linux" scheme="https://blog.ddlee.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记](DenseNet)Densely Connected Convolutional Networks</title>
    <link href="https://blog.ddlee.cn/posts/f8991abd/"/>
    <id>https://blog.ddlee.cn/posts/f8991abd/</id>
    <published>2018-01-06T13:23:16.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>DenseNet将shortcut-connection的思路发挥到极致。在一个DenseBlock内部，每一层的输出均跟后面的层建立shortcut，特别需要注意的是，不同于ResNet中的相加，DenseNet连接shortcut的方式是Concat，这样越深的层则输入channel数越大。</p>
<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/arch.png" alt="arch"></p>
<p>整个网络被分为Dense Block和Transition Layer，前者内部进行密集连接，保持同样大小的feature map，后者为DenseBlock之间的连接层，完成下采样操作。</p>
<p>在每个DenseBlock内部，接受的数据维度会随层数加深而变大（因为不断拼接了之前层的输出），增长的速率即为初始的channel数，文章称这一channel数为growth rate，作为模型的一个超参数。初始的growth rate为32时，在DenseNet121架构下，最后一层的channel数将增长到1024。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/56cb18697f42eb0374d933446f45b151" target="_blank" rel="external">Netscope Vis</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>作者在CIFAR和ImageNet上都做了实验，DenseNet取得了跟ResNet相当的表现，加入Bottleneck和一部分压缩技巧后，用较少的参数就能达到跟ResNet相当的效果：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/result.png" alt="arch"></p>
<p>论文链接：<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">https://arxiv.org/abs/1608.06993</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DenseNet将shortcut-connection的思路发挥到极致。在一个DenseBlock内部，每一层的输出均跟后面的层建立shortcut，特别需要注意的是，不同于ResNet中的相加，DenseNet连接shortcut的方式是Concat，这样越深的层则输入
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="https://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记](ResNeXt)Aggregated Residual Transformations for Deep Neural Networks</title>
    <link href="https://blog.ddlee.cn/posts/423c678a/"/>
    <id>https://blog.ddlee.cn/posts/423c678a/</id>
    <published>2018-01-06T13:19:34.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为”cardinality”。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也添加了层与层间的shortcut connection。但由于其多路径的设计特征，我将其归为Inception系网络。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>深度网络结构上的设计已经有三种经典的范式：</p>
<ul>
<li>Repeat. 由AlexNet和VGG等开拓，几乎被之后所有的网络采用。即堆叠相同的拓扑结构，整个网络成为模块化的结构。</li>
<li>Multi-path. 由Inception系列发扬，将前一层的输入分割到不同的路径上进行变换，最后拼接结果。</li>
<li>Skip-connection. 最初出现于Highway Network，由ResNet发扬并成为标配。即建立浅层信息与深层信息的传递通道，改变原有的单一线性结构。</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>文章将残差函数表示为：</p>
<p>其中，C为本层进行的变换数目，即”cardinality”。</p>
<p>相比Inception-ResNet，ResNeXt相当于将其Inception Module的每条路径规范化了，并将规范后的路径数目作为新的超参数。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/multi-path.png" alt="multi-path"></p>
<p>上图中，路径被扩展为多条，而每条路径的宽度（channel数）也变窄了（64-&gt;4）。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/c2ba521fcb60520abb0b0da0e9c0f2ef" target="_blank" rel="external">NetScope Vis</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiements"><a href="#Experiements" class="headerlink" title="Experiements"></a>Experiements</h3><p>ResNeXt试图在保持参数数目的情况下提高网络性能，提升cardinality的同时使每条路径的宽度变窄。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/setting.png" alt="setting"></p>
<p>对比其他网络的结果：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/result.png" alt="result"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>ResNeXt较为突出的是把Inception单元规范化了，摆脱了需要精心设计Inception单元中卷积结构的问题，更好地组织了参数。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">https://arxiv.org/abs/1611.05431</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为”cardinality”。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <link href="https://blog.ddlee.cn/posts/ae26dc1c/"/>
    <id>https://blog.ddlee.cn/posts/ae26dc1c/</id>
    <published>2018-01-04T13:10:09.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>MobileNets系列可以看做是继Xception之后对Depthwise Separable Convolution的又一推动。利用深度可分离的特征，MobileNets系列引入两个模型精度和大小的超参，在保持相当精度的同时享有非常小的计算消耗，适用于移动端情形，因而被命名为”MobileNets”。</p>
<h3 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a>Depthwise Separable Convolution</h3><p>深度可分离卷积是近期深度网络设计的重要趋势。最早见于L. Sifre的PhD论文Rigid-motion scattering for image classification，其1×1卷积在Inception, ResNet, SqueezeNet等网络中作为降维bottleneck使用。Xception指出，Inception单元本质上假设了跨通道和跨空间相关性的解耦关系，并将这一解耦关系推向极端，用Depthwise Separable Convolution改造了Inception结构。</p>
<p>深度可分离卷积受欢迎的另一重要原因是其参数高效性。将原有卷积换成深度可分离卷积后，可以享受到模型压缩的增益。</p>
<p>标准的卷积操作，可以认为是大小为DK的窗口在DF大小的特征图上滑动计算，计算复杂性为：</p>
<p>DK×DK × M×N × DF×DF</p>
<p>其中，M和N分别代表输入channel数和输出channel数。</p>
<p>替换为深度可分离卷积后，先进行Depthwise Convolution，再进行1×1 Pointwise Convolution，计算复杂性为：</p>
<p>DK×DK × M × DF×DF + M×N × DF×DF</p>
<p>相比下，深度可分离卷积的计算复杂性约为原来的(1/N+1/DK^2)。</p>
<p>进一步地，MobileNet添加两个超参来控制这一压缩程度，alpha为channel数压缩系数，rho为分辨率压缩系数：</p>
<p>DK×DK × alpha×M × rho×DF× rho×DF + alpha×M × alpha×N × rho×DF × rho×DF</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/depthwise-seperable.png" alt="depthwise-seperable"></p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>MobileNet的基本结构如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/arch.png" alt="arch"></p>
<p>论文链接：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MobileNets系列可以看做是继Xception之后对Depthwise Separable Convolution的又一推动。利用深度可分离的特征，MobileNets系列引入两个模型精度和大小的超参，在保持相当精度的同时享有非常小的计算消耗，适用于移动端情形，因而被
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
      <category term="Mobile" scheme="https://blog.ddlee.cn/tags/Mobile/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Xception: Deep Learning with Depthwise Seperable Convolutions</title>
    <link href="https://blog.ddlee.cn/posts/3b214d12/"/>
    <id>https://blog.ddlee.cn/posts/3b214d12/</id>
    <published>2018-01-02T13:04:30.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是keras库作者的文章，对Inception结构进行了改进：用Depth-wise seperable convolution替换了Inception单元中的1×1卷积和3×3卷积。</p>
<p>文章对Inception结构的评论非常有见地。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>文章指出，Inception单元背后的假设是跨Channel和跨空间的相关性可以充分解耦，类似的还有长度和高度方向上的卷积结构（在Inception-v3里的3×3卷积被1×3和3×1卷积替代）。</p>
<p>进一步的，Xception基于更强的假设：跨channel和跨空间的相关性完全解耦。这也是Depthwise Separable Convolution所建模的理念。</p>
<p>一个简化的Inception单元：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception.png" alt="inception"></p>
<p>等价于：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception2.png" alt="inception"></p>
<p>将channel推向极端，即每个channel都由独立的3×3卷积处理：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception3.png" alt="inception"></p>
<p>这样就得到了Depthwise Separable Convolution。</p>
<h3 id="Architectrue"><a href="#Architectrue" class="headerlink" title="Architectrue"></a>Architectrue</h3><p>简单讲，Xception是线性堆叠的Depthwise Separable卷积，附加了Skip-connection。</p>
<p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/931d7c91b22109f83bbbb7ff1a215f5f" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/arch.png" alt="inception"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文的实验部分并没有像其他论文那样集成一个在ImageNet上SOTA的结果，而是以Inception-v3为基线，对比了参数数量和性能，认为提升正来自于更合理的参数利用。文章还对比了Residual的作用，在Xception网络中，Skip-connection不仅能提高训练速度，还能增强模型的性能。</p>
<h3 id="Concolusion"><a href="#Concolusion" class="headerlink" title="Concolusion"></a>Concolusion</h3><p>本文贡献主要对Inception单元的解读和引入Depthwise Seperable Convolution。更多对于Depthwise Seperable Convolution的描述，请参考<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">MobileNets</a>的笔记。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="external">https://arxiv.org/abs/1610.02357</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是keras库作者的文章，对Inception结构进行了改进：用Depth-wise seperable convolution替换了Inception单元中的1×1卷积和3×3卷积。&lt;/p&gt;
&lt;p&gt;文章对Inception结构的评论非常有见地。&lt;/p&gt;
&lt;h3 id=
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
      <category term="Inception" scheme="https://blog.ddlee.cn/tags/Inception/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
    <link href="https://blog.ddlee.cn/posts/ea323d66/"/>
    <id>https://blog.ddlee.cn/posts/ea323d66/</id>
    <published>2017-12-26T12:59:02.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>在15年，ResNet成为那年最耀眼的卷积网络结构，skip-connection的结构也成为避不开的考虑选项。Inception系列也参考ResNet更新了自己的结构。同时推出了第四代和跟ResNet的结合版：Inception-v4和Inception-ResNet。</p>
<p>然而，这是一篇几乎都是图的论文。</p>
<p>所以，上图。</p>
<h3 id="Inception-v4-Architecture"><a href="#Inception-v4-Architecture" class="headerlink" title="Inception-v4 Architecture"></a>Inception-v4 Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/e0ac64013b167844053184d97b380978" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch1.jpg" alt="arch1"></p>
<h3 id="Inception-ResNet-v2-Architecture"><a href="#Inception-ResNet-v2-Architecture" class="headerlink" title="Inception-ResNet(v2) Architecture"></a>Inception-ResNet(v2) Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/aadd97383baccabb8b827ba507c24162" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch2.jpg" alt="arch1"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章在实验部分提到，不借助Skip-connection的结构也可以将Inception网络提升到SOTA的水准，但加入Skip-connection可以有效增加训练速度。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>卷积网络结构的演进遇到了瓶颈，在ImageNet上的提升边界似乎碰到天花板，且更多来自训练技巧和集成。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">https://arxiv.org/abs/1602.07261</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在15年，ResNet成为那年最耀眼的卷积网络结构，skip-connection的结构也成为避不开的考虑选项。Inception系列也参考ResNet更新了自己的结构。同时推出了第四代和跟ResNet的结合版：Inception-v4和Inception-ResNet。&lt;
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Speed/accuracy trade-offs for modern convolutional object detectors</title>
    <link href="https://blog.ddlee.cn/posts/b58d5aab/"/>
    <id>https://blog.ddlee.cn/posts/b58d5aab/</id>
    <published>2017-12-24T13:55:22.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。</p>
<p>文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了Proposal个数、输入图片尺寸，生成Feature map的大小等作为超参，并行实验，探索精度和速度方面的trade-off。</p>
<p>文章也将源码公开，作为Tensorflow的Object Detection API。</p>
<p>下图是三种元结构的图示：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/meta-arch.png" alt="meta-arch"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig2.png" alt="meta-arch"></p>
<p>信息量非常大的一张图。</p>
<ul>
<li>横纵两个维度分别代表速度和准确度，横轴越靠左说明用时越少，纵轴越靠上说明mAP表现越好，因而，sweet spot应分布在左上角</li>
<li>两个超维是元结构和特征提取网络，元结构由形状代表，特征提取网络由颜色代表</li>
<li>虚线代表理想中的trade-off边界</li>
</ul>
<p>分析：</p>
<ul>
<li>准确度最高的由Faster-RCNN元结构、Inception-ResNet提取网络，高分图片，使用较大的feature map达到，如图右上角</li>
<li>较快的网络中准确度表现最好的由使用Inception和Mobilenet的SSD达到</li>
<li>sweet spot区特征提取网络由ResNet统治，较少Proposal的Faster-RCNN可以跟R-FCN相当</li>
<li>特征提取网络方面，Inception V2和MobileNet在高速度区，Incep-ResNet和ResNet在sweet spot和高精度区，Inception V3和VGG则远离理想边界（虚线）</li>
</ul>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig3.png" alt="meta-arch"></p>
<p>上图是特征提取网络对三种元结构的影响，横轴是特征提取网络的分类准确率，纵轴是检测任务上的mAP表现，可以看到，SSD在纵轴方向上方差最小，而Faster-RCNN和R-FCN对特征提取网络更为敏感。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig4.png" alt="meta-arch"></p>
<p>上图的横轴是不同的特征提取网络，组内是三种元结构的对比，纵轴是不同尺寸物体的mAP。</p>
<p>可以看到，在大物体的检测上，使用较小的网络时，SSD的效果跟两阶段方法相当，更深的特征提取网络则对两阶段方法的中型和小型物体的检测提升较大（ResNet101和Incep-ResNet都显现了两阶段方法在小物体上的提升）</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig5.png" alt="meta-arch"></p>
<p>上图显示了输入图片尺寸对mAP的影响。高分的图片对小物体检测帮助明显，因而拥有更高的精度，但相对运行速度会变慢。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig6.png" alt="meta-arch"></p>
<p>上图探究了两阶段方法中Proposal个数的影响，左边是Faster-RCNN，右边是R-FCN，实线是mAP，虚线是推断时间。<br>分析：</p>
<ul>
<li>相比R-FCN，Faster-RCNN推断时间对Proposal个数相当敏感（因为有per ROI的计算）</li>
<li>减少Proposal的个数，并不会给精度带来致命的下降</li>
</ul>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig7.png" alt="meta-arch"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig8.png" alt="meta-arch"></p>
<p>上面两图是对FLOPS的记录，相对GPU时间更为中立，在图8中，GPU部分显现了ResNet跟Inception的分野（关于45度线，此时FLOPS跟GPU时间相当），文章认为分解操作(Factorization)减少了FLOPs，但增加了内存的IO时间，或者是GPU指令集更适合密集的卷积计算。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig9.png" alt="meta-arch"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig10.png" alt="meta-arch"></p>
<p>上两图是对内存占用的分析，总体来说，特征提取网络越精简、feature map尺寸越小，占用内存越少，运行时间也越短。</p>
<p>最后，文章描述了他们ensemble的思路，在一系列不同stride、loss和配置的Faster-RCNN中（ResNet和Incep-ResNet为特征提取网络），贪心地选择验证集上AP较高的，并且去除类AP相似的模型。选择的5个用于ensemble的模型如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/table5.png" alt="meta-arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇文章是不错的实验结果报告，测试了足够多的模型，也得出了合理的和有启发的结论。几点想法：</p>
<ul>
<li>RFCN并没有很好的解决定位跟分类的矛盾，per ROI的子网络最好还是要有，但要限制Proposal的个数（实际大部分都是负样本）来减少冗余</li>
<li>小物体的检测仍然是最大的难点，增大分辨率和更深的网络确有帮助，但不是实质的。</li>
</ul>
<p>论文链接： <a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="external">https://arxiv.org/abs/1611.10012</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。&lt;/p&gt;
&lt;p&gt;文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了P
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Light-Head R-CNN: In Defense of Two-Stage Object Detector</title>
    <link href="https://blog.ddlee.cn/posts/50848bae/"/>
    <id>https://blog.ddlee.cn/posts/50848bae/</id>
    <published>2017-12-22T13:55:36.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。</p>
<p>针对这两种元结构(Faster-RCNN和RFCN)，文章提出了“头”轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。</p>
<h2 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/arch.png" alt="arch"></p>
<p>如上图，虚线框出的部分是三种结构的RCNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为sperable convolution，产生的Score map也减少至10×p×p（相比原先的#class×p×p）。</p>
<p>一个可能的解释是，“瘦”（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较“厚”的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。</p>
<p>进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。</p>
<p>light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。</p>
<p>粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分，文章验证了较“瘦”的Score map不会对精度产生太大损害，也展现了ROI Align, Multiscale train等技巧对基线的提升过程。</p>
<p>文章的主要结果如下面两图（第一个为高精度，第二个为高速度）：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result1.png" alt="result1"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result2.png" alt="result2"></p>
<p>只能说这样的对比比较诡异。</p>
<p>第一张图中三个light-head结果并不能跟上面的其他结构构成多少有效的对照组，要么scale不同，要么FPN, multi-scale, ROI Align不同。唯一的有效对照是跟Mask-RCNN。</p>
<p>在高精度方面，基础网络不同，采用的scale也不同，没有有效的对照组。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我并不觉得这是对两阶段方法的Defense。文章对两阶段方法在精度和速度方面的分析比较有见地，但实验的结果并不能可靠地支撑light-head的有效性。相比之下Google的那篇trade-off可能更有参考价值。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="external">https://arxiv.org/abs/1711.07264</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]You Only Look Once: Unified, Real Time Object Detection</title>
    <link href="https://blog.ddlee.cn/posts/41036331/"/>
    <id>https://blog.ddlee.cn/posts/41036331/</id>
    <published>2017-12-20T13:38:31.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。</p>
<p>YOLO的主要优点：</p>
<ul>
<li>快。</li>
<li>全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。</li>
<li>泛化性能好，在艺术作品上做检测时，YOLO表现好。</li>
</ul>
<h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><p>YOLO的大致工作流程如下：<br>1.准备数据：将图片缩放，划分为等分的网格，每个网格按跟ground truth的IOU分配到所要预测的样本。<br>2.卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IOU表示）。测试时，分数如下计算：</p>
<p>等式左边第一项由网格预测，后两项由每个box预测，综合起来变得到每个box含有不同类别物体的分数。<br>因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，S为网格数，B为每个网格生成box个数，C为类别数。<br>3.后处理：使用NMS过滤得到的box</p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><img src="https://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/loss.jpg" alt="loss-function"></p>
<p>图片来自<a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/24916786</a></p>
<p>损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，loss中添加了权重并将长宽取根号。</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/error.png" alt="error"></p>
<p>相比Fast-RCNN，YOLO的背景误检在错误中占比重小，而位置错误占比大（未采用log编码）。</p>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>YOLO划分网格的思路还是比较粗糙的，每个网格生成的box个数也限制了其对小物体和相近物体的检测。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">https://arxiv.org/abs/1506.02640</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。&lt;/p&gt;
&lt;p&gt;YOLO的主要优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;快。&lt;/li&gt;
&lt;li&gt;全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="YOLO" scheme="https://blog.ddlee.cn/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记](Inception V3)Rethinking the Inception Architecture for Computer Vision</title>
    <link href="https://blog.ddlee.cn/posts/5e3f4a2c/"/>
    <id>https://blog.ddlee.cn/posts/5e3f4a2c/</id>
    <published>2017-12-16T12:53:29.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一网络结构，v3版本相比v2版本加入了RMSProp，Label Smoothing等技巧。</p>
<p>文章表述了Inception系列的几个设计原则，并根据这些原则改进了GoogLeNet的结构。</p>
<h3 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h3><ul>
<li>Avoid representational bottlenecks, especially early in the network. 建议不要在过浅的阶段进行特征压缩，而维度只是一个表达复杂性的参考，并不能作为特征复杂性的绝对衡量标准。</li>
<li>Higher dimensional representations are easier to process locally with a network. 高阶的表示更有局部描述力，增加非线性有助于固化这些描述力。</li>
<li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. 基于空间的聚合信息可以在低维空间里处理，而不必担心有太多信息损失。这一点也佐证了1×1卷积的降维作用。</li>
<li>Balance the width and depth of the network.  宽度和深度的增加都有助于网络的表达能力，最好的做法是同时在这两个方向上推进，而非只顾及一个。</li>
</ul>
<h3 id="Factorizing-Convolution"><a href="#Factorizing-Convolution" class="headerlink" title="Factorizing Convolution"></a>Factorizing Convolution</h3><p>分解一直是计算数学里经典的思路。从牛顿法到BFGS，就是把Hessian矩阵（或其逆）用一系列的向量操作来表示和近似，避免矩阵的计算。</p>
<p>本文提出了两种卷积结构方面的分解，一个是在卷积核的层面，另一个是在空间方面。</p>
<p>第一种分解是将大核卷积分解成串联的小核卷积。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor3.png" alt="factor5"></p>
<p>用两个3×3的卷积代替5×5的卷积，带来的参数减少为(9+9)/(5×5).</p>
<p>第二种分解是在卷积核本身上，引入非对称卷积：用3×1和1×3的卷积串联代替3×3卷积。如下图所示。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor1.png" alt="factor3"></p>
<p>这种分解也可以推广到n维情况，且n越大，带来的收益越明显。</p>
<p>空间上的卷积分解建模了这样的情形：两个方向上的卷积参数互相正交，便被空间分解卷积解耦。</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/spatial-seperable.png" alt="factor5"></p>
<h3 id="Utility-of-Auxiliary-Classifiers"><a href="#Utility-of-Auxiliary-Classifiers" class="headerlink" title="Utility of Auxiliary Classifiers"></a>Utility of Auxiliary Classifiers</h3><p>在GoogLeNet中，作者用loss监督了低维的特征图的学习，但进一步的实验发现，加入BN层后，这些增益被抵消了，于是Auxiliary Classifier可被看做是某种正则化技术，在加入BN的网络中便不再应用。</p>
<h3 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h3><p>这一节讨论网络中的特征降维，即下采样的过程，通常由卷积层或Pooling层的stride参数控制。文章为避免原则一中提到的Representation Bottleneck，在进行Pooling之前将网络加宽（通过Channel数的增加），这也对应了平衡宽度和深度的原则。</p>
<p>最终结合了Inception结构和下采样需求的单元如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/downsample.png" alt="factor5"></p>
<p>不同于Inception单元，上面的1×1卷积扩展了Channel，并且3×3卷积采用了stride=2。</p>
<h3 id="Inception-v2-amp-Inception-v3-Architecture"><a href="#Inception-v2-amp-Inception-v3-Architecture" class="headerlink" title="Inception-v2 &amp; Inception-v3 Architecture"></a>Inception-v2 &amp; Inception-v3 Architecture</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/arch.png" alt="factor5"></p>
<p>可以看到随深度增加，Channel数也在扩展，而Inception单元也遵从了堆叠的范式。</p>
<p>其中三种Inception单元分别为：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptiona.png" alt="factor5"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionb.png" alt="factor5"></p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionc.png" alt="factor5"></p>
<p>另外，也可以查看<a href="http://ethereon.github.io/netscope/#gist/a2394c1c4a9738469078f096a8979346" target="_blank" rel="external">NetScope Vis</a>来熟悉Inception-v3的结构，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下面是Inception结构演化带来的增益分解：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/experiment.png" alt="factor5"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本篇是对Inception系网络的推进，其分解的思想成为又一网络设计的指导原则。</p>
<p>对卷积的进一步理解，可以参考这个<a href="https://graphics.stanford.edu/courses/cs178-10/applets/convolution.html" target="_blank" rel="external">页面</a>，这一工具可视化了不同卷积核对输入的处理，给出的例子都是在早期人们手工设计的滤波器，而深度网络隐式地学习到了这些滤波器的卷积表达。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">https://arxiv.org/abs/1512.00567</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://blog.ddlee.cn/tags/CNN/"/>
    
      <category term="Inception" scheme="https://blog.ddlee.cn/tags/Inception/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]SSD: Single Shot MultiBox Detector</title>
    <link href="https://blog.ddlee.cn/posts/ac9fbfcc/"/>
    <id>https://blog.ddlee.cn/posts/ac9fbfcc/</id>
    <published>2017-12-12T13:37:56.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保持较快的运行速度。</p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><img src="https://static.ddlee.cn/static/img/论文笔记-SSD-Single-Shot-MultiBox-Detector/arch.jpg" alt="arch"></p>
<ul>
<li><p>多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。</p>
</li>
<li><p>更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SSD有点像多分类的RPN，生成anchor box，再对box预测分数和位置调整值。</p>
<p>论文链接：<a href="https://arxiv.org/abs/151.023325" target="_blank" rel="external">https://arxiv.org/abs/151.023325</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记](FPN)Feature Pyramid Networks for Object Detection</title>
    <link href="https://blog.ddlee.cn/posts/6a10f2e/"/>
    <id>https://blog.ddlee.cn/posts/6a10f2e/</id>
    <published>2017-12-07T13:55:59.000Z</published>
    <updated>2018-03-18T13:28:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。</p>
<p>回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。</p>
<p>一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的内存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。</p>
<p>本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。</p>
<h3 id="Feature-Pyramid-Networks"><a href="#Feature-Pyramid-Networks" class="headerlink" title="Feature Pyramid Networks"></a>Feature Pyramid Networks</h3><p><img src="https://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/arch.png" alt="arch"></p>
<p>从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行元素位置上的叠加（lateral connection），构成这一深度上的特征。</p>
<p>这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章的主要实验结果如下：</p>
<p><img src="https://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/result.png" alt="Experiments results"></p>
<p>对比不同head部分，输入feature的变化对检测精度确实有提升，而且，lateral和top-down两个操作也是缺一不可。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。</p>
<p>个人方面，FPN跟multi-scale的区别在哪，还值得进一步探索。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">https://arxiv.org/abs/1612.03144</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感
    
    </summary>
    
      <category term="Papers" scheme="https://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Papers" scheme="https://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="https://blog.ddlee.cn/tags/Object-Detection/"/>
    
      <category term="Computer Vision" scheme="https://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
</feed>
