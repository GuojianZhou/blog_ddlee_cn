<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>萧爽楼|李东东的安静一隅</title>
  <subtitle>李东东的安静一隅</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.ddlee.cn/"/>
  <updated>2017-06-14T15:22:19.907Z</updated>
  <id>http://blog.ddlee.cn/</id>
  
  <author>
    <name>ddlee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[论文笔记]Accurate, Large Minibatch SGD: Training ImageNet in One Hour</title>
    <link href="http://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/"/>
    <id>http://blog.ddlee.cn/2017/06/14/论文笔记-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/</id>
    <published>2017-06-14T14:43:41.000Z</published>
    <updated>2017-06-14T15:22:19.907Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="external">Accurate, Large Minibatch SGD: Training ImageNet in One Hour</a></p>
<p>这篇文章在各处都有很广泛的讨论，作为实验经验并不多的小白，将文中tricks只做些记录。</p>
<h3 id="Linear-Scaling-Rule"><a href="#Linear-Scaling-Rule" class="headerlink" title="Linear Scaling Rule"></a>Linear Scaling Rule</h3><p>进行大批量的Minibatch SGD时会有批量越大，误差越大的问题。本文提出的Linear Scaling Rule正是试图解决这一问题。</p>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>设想两个情景：一是在一次参数更新中使用kn个样本梯度，二是分为k次更新，每次取n个样本梯度。</p>
<p>第一种情景的参数更新公式：<br>
$$w_t+1^(1) = w_t^(1) - \mu^(1) \frac{1}{kn} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t)$$
</p>
<p>第二种情景的参数更新公式：<br>
$$w_t+k^(2) = w_t^(2) - \mu^(2) \frac{1}{n} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t+j)$$
</p>
<p>由上面可以看出，主要的区别是梯度平均时批量的大小不同，前者为kn，后者为每次n，更新k次。</p>
<p>再假设双重求和号内项变化不大时，为使情景二更新k次（即使用同样数量的样本）之后参数与情景一类似，我们自然要将学习速率$\mu$线性提升。</p>
<h3 id="Gradual-Warmup"><a href="#Gradual-Warmup" class="headerlink" title="Gradual Warmup"></a>Gradual Warmup</h3><p>上面提到的Linear Scaling Rule使用的假设是梯度变化不大。但在训练初期，参数随机初始化，梯度变化很大，因而Linear Scaling Rule不再适用。在实践中，可以使学习速率在初始时较小，在经过几个epoch训练后再升至与kn批量相应的大小。</p>
<h3 id="BN-statistics"><a href="#BN-statistics" class="headerlink" title="BN statistics"></a>BN statistics</h3><p>在分布式训练的系统中，对于BN中要估计的均值和方差，文中给出的建议是对所有worker上的样本计算均值和方差，而不是每个worker单独计算。</p>
<h3 id="Weight-Decay"><a href="#Weight-Decay" class="headerlink" title="Weight Decay"></a>Weight Decay</h3><p>由于weight decay的存在，Linear Scaling Rule最好用于学习速率，而非用于Loss Function</p>
<h3 id="Momentum-Correction"><a href="#Momentum-Correction" class="headerlink" title="Momentum Correction"></a>Momentum Correction</h3><p>加入Linear Scaling Rule之后，适用动量加速的SGD需要进行动量更正。</p>
<h3 id="Data-Shuffling"><a href="#Data-Shuffling" class="headerlink" title="Data Shuffling"></a>Data Shuffling</h3><p>在分布式的系统中，先进行Data Shuffling，再分配数据到每个worker上。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1706.02677&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Accurate, Large Minibatch SGD: Training ImageNet in One Hour
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>自用LaTeX中英文简历模板</title>
    <link href="http://blog.ddlee.cn/2017/06/14/%E8%87%AA%E7%94%A8LaTeX%E4%B8%AD%E8%8B%B1%E6%96%87%E5%BB%BA%E7%AB%8B%E6%A8%A1%E6%9D%BF/"/>
    <id>http://blog.ddlee.cn/2017/06/14/自用LaTeX中英文建立模板/</id>
    <published>2017-06-14T10:58:41.000Z</published>
    <updated>2017-06-14T10:58:41.185Z</updated>
    
    <content type="html"><![CDATA[<p>分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在<a href="https://www.sharelatex.com/templates/cv-or-resume/professional-cv" target="_blank" rel="external">ShareLaTeX</a>上分享的模板。</p>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li>Github仓库：<a href="https://github.com/ddlee96/latex_cv_template" target="_blank" rel="external">ddlee96/latex_cv_template</a></li>
<li>编译引擎： XeLaTeX</li>
<li>下载地址： <a href="https://github.com/ddlee96/latex_cv_template/releases/tag/0.1" target="_blank" rel="external">v0.1</a></li>
<li>压缩包内包含.tex文件和所用字体文件，解压后修改.tex文件再编译即可。</li>
<li>在Ubuntu 16.04, Texlive 2016环境下测试通过。</li>
<li>英文字体: Fontin，中文字体：方正兰亭黑</li>
</ul>
<h4 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h4><ul>
<li>.tex代码：Apache 2.0</li>
<li>字体： 仅供个人使用</li>
</ul>
<h4 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h4><h5 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h5><p><img src="http://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv-1.png" alt="en"></p>
<h5 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h5><p><img src="http://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv_zh-1.png" alt="zh"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在&lt;a href=&quot;https://www.sharelatex.com/templates/cv-or-resume/professional-cv&quot; target=&quot;_blank&quot; re
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/tags/Individual-Development/"/>
    
      <category term="LaTeX" scheme="http://blog.ddlee.cn/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]On the Effects and Weight Normalization in GAN</title>
    <link href="http://blog.ddlee.cn/2017/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-Effects-and-Weight-Normalization-in-GAN/"/>
    <id>http://blog.ddlee.cn/2017/06/10/论文笔记-On-the-Effects-and-Weight-Normalization-in-GAN/</id>
    <published>2017-06-10T14:01:21.000Z</published>
    <updated>2017-06-14T14:57:42.981Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1704.03971" target="_blank" rel="external">On the Effects and Weight Normalization in GAN</a></p>
<p>本文探索了参数标准化(Weight Normalization)这一技术在GAN中的应用。BN在mini-batch的层级上计算均值和方差，容易引入噪声，并不适用于GAN这种生成模型，而WN对参数进行重写，引入噪声更少。</p>
<p>我觉得本文的亮点有二：</p>
<h3 id="1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变"><a href="#1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变" class="headerlink" title="1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变"></a>1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变</h3><p>朴素的参数标准化层有如下的形式：<br>
$$y=\frac{{w}^{T}x}{\|w\|}$$
<br>文中称这样形式的层为“strict weight-normalized layer”。若将线性层换为这样的层，网络的表达能力会下降，因而需要添加如下的affine transformation:</p>

$$y=\frac{{w}^{T}x}{\|w\|} \gamma + \beta$$

<p>用于恢复网络的表达能力。</p>
<p>将上述变换带入ReLU，简化后可以得到如下T-ReLu:</p>
<script type="math/tex; mode=display">TReLU_\alpha (x) = ReLU(x-\alpha) + \alpha</script><p>文章的一个重要结论是，在网络的最后一层加入affine transformation层之后，堆叠的“线性层+ReLU”与“strict weight-normalized layer + T-ReLU”表达能力相同（在附录中给出证明）。</p>
<p>下面L表示线性层，R表示ReLU，TR表示TReLU，A表示affine transformation，S表示上述的strict weight-normalized layer。</p>
<p>证明的大致思路是，在ReLU与线性层之间加入affine transformation层，由于线性层的存在，affine transformation带来的效果会被吸收（相当于多个线性层叠在一起还是线性层），网络表达能力不变。而”L+R+A”的结构可以等价于”S+TR+A”。如此递归下去，即可得到结论。个人认为相当于把线性层中的bias转嫁成了TReLU中的threshold（即$\alpha$）。</p>
<h3 id="2-提出对生成图形的评估指标"><a href="#2-提出对生成图形的评估指标" class="headerlink" title="2. 提出对生成图形的评估指标"></a>2. 提出对生成图形的评估指标</h3><p>生成式模型的生成效果常常难以评价。DcGAN给出的结果也是生成图片的对比。本文中提出一个评价生成效果的指标，且与人的主观评价一致。</p>
<p>评价的具体指标是生成图片与测试集图片的欧氏距离，评价的对象是生成器是Generator。有如下形式：</p>

$$\frac{1}{m} \sum_{i=1}^{m} min_z {\|G(z)-x^{(i)}\|}^2$$

<p>其中的$min$指使用梯度下降方法等使生成图片的效果最好。但事实上这样做开销很高。</p>
<h3 id="PyTorch实现"><a href="#PyTorch实现" class="headerlink" title="PyTorch实现"></a>PyTorch实现</h3><p>作者将他们的实现代码公布在了<a href="https://github.com/stormraiser/GAN-weight-norm" target="_blank" rel="external">GitHub</a>上。</p>
<p>下面是利用PyTorch对T-ReLU的实现：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">class TPReLU(Module):

    def __init__(self, num_parameters=1, init=0.25):
        self.num_parameters = num_parameters
        super(TPReLU, self).__init__()
        self.weight = Parameter(torch.Tensor(num_parameters).fill_(init))
        self.bias = Parameter(torch.zeros(num_parameters))

    def forward(self, input):
        bias_resize = self.bias.view(1, self.num_parameters, *((1,) * (input.dim() - 2))).expand_as(input)
        return F.prelu(input - bias_resize, self.weight.clamp(0, 1)) + bias_resize
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对 Weigh-normalized layer 的实现：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">class WeightNormalizedLinear(Module):

    def __init__(self, in_features, out_features, scale=True, bias=True, init_factor=1, init_scale=1):
        super(WeightNormalizedLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = Parameter(torch.zeros(1, out_features))
        else:
            self.register_parameter('bias', None)
        if scale:
            self.scale = Parameter(torch.Tensor(1, out_features).fill_(init_scale))
        else:
            self.register_parameter('scale', None)
        self.reset_parameters(init_factor)

    def reset_parameters(self, factor):
        stdv = 1. * factor / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def weight_norm(self):
        return self.weight.pow(2).sum(1).add(1e-6).sqrt()

    def norm_scale_bias(self, input):
        output = input.div(self.weight_norm().transpose(0, 1).expand_as(input))
        if self.scale is not None:
            output = output.mul(self.scale.expand_as(input))
        if self.bias is not None:
            output = output.add(self.bias.expand_as(input))
        return output

    def forward(self, input):
        return self.norm_scale_bias(F.linear(input, self.weight))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>观察上面的forward函数可以发现，TReLU添加bias这一习得参数，而weight-normalized layer中则对传入的weight进行了标准化。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1704.03971&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;On the Effects and Weight Normalization in GAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="GAN" scheme="http://blog.ddlee.cn/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Large-Scale Evolution of Image Classifiers</title>
    <link href="http://blog.ddlee.cn/2017/06/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Large-Scale-Evolution-of-Image-Classifiers/"/>
    <id>http://blog.ddlee.cn/2017/06/05/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/</id>
    <published>2017-06-05T08:09:52.000Z</published>
    <updated>2017-06-05T08:09:52.667Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">Large-Scale Evolution of Image, Classifiers</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>深层网络在图片分类问题上表现优异，但网络结构的设计上并没有统一的指导。进化是构建深度网络架构的一种方式。利用本文的自动化方法得出的深度网络结构，已经能在CIFAR-10上取得可以跟人工设计的网络相媲美的结果</p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Evolution-Algorithm"><a href="#Evolution-Algorithm" class="headerlink" title="Evolution Algorithm"></a>Evolution Algorithm</h3><p>整个算法的核心是如下的tournament selection:</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/tournament.jpg" alt="tournament"></p>
<ul>
<li>population: 供筛选的群体</li>
<li>individual: 个体，带有指标fitness，特别地，指在CV集上的损失</li>
<li>worker: 筛选者，上帝</li>
</ul>
<ol>
<li><em>population</em> 中的 <em>individual</em> 均已在训练集上训练完毕，带有指标 <em>fitness</em></li>
<li><em>worker</em> 随机选择一对 <em>individual</em>，比较 <em>fitness</em>，较差的 <em>individual</em> 被舍弃</li>
<li>表现较好的 <em>individual</em> 成为parent，对其施加 <em>mutation</em> (变异)，得到 <em>child</em></li>
<li>训练 <em>child</em> 并在CV集上得到其 <em>fitness</em>，归还到 <em>population</em> 中</li>
</ol>
<h3 id="Encoding-and-Mutation"><a href="#Encoding-and-Mutation" class="headerlink" title="Encoding and Mutation"></a>Encoding and Mutation</h3><p>个体的网络结构和部分参数被编码为DNA。</p>
<p>能够施加的变异有：</p>
<ul>
<li>改变学习率</li>
<li>恒等（不变）</li>
<li>重设参数</li>
<li>加入卷积层</li>
<li>移除卷积层</li>
<li>更改卷积层的stride参数</li>
<li>更改卷积层的Channel参数</li>
<li>更改卷积核大小</li>
<li>加入skip连接（类似ResNet)</li>
<li>移除skip连接</li>
</ul>
<h3 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h3><p>计算方面采用了并行、异步、无锁的策略。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/comp.jpg" alt="comp"></p>
<p>建立约为 <em>population</em> 数1/4的 <em>worker</em>，分别运行于不同的机器上，之间独立异步。<em>population</em> 共享，若两个 <em>worker</em> 在一个 <em>individual</em> 上产生冲突，则后一个 <em>worker</em> 停止并等待再次尝试。</p>
<h3 id="Weight-Inheritance"><a href="#Weight-Inheritance" class="headerlink" title="Weight Inheritance"></a>Weight Inheritance</h3><p>除了架构之外，子模型还会继承父母模型未经变异影响的隐藏层参数（不仅是DNA中的），这样使子模型的训练时间大幅减小。</p>
<h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><p>文章的主要结果如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/results.png" alt="results"></p>
<p>最右边的结构是在CIFAR-10上发现的最好（CV集准确度最高）的结构，左边两个是它的祖先。其中白色块相当于简单的线性层，彩色块则带有非线性激活，可以看到，不同于人工设计的网络，某一线性层之后可能包含多个非线性层。</p>
<p>另外，利用本文的模型，也在CIFAR-100上做了实验，可以达到76.3%的准确率，一定程度上说明了算法的扩展性。</p>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/popu.png" alt="popu"></p>
<p>上图说明随着 <em>population</em> 规模和训练步数的增加，模型的整体水平在变好。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/mutation.png" alt="mutation"></p>
<p>在模型陷入局部最优值时，提高变异率和重设参数会使群体继续进化。这是由于变异中包含恒等映射等不改变模型架构的变异类型，再加上weight Inheritance，一些子模型只是训练次数比其他模型多很多的“活化石”。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Google I/O时就提到了自动筛选最优网络结构，但没有公布论文。但将网络结构自动化，必定是未来的方向。个人认为，ResNet就相当于自动化网络深度（一些层实际上被跳过了），而Inception单元似乎包含了太多的先验，而且也没有逻辑上的证据说明这样的结构更有效。网络结构本身就是先验信息，而要达到通用的人工智能，这些先验也必须由模型自行发觉。</p>
<p>强化学习本身也是一个进化过程，应该也有相关的工作将强化学习的框架应用于网络结构的学习上。</p>
<p>更进一步地，若数据是一阶信息，深度网络的隐藏层学到的表示是二阶信息，深度网络的结构则是三阶信息，从一阶到二阶的框架是不是都可以移植到二阶到三阶上来？关键之处在于我们还没有描述好深度网络的结构空间，但就现在的发展看，深度网络的一些基本结构(conv, BN)等，已经被作为基本单元（离散的）来进行构建和筛选了，也就是说，所有深度网络构成的空间之性质如何，还有大量的工作可以做。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1703.01041&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Large-Scale Evolution of Image, Classifiers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="autoML" scheme="http://blog.ddlee.cn/tags/autoML/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]An Analysis of Deep Neural Network Models for Practical Applications</title>
    <link href="http://blog.ddlee.cn/2017/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/"/>
    <id>http://blog.ddlee.cn/2017/06/03/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/</id>
    <published>2017-06-03T06:27:07.000Z</published>
    <updated>2017-06-03T06:27:07.962Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1605.07678" target="_blank" rel="external">An Analysis of Deep Neural Network Models for Practical Applications</a></p>
<p>本文是对现有（论文发表于2016年5月）深度网络的比较，从以下方面入手：</p>
<ul>
<li>accuracy</li>
<li>memory footprint</li>
<li>parameters</li>
<li>operations count</li>
<li>inference time</li>
<li>power consumption</li>
</ul>
<p>以下图片各模型的着色是统一的：蓝色是Inception系，绿色是VGG系，粉色是ResNet系，黄色为AlexNet系。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/top1.png" alt="top1"></p>
<p>上图是Top1准确率与模型参数数、操作数的关系。可以看到Inception系列网络以较少的参数取得相对高的准确率，而VGG系则在这一点上表现很差。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/infer-batch.png" alt="infer-batch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/power-batch.png" alt="power-batch"></p>
<p>上面两图分别是推断耗时和电量消耗与批量大小的关系。可以看到，两者均与批量大小无明显的相关关系。但电量消耗在不同的模型之间也非常类似，而推断时间与模型结构关系很大（VGG再次尴尬）。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/mem-batch.png" alt="mem-batch"></p>
<p>上图展示了模型占用内存大小与批量大小的关系，大部分网络都有相对固定的内存占用，随后随批量大小的上扬而上涨。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-infer.png" alt="infer-ops"></p>
<p>从上图可以发现推断耗时和模型的操作数大体上呈现线性关系。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-power.png" alt="ops-power"></p>
<p>电量消耗与模型的参数数、操作数并没有明显的相关性。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/accuracy-infer.png" alt="accuracy-infer"></p>
<p>注意，上图中点的大小代表模型操作数，横轴代表推断效率，纵轴表示准确率。灰色区域表示模型获得了额外的推断效率或准确率，而白色区域代表非最优。</p>
<p>操作数越多的模型推断效率越低，大部分模型都落在相对平衡的边界上，VGG和小批量情形下的AlexNet落在了非最优区域。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从这篇论文的比较中可以看到，在特定的任务中对网络特定结构的设计（如Inception单元），即加入更强的先验知识，比堆叠网络层数更有效。深度网络还是需要人类的指导才能发挥更大的作用。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1605.07678&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;An Analysis of Deep Neural Network Models for Practical Appl
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>深度学习和分布式表示</title>
    <link href="http://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/"/>
    <id>http://blog.ddlee.cn/2017/06/01/深度学习和分布式表示/</id>
    <published>2017-06-01T14:52:16.000Z</published>
    <updated>2017-06-03T04:29:31.304Z</updated>
    
    <content type="html"><![CDATA[<p>本文的两个主要参考资料：</p>
<ol>
<li>Yoshua Bengio在2016年九月<a href="https://www.bayareadlschool.org/" target="_blank" rel="external">Deep Learning School</a>的演讲Foundations and Challenges of Deep Learning。<a href="https://www.youtube.com/watch?v=11rsu_WwZTc" target="_blank" rel="external">YouTube</a></li>
<li><a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning</a>, Goodfellow et al, Section 15.4</li>
</ol>
<h3 id="从机器学习到人工智能"><a href="#从机器学习到人工智能" class="headerlink" title="从机器学习到人工智能"></a>从机器学习到人工智能</h3><p>在演讲中，Bengio提到从机器学习到人工智能有五个关键的飞跃：</p>
<ol>
<li>Lots of data</li>
<li>Very flexible models</li>
<li>Enough computing power</li>
<li>Powerful priors that can defeat the curse of dimensionality</li>
<li>Computationally efficient inference</li>
</ol>
<p>第一点已经发生，到处都提大数据，到处都在招数据分析师。<br>我在读高中时，就曾预感数据将是新时代的石油和煤炭，因为数据正是人类社会经验的总结，数据带来的知识和见解将在驱动社会进步中发挥越来越重要的作用，而自己要立志成为新时代的矿工。</p>
<p>第二点在我看来有两个例子，一是核技巧，通过核函数对分布空间的转换，赋予了模型更强大的表述能力；二是深度神经网络，多层的框架和非线性的引入使得模型理论上可以拟合任意函数。</p>
<p>第三点，借云计算的浪潮，计算力不再是一项资产而是一项可供消费的服务，我们学生也可以廉价地接触到根本负担不起的计算力资源。而GPU等芯片技术的进步也为AI的浩浩征程添砖加瓦。</p>
<p>第五点，近期发布的Tensorflow Lite和Caffe2等工具也有助于越来越多地将计算任务分配在终端上进行，而非作为一个发送与接收器。</p>
<p>最后第四点，也是这篇文章的中心话题：借助分布式表示的强大能力，深度学习正尝试解决维度带来的灾难。</p>
<h3 id="没有免费的午餐"><a href="#没有免费的午餐" class="headerlink" title="没有免费的午餐"></a>没有免费的午餐</h3><p>简单说，没有免费的午餐定理指出找不到一个在任何问题上都表现最优的模型/算法。不同的模型都有其擅长的问题，这由该模型建立时引入的先验知识决定。</p>
<p>那么，深度学习加入的先验知识是什么？</p>
<p>Bengio用的词是Compositionality，即复合性，<em>某一概念之意义由其组成部分的意义以及组合规则决定</em>。复合性的原则可以用于高效地描述我们的世界，而深度学习模型中隐藏的层正是去学习其组成部分，网络的结构则代表了组合规则。这正是深度学习模型潜在的信念。</p>
<h3 id="分布式表示带来的指数增益"><a href="#分布式表示带来的指数增益" class="headerlink" title="分布式表示带来的指数增益"></a>分布式表示带来的指数增益</h3><p>分布式表示(Distributed Representation)是连接主义的核心概念，与复合性的原理相合。整体由组成它的个体及其组合来表示。请看下面的例子：</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/distributed.webp" alt="Distributed"></p>
<p>描述一个形状，我们将其分解为不同的特征来表述。分布式表示是一种解耦，它试图复杂的概念分离成独立的部分。而这也引出了分布式表示带来的缺点：隐藏层学到的分解特征难以得到显式的解释。</p>
<p>传统的机器学习算法，如K-Means聚类、决策树等，大多使用的是非分布式表示，即用特定的参数去描述特定的区域。如K-Means聚类，我们要划分多少区域，就需要有多少个中心点。因而，这类算法的特点是，随着参数个数的提升，其能描述的概念线性增长。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/non-dist.png" alt="non-dist"><br>使用分布式表示的深度网络，则可以享受到指数级的增益，即，随着参数个数的提升，其表述能力是指数级的增长。具有$k$个值的$n$个特征，可以描述${k}^{n}$个不同的概念。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/dist.png" alt="dist"></p>
<h3 id="分布式表示在泛化上的优势"><a href="#分布式表示在泛化上的优势" class="headerlink" title="分布式表示在泛化上的优势"></a>分布式表示在泛化上的优势</h3><p>分布式的想法还可以得到额外的泛化优势。通过重新组合在原有数据中抽离出来的特征，可以表示得到原有数据中不存在的实例。在Radford et al.的工作中，生成模型区习得了性别，并能从“戴眼镜的男人”-“男人”+“女人”=“戴眼镜的女人”这样的抽象概念表达式中生成实例。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/generative.png" alt="generative"></p>
<h3 id="分布式表示与巻积神经网络"><a href="#分布式表示与巻积神经网络" class="headerlink" title="分布式表示与巻积神经网络"></a>分布式表示与巻积神经网络</h3><p>巻积神经网络不同的滤波器习得的特征可以为分布式表示的概念分解这一特性提供一些例子。下图是VGG16不同滤波器得到结果的可视化表示，<br>出自Francois Chollet的博文<a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" target="_blank" rel="external">How convolutional neural networks see the world</a></p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/filters.jpg" alt="filters"></p>
<p>可以看到，浅层的滤波器学到的是简单的颜色、线条走向等特征，较深的滤波器学到复杂的纹理。</p>
<h3 id="量子计算机与分布式表示"><a href="#量子计算机与分布式表示" class="headerlink" title="量子计算机与分布式表示"></a>量子计算机与分布式表示</h3><p>在我看来，量子计算机的激动人心之处也在于其表示能力。一个量子态可以表示原先两个静态表示的信息，原先需要8个单位静态存储表示的信息只需要3个量子态单位即可表示，这也是指数级的增益。在这一点上，计算模型和概念模型已然殊途同归。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从经验中总结原则，用原则生成套路，正是我们自己处理和解决新问题的途径。通过解耦得到的信息来消除未知和不确定性，是我们智能的一部分。我们眼中的世界，只是适合我们的一种表示而已。也许，真正的人工智能到来那一刻，会是我们创造的机器“理解”了自己的表示系统之时——我们所关注的可解释性，也就无关紧要了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文的两个主要参考资料：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Yoshua Bengio在2016年九月&lt;a href=&quot;https://www.bayareadlschool.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deep Learning Sc
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]On-the-fly Operation Batching in Dynamic Computation Graphs</title>
    <link href="http://blog.ddlee.cn/2017/05/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/"/>
    <id>http://blog.ddlee.cn/2017/05/30/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/</id>
    <published>2017-05-30T07:24:34.000Z</published>
    <updated>2017-05-30T07:24:34.453Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="http://arxiv.org/abs/1705.07860" target="_blank" rel="external">On-the-fly Operation Batching in Dynamic Computaion Graphs</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>基于动态图的深度学习框架如<code>Pytorch</code>,<code>DyNet</code>提供了更为灵活的结构和数据维度的选择，但要求开发者自行将数据批量化，才能最大限度地发挥框架的并行计算优势。</p>
<h2 id="当前的状况：灵活的结构与高效计算"><a href="#当前的状况：灵活的结构与高效计算" class="headerlink" title="当前的状况：灵活的结构与高效计算"></a>当前的状况：灵活的结构与高效计算</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/comparison.png" alt="左图为循环结构，右图将序列补齐，批量化"><br>左图为循环结构，右图将序列补齐，批量化</p>
<ol>
<li>灵活的结构和数据输入维度，采用朴素的循环结构实现，但不高效，因为尽管维度不同，在循环内数据接受的是同样的操作。</li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/loop.png" alt="loop"></p>
<ol>
<li>对数据做“Padding”，即用傀儡数据将输入维度对齐，进而实现向量化，但这种操作对开发者并不友好，会使开发者浪费掉很多本该投入到结构设计等方面的精力。</li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/padding.png" alt="padding"></p>
<h2 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h2><h3 id="三个部分"><a href="#三个部分" class="headerlink" title="三个部分"></a>三个部分</h3><ol>
<li>Graph Definition</li>
<li>Operation Batching</li>
<li>Computation</li>
</ol>
<p>第一步和第三步在当前已被大部分深度学习框架较好地实现。主要特点是，构建计算图与计算的分离，即”Lazy Evaluation”。比如在<code>Tensorflow</code>中，一个抽象层负责解析计算图各节点之间的依赖，决定执行计算的顺序，而另一个抽象层则负责分配计算资源。</p>
<h3 id="Operation-Batching"><a href="#Operation-Batching" class="headerlink" title="Operation Batching"></a>Operation Batching</h3><h4 id="Computing-compatibility-groups"><a href="#Computing-compatibility-groups" class="headerlink" title="Computing compatibility groups"></a>Computing compatibility groups</h4><p>这一步是建立可以批量化计算的节点组。具体做法是，给每一个计算节点建立 <em>signature</em>，用于描述节点计算的特性，文中举出了如下几个例子:</p>
<ol>
<li>Component-wise operations: 直接施加在每个张量元素上的计算，跟张量的维度无关，如$tanh$,$log$</li>
<li>Dimension-sensitive operations: 基于维度的计算，如线性传递$Wh+b$，要求$W$和$h$维度相符，<em>signature</em> 中要包含维度信息</li>
<li>Operations with shared elements: 包含共享元素的计算，如共享的权值$W$</li>
<li>Unbatchable operations: 其他</li>
</ol>
<h4 id="Determining-execution-order"><a href="#Determining-execution-order" class="headerlink" title="Determining execution order"></a>Determining execution order</h4><p>执行顺序要满足两个目标：</p>
<ol>
<li>每一节点的计算要在其依赖之后</li>
<li>带有同样 <em>signature</em> 且没有依赖关系的节点放在同一批量执行</li>
</ol>
<p>但在一般情况下找到最大化批量规模的执行顺序是个NP问题。有如下两种策略：</p>
<ol>
<li>Depth-based Batching: 库<code>Tensorflow Fold</code>中使用的方法。某一节点的深度定义为其子节点到其本身的最大长度，同一深度的节点进行批量计算。但由于输入序列长度不一，可能会错失一些批量化的机会。</li>
<li>Agenda-based Batching: 本文的方法，核心的想法是维护一个 <em>agenda</em> 序列，所有依赖已经被解析的节点入列，每次迭代时从 <em>agenda</em> 序列中按 <em>signature</em> 相同的原则取出节点进行批量计算。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章选取了四个模型：BiLSTM, BiLSTM w/char, Tree-structured LSTMs, Transition-based Dependency Parsing。</p>
<p>实验结果：（单位为Sentences/second）<br><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/result.png" alt="result"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本来读到题目还是蛮惊喜的，期待的是从模型构建的角度解决序列长度不一带来的计算上的不便。但通读下来发现是在计算图的计算这一层面进行的优化，有些失望但也感激，作者使用<code>DyNet</code>框架实现了他们的方法，希望自己也可以为<code>Pytorch</code>等框架该算法的实现出一份力。</p>
<p>感谢这些开源的框架，正一步步拉近人类构建模型和机器高效计算之间的距离。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;http://arxiv.org/abs/1705.07860&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;On-the-fly Operation Batching in Dynamic Computaion Graphs&lt;/a
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Paper" scheme="http://blog.ddlee.cn/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>LSTM:Pytorch实现</title>
    <link href="http://blog.ddlee.cn/2017/05/29/LSTM-Pytorch%E5%AE%9E%E7%8E%B0/"/>
    <id>http://blog.ddlee.cn/2017/05/29/LSTM-Pytorch实现/</id>
    <published>2017-05-28T17:06:44.000Z</published>
    <updated>2017-05-28T17:16:51.094Z</updated>
    
    <content type="html"><![CDATA[<p>本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM是一种循环神经网络，适用于对序列化的输入建模。Chris Olah的这篇<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">文章</a>细致地解释了一个LSTM单元的运作方式，建议阅读。</p>
<h3 id="两个想法"><a href="#两个想法" class="headerlink" title="两个想法"></a>两个想法</h3><h4 id="Gate：信息流动的闸门"><a href="#Gate：信息流动的闸门" class="headerlink" title="Gate：信息流动的闸门"></a>Gate：信息流动的闸门</h4><script type="math/tex; mode=display">i_t = sigmoid(W_{xi} x_t  +  W_{hi}h_{t-1} + b_i)</script><script type="math/tex; mode=display">f_t = sigmoid(W_{xf} x_t  +  W_{hf}h_{t-1} + b_f)</script><script type="math/tex; mode=display">o_t = sigmoid(W_{xo} x_t  +  W_{ho}h_{t-1} + b_o)</script><p>$x$ 表示输入，$h$表示隐藏状态，用$sigmoid$函数将输入二者的传递结果映射到$（0,1)$上，分别赋予输入门、遗忘门、输出门的含义，来控制不同神经单元（同一神经元不同时间点的状态）之间信息流动。</p>
<h4 id="Cell：记忆池"><a href="#Cell：记忆池" class="headerlink" title="Cell：记忆池"></a>Cell：记忆池</h4><script type="math/tex; mode=display">c_t = f_t \odot c_{t - 1} + i_t \odot tanh(W_{xc} x_t  +  W_{hc}h_{t-1} + b_c)\\
h_t = o_t \odot tanh(c_t)</script><p>$h$表示隐藏状态，$C$表示记忆池，通过Gate，上一单元（状态）的信息有控制地遗忘，当前的输入有控制地流入，记忆池中的信息有控制地流入隐藏状态。</p>
<h3 id="与普通RNN的对比"><a href="#与普通RNN的对比" class="headerlink" title="与普通RNN的对比"></a>与普通RNN的对比</h3><p><img src="http://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM3-SimpleRNN.png" alt="RNN"><br>普通RNN只有一个自更新的隐藏状态单元。</p>
<p><img src="http://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM.jpg" alt="LSTM"><br>LSTM增加了记忆池Cell，并通过几个Gate将信息有控制地更新在记忆池中，并通过记忆池中的信息来决定隐藏状态。</p>
<h2 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h2><p>下面是手动实现LSTM的代码，继承了基类<code>nn.Module</code>。</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">import torch.nn as nn
import torch
from torch.autograd import Variable

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, cell_size, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.cell_size = cell_size
        self.gate = nn.Linear(input_size + hidden_size, cell_size)
        self.output = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
        self.tanh = nn.Tanh()
        self.softmax = nn.LogSoftmax()

    def forward(self, input, hidden, cell):
        combined = torch.cat((input, hidden), 1)
        f_gate = self.gate(combined)
        i_gate = self.gate(combined)
        o_gate = self.gate(combined)
        f_gate = self.sigmoid(f_gate)
        i_gate = self.sigmoid(i_gate)
        o_gate = self.sigmoid(o_gate)
        cell_helper = self.gate(combined)
        cell_helper = self.tanh(cell_helper)
        cell = torch.add(torch.mul(cell, f_gate), torch.mul(cell_helper, i_gate))
        hidden = torch.mul(self.tanh(cell), o_gate)
        output = self.output(hidden)
        output = self.softmax(output)
        return output, hidden, cell

    def initHidden(self):
        return Variable(torch.zeros(1, self.hidden_size))

    def initCell(self):
        return Variable(torch.zeros(1, self.cell_size))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>几个关键点：</p>
<ol>
<li>Tensor的大小</li>
<li>信息的传递顺序</li>
</ol>
<h2 id="Pytorch-Module"><a href="#Pytorch-Module" class="headerlink" title="Pytorch Module"></a>Pytorch Module</h2><p>Pytorch库本身对LSTM的实现封装了更多功能，类和函数的组织也非常有借鉴意义。我对其实现的理解基于以下两点展开：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p>下面开始按图索骥，源码见<a href="https://github.com/pytorch/pytorch/tree/master/torch" target="_blank" rel="external">GitHub</a>。</p>
<h4 id="LSTM类"><a href="#LSTM类" class="headerlink" title="LSTM类"></a>LSTM类</h4><p>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" target="_blank" rel="external">nn/modules/rnn.py</a></p>
<pre class="line-numbers language-lang-Python"><code class="language-lang-Python"># nn/modules/rnn.py
class RNNBase(Module):
  def __init__(self, mode, input_size, output_size):
      pass
  def forward(self, input, hx=None):
      if hx is None:
          hx = torch.autograd.Variable()
      if self.mode == 'LSTM':
          hx = (hx, hx)
      func = self._backend.RNN() #!!!
      output, hidden = func(input, self.all_weights, hx) #!!!
      return output, hidden

class LSTM(RNNBase):
    def __init__(self, *args, **kwargs):
        super(LSTM, self).__init__('LSTM', *args, **kwargs)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>LSTM</code>类只是<code>RNNBase</code>类的一个装饰器。</li>
<li>在基类<code>nn.Module</code>中，把<code>__call__()</code>定义为调用<code>forward()</code>方法，因而真正的功能实现在<code>_backend.RNN()</code>中</li>
</ol>
<h4 id="AutogradRNN函数"><a href="#AutogradRNN函数" class="headerlink" title="AutogradRNN函数"></a>AutogradRNN函数</h4><p>下面寻找<code>_backend.RNN</code>。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/backends/thnn.py" target="_blank" rel="external">nn/backends/thnn.py</a></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python"># nn/backends/thnn.py
def _initialize_backend():
    from .._functions.rnn import RNN, LSTMCell
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>原来，<code>_backend</code>也是索引。</p>
<p>终于找到<code>RNN()</code>函数。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py" target="_blank" rel="external">nn/_functions/rnn.py</a></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python"># nn/_functions/rnn.py
def RNN(*args, **kwargs):
    def forward(input, *fargs, **fkwargs):
        func = AutogradRNN(*args, **kwargs)
        return func(input, *fargs, **fkwargs)
    return forward

def AutogradRNN(mode, input_size, hidden_size):
    cell = LSTMCell
    rec_factory = Recurrent
    layer = (rec_factory(cell),)
    func = StackedRNN(layer, num_layers)
    def forward(input, weight, hidden):
        nexth, output = func(input, hidden, weight)
        return output, nexth
    return forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>RNN()</code>是一个装饰器，根据是否有<code>cudnn</code>库决定调用<code>AutogradRNN()</code>还是<code>CudnnRNN()</code>，这里仅观察<code>AutogradRNN()</code></li>
<li><code>AutogradRNN()</code>选用了<code>LSTMCell</code>，用<code>Recurrent()</code>函数处理了<code>Cell</code>构成<code>Layer</code>，再将<code>Layer</code>传入<code>StackedRNN()</code>函数</li>
<li><code>RNN()</code>和<code>AutogradRNN()</code>返回的都是其<code>forward()</code>函数句柄</li>
</ol>
<p>下面是<code>Recurrent()</code>函数：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def Recurrent(inner):
    def forward(input, hidden, weight):
        output = []
        steps = range(input.size(0) - 1, -1, -1)
        for i in steps:
            hidden = inner(input[i], hidden, *weight)
            output.append(hidden[0])
        return hidden, output
    return forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>Recurrent()</code>函数实现了“递归”的结构，根据输入的大小组合<code>Cell</code>，完成了隐藏状态和参数的迭代。</li>
<li><code>Recurrent()</code>函数将<code>Cell(inner)</code>组合为<code>Layer</code>。</li>
</ol>
<h4 id="StackedRNN-函数"><a href="#StackedRNN-函数" class="headerlink" title="StackedRNN()函数"></a>StackedRNN()函数</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def StackedRNN(inners, num_layers):
    num_directions = len(inners)
    total_layers = num_layers * num_directions
    def forward(input, hidden, weight):
        next_hidden = []
        hidden = list(zip(*hidden))
        for i in range(num_layers):
          all_output = []
          for j, inner in enumerate(inners):
              hy, output = inner(input, hidden[l], weight[l])
              next_hidden.append(hy)
              all_output.append(output)
          input = torch.cat(all_output, input.dim() - 1)
        next_h, next_c = zip(*next_hidden)
        next_hidden = (torch.cat(next_h, 0).view(total_layers, *next_h[0].size()),
                  torch.cat(next_c, 0).view(total_layers, *next_c[0].size()))
        return next_hidden, input
    return forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>StackedRNN()</code>函数将<code>Layer(inner)</code>组合为栈</li>
</ol>
<p>最后的最后，一个基本的LSTM单元内的计算由<code>LSTMCell()</code>函数实现。</p>
<h4 id="LSTMCell-函数"><a href="#LSTMCell-函数" class="headerlink" title="LSTMCell()函数"></a>LSTMCell()函数</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">def LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):
    if input.is_cuda:
        igates = F.linear(input, w_ih)
        hgates = F.linear(hidden[0], w_hh)
        state = fusedBackend.LSTMFused()
        return state(igates, hgates, hidden[1]) if b_ih is None else state(igates, hgates, hidden[1], b_ih, b_hh)

    hx, cx = hidden
    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)

    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)

    ingate = F.sigmoid(ingate)
    forgetgate = F.sigmoid(forgetgate)
    cellgate = F.tanh(cellgate)
    outgate = F.sigmoid(outgate)

    cy = (forgetgate * cx) + (ingate * cellgate)
    hy = outgate * F.tanh(cy)

    return hy, cy
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>观察上面的代码，即是LSTM的基本信息传递公式。至此，我们的旅程完成。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><blockquote>
<p>没有什么是增加一层抽象不能解决的，如果不能，那就再加一层。</p>
</blockquote>
<p>重复一下我对上述代码的理解：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/LSTM-Pytorch实现/str.jpg" alt="str"></p>
<p>如洋葱一般，我们剥到最后，发现处理的信息正是输入、隐藏状态和LSTM单元几个控制门的参数。在一层一层的抽象之中，Pytorch在不同的层面处理了不同的参数，保证了扩展性和抽象层之间的解耦。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。&lt;/p&gt;
&lt;h2 id=&quot;LSTM&quot;&gt;&lt;a href=&quot;#LSTM&quot; class=&quot;headerlink&quot; title=&quot;LSTM&quot;&gt;&lt;/a&gt;LSTM&lt;/h2&gt;&lt;p&gt;LSTM是一种循环神
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.ddlee.cn/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pandas速度优化</title>
    <link href="http://blog.ddlee.cn/2017/05/28/Pandas%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.ddlee.cn/2017/05/28/Pandas速度优化/</id>
    <published>2017-05-28T12:06:14.000Z</published>
    <updated>2017-05-28T12:17:21.127Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要内容取自Sofia Heisler在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=HN5d490_KKk" target="_blank" rel="external">No More Sad Pandas Optimizing Pandas Code for Speed and Efficiency</a>，讲稿代码和幻灯片见<a href="https://github.com/sversh/pycon2017-optimizing-pandas" target="_blank" rel="external">GitHub</a>。</p>
<h2 id="Set-Up"><a href="#Set-Up" class="headerlink" title="Set Up"></a>Set Up</h2><h4 id="示例数据"><a href="#示例数据" class="headerlink" title="示例数据"></a>示例数据</h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>ean_hotel_id</th>
<th>name</th>
<th>address1</th>
<th>city</th>
<th>state_province</th>
<th>postal_code</th>
<th>latitude</th>
<th>longitude</th>
<th>star_rating</th>
<th>high_rate</th>
<th>low_rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>269955</td>
<td>Hilton Garden Inn Albany/SUNY Area</td>
<td>1389 Washington Ave</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68751</td>
<td>-73.81643</td>
<td>3.0</td>
<td>154.0272</td>
<td>124.0216</td>
</tr>
<tr>
<td>1</td>
<td>113431</td>
<td>Courtyard by Marriott Albany Thruway</td>
<td>1455 Washington Avenue</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68971</td>
<td>-73.82021</td>
<td>3.0</td>
<td>179.0100</td>
<td>134.0000</td>
</tr>
<tr>
<td>2</td>
<td>108151</td>
<td>Radisson Hotel Albany</td>
<td>205 Wolf Rd</td>
<td>Albany</td>
<td>NY</td>
<td>12205</td>
<td>42.72410</td>
<td>-73.79822</td>
<td>3.0</td>
<td>134.1700</td>
<td>84.1600</td>
</tr>
</tbody>
</table>
</div>
<h4 id="示例函数：Haversine-Distance"><a href="#示例函数：Haversine-Distance" class="headerlink" title="示例函数：Haversine Distance"></a>示例函数：Haversine Distance</h4><pre class="line-numbers language-lang-Python"><code class="language-lang-Python">def haversine(lat1, lon1, lat2, lon2):
    miles_constant = 3959
    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    mi = miles_constant * c
    return mi
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="优化它之前，先测量它"><a href="#优化它之前，先测量它" class="headerlink" title="优化它之前，先测量它"></a>优化它之前，先测量它</h2><h4 id="IPython-Notebook的Magic-Command-timeit"><a href="#IPython-Notebook的Magic-Command-timeit" class="headerlink" title="IPython Notebook的Magic Command: %timeit"></a>IPython Notebook的Magic Command: <code>%timeit</code></h4><p>既可以测量某一行代码的执行时间，又可以测量整个单元格里代码快的执行时间。</p>
<h4 id="Package-line-profiler"><a href="#Package-line-profiler" class="headerlink" title="Package: line_profiler"></a>Package: line_profiler</h4><p>记录每行代码的执行次数和执行时间。</p>
<p>在IPython Notebook中使用时，先运行<code>%load_ext line_profiler</code>， 之后可以用<code>%lprun -f [function name]</code>命令记录指定函数的执行情况。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h4 id="对行做循环-Baseline"><a href="#对行做循环-Baseline" class="headerlink" title="对行做循环(Baseline)"></a>对行做循环(Baseline)</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">%%timeit
haversine_series = []
for index, row in df.iterrows():
    haversine_series.append(haversine(40.671, -73.985,\
                                      row['latitude'], row['longitude']))
df['distance'] = haversine_series
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>197 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><h4 id="pd-DataFrame-apply-方法"><a href="#pd-DataFrame-apply-方法" class="headerlink" title="pd.DataFrame.apply()方法"></a>pd.DataFrame.apply()方法</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python">%lprun -f haversine \
df.apply(lambda row: haversine(40.671, -73.985,\
                               row['latitude'], row['longitude']), axis=1)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>90.6 ms ± 7.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.049982 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2      1631         1535      0.9      3.1      miles_constant = 3959
     3      1631        16602     10.2     33.2      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4      1631         2019      1.2      4.0      dlat = lat2 - lat1
     5      1631         1143      0.7      2.3      dlon = lon2 - lon1
     6      1631        18128     11.1     36.3      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7      1631         7857      4.8     15.7      c = 2 * np.arcsin(np.sqrt(a))
     8      1631         1708      1.0      3.4      mi = miles_constant * c
     9      1631          990      0.6      2.0      return mi
</code></pre><p>观察Hits这一列可以看到，<code>apply()</code>方法还是将函数一行行地应用于每行。</p>
<h4 id="向量化：将pd-Series传入函数"><a href="#向量化：将pd-Series传入函数" class="headerlink" title="向量化：将pd.Series传入函数"></a>向量化：将pd.Series传入函数</h4><pre><code>%lprun -f haversine haversine(40.671, -73.985,\
                              df[&#39;latitude&#39;], df[&#39;longitude&#39;])
</code></pre><p>Output:</p>
<pre><code>2.21 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.008601 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.0      miles_constant = 3959
     3         1          838    838.0      9.7      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1          597    597.0      6.9      dlat = lat2 - lat1
     5         1          572    572.0      6.7      dlon = lon2 - lon1
     6         1         5033   5033.0     58.5      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1         1060   1060.0     12.3      c = 2 * np.arcsin(np.sqrt(a))
     8         1          496    496.0      5.8      mi = miles_constant * c
     9         1            2      2.0      0.0      return mi
</code></pre><p>向量化之后，函数内的每行操作只被访问一次，达到了行结构上的并行。</p>
<h3 id="向量化：将np-array传入函数"><a href="#向量化：将np-array传入函数" class="headerlink" title="向量化：将np.array传入函数"></a>向量化：将np.array传入函数</h3><pre class="line-numbers language-lang-python"><code class="language-lang-python">%lprun -f haversine df['distance'] = haversine(40.671, -73.985,\
                        df['latitude'].values, df['longitude'].values)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>Output：</p>
<pre><code>370 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.001382 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.2      miles_constant = 3959
     3         1          292    292.0     21.1      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1           40     40.0      2.9      dlat = lat2 - lat1
     5         1           29     29.0      2.1      dlon = lon2 - lon1
     6         1          815    815.0     59.0      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1          183    183.0     13.2      c = 2 * np.arcsin(np.sqrt(a))
     8         1           18     18.0      1.3      mi = miles_constant * c
     9         1            2      2.0      0.1      return mi
</code></pre><p>相比<code>pd.Series</code>，<code>np.array</code>不含索引等额外信息，因而更加高效。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Methodology</th>
<th>Avg.    single    run    time</th>
<th>Marginal    performance    improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Looping    with    iterrows</td>
<td>184.00</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Looping    with    apply</td>
<td>78.10</td>
<td>2.4x</td>
</tr>
<tr>
<td>Vectorization    with    Pandas    series</td>
<td>1.79</td>
<td>43.6x</td>
</tr>
<tr>
<td>Vectorization    with    NumPy    arrays</td>
<td>0.37</td>
<td>4.8x</td>
</tr>
</tbody>
</table>
</div>
<p>通过上面的对比，我们比最初的baseline快了近500倍。最大的提升来自于向量化。因而，实现的函数能够很方便地向量化是高效处理的关键。</p>
<h2 id="用Cython优化"><a href="#用Cython优化" class="headerlink" title="用Cython优化"></a>用<code>Cython</code>优化</h2><p><code>Cython</code>可以将<code>python</code>代码转化为<code>C</code>代码来执行，可以进行如下优化（静态化变量类型，调用C函数库）</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">%load_ext cython

%%cython -a
# Haversine cythonized
from libc.math cimport sin, cos, acos, asin, sqrt

cdef deg2rad_cy(float deg):
    cdef float rad
    rad = 0.01745329252*deg
    return rad

cpdef haversine_cy_dtyped(float lat1, float lon1, float lat2, float lon2):
    cdef:
        float dlon
        float dlat
        float a
        float c
        float mi

    lat1, lon1, lat2, lon2 = map(deg2rad_cy, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    mi = 3959 * c
    return mi
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>嵌套于循坏中：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">%timeit df['distance'] =\
df.apply(lambda row: haversine_cy_dtyped(40.671, -73.985,\
                              row['latitude'], row['longitude']), axis=1)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>10 loops, best of 3: 68.4 ms per loop
</code></pre><p>可以看到，<code>Cython</code>确实带来速度上的提升，但效果不及向量化（并行化）。</p>
]]></content>
    
    <summary type="html">
    
      一些优化Pandas库处理大批量数据速度的技巧
    
    </summary>
    
      <category term="Data Science" scheme="http://blog.ddlee.cn/categories/Data-Science/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Data Science" scheme="http://blog.ddlee.cn/tags/Data-Science/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
  </entry>
  
  <entry>
    <title>Python可视化工具指引</title>
    <link href="http://blog.ddlee.cn/2017/05/28/Python%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E6%8C%87%E5%BC%95/"/>
    <id>http://blog.ddlee.cn/2017/05/28/Python可视化工具指引/</id>
    <published>2017-05-27T16:12:28.000Z</published>
    <updated>2017-06-03T04:27:24.027Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=FytuB8nFHPQ" target="_blank" rel="external">Python’s Visualization Landscape</a></p>
<p>Python真是越来越火了。活跃的开源社区为Python这门语言贡献着长青的活力。</p>
<p>子曾经曰过：轮子多了，车就稳了。</p>
<p>本文帮助你选好轮子，也祝愿可视化的车开得越来越稳。</p>
<h2 id="The-Landscape"><a href="#The-Landscape" class="headerlink" title="The Landscape"></a>The Landscape</h2><p><img src="http://static.ddlee.cn/static/img/Python可视化工具指引/landscape.png" alt="landscape"></p>
<p>如图。</p>
<p>VanderPlas在展示完这张全景图后给大家贴了这张图：</p>
<p><img src="http://static.ddlee.cn/static/img/Python可视化工具指引/chan.png" alt="chan"></p>
<p>我差点笑喷。我们的表情包可能要在人民币之前走向国际化了。</p>
<p>回到正题，可视化工具有两个主要阵营，一是基于matplotlib，二是基于JavaScript。还有的接入了JS下著名的D3.js库。</p>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>numpy, pandas, matplotlib可以说是python数据科学的三驾马车。凡以python为教学语言的数据科学相关课程必提这三个库。而matplotlib又有什么特点呢？</p>
<p>先说优点：</p>
<ol>
<li>像MATLAB的语法，对MATLAB用户好上手</li>
<li>稳定，久经考验</li>
<li>渲染后端丰富，跨平台（GTK, Qt5, svg, pdf等）</li>
</ol>
<p>缺点也有很多：</p>
<ol>
<li>API过于繁琐</li>
<li>默认配色太丑</li>
<li>对web支持差，交互性差</li>
<li>对大数据集处理较慢</li>
</ol>
<p>于是就有了很多基于matplotlib的扩展，提供了更丰富、更人性化的API。</p>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/matplotlib.png" alt="matplotlib"></p>
<p>下面是几个比较受欢迎的包：</p>
<h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p>pandas的DataFrame对象是有plot()方法的，如：<br><code>iris.plot.scatter(&#39;petalLength&#39;, &#39;petalWidth&#39;)</code>生成二维散点图，只需指明两个轴取自哪一列数据即可。</p>
<h3 id="seaborn"><a href="#seaborn" class="headerlink" title="seaborn"></a>seaborn</h3><p>seaborn(<a href="http://seaborn.pydata.org/examples/" target="_blank" rel="external">gallery</a>)专注于统计数据可视化，默认配色也还可以。语法示例：</p>
<pre class="line-numbers language-lang-Python"><code class="language-lang-Python">import seaborn as sns
sns.lmplot('petalLength', 'sepalWidth', iris, hue='species', fit_reg=False)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="类ggplot"><a href="#类ggplot" class="headerlink" title="类ggplot"></a>类ggplot</h3><p>对于R用户，最熟悉的可视化包可能是ggplot2，在python中可以考虑ggpy(<a href="https://github.com/yhat/ggpy)和近期上了Github" target="_blank" rel="external">https://github.com/yhat/ggpy)和近期上了Github</a> Trends的plotnie(<a href="https://github.com/has2k1/plotnine)。" target="_blank" rel="external">https://github.com/has2k1/plotnine)。</a></p>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>基于JS的包常常具有非常好的交互性，其共同点是将图形格式化为json文件，再由JS完成渲染。</p>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/js.png" alt="js"></p>
<h3 id="Bokeh"><a href="#Bokeh" class="headerlink" title="Bokeh"></a>Bokeh</h3><p>Bokeh(<a href="http://bokeh.pydata.org/en/latest/docs/gallery.html" target="_blank" rel="external">Gallery</a>)定位于绘制用于浏览器展示的交互式图形。其优点是交互性、能够处理大量数据和流数据。语法示例：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">p = figure()
p.circle(iris.petalLength, iris.sepalWidth)
show(p)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="Plotly"><a href="#Plotly" class="headerlink" title="Plotly"></a>Plotly</h3><p>Plotly(<a href="https://plot.ly/python/" target="_blank" rel="external">Gallery</a>)跟Bokeh类似。但其提供了多种语言接口(JS, R, Python, MATLAB)，并且支持3D和动画效果，缺点是有些功能需要付费。<br>语法示例：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">from plotly.graph_objs import Scatter
from plotly.offline import iplot
p = Scatter(x=iris.petalLength,
            y=iris.sepalWidth,
            mode='markers')
iplot(p)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="处理大型数据集"><a href="#处理大型数据集" class="headerlink" title="处理大型数据集"></a>处理大型数据集</h2><p>对于大型数据集，可以考虑的包包括datashader, Vaex, 基于OpenGL的Vispy和Glumpy，GlueViz等。这里介绍datashader。</p>
<h3 id="datashader"><a href="#datashader" class="headerlink" title="datashader"></a>datashader</h3><p><a href="https://github.com/bokeh/datashader" target="_blank" rel="external">datashader</a>是Bokeh的子项目，为处理大型数据集而生。</p>
<p>示例语法：</p>
<pre class="line-numbers language-lang-Python"><code class="language-lang-Python">from colorcet import fire
export(tf.shade(agg, cmap=cm(fire, 0.2), how='eq_hist'), 'census_ds_fier_eq_hist')
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/fire.jpg" alt="fire"></p>
<h2 id="最终的建议"><a href="#最终的建议" class="headerlink" title="最终的建议"></a>最终的建议</h2><p>上车忠告：</p>
<ol>
<li>matplotlib必会</li>
<li>R用户：ggpy/plotnine</li>
<li>交互式：plotly(与R接口统一)/bokeh(免费)</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲&lt;a href=&quot;https://www.youtube.com/watch?v=FytuB8nFHPQ&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Python’s Visua
    
    </summary>
    
      <category term="Data Science" scheme="http://blog.ddlee.cn/categories/Data-Science/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Visualization" scheme="http://blog.ddlee.cn/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Deep Learning</title>
    <link href="http://blog.ddlee.cn/2017/05/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Learning/"/>
    <id>http://blog.ddlee.cn/2017/05/23/论文笔记-Deep-Learning/</id>
    <published>2017-05-23T11:09:09.000Z</published>
    <updated>2017-06-02T11:24:55.784Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" target="_blank" rel="external">Deep Learning</a></p>
<p>这篇文章是三位大牛15年发表在Nature上有关深度学习的综述，尽管这两年深度学习又有更多的模型和成果出现，文章显得有些过时，但来自三位领军人物对深度学习的深度阐述还是值得反复回味。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>摘要的第一句话可以说给深度学习下了定义。有一些观点认为深度学习就是堆叠了很多层的神经网络，因计算力的提升而迎来第二春。但请看三位是怎么说的：</p>
<blockquote>
<p>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.</p>
</blockquote>
<p>也就是说，深度学习是允许由 <em>多个处理层构成的计算模型</em> 用多个层次的 <em>抽象</em> 来习得 <em>数据表示</em> 的技术。我的解读如下：</p>
<ol>
<li>深度学习不限于神经网络模型，其关键之处在于多层的表示</li>
<li>深度学习属于表示学习，目的是习得数据的某种表示，而这种表示由多个层次的抽象完成</li>
</ol>
<p>在第一段的导言中，文章总结了深度学习技术取得突破性成果的各个领域，也再次指出了深度学习与传统学习算法的不同之处：</p>
<ul>
<li>传统学习模型需要特征工程和领域知识来从数据构建较好的特征</li>
<li>深度学习中，多层的特征由通用的学习过程得到，而不需要人类工程师的参与</li>
</ul>
<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><p>这一段概述了监督学习的一般框架、优化策略，并指出浅层学习需要Feature Extractor来提取对最适合目标问题的特征。</p>
<h2 id="Backpropagation-to-train-multilayer-architectures"><a href="#Backpropagation-to-train-multilayer-architectures" class="headerlink" title="Backpropagation to train multilayer architectures"></a>Backpropagation to train multilayer architectures</h2><p>这一段指出BP算法的关键在于目标函数关于某一子模块输入的导数可以反向通过目标函数关于该子模块输出的导数得出，而这一过程是可迭代的。BP算法曾因容易陷于局部最优解而被冷落，但对于大型网络，在实践中，理论和经验都表明尽管落于局部最优解，但这个解的效果却和全局最优解相差无几，而且几乎所有的局部最优解都可以取得类似的效果。</p>
<h2 id="Convolutional-neural-networks"><a href="#Convolutional-neural-networks" class="headerlink" title="Convolutional neural networks"></a>Convolutional neural networks</h2><p>巻积网络背后有四个关键想法：</p>
<ul>
<li>local connections</li>
<li>shared weights</li>
<li>pooling</li>
<li>the use of many layers</li>
</ul>
<p>巻积网络常由巻积层、池化层和激活层构成，巻积层用于提取局部特征，池化层用于整合相似的特征，激活层用于加入非线性。这样的结构有两点理由：</p>
<ol>
<li>张量性数据的局部数值常常高度相关，局部特征容易发现</li>
<li>局部特征跟位置无关（平移不变性）</li>
</ol>
<p>文章也提到了这种巻积结构的仿生学证据。</p>
<h2 id="Image-understanding-with-deep-convolutional-networks"><a href="#Image-understanding-with-deep-convolutional-networks" class="headerlink" title="Image understanding with deep convolutional networks"></a>Image understanding with deep convolutional networks</h2><p>这一段总结了巻积网路在图像方面取得的成就。</p>
<h2 id="Distributed-representations-and-language-processing"><a href="#Distributed-representations-and-language-processing" class="headerlink" title="Distributed representations and language processing"></a>Distributed representations and language processing</h2><p>分布式表示在两点上可以取得指数级增益：</p>
<ol>
<li>习得特征的不同组合可以泛化出训练数据中不存在的类型</li>
<li>特征组合的个数的增加关于层数是指数级的</li>
</ol>
<p>文章还比较了分布式表示相比传统的词频统计在表述人类语言方面的优势。</p>
<h2 id="Recurrent-neural-networks"><a href="#Recurrent-neural-networks" class="headerlink" title="Recurrent neural networks"></a>Recurrent neural networks</h2><p>这一段概述了循环神经网络的动态特性和LSTM等结构上的改进。</p>
<h2 id="The-future-of-deep-learning"><a href="#The-future-of-deep-learning" class="headerlink" title="The future of deep learning"></a>The future of deep learning</h2><p>作者认为在长期看来，无监督学习会更为重要，人工智能领域的重大飞跃将由组合了表示学习和复杂推理的系统取得。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章是三位
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>Dropout-Pytorch实现</title>
    <link href="http://blog.ddlee.cn/2017/05/17/Dropout-Pytorch%E5%AE%9E%E7%8E%B0/"/>
    <id>http://blog.ddlee.cn/2017/05/17/Dropout-Pytorch实现/</id>
    <published>2017-05-17T11:30:44.000Z</published>
    <updated>2017-06-03T08:32:34.577Z</updated>
    
    <content type="html"><![CDATA[<p>Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。</p>
<h3 id="Dropout的洞察"><a href="#Dropout的洞察" class="headerlink" title="Dropout的洞察"></a>Dropout的洞察</h3><p>关于Dropout，一个流行的解释是，通过随机行为训练网络，并平均多个随机决定的结果，实现了参数共享的Bagging。如下图，通过随机地冻结/抛弃某些隐藏单元，我们得到了新的子网络，而参数共享是说，与Bagging中子模型相互独立的参数不同，深度网络中Dropout生成的子网络是串行的，后一个子模型继承了前一个子模型的某些参数。</p>
<p><img src="http://static.ddlee.cn/static/img/Dropout-Pytorch实现/dropout.jpg" alt="dropout"></p>
<p>Dropout是模型自我破坏的一种形式，这种破坏使得存活下来的部分更加鲁棒。例如，某一隐藏单元学得了脸部鼻子的特征，而在Dropout中遭到破坏，则在之后的迭代中，要么该隐藏单元重新学习到鼻子的特征，要么学到别的特征，后者则说明，鼻子特征对该任务来说是冗余的，因而，通过Dropout，保留下来的特征更加稳定和富有信息。</p>
<p>Hinton曾用生物学的观点解释这一点。神经网络的训练过程可以看做是生物种群逐渐适应环境的过程，在迭代中传递的模型参数可以看做种群的基因，Dropout以随机信号的方式给环境随机的干扰，使得传递的基因不得不适应更多的情况才能存活。</p>
<p>另一个需要指出的地方是，Dropout给隐藏单元加入的噪声是乘性的，不像Bias那样加在隐藏单元上，这样在进行反向传播时，Dropout引入的噪声仍能够起作用。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面看在实践中，Dropout层是如何实现的。简单来说，就是生成一系列随机数作为mask，然后再用mask点乘原有的输入，达到引入噪声的效果。</p>
<h4 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h4><pre class="line-numbers language-lang-python"><code class="language-lang-python"># forward pass
def dropout_forward(x, dropout_param):
  p, mode = dropout_param['p'], dropout_param['mode']
  # p: dropout rate; mode: train or test
  if 'seed' in dropout_param:
    np.random_seed(dropout_param['seed'])
  # seed: random seed
  mask = None
  out = None
  if mode == 'train':
    mask = (np.random.rand(*x.shape) >= p)/(1-p)
    # 1-p as normalization multiplier: to keep the size of input
    out = x * mask
  elif mode == 'test':
    # do nothing when perform inference
    out = x
  cache = (dropout_param, mask)
  out = out.astype(x.dtype, copy=False)
  return out, cache

# backward pass
def dropout_backward(dout, cache):
  dropout_param, mask = cache
  mode = dropout_param['mode']

  dx = None
  if mode == 'train':
    dx = dout * mask
  elif mode == 'test':
    dx = dout
  return dx
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><p>file: <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/dropout.py" target="_blank" rel="external">/torch/nn/_functions/dropout.py</a></p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">class Dropout(InplaceFunction):

    def __init__(self, p=0.5, train=False, inplace=False):
        super(Dropout, self).__init__()
        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, "
                             "but got {}".format(p))
        self.p = p
        self.train = train
        self.inplace = inplace

    def _make_noise(self, input):
    # generate random signal
        return input.new().resize_as_(input)

    def forward(self, input):
        if self.inplace:
            self.mark_dirty(input)
            output = input
        else:
            output = input.clone()

        if self.p > 0 and self.train:
            self.noise = self._make_noise(input)
            # multiply mask to input
            self.noise.bernoulli_(1 - self.p).div_(1 - self.p)
            if self.p == 1:
                self.noise.fill_(0)
            self.noise = self.noise.expand_as(input)
            output.mul_(self.noise)

        return output

    def backward(self, grad_output):
        if self.p > 0 and self.train:
            return grad_output.mul(self.noise)
        else:
            return grad_output
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。&lt;/p&gt;
&lt;h3 id=&quot;Dropout的洞察&quot;&gt;&lt;a href=&quot;#Dropout的洞察&quot; class=&quot;he
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://blog.ddlee.cn/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Visualizing and Understanding Recurrent Networks</title>
    <link href="http://blog.ddlee.cn/2017/05/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Visualizing-and-Understanding-Recurrent-Networks/"/>
    <id>http://blog.ddlee.cn/2017/05/13/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/</id>
    <published>2017-05-13T06:06:51.000Z</published>
    <updated>2017-06-03T04:35:09.919Z</updated>
    
    <content type="html"><![CDATA[<p>论文： <a href="http://arxiv.org/abs/1506.02078" target="_blank" rel="external">Visualizing and Understanding Recurrent Networks</a></p>
<h2 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h2><p>字母级的循环神经网络，用Torch实现，代码见<a href="http://github.com/karpathy/char-rnn" target="_blank" rel="external">GitHub</a>。字母嵌入成One-hot向量。优化方面，采用了RMSProp算法，加入了学习速率的decay和early stopping。</p>
<p>数据集采用了托尔斯泰的《战争与和平》和Linux核心的代码。</p>
<h2 id="可解释性激活的例子"><a href="#可解释性激活的例子" class="headerlink" title="可解释性激活的例子"></a>可解释性激活的例子</h2><p>$tanh$函数激活的例子，$-1$为红色，$+1$为蓝色。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane1.png" alt="pane1"></p>
<p>上图分别是记录了行位置、引文和if语句特征的例子和失败的例子。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane2.png" alt="pane2"></p>
<p>上图分别是记录代码中注释、代码嵌套深度和行末标记特征的例子。</p>
<h2 id="Gates数值的统计"><a href="#Gates数值的统计" class="headerlink" title="Gates数值的统计"></a>Gates数值的统计</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/gates.png" alt="gates"></p>
<p>此图信息量很大。</p>
<ol>
<li>left-saturated和right-saturated表示各个Gates激活函数（$sigmoid$）小于0.1和大于0.9，即总是阻止信息流过和总是允许信息流过。</li>
<li>横轴和纵轴表示该Gate处于这两种状态的时间比例，即有多少时间是阻塞状态，有多少时间是畅通状态。</li>
<li>三种颜色表示不同的层。</li>
</ol>
<p>有以下几个观察：</p>
<ol>
<li>第一层的门总是比较中庸，既不阻塞，也不畅通</li>
<li>第二三层的门在这两种状态间比较分散，经常处于畅通状态的门可能记录了长期的依赖信息，而经常处于阻塞状态的门则负责了短期信息的控制。</li>
</ol>
<h2 id="错误来源分析"><a href="#错误来源分析" class="headerlink" title="错误来源分析"></a>错误来源分析</h2><p>在这一节，作者用了“剥洋葱”的方法，建立了不同的模型将错误进行分解。此处错误指LSTM预测下一个字母产生的错误，数据集为托尔斯泰的《战争与和平》。</p>
<ol>
<li>n-gram</li>
<li>Dynamic n-long memory，即对已经出现过得单词的复现。如句子”Jon yelled at<br>Mary but Mary couldn’t hear him.”中的Mary。</li>
<li>Rare words，不常见单词</li>
<li>Word model，单词首字母、新行、空格之后出现的错误</li>
<li>Punctuation，标点之后</li>
<li>Boost，其他错误</li>
</ol>
<p>根据作者的实验，错误的来源有如下分解：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/error.png" alt="error"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇文章是打开LSTM黑箱的尝试，提供了序列维度上共享权值的合理性证据，对Gates状态的可视化也非常值得关注，最后对误差的分解可能对新的网络结构有所启发（比如，如何将单词级别和字母级别的LSTM嵌套起来，解决首字母预测的问题？）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文： &lt;a href=&quot;http://arxiv.org/abs/1506.02078&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Visualizing and Understanding Recurrent Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Deep Residual Learning for Image Recognition</title>
    <link href="http://blog.ddlee.cn/2017/04/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <id>http://blog.ddlee.cn/2017/04/30/论文笔记-Deep-Residual-Learning-for-Image-Recognition/</id>
    <published>2017-04-30T15:12:11.000Z</published>
    <updated>2017-05-30T13:37:55.644Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>网络在堆叠到越来越深之后，由于BP算法所依赖的链式法则的连乘形式，会出现梯度消失和梯度下降的问题。初始标准化和中间标准化参数在一定程度上缓解了这一问题，但仍然存在更深的网络比浅层网络具有更大的训练误差的问题。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/error.png" alt="error"></p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><p>多层的网络结构能够任意接近地拟合目标映射$H(x)$，那么也能任意接近地拟合其关于恒等映射的残差函数$H(x)-x$。记$F(x)=H(x)-x$，则原来的目标映射表为$F(x)+x$。由此，可以设计如下结构。</p>
<h3 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/block.jpg" alt="Residual Learning: a building block"></p>
<p>残差单元包含一条恒等映射的捷径，不会给原有的网络结构增添新的参数。</p>
<h2 id="动机-启发"><a href="#动机-启发" class="headerlink" title="动机/启发"></a>动机/启发</h2><p>层数的加深会导致更大的训练误差，但只增加恒等映射层则一定不会使训练误差增加，而若多层网络块要拟合的映射与恒等映射十分类似时，加入的捷径便可方便的发挥作用。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章中列举了大量在ImagNet和CIFAR-10上的分类表现，效果很好，在此不表。</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><h4 id="Deeper-Bottleneck-Architectures"><a href="#Deeper-Bottleneck-Architectures" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/Bottleneck.png" alt="Bottleneck"></p>
<p>两头的1 * 1巻积核先降维再升维，中间的3 * 3巻积核成为“瓶颈”，用于提取重要的特征。这样的结构跟恒等映射捷径配合，在ImageNet上有很好的分类效果。</p>
<h4 id="Standard-deviations-of-layer-responses"><a href="#Standard-deviations-of-layer-responses" class="headerlink" title="Standard deviations of layer responses"></a>Standard deviations of layer responses</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/std.png" alt="std"><br>上图是在CIFAR-10数据集上训练的网络各层的相应方差（Batch-Normalization之后，激活之前）。可以看到，残差网络相对普通网络有更小的方差。这一结果支持了残差函数比非残差函数更接近于0的想法（即更接近恒等映射）。此外，还显示出网络越深，越倾向于保留流过的信息。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>深度残差网络在当年的比赛中几乎是满贯。<br>下面是我的一些（未经实验证实的）理解：</p>
<p>首先，其”跳级”的网络结构对深度网络的设计是一种启发，通过“跳级”，可以把之前网络的信息相对完整的跟后层网络结合起来，即低层次解耦得到的特征和高层次解耦得到的特征再组合。<br>再者，这种分叉的结构可以看作网络结构层面的”Dropout”: 如果被跳过的网络块不能习得更有用的信息，就被恒等映射跳过了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;http://arxiv.org/abs/1512.03385&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Tensorflow White Paper</title>
    <link href="http://blog.ddlee.cn/2017/04/20/Tensorflow-White-Paper/"/>
    <id>http://blog.ddlee.cn/2017/04/20/Tensorflow-White-Paper/</id>
    <published>2017-04-20T13:32:29.000Z</published>
    <updated>2017-05-30T13:33:20.407Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://www.tensorflow.org/about/bib" target="_blank" rel="external">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</a></p>
<h2 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h2><h3 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h3><p>整张图如同管道结构，数据流就是其中的水流。<em>Control Dependency</em> 描述了管道的有向结构，而反向传播可以通过增加新的管道节点来实现。</p>
<h3 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h3><p>即计算操作的抽象，相当于映射、函数。</p>
<h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><p>执行计算的单元，CPU或GPU</p>
<h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>Client-Server结构，进行计算或者调整图结构则视为一次会话</p>
<h3 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h3><p>特殊的Operation，返回一个句柄，指向持久化的张量，这些张量在整张图的计算中不会被释放。</p>
<h3 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h3><p>对Kernel的封装，包含类型属性，实行注册机制维护可供使用的Device列表。</p>
<h2 id="多机实现"><a href="#多机实现" class="headerlink" title="多机实现"></a>多机实现</h2><p>要考虑两个问题：</p>
<ol>
<li>计算节点在Device间的分配问题</li>
<li>Devices之间的通信</li>
</ol>
<p>针对这两个问题，分别建立了两个抽象层。</p>
<h3 id="计算节点分配的C-S机制"><a href="#计算节点分配的C-S机制" class="headerlink" title="计算节点分配的C/S机制"></a>计算节点分配的C/S机制</h3><p><img src="http://static.ddlee.cn/static/img/Tensorflow-White-Paper/master.png" alt="master"><br>client提出计算请求，master负责切割计算图为子图，分配子图到Devices。分配时，会模拟执行子图，并采取贪心的策略分配。</p>
<h3 id="不同Device之间的发送和接收节点"><a href="#不同Device之间的发送和接收节点" class="headerlink" title="不同Device之间的发送和接收节点"></a>不同Device之间的发送和接收节点</h3><p><img src="http://static.ddlee.cn/static/img/Tensorflow-White-Paper/rev-send.png" alt="rec-send"></p>
<p>在每个Device上建立Receive和Send节点，负责与其他Device通信。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="数据化并行"><a href="#数据化并行" class="headerlink" title="数据化并行"></a>数据化并行</h3><p><img src="http://static.ddlee.cn/static/img/Tensorflow-White-Paper/Parallelize.png" alt="Parallelize"></p>
<p>上：单线程，同步数据并行<br>下：多线程，异步更新</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><p>文章中很多内容并没涉及到（看不懂）。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>TensorFlow是个庞大的计算框架，不仅仅定位于深度网络。其对计算图的抽象和数据、计算资源的分配的处理是值得关注的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://www.tensorflow.org/about/bib&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;TensorFlow: Large-Scale Machine Learning on Heterogeneou
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Tensorflow" scheme="http://blog.ddlee.cn/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>编程方法论(一):重构</title>
    <link href="http://blog.ddlee.cn/2017/04/08/%E7%BC%96%E7%A8%8B%E6%96%B9%E6%B3%95%E8%AE%BA-%E9%87%8D%E6%9E%84/"/>
    <id>http://blog.ddlee.cn/2017/04/08/编程方法论-重构/</id>
    <published>2017-04-07T18:21:19.000Z</published>
    <updated>2017-04-09T16:38:15.126Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容主要整理自lynda.com课程<a href="https://www.lynda.com/Developer-Programming-Foundations-tutorials/Foundations-Programming-Refactoring-Code/122457-2.html" target="_blank" rel="external">Programming Foudations: Refactoring Code</a>和<em>Martin Fowler</em>的<a href="https://martinfowler.com/books/refactoring.html" target="_blank" rel="external">重构</a>。全部例子来源于<a href="https://refactoring.com" target="_blank" rel="external">refactoring.com</a>。</p>
<p>内容大纲：<br><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring.png" alt="structure"></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-1.png" alt="intro-method"></p>
<h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>重构是在不影响软件功能的情况下，重新组织代码，使之更清晰、更容易理解的过程。</p>
<ul>
<li>前提：功能不变</li>
<li>行为：改写代码</li>
<li>目的：提高可理解性</li>
</ul>
<p>大白话讲，重构就是改写，造福以后需要理解这段代码的人们。</p>
<h5 id="重构不是什么"><a href="#重构不是什么" class="headerlink" title="重构不是什么"></a>重构不是什么</h5><p>给一件事物下定义，有时候从反方面更好讲些。比如你难给正义下一个定义，但很容易举出什么是非正义的例子。</p>
<ul>
<li>重构不是Debug，代码已经运行良好</li>
<li>重构不是优化</li>
<li>重构不是添加新功能</li>
</ul>
<p>也就是说，重构对使用代码的人没有任何好处，对使用者来讲，代码是黑箱。重构是准备给要打开黑箱的人，而那个人常常是你自己。</p>
<h5 id="玄学：Code-Smells"><a href="#玄学：Code-Smells" class="headerlink" title="玄学：Code Smells"></a>玄学：Code Smells</h5><p>玄学二字是我自己加的。Martin Fowler当然没有这样说。我只是表达一下对无法精确描述的定义的敬意。</p>
<p>我的理解是Code Smells是best practice和code style的总和，直接和根本来源是自己的代码经验。所谓语感、文笔、血淋淋的人生道理。</p>
<h5 id="准备工作：自动化测试"><a href="#准备工作：自动化测试" class="headerlink" title="准备工作：自动化测试"></a>准备工作：自动化测试</h5><p>重构当然不是breaking the code，写好测试，保证代码仍能正常运行。</p>
<h3 id="重构范例：方法层面"><a href="#重构范例：方法层面" class="headerlink" title="重构范例：方法层面"></a>重构范例：方法层面</h3><p>首先是一句良言：哪里加了注视，哪里可能就需要重构。</p>
<p>这一点的潜在信念是，好的代码是self-explained的，通过合理的命名、清晰的组织，代码应该像皇帝的新装那样一目了然。</p>
<h4 id="可以用于重构的工具"><a href="#可以用于重构的工具" class="headerlink" title="可以用于重构的工具"></a>可以用于重构的工具</h4><p>常见的IDE会有重构的功能，如重命名变量。另外，一个严厉的Linter加上像我这样的强迫癌患者会将风格问题扼杀在摇篮之中。</p>
<h4 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h4><p>举例均以Code smell和重构建议两部分构成，较抽象(wo kan bu dong)的给出代码。</p>
<h5 id="Extract-Method"><a href="#Extract-Method" class="headerlink" title="Extract Method"></a><em>Extract Method</em></h5><p>Code smell: 太长的方法，带注释的代码块</p>
<p>重构： 提取，新建，用评论命名</p>
<h5 id="Remove-temps"><a href="#Remove-temps" class="headerlink" title="Remove temps"></a>Remove temps</h5><p>Code smell: 冗余的临时变量（本地）</p>
<p>重构：</p>
<ul>
<li><em>Replace with Query</em>: 把表达式提取为方法（规模较大）</li>
<li><em>Inline temps</em>: 直接用表达式代替这个变量（规模较小）</li>
</ul>
<h5 id="Add-temps"><a href="#Add-temps" class="headerlink" title="Add temps"></a>Add temps</h5><p>Code smell: 同样的变量有多重含义</p>
<p>重构：</p>
<ul>
<li><em>Split temporary variable</em>: 同一个临时变量在上下文赋予了不同含义（复用），拆</li>
<li><em>Remove assignment to parameters</em>: 对参数默认值的设定，在函数内新建变量，初始化这个新变量</li>
</ul>
<p>Remove assignment to parameters的例子：</p>
<pre class="line-numbers language-lang-java"><code class="language-lang-java">//Before
int discount (int inputVal, int quantity, int yearToDate) {
  if (inputVal > 50) inputVal -= 2;

//After Refactoring
int discount (int inputVal, int quantity, int yearToDate) {
  int result = inputVal;
  if (inputVal > 50) result -= 2;
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这一点基于的信念是，参数只能代表被传进来的变量，不应该在本地再赋予别的含义。</p>
<h3 id="重构范例：类与方法"><a href="#重构范例：类与方法" class="headerlink" title="重构范例：类与方法"></a>重构范例：类与方法</h3><p><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-2.png" alt="class and method"></p>
<h4 id="Move-Method"><a href="#Move-Method" class="headerlink" title="Move Method"></a><em>Move Method</em></h4><p>Code smell: feature envy（依恋情结）</p>
<p>用中文来说，是指某个方法操作/使用（依恋）某一个类多过自己所处的类，我们用“出轨”这个词来表示这种现象。但这是违反婚姻法的，因而，我们的重构手段就是，把这个方法移动到它依恋的类中，圆满一段木石良缘。*</p>
<p>重构： 圆满木石良缘。</p>
<h4 id="Extract-Class"><a href="#Extract-Class" class="headerlink" title="Extract Class"></a><em>Extract Class</em></h4><p>Code smell: 规模太大的类</p>
<p>重构： 把部分移出，自立门户</p>
<h4 id="Inline-Class"><a href="#Inline-Class" class="headerlink" title="Inline Class"></a><em>Inline Class</em></h4><p>Code smell: 冗余的类</p>
<p>重构： 像我这中请天假组内运转几乎不受影响的人，应该清除掉（这是瞎话）</p>
<h4 id="Condition-Focused（条件语句相关）"><a href="#Condition-Focused（条件语句相关）" class="headerlink" title="Condition Focused（条件语句相关）"></a>Condition Focused（条件语句相关）</h4><p>Code smell: 写完判断条件自己都看不懂/看着难受</p>
<p>重构：</p>
<ul>
<li><em>Decompose conditional</em>: 分解</li>
<li><em>Consolidate conditional expression</em>: 多项条件指向同一段后续操作，提取这些条件为方法</li>
<li><em>Consolidate duplicate conditional fragments</em>: 不同条件的后续操作中含有共同的部分，将共有部分提取出来（不管哪个条件总要执行）</li>
<li><em>Replace condition with polymorphism</em>: 针对有判断分支的方法，替换成多态方法</li>
<li><em>Replace type code with subclass*</em>: 针对有判断分支的类，替换为子类</li>
</ul>
<p>Consolidate conditional expression的例子：</p>
<pre class="line-numbers language-lang-java"><code class="language-lang-java">//Before
double disabilityAmount() {
  if (_seniority < 2) return 0;
  if (_monthsDisabled > 12) return 0;
  if (_isPartTime) return 0;
  // compute the disability amount
//After Refactoring
double disabilityAmount() {
  if (isNotEligableForDisability()) return 0;
  // compute the disability amount
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="重构范例：数据相关"><a href="#重构范例：数据相关" class="headerlink" title="重构范例：数据相关"></a>重构范例：数据相关</h3><p><img src="http://static.ddlee.cn/static/img/编程方法论-重构/Refactoring-3.png" alt="class and method"></p>
<h4 id="Move-field"><a href="#Move-field" class="headerlink" title="Move field"></a><em>Move field</em></h4><p>code smell: inverse feature envy（自造）</p>
<p>某一个类使用某一数据比该数据所属的类还多。</p>
<p>重构：送给你了还不行吗！？</p>
<h4 id="Data-Clumps（数据团）"><a href="#Data-Clumps（数据团）" class="headerlink" title="Data Clumps（数据团）"></a>Data Clumps（数据团）</h4><p>code smell: 某些数据总是抱团出现</p>
<p>重构：</p>
<ul>
<li><em>Preserve whole object</em>: 在一个方法中反复提取某个类的一些属性，将整个对象传入</li>
<li><em>Introducing parameter object</em>: 把这些参数合并为一个类，把新建的类传入</li>
</ul>
<h4 id="Similifying"><a href="#Similifying" class="headerlink" title="Similifying"></a>Similifying</h4><p>重构：</p>
<ul>
<li><em>Renaming</em>: 顾名思义</li>
<li><em>Add or remove parameters</em>: 顾名思义</li>
<li><em>Replace parameter with explicit Method</em>: 根据不同参数值新建专属的方法</li>
<li><em>Parameterize Method</em>: 与上者相反，把不同方法合并，传入参数</li>
<li><em>Separate queries form modifiers</em>: 将找到数据和更该数据两个操作拆成两个方法</li>
</ul>
<p>Replace parameter with explicit Method的例子：</p>
<pre class="line-numbers language-lang-java"><code class="language-lang-java">//Before
void setValue (String name, int value) {
  if (name.equals("height")) {
    _height = value;
    return;
  }
  if (name.equals("width")) {
    _width = value;
    return;
  }
  Assert.shouldNeverReachHere();
}
//After Refactoring
void setHeight(int arg) {
  _height = arg;
}
void setWidth (int arg) {
  _width = arg;
}
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Pulling-and-pushing-升级与降级"><a href="#Pulling-and-pushing-升级与降级" class="headerlink" title="Pulling and pushing(升级与降级)"></a>Pulling and pushing(升级与降级)</h4><ul>
<li><em>Pull up method</em> and <em>pull up field</em></li>
<li><em>Push down method</em> and <em>push down field</em></li>
</ul>
<p>解决方法、数据归属不合理的问题。</p>
<h3 id="高阶重构（大坑，大坑）"><a href="#高阶重构（大坑，大坑）" class="headerlink" title="高阶重构（大坑，大坑）"></a>高阶重构（大坑，大坑）</h3><h4 id="Convert-procedural-design-to-objects"><a href="#Convert-procedural-design-to-objects" class="headerlink" title="Convert procedural design to objects"></a><em>Convert procedural design to objects</em></h4><p>化函数式变成为面向对象，祝好运。</p>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>写代码和改代码是一个不断被自己坑和被别人坑的旅程。且行且珍惜。</p>
<p>Cheers, have a good one.</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文内容主要整理自lynda.com课程&lt;a href=&quot;https://www.lynda.com/Developer-Programming-Foundations-tutorials/Foundations-Programming-Refactoring-Code/1
    
    </summary>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/categories/Programming/"/>
    
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Refactoring" scheme="http://blog.ddlee.cn/tags/Refactoring/"/>
    
  </entry>
  
  <entry>
    <title>ddlee约书计划（第二弹）</title>
    <link href="http://blog.ddlee.cn/2017/04/05/ddlee%E7%BA%A6%E4%B9%A6%E8%AE%A1%E5%88%92%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%BC%B9%EF%BC%89/"/>
    <id>http://blog.ddlee.cn/2017/04/05/ddlee约书计划（第二弹）/</id>
    <published>2017-04-04T16:45:28.000Z</published>
    <updated>2017-04-07T18:26:39.880Z</updated>
    
    <content type="html"><![CDATA[<h4 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h4><p>读书这种事情，每几个月都会有那么几天。</p>
<p>浑身难受。不干点什么，眼睛闲得团团转，双手也不知道往哪搁。</p>
<p>大概是愧疚吧。要立个FLAG把这压抑着的自卑和热情释放一下。</p>
<p>花大概十分钟的时间，下载/买下十个月都读不完的书。</p>
<h4 id="书单"><a href="#书单" class="headerlink" title="书单"></a>书单</h4><p>选书原则：</p>
<ol>
<li>我感兴趣</li>
<li>拒绝大部头</li>
<li>均为论述类，有的聊</li>
<li>我能提供电子版</li>
</ol>
<p>书单：</p>
<ul>
<li><a href="https://book.douban.com/subject/26943161/" target="_blank" rel="external">《未来简史》尤瓦尔·赫拉利 </a></li>
<li><a href="https://book.douban.com/subject/26279954/" target="_blank" rel="external">《知识的边界》戴维·温伯格 </a></li>
<li><a href="https://book.douban.com/subject/1089508/" target="_blank" rel="external">《数字化生存》尼葛洛庞帝 </a></li>
<li><a href="https://book.douban.com/subject/25846075/" target="_blank" rel="external">《技术的本质》布莱恩•阿瑟</a></li>
<li><a href="https://book.douban.com/subject/6965746/" target="_blank" rel="external">《科技想要什么》凯文·凯利 </a></li>
<li><a href="https://book.douban.com/subject/1813841/" target="_blank" rel="external">《枪炮、病菌与钢铁》贾雷德·戴蒙德 </a></li>
<li><a href="https://book.douban.com/subject/4850629/" target="_blank" rel="external">《言论的边界》安东尼·刘易斯 </a></li>
<li><a href="https://book.douban.com/subject/1087547/" target="_blank" rel="external">《理解媒介》马歇尔·麦克卢汉 </a></li>
<li><a href="https://book.douban.com/subject/1292405/" target="_blank" rel="external">《自私的基因》里查德.道金斯 </a></li>
<li><a href="https://book.douban.com/subject/1003692/" target="_blank" rel="external">《真实世界的脉络》戴维·多伊奇 </a></li>
</ul>
<p>豆列<a href="https://www.douban.com/doulist/45917752/?start=0&amp;sort=" target="_blank" rel="external">在此</a></p>
<p>剩下一些，我读过，但意犹未尽，也可以聊。</p>
<ul>
<li>《中国近代史》徐中约</li>
<li>《人类简史》尤瓦尔·赫拉利</li>
<li>《娱乐至死》尼尔·波兹曼</li>
<li>《浅薄：互联网如何毒害了我们的大脑》尼古拉斯·卡尔</li>
</ul>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>有条件的，我们用Google Docs共享想法。没条件的，用Evernote。</p>
<p>如果你不习惯写下来，我们可以线下聊（聊到风花雪月人生哲学就不保证了，所以最好还是写下来）。</p>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><ol>
<li>精力有限，同时运行三个线程，多了就溢出了。</li>
<li>其他书也可以推荐，如果长得足够好看的话，我会同意的。</li>
<li>谁也是诸事缠身，有事情不能坚持的，随时退出，我太能理解了；能坚持读完的，我陪你到最后。</li>
<li>这种计划似乎跟熟人约过一次，没成，向我骚扰过的人抱歉。</li>
<li>如果你感兴趣，私信/微信/邮箱联系我。</li>
</ol>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;缘起&quot;&gt;&lt;a href=&quot;#缘起&quot; class=&quot;headerlink&quot; title=&quot;缘起&quot;&gt;&lt;/a&gt;缘起&lt;/h4&gt;&lt;p&gt;读书这种事情，每几个月都会有那么几天。&lt;/p&gt;
&lt;p&gt;浑身难受。不干点什么，眼睛闲得团团转，双手也不知道往哪搁。&lt;/p&gt;
&lt;p&gt;大概是愧疚吧
    
    </summary>
    
      <category term="Reading" scheme="http://blog.ddlee.cn/categories/Reading/"/>
    
    
      <category term="Reading" scheme="http://blog.ddlee.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>Coroutine,Generator,Async与Await</title>
    <link href="http://blog.ddlee.cn/2017/04/03/Coroutine-Generator-Async%E4%B8%8EAwait/"/>
    <id>http://blog.ddlee.cn/2017/04/03/Coroutine-Generator-Async与Await/</id>
    <published>2017-04-03T14:57:54.000Z</published>
    <updated>2017-04-07T18:27:24.796Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p>Generator能保存自己的状态，进入一种“Paused”状态，再次调用时会继续执行。</p>
<p>Generator的好处之一是节省了存储空间开销，带一些”流处理”的思想。</p>
<p>其实，我们也可以对Generator进行传入数据的操作：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">def coro():
    hello = yield "Hello"
    yield hello

c = coro()
print(next(c))
print(c.send("World"))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Coroutine"><a href="#Coroutine" class="headerlink" title="Coroutine"></a>Coroutine</h4><p>coroutine可以认为是generator思想的泛化：</p>
<ul>
<li>generator一个一个地吐出数据（返回值）</li>
<li>coroutine一个一个地吃掉数据（传入参数）并返回结果，即可控地执行函数</li>
</ul>
<p>关键点在于，generator与coroutine都能保存自己的状态，而这种特点正可以用于任务切换。yield可以看做是操作系统在进行进程管理时的traps:</p>
<p><img src="http://static.ddlee.cn/static/img/coroutine/os.png" alt="traps"></p>
<p>实际上，coroutine可以看做”用户自定义”的进程，状态、启用和暂停都可控，David Beazley就利用这一点用coroutine实现了Python上的操作系统（参见Reference)。</p>
<h4 id="Conroutine与Concurrent-Programming"><a href="#Conroutine与Concurrent-Programming" class="headerlink" title="Conroutine与Concurrent Programming"></a>Conroutine与Concurrent Programming</h4><p>Concurrent Programming中有Task的概念，有如下特点：</p>
<ul>
<li>独立的控制流</li>
<li>内部状态变量</li>
<li>支持计划任务（暂停、恢复执行）</li>
<li>与其他Task通信</li>
</ul>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">@coroutine
def grep(pattern):  #正则匹配
    print "Looking for %s" % pattern    
    while True:
        line = (yield)
        if pattern in line:
            print line,
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>conroutine有自己的控制流（while/if），有局部变量（pattern, line），能暂停和恢复（yield()/send()），能相互通信（send()）</p>
<p>====》coroutine就是一种Task！</p>
<p>Python Docs中提供了一个例子：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">import asyncio

async def compute(x, y):
    print("Compute %s + %s ..." % (x, y))
    await asyncio.sleep(1.0)
    return x + y

async def print_sum(x, y):
    result = await compute(x, y)
    print("%s + %s = %s" % (x, y, result))

loop = asyncio.get_event_loop()
loop.run_until_complete(print_sum(1, 2))
loop.close()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>执行方式如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/coroutine/tulip_coro.png" alt="Chaining coroutines"></p>
<p>利用coroutine，可以在一个线程(Task)上实现异步。</p>
<h4 id="Impletation"><a href="#Impletation" class="headerlink" title="Impletation"></a>Impletation</h4><p>coroutine有两种实现方式，基于generator和原生async, awati关键字。</p>
<h5 id="generator-based-coroutine"><a href="#generator-based-coroutine" class="headerlink" title="generator based coroutine"></a>generator based coroutine</h5><pre class="line-numbers language-lang-python"><code class="language-lang-python">import asyncio
import datetime
import random

@asyncio.coroutine
def display_date(num, loop):
    end_time = loop.time() + 50.0
    while True:
        print("Loop: {} Time: {}".format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) >= end_time:
            break
        yield from asyncio.sleep(random.randint(0, 5))

loop = asyncio.get_event_loop()

asyncio.ensure_future(display_date(1, loop))
asyncio.ensure_future(display_date(2, loop))

loop.run_forever()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上面的程序实现了在同一个线程里交互执行两个函数（sleep），而又能保持各自的状态</p>
<h5 id="Native-support-python-3-5"><a href="#Native-support-python-3-5" class="headerlink" title="Native support(python 3.5+)"></a>Native support(python 3.5+)</h5><p>只需要修改函数定义头和<code>yield from</code>为关键字<code>await</code>即可。</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">async def display_date(num, loop, ):
    end_time = loop.time() + 50.0
    while True:
        print("Loop: {} Time: {}".format(num, datetime.datetime.now()))
        if (loop.time() + 1.0) >= end_time:
            break
        await asyncio.sleep(random.randint(0, 5))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><p>Coroutine常翻译成“协程”。</p>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h4><ul>
<li><a href="http://www.dabeaz.com/coroutines/Coroutines.pdf" target="_blank" rel="external">David Beazley @ PyCon2009 Slides</a></li>
<li><a href="http://masnun.com/2015/11/13/python-generators-coroutines-native-coroutines-and-async-await.html" target="_blank" rel="external">PYTHON: GENERATORS, COROUTINES, NATIVE COROUTINES AND ASYNC/AWAIT</a></li>
<li><a href="https://docs.python.org/3/library/asyncio-task.html" target="_blank" rel="external">Python 3.6 Docs: Taks and coroutines</a></li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Generator&quot;&gt;&lt;a href=&quot;#Generator&quot; class=&quot;headerlink&quot; title=&quot;Generator&quot;&gt;&lt;/a&gt;Generator&lt;/h4&gt;&lt;p&gt;Generator能保存自己的状态，进入一种“Paused”状态，再次调用时会继续执
    
    </summary>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/categories/Programming/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="异步" scheme="http://blog.ddlee.cn/tags/%E5%BC%82%E6%AD%A5/"/>
    
  </entry>
  
  <entry>
    <title>500lines项目Crawler源码阅读笔记</title>
    <link href="http://blog.ddlee.cn/2017/04/03/500lines%E9%A1%B9%E7%9B%AECrawler%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://blog.ddlee.cn/2017/04/03/500lines项目Crawler阅读笔记/</id>
    <published>2017-04-03T14:57:20.000Z</published>
    <updated>2017-04-07T18:27:34.968Z</updated>
    
    <content type="html"><![CDATA[<p>源码来自GitHub上著名的Repo: <a href="https://github.com/aosabook/500lines" target="_blank" rel="external">500lines or less</a>。</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p><img src="http://static.ddlee.cn/static/img/500lines_crawler/500lines_crawler.png" alt="structure"></p>
<p>代码结构由crawling, crawl, reporting三大部分组成。</p>
<ul>
<li>crawl: 驱动，解析传入的参数，管理loop，调用crawler，生成report</li>
<li>Crawling: 实现crawler类及一系列辅助函数</li>
<li>reporting： 生成记录</li>
</ul>
<h3 id="Crawler类"><a href="#Crawler类" class="headerlink" title="Crawler类"></a>Crawler类</h3><p>Crawler类实现了解析网址，抓取内容等基本功能，利用<code>asyncio</code>库构建<code>coroutine</code>（parse_lings(), fetch(), work()）。</p>
<p><img src="http://static.ddlee.cn/static/img/500lines_crawler/500lines_crawler_class.png" alt="Class Crawler"></p>
<p>核心之处是组织管理异步的抓取任务，代码块结构如下：</p>
<pre class="line-numbers language-lang-python"><code class="language-lang-python">class Crawler:
  def __init__(self, roots, ....., loop):
    self.q = Queue(loop=self.loop) # 建立队列

  @asyncio.coroutine
  def parse_links(self, response):
    '''从返回内容中解析出要抓取的链接'''
    body = yield from response.read()
    if response.status == 200:
      if content_type:
        text = yield from response.text()
        urls = set(re.findall())
        for url in urls:
          if self.url_allowed():
            links.add()
  @asynico.coroutine
  def fetch(self, url):
    '''访问链接，抓取返回结果'''
    while tries:
      try:
        response = yield from self.session.get(url)
    try:
      if is_redirect():
        pass
      else:
        links = yield from self.parse_links(response)
    finally:
      yield from response.releas()
  @asyncio.coroutine
  def work(self):
    '''封装抓取过程，与队列交互'''
    try:
      while True:
        url = yield from self.q.get()
        assert url in self.seen_urls
        yield from self.fetch(url)
        self.q.task_done()
  @asyncio.coroutine
  def crawl(self):
    '''建立Tasks，启动Task'''
    workers = [asyncio.Task(self.work(), loop=self.loop)
                for _ in range(self.max_tasks)]
    yield from self.q.join()
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>下图是我对上述代码结构的理解：</p>
<p><img src="http://static.ddlee.cn/static/img/500lines_crawler/coroutine.JPG" alt="coroutines"></p>
<p>对coroutine的进一步介绍，参见<a href="/2017/04/03/Coroutine-Generator-Async与Await/">Coroutine-Generator-Async与Await</a>。</p>
<h3 id="A-Web-Crawler-With-asyncio-Coroutines导读"><a href="#A-Web-Crawler-With-asyncio-Coroutines导读" class="headerlink" title="A Web Crawler With asyncio Coroutines导读"></a><a href="http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html" target="_blank" rel="external">A Web Crawler With asyncio Coroutines</a>导读</h3><p>文章整体结构：</p>
<ul>
<li>分析爬虫任务</li>
<li><ul>
<li>传统方式：抢锁</li>
</ul>
</li>
<li><ul>
<li>异步方式的特点：无锁；单线程上同时运行多操作</li>
</ul>
</li>
<li>回调函数：fetch(),connecte(),read_response()的实现</li>
<li>Coroutine</li>
<li><ul>
<li>Generator的工作原理</li>
</ul>
</li>
<li><ul>
<li>用Generator实现Coroutine</li>
</ul>
</li>
<li>Asyncio库中的Coroutine</li>
<li><ul>
<li>crawl()</li>
</ul>
</li>
<li><ul>
<li>work()</li>
</ul>
</li>
<li><ul>
<li>fetch(), handle redirections</li>
</ul>
</li>
<li><ul>
<li>Queue()</li>
</ul>
</li>
<li><ul>
<li>EventLoop()</li>
</ul>
</li>
<li><ul>
<li>Task()</li>
</ul>
</li>
<li>Conclusion</li>
</ul>
<p>文章最后，作者点明了主题思想：</p>
<blockquote>
<p>Increasingly often, modern programs are I/O-bound instead of CPU-bound. For such programs, Python threads are the worst of both worlds: the global interpreter lock prevents them from actually executing computations in parallel, and preemptive switching makes them prone to races. Async is often the right pattern. But as callback-based async code grows, it tends to become a dishevelled mess. Coroutines are a tidy alternative. They factor naturally into subroutines, with sane exception handling and stack traces.</p>
</blockquote>
<p>大意是说，对I/O密集型的程序，Python多线程在两方面令人失望：全局锁的设定使之不能真正并行；抢占式多任务处理机制又让多个线程间形成竞争关系。异步通常是正确的选择。但持续增长的回调函数会使代码丧失可读性，Coroutine便是一种保持整洁性的替代方案。</p>
<h4 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h4><p>这样说，Python的多线程效率带来的提高只是Python程序抢占了系统中非Python进程的资源（参考召集一波狐朋狗友帮你抢选修课），多个线程提高了Python作为一个整体在系统资源调配中的竞争力。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;源码来自GitHub上著名的Repo: &lt;a href=&quot;https://github.com/aosabook/500lines&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;500lines or less&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;整体结构&quot;
    
    </summary>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/categories/Programming/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="爬虫" scheme="http://blog.ddlee.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Feedly+Reeder3+FeedMe:信息获取与处理</title>
    <link href="http://blog.ddlee.cn/2017/04/03/Feedly-Reeder3-FeedMe-%E4%BF%A1%E6%81%AF%E8%8E%B7%E5%8F%96%E4%B8%8E%E5%A4%84%E7%90%86/"/>
    <id>http://blog.ddlee.cn/2017/04/03/Feedly-Reeder3-FeedMe-信息获取与处理/</id>
    <published>2017-04-03T03:13:37.000Z</published>
    <updated>2017-04-05T13:13:10.337Z</updated>
    
    <content type="html"><![CDATA[<p>我蛮早就意识到自己被信息淹没了。于是关了票圈，屏蔽了空间，不装任何新闻APP，失去效力的微信群一概不留，知乎上的关注也缩减了很多很多。</p>
<blockquote>
<p>世界终于清静了。</p>
</blockquote>
<p>但仍然需要有关注的动向。我又捡起了RSS这个老朋友，建立起的信息获取跟处理流如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/Feedly_Reeder/Feedly+Reeder.png" alt="Feedly+Reeder3"></p>
<h3 id="服务与APP"><a href="#服务与APP" class="headerlink" title="服务与APP"></a>服务与APP</h3><p>主要涉及的服务：Feedly（免费，有高级版）</p>
<p>IOS APP：</p>
<ul>
<li>Reeder3（￥30）</li>
<li>Pocket（免费）</li>
<li>Pushbullet（免费）</li>
<li>Evernote（免费版限制客户端个数）</li>
</ul>
<p>Android APP:</p>
<ul>
<li>FeedMe（免费）</li>
<li>Pocket</li>
<li>Inbox</li>
<li>Evernote</li>
<li>Google Keep</li>
</ul>
<h3 id="获取：Feedly整合"><a href="#获取：Feedly整合" class="headerlink" title="获取：Feedly整合"></a>获取：Feedly整合</h3><p><a href="https://feedly.com/i/welcome" target="_blank" rel="external">Feedly</a>是著名的信息聚合服务，能从媒体RSS、博客、YouTube Chanel等拉取文章/动态，还提供Google关键词动态提醒服务。</p>
<p>这里先推荐两个Chrome插件，可以更方便地将网页端想要订阅的信息整合到Feedly中。</p>
<ul>
<li><a href="https://chrome.google.com/webstore/detail/save-to-feedly-board/hdhblphcdjcicefneapkhmleapfaocih?hl=en-US" target="_blank" rel="external">Follow Feed</a>: 识别跟当前网页内容相关的信息源，添加到Feedly订阅中。</li>
<li><a href="https://chrome.google.com/webstore/detail/save-to-feedly-board/hdhblphcdjcicefneapkhmleapfaocih?hl=en-US" target="_blank" rel="external">Save to Feedly Board</a>: 将当前网页添加到Feedly Board中，可以标记后分享给团队，实时更新。</li>
</ul>
<h4 id="信息源"><a href="#信息源" class="headerlink" title="信息源"></a>信息源</h4><h5 id="微信公众号"><a href="#微信公众号" class="headerlink" title="微信公众号"></a>微信公众号</h5><p>先直接在Feedly中搜索公众号，若找不到订阅源，则可通过<a href="http://www.iwgc.cn/" target="_blank" rel="external">微广场</a>等服务转成RSS。</p>
<h5 id="媒体-博客RSS源"><a href="#媒体-博客RSS源" class="headerlink" title="媒体/博客RSS源"></a>媒体/博客RSS源</h5><p>很多在线媒体会在主页提供RSS地址，也可直接在Feedly中搜索媒体名。</p>
<h5 id="知乎专栏"><a href="#知乎专栏" class="headerlink" title="知乎专栏"></a>知乎专栏</h5><p>有些<a href="https://rss.lilydjwg.me/" target="_blank" rel="external">工具</a>可以将知乎专栏转成RSS订阅源。</p>
<h5 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h5><p>Feedly支持设置关键词动态提醒。</p>
<h3 id="处理：Reeder3-FeedMe"><a href="#处理：Reeder3-FeedMe" class="headerlink" title="处理：Reeder3 + FeedMe"></a>处理：Reeder3 + FeedMe</h3><p>支持Feedly的APP实在太多，<a href="https://feedly.com/apps.html" target="_blank" rel="external">这里</a>是官方给出的列表，可看脸挑选。</p>
<p>我平常同时用Android和IOS处理订阅的信息，大屏精读，小屏浏览。</p>
<h4 id="IOS-Reeder3"><a href="#IOS-Reeder3" class="headerlink" title="IOS: Reeder3"></a>IOS: Reeder3</h4><p>不幸的是Reeder3是要付费的， 30块，几乎没有降过价。</p>
<p>替代品可以考虑自家的Feedly，Ziner等，可以参考<a href="http://www.makeuseof.com/tag/5-best-ipad-rss-readers/" target="_blank" rel="external">这篇文章</a>对比的结果选择。</p>
<h4 id="Android-FeedMe"><a href="#Android-FeedMe" class="headerlink" title="Android: FeedMe"></a>Android: FeedMe</h4><p>这里强推FeedMe(<a href="https://play.google.com/store/apps/details?id=com.seazon.feedme&amp;hl=en" target="_blank" rel="external">Google Play</a>)，抓取、缓存迅速，界面简洁，还有“中国大陆”模式。</p>
<h3 id="消化：Pocket-Inbox-Evernote-Keep"><a href="#消化：Pocket-Inbox-Evernote-Keep" class="headerlink" title="消化：Pocket, Inbox, Evernote/Keep"></a>消化：Pocket, Inbox, Evernote/Keep</h3><p>我个人将信息处理的结果分为三类：</p>
<ul>
<li>Read Later: 没消化</li>
<li>Links to save: 还想接着吃</li>
<li>Favorite:　想学着做</li>
</ul>
<p>稍后再读用Pocket，接口丰富，功能专一（尽管也有了“发现”模块）。</p>
<p>文章中挂的一些外链，移动端不好处理，要发往PC，手机端存在Inbox中，当临时的标签栏，iPad端用Pushbullet发给Chrome，下次打开Chrome时浏览处理。</p>
<p>收藏的文章存到Evernote，打好tags，长篇干货/可反复参考的转到OneNote。</p>
<h3 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h3><p>其他情境下遇到的好文章、信息等尽量文字存到Google Keep，链接存到Inbox，或者给自己写封邮件。</p>
<p>微信的Favorite尽量不用，收藏的目的就在于情景分离，在不同的上下文中，我门信息获取的效率和质量区别实在太大了。详情参考拉微信群异地参加美赛的战友们。</p>
<p>最后推荐几个不错的订阅源(右击复制链接)：</p>
<ul>
<li><a href="http://www.sbnation.com/authors/mike-prada/rss" target="_blank" rel="external">SBNation上Mike Prada的文章</a>: 对NBA比赛、球队战术的分析</li>
<li><a href="http://rsarxiv.github.io/atom.xml" target="_blank" rel="external">Paper Weekly</a>: 机器学习方面的论文解读</li>
<li><a href="http://cos.name/feed/" target="_blank" rel="external">统计之都</a>: 统计学及应用、R语言方面的优秀内容</li>
<li><a href="http://github-trends.ryotarai.info/rss/github_trends_all_weekly.rss" target="_blank" rel="external">GitHub Trends</a></li>
<li><a href="https://blog.ddlee.cn/atom.xml">blog.ddlee.cn</a>: 大言不惭 -_-！</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我蛮早就意识到自己被信息淹没了。于是关了票圈，屏蔽了空间，不装任何新闻APP，失去效力的微信群一概不留，知乎上的关注也缩减了很多很多。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;世界终于清静了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但仍然需要有关注的动向。我又捡起了RS
    
    </summary>
    
      <category term="Individual Management" scheme="http://blog.ddlee.cn/categories/Individual-Management/"/>
    
    
      <category term="Individual Management" scheme="http://blog.ddlee.cn/tags/Individual-Management/"/>
    
      <category term="RSS" scheme="http://blog.ddlee.cn/tags/RSS/"/>
    
  </entry>
  
</feed>
