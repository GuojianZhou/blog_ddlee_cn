<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>萧爽楼</title>
  <subtitle>李家丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.ddlee.cn/"/>
  <updated>2018-01-17T17:03:29.326Z</updated>
  <id>http://blog.ddlee.cn/</id>
  
  <author>
    <name>ddlee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[论文笔记]Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</title>
    <link href="http://blog.ddlee.cn/2018/01/18/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/"/>
    <id>http://blog.ddlee.cn/2018/01/18/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/</id>
    <published>2018-01-17T16:52:08.000Z</published>
    <updated>2018-01-17T17:03:29.326Z</updated>
    
    <content type="html"><![CDATA[<p>本文是MobileNets的第二版。<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">第一版</a>中，MobileNets全面应用了Depth-wise Seperable Convolution并提出两个超参来控制网络容量，在保持移动端可接受的模型复杂性的基础上达到了相当的精度。而第二版中，MobileNets应用了新的单元：Inverted residual with linear bottleneck，主要的改动是添加了线性Bottleneck和将skip-connection转移到低维bottleneck层。</p>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>本篇比较丰富的地方是对网络中bottleneck结构的探讨。</p>
<p>在最早的Network in Network工作中，1x1卷积被作为一个降维的操作而引入，后来逐渐发展为Depth-wise Seperable Convolution（可分离卷积）并被广泛应用，堪称跟skip-connection同样具有影响力的网络部件。在<a href="https://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/">Inception单元</a>最初提出之时，具有较多channel的feature map被认为是可供压缩的，作者引入1x1卷积将它们映射到低维（较少channel数）空间上并添加多路径处理的范式。之后的<a href="https://blog.ddlee.cn/2018/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/">Xception</a>、MobileNets等工作则将可分离卷积应用到极致：前者指出可分离卷积背后的假设是跨channel相关性和跨spatial相关性的解耦，后者则利用可控的两个超参来获得在效率和精度上取得较好平衡的网络。</p>
<p>文中，经过激活层后的张量被称为兴趣流形，具有维HxWxD，其中D即为通常意义的channel数，部分文章也将其称为网络的宽度（width）。</p>
<p>根据之前的研究，兴趣流形可能仅分布在激活空间的一个低维子空间里，利用这一点很容易使用1x1卷积将张量降维（即MobileNet V1的工作），但由于ReLU的存在，这种降维实际上会损失较多的信息。下图是一个例子。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/collapse.png" alt="envolve"></p>
<p>上图中，利用MxN的矩阵B将张量（2D，即N=2）变换到M维的空间中，通过ReLUctant后（y=ReLU(Bx)），再用此矩阵之逆恢复原来的张量。可以看到，当M较小时，恢复后的张量坍缩严重，M较大时则恢复较好。</p>
<p>这意味着，在较低维度的张量表示（兴趣流形）上进行ReLU等线性变换会有很大的信息损耗。因而本文提出使用线性变换替代Bottleneck的激活层，而在需要激活的卷积层中，使用较大的M使张量在进行激活前先扩张，整个单元的输入输出是低维张量，而中间的层则用较高维的张量。文中所用单元的演化过程如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/envolve.png" alt="envolve"></p>
<p>。图a中普通卷积将channel和spatial的信息同时进行映射，参数量较大；图b为可分离卷积，解耦了channel和spatial，化乘法为加法，有一定比例的参数节省；图c中进行可分离卷积后又添加了bottleneck，映射到低维空间中；图d则是从低维空间开始，进行可分离卷积时扩张到较高的维度（前后维度之比被称为expansion factor，扩张系数），之后再通过1x1卷积降到原始维度。</p>
<p>实际上，图c和图d的结构在堆叠时是等价的，只是观察起点的不同。但基于兴趣流形应该分布在一个低维子空间上的假设，这引出了文章的第二个关键点：将skip-connection转移到低维表达间，即Inverted residual block。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/invert_residual.png" alt="inverted"></p>
<p>综合以上两点，文章中网络所用的基本单元如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/module.png" alt="envolve"></p>
<p>文章指出，这种设计将层输入、输出空间跟层变换分离，即网络容量（capacity）和表达力（expressIveness）的解耦。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/module2.png" alt="envolve"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>MobileNet V2的整体结构如下表：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/arch.png" alt="envolve"></p>
<p>上图中，t代表单元的扩张系数，c代表channel数，n为单元重复个数，s为stride数。可见，网络整体上遵循了重复相同单元和加深则变宽等设计范式。也不免有人工设计的成分（如28^2*64单元的stride，单元重复数等）。</p>
<h4 id="ImageNet-Classification"><a href="#ImageNet-Classification" class="headerlink" title="ImageNet Classification"></a>ImageNet Classification</h4><p>MoblieNets V2仍然集成了V1版本的两个超参数，在ImageNet上的实验结果如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/imagenet.png" alt="envolve"></p>
<p>可以看到相比V1版本优势明显，在精度方面跟NAS搜索出的结构有相当的表现。</p>
<h4 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h4><p>文章还提出SSDLite来更好适应移动端需求，改动是将head部分的普通卷积都替换为了可分离卷积。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/coco.png" alt="envolve"></p>
<p>上面是在COCO上的表现，可以看到精度方面跟YOLOv2和SSD300相当（尽管很低，相比SOTA差距还很大），但模型参数和运算复杂度都有一个数量级的减少。最后的CPU时间是在Pixel上测得，可以到5FPS，达不到真正移动实时的要求，但也是不小的推进了（并没有给出GPU上的推断时间，而Pixel+TF-Lite的benchmark又跟其他网络难以产生有效的比较）。</p>
<h4 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h4><p>下图是在VOC上分割的结果：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/seg.png" alt="envolve"></p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>文章还做了关于线性变换bottleneck替代ReLU和skip-connection位置的实验，进一步支撑之前的分析。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/ablation.png" alt="envolve"></p>
<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p>本篇文章的附录部分提供了紧的n维流形在经过升维线性变化加ReLU后被映射到子集的期望大小的界，这个界说明在扩张到足够高的维度后，升维线性变换加ReLu能以较高的概率可逆（保持信息）并加入非线性。</p>
<p>上面的结论是非常拗口的。自己的理解是，使用ReLU引入非线性的同时会导致信息损失（非线性指不会被卷积、全连接等线性映射吸收掉，信息损失则是指ReLU将&lt;0的输入置0，输入变得稀疏，而若所有输入的某一维度都被置0，则会使输入空间本身降维），我们要对抗这一可能的信息损失，需要将输入先扩张，即y=ReLU(Bx)，x为R^n空间上的输入，B为m×n矩阵，我们期望m足够大，以达到扩张的效果并在经过ReLU后保持y跟x的信息量同样多（文中的引理二，即是证此变换可逆性的一个条件，应该是借用了代数的概念，矩阵在经过可逆变换后不会降秩，秩成为衡量信息损失的指标）。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inverted-Residuals-and-Linear-Bottlenecks-Mobile-Networks-for-Classification-Detection-and-Segmentation/distribution.png" alt="envolve"></p>
<p>在ReLU(Bx)算符可逆性的问题上，作者做了一些经验性实验，如上图。a和b分别为训练前后，每层正激活channel数（可逆性条件）和其占总channel数比例的分布。图a和图b的左图，随网络加深，channel数增多，即变宽；训练前后，方差增大，且有两层低于了可逆性条件阈值（图b左图中绿色线低于紫色阈值的部分）。右图是一个比例，由于随机初始化，均值在0.5附近，训练后同样方差增大，而可逆性条件阈值一直为1/6（即为MobileNet V2扩张系数的倒数）。</p>
<p>附录的Theorem 1则证明了ReLU(Bx)算符将输入x压缩后的空间维度(n-volume)的界，此界在扩张系数较大时可以跟原空间相当，即信息损失很小。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>能够看到附录里给出文中假定或观察的数学证明的论文还是开心的。太多论文只是终于实验的SOTA而避而不谈Insights，况且给出证明。尽管本篇附录中的证明仍有经验主义的部分，且并没有完全定义清楚问题和结论，其对后续工作的启发价值还是有的。</p>
<p>这也暴露了当前领域的通病，我们没有共通的一套语言来描述自己的网络，譬如，如何定义网络的容量、表达力，如何衡量信息的损失。没有通用的定义造成了论文表述常常有令经验少者难以理解的表达。去定义这样一套语言和标准来为网络设计提供参考，希望成为以后的研究热点，也是我自己的一个思考方向。</p>
<p>总体来看，本篇文章提供的两点改进都是有启发性的，但并不完整，需要更多工作来补充。另外源码没有给出，会尝试复现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是MobileNets的第二版。&lt;a href=&quot;https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neura
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Chrome扩展推荐</title>
    <link href="http://blog.ddlee.cn/2018/01/16/Chrome%E6%89%A9%E5%B1%95%E6%8E%A8%E8%8D%90/"/>
    <id>http://blog.ddlee.cn/2018/01/16/Chrome扩展推荐/</id>
    <published>2018-01-16T12:59:51.000Z</published>
    <updated>2018-01-16T12:59:51.235Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是一个Chrome扩展的推荐列表。所有扩展插件在<a href="https://chrome.google.com/webstore/" target="_blank" rel="external">Chrome Web Store</a>都可搜索得到。</p>
<ul>
<li>Adblock Plus，用于屏蔽广告，效果出众，必备</li>
<li>Chrono Download Manager，下载任务管理，必备</li>
<li>Draw.io Desktop，流程图编辑，轻量方便</li>
<li>Emoji Keyboard (2016) by EmojiOne，表情输入</li>
<li>Evernote Web Clipper，保存有价值的信息到Evernote</li>
<li>Fatkun Batch Download Image，批量下载图片的插件，配合Google搜图</li>
<li>Follow Feed(by Feedly)，在当前网页搜索RSS订阅源并订阅至Feedly，用于订阅浏览到的价值博客等</li>
<li>GNOME Shell Integration，GNOME插件集成，用于在extension.gnome.org给GNOME安装插件，GNOME用户必备（有关GNOME可参考<a href="https://blog.ddlee.cn/tags/Gnome/">这里</a>）</li>
<li>Google Dictionary (by Google)，字典，设置快捷键Ctrl+D，搜词很方便</li>
<li>Google Input Tools，Google输入工具，应急之用</li>
<li>Google Scholar Button，用于在当前网页识别论文并在Google Scholar上检索相关内容</li>
<li>Google Translate，选取网页内容，可进行方便翻译</li>
<li>Inbox by Gmail，用于保存连接等内容到邮箱，方便分享</li>
<li>LastPass，跨平台的免费密码管理</li>
<li>Mega，如名</li>
<li>Mendeley Importer，导入文章到Mendeley Library</li>
<li>Mercury Reader，渲染网页到阅读模式，清爽干净</li>
<li>Momentum，增强新标签页，显示时钟、天气、To Do List等</li>
<li>Octotree，显示Github Repo的目录结构，必备</li>
<li>One-Click Extensions Manager，管理扩展用，用于节省内存</li>
<li>PDF Viewer，用于阅读PDF（免于直接下载），记得勾选Allow access to file URLs</li>
<li>Proxy SwitchyOmega，代理，善用auto switch和备份等功能</li>
<li>Pushbullet，跨设备文字通信，精分（PC用Ubuntu，平板iOS，手机Android）推荐</li>
<li>Quick QRCode，利器，用于把文字、连接等转成二维码，方便分享</li>
<li>Save to Google，保存网页到Google</li>
<li>Save to Pocket，保存到Pocket，稍后再读工具</li>
<li>Secure Shell，网页版SSH</li>
<li>Tab Snooze，折叠暂时不必要的标签页，利器</li>
<li>Telegram，如名</li>
<li>Text，轻量文本编辑器</li>
<li>Turn Off the Lights，利器，关灯。YouTube和Bilibili可用，其他未测试</li>
<li>Vimium，利器，脱离鼠标的网页浏览体验</li>
<li>微软雅黑字体，强迫症推荐</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是一个Chrome扩展的推荐列表。所有扩展插件在&lt;a href=&quot;https://chrome.google.com/webstore/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Chrome Web Store&lt;/a&gt;都可搜索得到。&lt;/p&gt;
&lt;u
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
      <category term="Chrome" scheme="http://blog.ddlee.cn/tags/Chrome/"/>
    
  </entry>
  
  <entry>
    <title>Laptop Reborn: 系统重装侧记</title>
    <link href="http://blog.ddlee.cn/2018/01/15/Laptop-Reborn-%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A3%85%E4%BE%A7%E8%AE%B0/"/>
    <id>http://blog.ddlee.cn/2018/01/15/Laptop-Reborn-系统重装侧记/</id>
    <published>2018-01-14T18:06:16.000Z</published>
    <updated>2018-01-14T18:06:16.783Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是个人向的系统重装记录，以便自己日后参考。</p>
<p>我使用的系统是Win10+Ubuntu16.04。</p>
<p>Ubuntu为主力，Windows在两种情况下发挥作用：</p>
<ol>
<li>用只支持win平台下解锁的移动硬盘存取文件</li>
<li>用iTunes备份iPad和传输文件</li>
</ol>
<h2 id="备份-Backup"><a href="#备份-Backup" class="headerlink" title="备份 Backup"></a>备份 Backup</h2><p>备份必然是第一步，几乎只需要备份用户文件夹就好。</p>
<p>Ubuntu:<br>/home/Documents/<br>/home/Pictures(Music, Video, etc.)<br>/home/Downloads(Maybe)</p>
<p>Windows<br>~/Documents/(Music, Picture, Video, Desktop)</p>
<h2 id="镜像准备-ISO-Preparation"><a href="#镜像准备-ISO-Preparation" class="headerlink" title="镜像准备 ISO Preparation"></a>镜像准备 ISO Preparation</h2><p>准备系统镜像。这次用的刻录软件分别是Ruff(Windows)和Etcher(Ubuntu)。</p>
<p>Ubuntu: Ubuntu Gnome 16.04(From TUNA Mirror)</p>
<p>Windows: Win10-multi-ver1709(msdn.itellyou.cn)</p>
<p>Burning Tool: etcher portable</p>
<h2 id="硬盘分区-Disk-Partition"><a href="#硬盘分区-Disk-Partition" class="headerlink" title="硬盘分区 Disk Partition"></a>硬盘分区 Disk Partition</h2><p>双系统共存的话，要先装Windows(反之Windows安装时会影响已存在的Ubuntu)，因此分区这一部分在安装Windows的时候完成。</p>
<p>我的SSD只有240G，因此选择50+50给Windows，剩下的给Ubuntu(包括swap)</p>
<p>Windows(sys 50G, data 50G)<br>Ubuntu(sys 120G)</p>
<h2 id="驱动准备-Driver-Preparation"><a href="#驱动准备-Driver-Preparation" class="headerlink" title="驱动准备 Driver Preparation"></a>驱动准备 Driver Preparation</h2><p>Ubuntu还好，全新的Windows可能没法驱动网卡，这样就没办法更新和后续操作。我之前的方案是用驱动人生或者驱动精灵的网卡版，用完之后，良犬烹。但这次洁癖发作，因此选择去联想官网下载网卡驱动。再用相对干净的Driver boost更新其他驱动。</p>
<p>netcard: <a href="https://pcsupport.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-t-series-laptops/thinkpad-t460p/downloads" target="_blank" rel="external">https://pcsupport.lenovo.com/us/en/products/laptops-and-netbooks/thinkpad-t-series-laptops/thinkpad-t460p/downloads</a></p>
<p>Driver update tool(Driver boost 3): <a href="http://download.cnet.com/Driver-Booster-2/3000-18513_4-75992725.html?part=dl-&amp;subj=dl&amp;tag=button" target="_blank" rel="external">http://download.cnet.com/Driver-Booster-2/3000-18513_4-75992725.html?part=dl-&amp;subj=dl&amp;tag=button</a></p>
<h2 id="软件安装-Software-Installation"><a href="#软件安装-Software-Installation" class="headerlink" title="软件安装 Software Installation"></a>软件安装 Software Installation</h2><p>这一部分只是一个list，主要给自己参考用。</p>
<p>Windows:</p>
<ul>
<li>Minimal-Net-Pack</li>
<li>Office 2016</li>
<li>iTunes</li>
<li>CCleaner</li>
<li>Picasa</li>
<li>PotPlayer</li>
<li>mini-Thunder</li>
<li>Launchy</li>
</ul>
<p>Ubuntu:</p>
<ul>
<li>Minimal-Net-Pack</li>
<li>Google Chrome</li>
<li>Sougou Pinyin, Nutstore, Stacer</li>
<li>Netease Music, Variety</li>
<li>Gnome themes, Telegram, Xmind, FileZilla</li>
</ul>
<h2 id="开发环境配置-Environment-Configuration"><a href="#开发环境配置-Environment-Configuration" class="headerlink" title="开发环境配置 Environment Configuration"></a>开发环境配置 Environment Configuration</h2><p>这一部分完成自己开发环境的配置。</p>
<h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h4><p>SSH key: ssh-keygen and ssh-add, git config name&amp;email</p>
<h4 id="Python-Annaconda"><a href="#Python-Annaconda" class="headerlink" title="Python(Annaconda)"></a>Python(Annaconda)</h4><p>(pypi: 修改 ~/.config/pip/pip.conf (Linux), %APPDATA%\pip\pip.ini (Windows 10) 或 $HOME/Library/Application Support/pip/pip.conf (macOS) (没有就创建一个)， 修改 index-url至tuna，例如<br>[global]<br>index-url = <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="external">https://pypi.tuna.tsinghua.edu.cn/simple</a> )<br>(conda TUNA 还提供了 Anaconda 仓库的镜像，运行以下命令:<br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a></p>
<p>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a></p>
<p>conda config –set show_channel_urls yes<br>即可添加 Anaconda Python 免费仓库。)</p>
<h4 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h4><p>install: <a href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="external">https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/</a></p>
<p>To verify: sudo docker run hello-world</p>
<p>mirror: <a href="https://www.docker-cn.com/registry-mirror" target="_blank" rel="external">https://www.docker-cn.com/registry-mirror</a><br>chmod 755 /etc/docker<br>为了永久性保留更改，您可以修改 /etc/docker/daemon.json 文件并添加上 registry-mirrors 键值。<br>{ “registry-mirrors”: [“<a href="https://registry.docker-cn.com" target="_blank" rel="external">https://registry.docker-cn.com</a>“] }<br>修改保存后重启 Docker 以使配置生效</p>
<p>ref: <a href="https://docker_practice.gitee.io" target="_blank" rel="external">https://docker_practice.gitee.io</a></p>
<h4 id="R"><a href="#R" class="headerlink" title="R"></a>R</h4><p>sudo apt install r-base r-base-dev</p>
<h4 id="Hexo-Node-js"><a href="#Hexo-Node-js" class="headerlink" title="Hexo(Node.js)"></a>Hexo(Node.js)</h4><p>Node.js:<br>curl -sL <a href="https://deb.nodesource.com/setup_8.x" target="_blank" rel="external">https://deb.nodesource.com/setup_8.x</a> | sudo -E bash -<br>sudo apt-get install -y nodejs<br>hexo:<br>sudo npm install -g hexo-cli<br>rebuild bind in hexo directory:<br>npm rebuild node-sass</p>
<h2 id="杂项-Minor-Changes"><a href="#杂项-Minor-Changes" class="headerlink" title="杂项 Minor Changes"></a>杂项 Minor Changes</h2><p>一些需要操作的小地方备忘。</p>
<p>1.install Nvidia Driver<br><a href="http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux" target="_blank" rel="external">http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux</a><br><a href="https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07" target="_blank" rel="external">https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07</a></p>
<p>sudo add-apt-repository ppa:graphics-drivers/ppa<br>sudo apt-get update<br>sudo apt-get install nvidia-384<br>(ref at <a href="http://www.nvidia.com/object/unix.html" target="_blank" rel="external">http://www.nvidia.com/object/unix.html</a> to determine driver version)<br>optional: pip install gpustat</p>
<p>Proxy for ppa.launchpad.net<br>修改/etc/apt/sources.list.d下面需要代理的仓库，将ppa.launchpad.net换成代理地址，执行sudo apt update更新即可。 可用代理地址：<a href="http://launchpad.proxy.ustclug.org/" target="_blank" rel="external">http://launchpad.proxy.ustclug.org/</a></p>
<p>2.Grub skip time<br><a href="https://askubuntu.com/questions/157925/how-do-i-skip-the-grub-menu-on-a-dual-boot-system" target="_blank" rel="external">https://askubuntu.com/questions/157925/how-do-i-skip-the-grub-menu-on-a-dual-boot-system</a></p>
<p>Edit /etc/default/grub to contain</p>
<p>if you prefer to see the menu for 1 second:<br>GRUB_HIDDEN_TIMEOUT=<br>GRUB_TIMEOUT=1</p>
<p>When you’re done, run sudo update-grub to save your changes.</p>
<p>3.cpufrequtils(set performance &amp; get temperature)<br><a href="http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Power-Management-with-cpufrequtils" target="_blank" rel="external">http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Power-Management-with-cpufrequtils</a><br><a href="https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature" target="_blank" rel="external">https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature</a></p>
<ul>
<li>Disable intel turbo boost technology: terminal:<br>sudo -i<br>echo “1” | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo<br>exit</li>
</ul>
<p>4.tracke-miner-fs bug<br><a href="https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup" target="_blank" rel="external">https://askubuntu.com/questions/346211/tracker-store-and-tracker-miner-fs-eating-up-my-cpu-on-every-startup</a></p>
<p>echo -e “\nHidden=true\n” | sudo tee –append /etc/xdg/autostart/tracker-extract.desktop /etc/xdg/autostart/tracker-miner-apps.desktop /etc/xdg/autostart/tracker-miner-fs.desktop /etc/xdg/autostart/tracker-miner-user-guides.desktop /etc/xdg/autostart/tracker-store.desktop &gt; /dev/null<br>gsettings set org.freedesktop.Tracker.Miner.Files crawling-interval -2<br>gsettings set org.freedesktop.Tracker.Miner.Files enable-monitors false<br>tracker reset –hard</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是个人向的系统重装记录，以便自己日后参考。&lt;/p&gt;
&lt;p&gt;我使用的系统是Win10+Ubuntu16.04。&lt;/p&gt;
&lt;p&gt;Ubuntu为主力，Windows在两种情况下发挥作用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用只支持win平台下解锁的移动硬盘存取文件&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>FlintOS轻体验</title>
    <link href="http://blog.ddlee.cn/2018/01/10/FlintOS%E8%BD%BB%E4%BD%93%E9%AA%8C/"/>
    <id>http://blog.ddlee.cn/2018/01/10/FlintOS轻体验/</id>
    <published>2018-01-09T17:36:06.000Z</published>
    <updated>2018-01-14T18:06:43.322Z</updated>
    
    <content type="html"><![CDATA[<p>呃，我在<a href="https://ddlee.cn/index_ver2016.html" target="_blank" rel="external">2016版的个人主页</a>里曾表达自己特别想拥有一台Chromebook，可终于还是屈服在了网络环境面前。</p>
<p>离了我的路由器，我的Chromebook几乎就是个废物了。</p>
<p>不过，FlintOS的出现让我重拾了这个想法，我可能还是需要一台轻便的上网本。</p>
<p>FlintOS是Chromium OS的中文本地化项目，而后者正是Chromebook的操作系统，在美国的低端笔记本市场和教育市场占有很大的份额。</p>
<p>只不过在大陆呵呵。</p>
<p>言归正传，FlintOS背后的公司是成立不久的燧炻科技，这里是他们的<a href="https://flintos.com" target="_blank" rel="external">官网</a>，可以在这个<a href="https://flintos.com/faq/" target="_blank" rel="external">页面</a>了解更多FlintOS的信息，本篇中的版本是在<a href="https://flintos.com/community/topic/flint-os-for-pc-dev-3-1-%e5%8f%91%e5%b8%83%e9%80%9a%e7%9f%a5/" target="_blank" rel="external">论坛</a>的DEV3.2中国版。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>DEV3.2中国版的最大亮点是支持本地账户。这意味着在大陆的网络环境下也有使用的可能性。而且，这个版本内置了可供科学上网的服务，测试可用。另外，FlintOS settings里面也包括了安装Flash的快捷方式。</p>
<p>目前建议是在U盘上体验，安装教程在<a href="https://flintos.com/instructions-pc" target="_blank" rel="external">这里</a>。</p>
<p>也可以选择安装双启动和硬盘独占安装，可以参考社区的<a href="https://flintos.com/community/forum/flint-os-for-pc/" target="_blank" rel="external">置顶帖</a>。</p>
<h2 id="界面UI"><a href="#界面UI" class="headerlink" title="界面UI"></a>界面UI</h2><p>桌面和右下角的通知栏，遵从了Material Design，看着舒服。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/desktop.png" alt="desktop"></p>
<p>APP Launcher，跟Gnome的风格很像。搜索栏可以直接Google搜索，也可以搜索文件和APP。实际上大部分APP可以当做交互逻辑级别更高的书签，打开即是新建相应网站的标签页（如YouTube）。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/apps.png" alt="desktop"></p>
<p>这个是Chrome Web Store，跟作为浏览器的Chrome完全一致，只不过这个平台上可没有homebrew也没有dpkg。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/webstore.png" alt="desktop"></p>
<p>文件管理应用，跟Google Drive深度集成。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/files.png" alt="desktop"></p>
<h2 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h2><p>我也尝试探索用于开发的可能性，由于ssh的存在，可操作性还是很高的。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/ssh.png" alt="desktop"></p>
<p>Texts是一个比较轻亮的文本编辑器。当然也有仿atom的付费APP。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/texts.png" alt="desktop"></p>
<p>crosh（通过Crtl+Shift+T打开）是内置的shell，功能上还有待探索，图为运行top的效果。</p>
<p><img src="http://static.ddlee.cn/static/img/FlintOS轻体验/crosh.png" alt="desktop"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Chromium OS的理念是web为王，浏览器即一切。这种理念配合Google相当完整的生态使得Chromebook在廉价本市场几乎是统治地位。去年的MS build上，微软也发布了对标的Windows版本，demo用的场景就是老师为每个学生配备PC。</p>
<p>相比之下，大陆的这个市场还是空白（或许不一定有，毕竟教育方面并不普及）。看到有把这种理念本地化的公司出现还是很惊艳，在此默默祝福他们。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;呃，我在&lt;a href=&quot;https://ddlee.cn/index_ver2016.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;2016版的个人主页&lt;/a&gt;里曾表达自己特别想拥有一台Chromebook，可终于还是屈服在了网络环境面前。&lt;/
    
    </summary>
    
      <category term="Linux" scheme="http://blog.ddlee.cn/categories/Linux/"/>
    
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
      <category term="Chrome" scheme="http://blog.ddlee.cn/tags/Chrome/"/>
    
      <category term="Geek" scheme="http://blog.ddlee.cn/tags/Geek/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Densely Connected Convolutional Networks</title>
    <link href="http://blog.ddlee.cn/2018/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/"/>
    <id>http://blog.ddlee.cn/2018/01/06/论文笔记-Densely-Connected-Convolutional-Networks/</id>
    <published>2018-01-06T13:23:16.000Z</published>
    <updated>2018-01-23T13:13:59.898Z</updated>
    
    <content type="html"><![CDATA[<p>DenseNet将shortcut-connection的思路发挥到极致。在一个DenseBlock内部，每一层的输出均跟后面的层建立shortcut，特别需要注意的是，不同于ResNet中的相加，DenseNet连接shortcut的方式是Concat，这样越深的层则输入channel数越大。</p>
<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/arch.png" alt="arch"></p>
<p>整个网络被分为Dense Block和Transition Layer，前者内部进行密集连接，保持同样大小的feature map，后者为DenseBlock之间的连接层，完成下采样操作。</p>
<p>在每个DenseBlock内部，接受的数据维度会随层数加深而变大（因为不断拼接了之前层的输出），增长的速率即为初始的channel数，文章称这一channel数为growth rate，作为模型的一个超参数。初始的growth rate为32时，在DenseNet121架构下，最后一层的channel数将增长到1024。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/56cb18697f42eb0374d933446f45b151" target="_blank" rel="external">Netscope Vis</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>作者在CIFAR和ImageNet上都做了实验，DenseNet取得了跟ResNet相当的表现，加入Bottleneck和一部分压缩技巧后，用较少的参数就能达到跟ResNet相当的效果：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/result.png" alt="arch"></p>
<p>论文链接：<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">https://arxiv.org/abs/1608.06993</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DenseNet将shortcut-connection的思路发挥到极致。在一个DenseBlock内部，每一层的输出均跟后面的层建立shortcut，特别需要注意的是，不同于ResNet中的相加，DenseNet连接shortcut的方式是Concat，这样越深的层则输入
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Aggregated Residual Transformations for Deep Neural Networks</title>
    <link href="http://blog.ddlee.cn/2018/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/"/>
    <id>http://blog.ddlee.cn/2018/01/06/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/</id>
    <published>2018-01-06T13:19:34.000Z</published>
    <updated>2018-01-23T13:13:31.520Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为”cardinality”。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也添加了层与层间的shortcut connection。但由于其多路径的设计特征，我将其归为Inception系网络。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>深度网络结构上的设计已经有三种经典的范式：</p>
<ul>
<li>Repeat. 由AlexNet和VGG等开拓，几乎被之后所有的网络采用。即堆叠相同的拓扑结构，整个网络成为模块化的结构。</li>
<li>Multi-path. 由Inception系列发扬，将前一层的输入分割到不同的路径上进行变换，最后拼接结果。</li>
<li>Skip-connection. 最初出现于Highway Network，由ResNet发扬并成为标配。即建立浅层信息与深层信息的传递通道，改变原有的单一线性结构。</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>文章将残差函数表示为：</p>
<p>其中，C为本层进行的变换数目，即”cardinality”。</p>
<p>相比Inception-ResNet，ResNeXt相当于将其Inception Module的每条路径规范化了，并将规范后的路径数目作为新的超参数。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/multi-path.png" alt="multi-path"></p>
<p>上图中，路径被扩展为多条，而每条路径的宽度（channel数）也变窄了（64-&gt;4）。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/c2ba521fcb60520abb0b0da0e9c0f2ef" target="_blank" rel="external">NetScope Vis</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiements"><a href="#Experiements" class="headerlink" title="Experiements"></a>Experiements</h3><p>ResNeXt试图在保持参数数目的情况下提高网络性能，提升cardinality的同时使每条路径的宽度变窄。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/setting.png" alt="setting"></p>
<p>对比其他网络的结果：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/result.png" alt="result"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>ResNeXt较为突出的是把Inception单元规范化了，摆脱了需要精心设计Inception单元中卷积结构的问题，更好地组织了参数。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">https://arxiv.org/abs/1611.05431</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为”cardinality”。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <link href="http://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/"/>
    <id>http://blog.ddlee.cn/2018/01/04/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/</id>
    <published>2018-01-04T13:10:09.000Z</published>
    <updated>2018-01-09T16:56:34.009Z</updated>
    
    <content type="html"><![CDATA[<p>MobileNets系列可以看做是继Xception之后对Depthwise Separable Convolution的又一推动。利用深度可分离的特征，MobileNets系列引入两个模型精度和大小的超参，在保持相当精度的同时享有非常小的计算消耗，适用于移动端情形，因而被命名为”MobileNets”。</p>
<h3 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a>Depthwise Separable Convolution</h3><p>深度可分离卷积是近期深度网络设计的重要趋势。最早见于L. Sifre的PhD论文Rigid-motion scattering for image classification，其1×1卷积在Inception, ResNet, SqueezeNet等网络中作为降维bottleneck使用。Xception指出，Inception单元本质上假设了跨通道和跨空间相关性的解耦关系，并将这一解耦关系推向极端，用Depthwise Separable Convolution改造了Inception结构。</p>
<p>深度可分离卷积受欢迎的另一重要原因是其参数高效性。将原有卷积换成深度可分离卷积后，可以享受到模型压缩的增益。</p>
<p>标准的卷积操作，可以认为是大小为DK的窗口在DF大小的特征图上滑动计算，计算复杂性为：</p>
<p>DK×DK × M×N × DF×DF</p>
<p>其中，M和N分别代表输入channel数和输出channel数。</p>
<p>替换为深度可分离卷积后，先进行Depthwise Convolution，再进行1×1 Pointwise Convolution，计算复杂性为：</p>
<p>DK×DK × M × DF×DF + M×N × DF×DF</p>
<p>相比下，深度可分离卷积的计算复杂性约为原来的(1/N+1/DK^2)。</p>
<p>进一步地，MobileNet添加两个超参来控制这一压缩程度，alpha为channel数压缩系数，rho为分辨率压缩系数：</p>
<p>DK×DK × alpha×M × rho×DF× rho×DF + alpha×M × alpha×N × rho×DF × rho×DF</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/depthwise-seperable.png" alt="depthwise-seperable"></p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>MobileNet的基本结构如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/arch.png" alt="arch"></p>
<p>论文链接：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MobileNets系列可以看做是继Xception之后对Depthwise Separable Convolution的又一推动。利用深度可分离的特征，MobileNets系列引入两个模型精度和大小的超参，在保持相当精度的同时享有非常小的计算消耗，适用于移动端情形，因而被
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Xception: Deep Learning with Depthwise Seperable Convolutions</title>
    <link href="http://blog.ddlee.cn/2018/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/"/>
    <id>http://blog.ddlee.cn/2018/01/02/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/</id>
    <published>2018-01-02T13:04:30.000Z</published>
    <updated>2018-01-23T13:15:00.097Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是keras库作者的文章，对Inception结构进行了改进：用Depth-wise seperable convolution替换了Inception单元中的1×1卷积和3×3卷积。</p>
<p>文章对Inception结构的评论非常有见地。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>文章指出，Inception单元背后的假设是跨Channel和跨空间的相关性可以充分解耦，类似的还有长度和高度方向上的卷积结构（在Inception-v3里的3×3卷积被1×3和3×1卷积替代）。</p>
<p>进一步的，Xception基于更强的假设：跨channel和跨空间的相关性完全解耦。这也是Depthwise Separable Convolution所建模的理念。</p>
<p>一个简化的Inception单元：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception.png" alt="inception"></p>
<p>等价于：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception2.png" alt="inception"></p>
<p>将channel推向极端，即每个channel都由独立的3×3卷积处理：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception3.png" alt="inception"></p>
<p>这样就得到了Depthwise Separable Convolution。</p>
<h3 id="Architectrue"><a href="#Architectrue" class="headerlink" title="Architectrue"></a>Architectrue</h3><p>简单讲，Xception是线性堆叠的Depthwise Separable卷积，附加了Skip-connection。</p>
<p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/931d7c91b22109f83bbbb7ff1a215f5f" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/arch.png" alt="inception"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文的实验部分并没有像其他论文那样集成一个在ImageNet上SOTA的结果，而是以Inception-v3为基线，对比了参数数量和性能，认为提升正来自于更合理的参数利用。文章还对比了Residual的作用，在Xception网络中，Skip-connection不仅能提高训练速度，还能增强模型的性能。</p>
<h3 id="Concolusion"><a href="#Concolusion" class="headerlink" title="Concolusion"></a>Concolusion</h3><p>本文贡献主要对Inception单元的解读和引入Depthwise Seperable Convolution。更多对于Depthwise Seperable Convolution的描述，请参考<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">MobileNets</a>的笔记。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="external">https://arxiv.org/abs/1610.02357</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是keras库作者的文章，对Inception结构进行了改进：用Depth-wise seperable convolution替换了Inception单元中的1×1卷积和3×3卷积。&lt;/p&gt;
&lt;p&gt;文章对Inception结构的评论非常有见地。&lt;/p&gt;
&lt;h3 id=
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
    <link href="http://blog.ddlee.cn/2017/12/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/"/>
    <id>http://blog.ddlee.cn/2017/12/26/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/</id>
    <published>2017-12-26T12:59:02.000Z</published>
    <updated>2018-01-23T13:14:21.300Z</updated>
    
    <content type="html"><![CDATA[<p>在15年，ResNet成为那年最耀眼的卷积网络结构，skip-connection的结构也成为避不开的考虑选项。Inception系列也参考ResNet更新了自己的结构。同时推出了第四代和跟ResNet的结合版：Inception-v4和Inception-ResNet。</p>
<p>然而，这是一篇几乎都是图的论文。</p>
<p>所以，上图。</p>
<h3 id="Inception-v4-Architecture"><a href="#Inception-v4-Architecture" class="headerlink" title="Inception-v4 Architecture"></a>Inception-v4 Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/e0ac64013b167844053184d97b380978" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch1.jpg" alt="arch1"></p>
<h3 id="Inception-ResNet-v2-Architecture"><a href="#Inception-ResNet-v2-Architecture" class="headerlink" title="Inception-ResNet(v2) Architecture"></a>Inception-ResNet(v2) Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/aadd97383baccabb8b827ba507c24162" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch2.jpg" alt="arch1"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章在实验部分提到，不借助Skip-connection的结构也可以将Inception网络提升到SOTA的水准，但加入Skip-connection可以有效增加训练速度。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>卷积网络结构的演进遇到了瓶颈，在ImageNet上的提升边界似乎碰到天花板，且更多来自训练技巧和集成。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">https://arxiv.org/abs/1602.07261</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在15年，ResNet成为那年最耀眼的卷积网络结构，skip-connection的结构也成为避不开的考虑选项。Inception系列也参考ResNet更新了自己的结构。同时推出了第四代和跟ResNet的结合版：Inception-v4和Inception-ResNet。&lt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Speed/accuracy trade-offs for modern convolutional object detectors</title>
    <link href="http://blog.ddlee.cn/2017/12/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/"/>
    <id>http://blog.ddlee.cn/2017/12/24/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/</id>
    <published>2017-12-24T13:55:22.000Z</published>
    <updated>2018-01-09T16:56:34.010Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。</p>
<p>文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了Proposal个数、输入图片尺寸，生成Feature map的大小等作为超参，并行实验，探索精度和速度方面的trade-off。</p>
<p>文章也将源码公开，作为Tensorflow的Object Detection API。</p>
<p>下图是三种元结构的图示：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/meta-arch.png" alt="meta-arch"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig2.png" alt="meta-arch"></p>
<p>信息量非常大的一张图。</p>
<ul>
<li>横纵两个维度分别代表速度和准确度，横轴越靠左说明用时越少，纵轴越靠上说明mAP表现越好，因而，sweet spot应分布在左上角</li>
<li>两个超维是元结构和特征提取网络，元结构由形状代表，特征提取网络由颜色代表</li>
<li>虚线代表理想中的trade-off边界</li>
</ul>
<p>分析：</p>
<ul>
<li>准确度最高的由Faster-RCNN元结构、Inception-ResNet提取网络，高分图片，使用较大的feature map达到，如图右上角</li>
<li>较快的网络中准确度表现最好的由使用Inception和Mobilenet的SSD达到</li>
<li>sweet spot区特征提取网络由ResNet统治，较少Proposal的Faster-RCNN可以跟R-FCN相当</li>
<li>特征提取网络方面，Inception V2和MobileNet在高速度区，Incep-ResNet和ResNet在sweet spot和高精度区，Inception V3和VGG则远离理想边界（虚线）</li>
</ul>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig3.png" alt="meta-arch"></p>
<p>上图是特征提取网络对三种元结构的影响，横轴是特征提取网络的分类准确率，纵轴是检测任务上的mAP表现，可以看到，SSD在纵轴方向上方差最小，而Faster-RCNN和R-FCN对特征提取网络更为敏感。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig4.png" alt="meta-arch"></p>
<p>上图的横轴是不同的特征提取网络，组内是三种元结构的对比，纵轴是不同尺寸物体的mAP。</p>
<p>可以看到，在大物体的检测上，使用较小的网络时，SSD的效果跟两阶段方法相当，更深的特征提取网络则对两阶段方法的中型和小型物体的检测提升较大（ResNet101和Incep-ResNet都显现了两阶段方法在小物体上的提升）</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig5.png" alt="meta-arch"></p>
<p>上图显示了输入图片尺寸对mAP的影响。高分的图片对小物体检测帮助明显，因而拥有更高的精度，但相对运行速度会变慢。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig6.png" alt="meta-arch"></p>
<p>上图探究了两阶段方法中Proposal个数的影响，左边是Faster-RCNN，右边是R-FCN，实线是mAP，虚线是推断时间。<br>分析：</p>
<ul>
<li>相比R-FCN，Faster-RCNN推断时间对Proposal个数相当敏感（因为有per ROI的计算）</li>
<li>减少Proposal的个数，并不会给精度带来致命的下降</li>
</ul>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig7.png" alt="meta-arch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig8.png" alt="meta-arch"></p>
<p>上面两图是对FLOPS的记录，相对GPU时间更为中立，在图8中，GPU部分显现了ResNet跟Inception的分野（关于45度线，此时FLOPS跟GPU时间相当），文章认为分解操作(Factorization)减少了FLOPs，但增加了内存的IO时间，或者是GPU指令集更适合密集的卷积计算。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig9.png" alt="meta-arch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig10.png" alt="meta-arch"></p>
<p>上两图是对内存占用的分析，总体来说，特征提取网络越精简、feature map尺寸越小，占用内存越少，运行时间也越短。</p>
<p>最后，文章描述了他们ensemble的思路，在一系列不同stride、loss和配置的Faster-RCNN中（ResNet和Incep-ResNet为特征提取网络），贪心地选择验证集上AP较高的，并且去除类AP相似的模型。选择的5个用于ensemble的模型如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/table5.png" alt="meta-arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇文章是不错的实验结果报告，测试了足够多的模型，也得出了合理的和有启发的结论。几点想法：</p>
<ul>
<li>RFCN并没有很好的解决定位跟分类的矛盾，per ROI的子网络最好还是要有，但要限制Proposal的个数（实际大部分都是负样本）来减少冗余</li>
<li>小物体的检测仍然是最大的难点，增大分辨率和更深的网络确有帮助，但不是实质的。</li>
</ul>
<p>论文链接： <a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="external">https://arxiv.org/abs/1611.10012</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。&lt;/p&gt;
&lt;p&gt;文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了P
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Light-Head R-CNN: In Defense of Two-Stage Object Detector</title>
    <link href="http://blog.ddlee.cn/2017/12/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/"/>
    <id>http://blog.ddlee.cn/2017/12/22/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/</id>
    <published>2017-12-22T13:55:36.000Z</published>
    <updated>2018-01-09T16:56:34.011Z</updated>
    
    <content type="html"><![CDATA[<p>文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。</p>
<p>针对这两种元结构(Faster-RCNN和RFCN)，文章提出了“头”轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。</p>
<h2 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/arch.png" alt="arch"></p>
<p>如上图，虚线框出的部分是三种结构的RCNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为sperable convolution，产生的Score map也减少至10×p×p（相比原先的#class×p×p）。</p>
<p>一个可能的解释是，“瘦”（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较“厚”的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。</p>
<p>进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。</p>
<p>light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。</p>
<p>粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分，文章验证了较“瘦”的Score map不会对精度产生太大损害，也展现了ROI Align, Multiscale train等技巧对基线的提升过程。</p>
<p>文章的主要结果如下面两图（第一个为高精度，第二个为高速度）：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result1.png" alt="result1"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result2.png" alt="result2"></p>
<p>只能说这样的对比比较诡异。</p>
<p>第一张图中三个light-head结果并不能跟上面的其他结构构成多少有效的对照组，要么scale不同，要么FPN, multi-scale, ROI Align不同。唯一的有效对照是跟Mask-RCNN。</p>
<p>在高精度方面，基础网络不同，采用的scale也不同，没有有效的对照组。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我并不觉得这是对两阶段方法的Defense。文章对两阶段方法在精度和速度方面的分析比较有见地，但实验的结果并不能可靠地支撑light-head的有效性。相比之下Google的那篇trade-off可能更有参考价值。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="external">https://arxiv.org/abs/1711.07264</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]You Only Look Once: Unified, Real Time Object Detection</title>
    <link href="http://blog.ddlee.cn/2017/12/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-You-Only-Look-Once-Unified-Real-Time-Object-Detection/"/>
    <id>http://blog.ddlee.cn/2017/12/20/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/</id>
    <published>2017-12-20T13:38:31.000Z</published>
    <updated>2018-01-09T16:56:34.010Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。</p>
<p>YOLO的主要优点：</p>
<ul>
<li>快。</li>
<li>全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。</li>
<li>泛化性能好，在艺术作品上做检测时，YOLO表现好。</li>
</ul>
<h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><p>YOLO的大致工作流程如下：<br>1.准备数据：将图片缩放，划分为等分的网格，每个网格按跟ground truth的IOU分配到所要预测的样本。<br>2.卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IOU表示）。测试时，分数如下计算：</p>
<p>等式左边第一项由网格预测，后两项由每个box预测，综合起来变得到每个box含有不同类别物体的分数。<br>因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，S为网格数，B为每个网格生成box个数，C为类别数。<br>3.后处理：使用NMS过滤得到的box</p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/loss.jpg" alt="loss-function"></p>
<p>图片来自<a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/24916786</a></p>
<p>损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，loss中添加了权重并将长宽取根号。</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/error.png" alt="error"></p>
<p>相比Fast-RCNN，YOLO的背景误检在错误中占比重小，而位置错误占比大（未采用log编码）。</p>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>YOLO划分网格的思路还是比较粗糙的，每个网格生成的box个数也限制了其对小物体和相近物体的检测。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">https://arxiv.org/abs/1506.02640</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。&lt;/p&gt;
&lt;p&gt;YOLO的主要优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;快。&lt;/li&gt;
&lt;li&gt;全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Rethinking the Inception Architecture for Computer Vision</title>
    <link href="http://blog.ddlee.cn/2017/12/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/"/>
    <id>http://blog.ddlee.cn/2017/12/16/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/</id>
    <published>2017-12-16T12:53:29.000Z</published>
    <updated>2018-01-23T13:14:43.796Z</updated>
    
    <content type="html"><![CDATA[<p>本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一网络结构，v3版本相比v2版本加入了RMSProp，Label Smoothing等技巧。</p>
<p>文章表述了Inception系列的几个设计原则，并根据这些原则改进了GoogLeNet的结构。</p>
<h3 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h3><ul>
<li>Avoid representational bottlenecks, especially early in the network. 建议不要在过浅的阶段进行特征压缩，而维度只是一个表达复杂性的参考，并不能作为特征复杂性的绝对衡量标准。</li>
<li>Higher dimensional representations are easier to process locally with a network. 高阶的表示更有局部描述力，增加非线性有助于固化这些描述力。</li>
<li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. 基于空间的聚合信息可以在低维空间里处理，而不必担心有太多信息损失。这一点也佐证了1×1卷积的降维作用。</li>
<li>Balance the width and depth of the network.  宽度和深度的增加都有助于网络的表达能力，最好的做法是同时在这两个方向上推进，而非只顾及一个。</li>
</ul>
<h3 id="Factorizing-Convolution"><a href="#Factorizing-Convolution" class="headerlink" title="Factorizing Convolution"></a>Factorizing Convolution</h3><p>分解一直是计算数学里经典的思路。从牛顿法到BFGS，就是把Hessian矩阵（或其逆）用一系列的向量操作来表示和近似，避免矩阵的计算。</p>
<p>本文提出了两种卷积结构方面的分解，一个是在卷积核的层面，另一个是在空间方面。</p>
<p>第一种分解是将大核卷积分解成串联的小核卷积。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor3.png" alt="factor5"></p>
<p>用两个3×3的卷积代替5×5的卷积，带来的参数减少为(9+9)/(5×5).</p>
<p>第二种分解是在卷积核本身上，引入非对称卷积：用3×1和1×3的卷积串联代替3×3卷积。如下图所示。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor1.png" alt="factor3"></p>
<p>这种分解也可以推广到n维情况，且n越大，带来的收益越明显。</p>
<p>空间上的卷积分解建模了这样的情形：两个方向上的卷积参数互相正交，便被空间分解卷积解耦。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/spatial-seperable.png" alt="factor5"></p>
<h3 id="Utility-of-Auxiliary-Classifiers"><a href="#Utility-of-Auxiliary-Classifiers" class="headerlink" title="Utility of Auxiliary Classifiers"></a>Utility of Auxiliary Classifiers</h3><p>在GoogLeNet中，作者用loss监督了低维的特征图的学习，但进一步的实验发现，加入BN层后，这些增益被抵消了，于是Auxiliary Classifier可被看做是某种正则化技术，在加入BN的网络中便不再应用。</p>
<h3 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h3><p>这一节讨论网络中的特征降维，即下采样的过程，通常由卷积层或Pooling层的stride参数控制。文章为避免原则一中提到的Representation Bottleneck，在进行Pooling之前将网络加宽（通过Channel数的增加），这也对应了平衡宽度和深度的原则。</p>
<p>最终结合了Inception结构和下采样需求的单元如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/downsample.png" alt="factor5"></p>
<p>不同于Inception单元，上面的1×1卷积扩展了Channel，并且3×3卷积采用了stride=2。</p>
<h3 id="Inception-v2-amp-Inception-v3-Architecture"><a href="#Inception-v2-amp-Inception-v3-Architecture" class="headerlink" title="Inception-v2 &amp; Inception-v3 Architecture"></a>Inception-v2 &amp; Inception-v3 Architecture</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/arch.png" alt="factor5"></p>
<p>可以看到随深度增加，Channel数也在扩展，而Inception单元也遵从了堆叠的范式。</p>
<p>其中三种Inception单元分别为：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptiona.png" alt="factor5"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionb.png" alt="factor5"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionc.png" alt="factor5"></p>
<p>另外，也可以查看<a href="http://ethereon.github.io/netscope/#gist/a2394c1c4a9738469078f096a8979346" target="_blank" rel="external">NetScope Vis</a>来熟悉Inception-v3的结构，源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下面是Inception结构演化带来的增益分解：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/experiment.png" alt="factor5"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本篇是对Inception系网络的推进，其分解的思想成为又一网络设计的指导原则。</p>
<p>对卷积的进一步理解，可以参考这个<a href="https://graphics.stanford.edu/courses/cs178-10/applets/convolution.html" target="_blank" rel="external">页面</a>，这一工具可视化了不同卷积核对输入的处理，给出的例子都是在早期人们手工设计的滤波器，而深度网络隐式地学习到了这些滤波器的卷积表达。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">https://arxiv.org/abs/1512.00567</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]SSD: Single Shot MultiBox Detector</title>
    <link href="http://blog.ddlee.cn/2017/12/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-SSD-Single-Shot-MultiBox-Detector/"/>
    <id>http://blog.ddlee.cn/2017/12/12/论文笔记-SSD-Single-Shot-MultiBox-Detector/</id>
    <published>2017-12-12T13:37:56.000Z</published>
    <updated>2018-01-09T16:56:34.010Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保持较快的运行速度。</p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-SSD-Single-Shot-MultiBox-Detector/arch.jpg" alt="arch"></p>
<ul>
<li><p>多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。</p>
</li>
<li><p>更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SSD有点像多分类的RPN，生成anchor box，再对box预测分数和位置调整值。</p>
<p>论文链接：<a href="https://arxiv.org/abs/151.023325" target="_blank" rel="external">https://arxiv.org/abs/151.023325</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Feature Pyramid Networks for Object Detection</title>
    <link href="http://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Feature-Pyramid-Networks-for-Object-Detection/"/>
    <id>http://blog.ddlee.cn/2017/12/07/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/</id>
    <published>2017-12-07T13:55:59.000Z</published>
    <updated>2018-01-09T16:56:34.011Z</updated>
    
    <content type="html"><![CDATA[<p>对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。</p>
<p>回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。</p>
<p>一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的内存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。</p>
<p>本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。</p>
<h3 id="Feature-Pyramid-Networks"><a href="#Feature-Pyramid-Networks" class="headerlink" title="Feature Pyramid Networks"></a>Feature Pyramid Networks</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/arch.png" alt="arch"></p>
<p>从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行元素位置上的叠加（lateral connection），构成这一深度上的特征。</p>
<p>这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章的主要实验结果如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/result.png" alt="Experiments results"></p>
<p>对比不同head部分，输入feature的变化对检测精度确实有提升，而且，lateral和top-down两个操作也是缺一不可。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。</p>
<p>个人方面，FPN跟multi-scale的区别在哪，还值得进一步探索。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">https://arxiv.org/abs/1612.03144</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]R-FCN: Object Detection via Region-based Fully Convolutinal Networks</title>
    <link href="http://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/"/>
    <id>http://blog.ddlee.cn/2017/12/07/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/</id>
    <published>2017-12-07T13:55:48.000Z</published>
    <updated>2018-01-09T16:56:34.011Z</updated>
    
    <content type="html"><![CDATA[<p>文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等），这一点是由分类任务的平移不变性和检测任务的平移敏感性之间的矛盾导致的。换句话说，检测模型采用了分类模型的特征提取器，丢失了位置信息。这篇文章提出采用“位置敏感分数图”的方法解决这一问题。</p>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI Pooling"></a>Position-sensitive score maps &amp; Position-sensitive RoI Pooling</h3><p>位置敏感分数图的生成有两个重要操作，一是生成更“厚”的feature map，二是在RoI Pooling时选择性地输入feature map。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/rfcn.png" alt="arch"></p>
<p>Faster R-CNN中，经过RPN得到RoI，转化成分类任务，还加入了一定量的卷积操作（ResNet中的conv5部分），而这一部分卷积操作是不能共享的。R-FCN则着眼于全卷积结构，利用卷积操作在Channel这一维度上的自由性，赋予其位置敏感的意义。下面是具体的操作：</p>
<ul>
<li>在全卷积网络的最后一层，生成k^2(C+1)个Channel的Feature map，其中C为类别数，k^2代表k×k网格，用于分别检测目标物体的k×k个部分。即是用不同channel的feature map代表物体的不同局部（如左上部分，右下部分）。</li>
<li>将RPN网络得到的Proposal映射到上一步得到的feature map（厚度为k×k×(C+1)，）后，相应的，将RoI等分为k×k个bin，对第(i,j)个bin，仅考虑对应(i,j)位置的(C+1)个feature map，进行如下计算：其中(x0,y0)是这个RoI的锚点，得到的即是(i,j)号bin对C类别的相应分数。</li>
<li>经过上一步，每个RoI得到的结果是k^2(C+1)大小的分数张量，k×k编码着物体的局部分数信息，进行vote（平均）后得到(C+1)维的分数向量，再接入softmax得到每一类的概率。</li>
</ul>
<p>上面第二步操作中“仅选取第(i, j)号feature map”是位置信息产生意义的关键。</p>
<p>这样设计的网络结构，所有可学习的参数都分布在可共享的卷积层，因而在训练和测试性能上均有提升。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>R-FCN是对Faster R-CNN结构上的改进，部分地解决了位置不变性和位置敏感性的矛盾。通过最大化地共享卷积参数，使得在精度相当的情况下训练和测试效率都有了很大的提升。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="external">https://arxiv.org/abs/1605.06409</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等），这一点是由分类任务的平移不变性和检测任务的平移敏感性之间的矛盾导致的。换句话说，检测模型采用了分类模型的特征提取器，丢失了位置信息
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Going deeper with convolutions</title>
    <link href="http://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/"/>
    <id>http://blog.ddlee.cn/2017/11/30/论文笔记-Going-deeper-with-convolutions/</id>
    <published>2017-11-30T12:39:58.000Z</published>
    <updated>2018-01-23T13:14:10.606Z</updated>
    
    <content type="html"><![CDATA[<p>本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的GoogLeNet拿下了ILSVRC14分类任务的头名。文章也探讨了网络在不断加深的情况下如何更好地利用计算资源，这一理念也是Inception系列网络的核心。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Inception单元的启发主要来自Network in Network结构和Arora等人在神经科学方面的工作。</p>
<p>提高深度模型的一个简单想法是增加深度，但这样带来过拟合的风险和巨大的计算资源消耗，对数据量和计算力的要求可能会超过网络加深带来的收益。</p>
<p>解决这些问题的基本思路是使用稀疏连接的网络，而这也跟Arora等人工作中的Hebbian principle吻合：共同激活的神经元常常集聚在一起。换句话说，某一层激活的神经元只向下一层中特定的几个神经元传递激活信号，而向其他神经元几乎不传递信息，即仅有少部分连接是真正有效的，这也是稀疏的含义。</p>
<p>然而另一方面，现代计算架构对稀疏的计算非常低效，更适合的是密集的计算，这样便产生了矛盾。而Inception单元的提出就是为了用密集的结构来近似稀疏结构，在建模稀疏连接的同时又能利用密集计算的优势。</p>
<p>很多文章认为inception结构的意义在于将不同大小核的卷积并行连接，然后让网络自行决定采用哪种卷积来提取特征，有些无监督的意味，然后将1×1的卷积解释为降维操作。这种想法有待验证，是否在5×5卷积有较强激活的时候，3×3卷积大部分没有激活，还是两者能够同时有较强的激活？不同的处理阶段这两种卷积核的选择有没有规律？</p>
<p>在此提出一个个人的理解，欢迎讨论。</p>
<p>首先是channel的意义。我们知道，卷积之所以有效，是因为它建模了张量数据在空间上的局部相关性，加之Pooling操作，将这些相关性赋予平移不变性（即泛化能力）。而channel则是第三维，它实际上是卷积结构中的隐藏单元，是中间神经元的个数。卷积层在事实上是全连接的：每个Input channel都会和output channel互动，互动的信息只不过从全连接层的weight和bias变成了卷积核的weight。</p>
<p>这种全连接是冗余的，本质上应是一个稀疏的结构。Inception单元便在channel这个维度上做文章，采用的是类似矩阵分块的思想。</p>
<p>根据Hebbian principle，跨channel的这些神经元，应是高度相关的，于是有信息压缩的空间，因而使用跨channel的1×1的卷积将它们嵌入到低维的空间里（比如，Inception4a单元的输入channel是512，不同分支的1×1卷积输出channel则是192,96,16和64，见下面GoogLeNet结构表），在这个低维空间里，用密集的全连接建模（即3×5和5×5卷积），它们的输出channel相加也再恢复到原来的输入channel维度（Inception4a分别是192+208+48+64），最后的连接由Concat操作完成（分块矩阵的合并），这样就完成了分块密集矩阵对稀疏矩阵的近似。</p>
<p>这样来看，3×3和5×5大小的选择并不是本质的，本质的是分块低维嵌入和concat的分治思路。而在ResNeXt的工作中，这里的分块被认为是新的维度（称为cardinality），采用相同的拓扑结构。</p>
<h3 id="Stacked-Inception-Module"><a href="#Stacked-Inception-Module" class="headerlink" title="Stacked Inception Module"></a>Stacked Inception Module</h3><p>在GoogLeNet中，借鉴了AlexNet和VGG的stack(repeat)策略，将Inception单元重复串联起来，构成基本的特征提取结构。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/arch.png" alt="arch"></p>
<h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><p>朴素版本的Inception单元会带来Channel维数的不断增长，加入的1×1卷积则起到低维嵌入的作用，使Inception单元前后channel数保持稳定。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/inception.png" alt="arch"></p>
<h3 id="Auxililary-Classifier"><a href="#Auxililary-Classifier" class="headerlink" title="Auxililary Classifier"></a>Auxililary Classifier</h3><p>这里是本文的另一个贡献，将监督信息传入中间的feature map，构成一个整合loss，作者认为这样有助于浅层特征的学习。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/auxililary.png" alt="arch"></p>
<h3 id="Architecture-of-GoogLeNet"><a href="#Architecture-of-GoogLeNet" class="headerlink" title="Architecture of GoogLeNet"></a>Architecture of GoogLeNet</h3><p>下面的表显示了GoogLeNet的整体架构，可以留意到Inception单元的堆叠和Channel数在子路径中的变化。NetScope可视化可参见<a href="http://ethereon.github.io/netscope/#/gist/db8754ee4b239920b3df5ab93220a84b" target="_blank" rel="external">GoogLeNet Vis</a>。源文件位于<a href="https://github.com/ddlee96/awesome_cnn" target="_blank" rel="external">awesome_cnn</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/table.png" alt="arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>文章是对NiN思想的继承和推进，不同于AlexNet和VGG，网络的模块化更加凸显，多路径的结构也成为新的网络设计范本，启发了众多后续网络结构的设计。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的GoogLeNet拿下了ILSVRC14分类任务的头名。文章也探讨了网络在不断加深的情况下如何更好地利用计算资源，这一理念也是Inception系列网络的核心。&lt;/p&gt;
&lt;h3 id=
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]MegDet: A Large Mini-Batch Object Detector</title>
    <link href="http://blog.ddlee.cn/2017/11/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MegDet-A-Large-Mini-Batch-Object-Detector/"/>
    <id>http://blog.ddlee.cn/2017/11/21/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/</id>
    <published>2017-11-21T15:30:56.000Z</published>
    <updated>2018-01-09T16:56:34.007Z</updated>
    
    <content type="html"><![CDATA[<p>本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。</p>
<h2 id="Object-Detection-Progress-Summay"><a href="#Object-Detection-Progress-Summay" class="headerlink" title="Object Detection Progress Summay"></a>Object Detection Progress Summay</h2><p>检测方法回顾：R-CNN, Fast/Faster R-CNN, Mask RCNN, RetinaNet(Focal Loss), ResNet(backbone network),</p>
<p>文章先指出前述方法大多是框架、loss等的更新，而均采用非常小的batch（2张图片）训练，有如下不足：</p>
<ul>
<li>training slow</li>
<li>fails to provide accurate statistics for BN</li>
</ul>
<p>这里涉及一个问题，检测任务的源数据，到底应该是图片还是标注框。在Fast R-CNN中，RBG提到SPPNet等每个batch采样的标注框来自不同的图片，之间不能共享卷积运算（卷积运算是以图片为单位的）。为了共享这部分计算，Fast R-CNN采用了“先选图片，再选标注框”的策略来确定每个batch，文章提到这种操作会引入相关性，但在实际中却影响不大。之后的Faster R-CNN，每张图片经过RPN产生约300个Proposal，传入RCNN做法也成了通用做法。</p>
<p>个人认为检测任务的数据，应该是以图片为单位的。物体在图片的背景中才会产生语义，而尽管每张图片有多个Proposal（近似分类任务中的batch大小），但它们共享的是同一个语义（场景），而单一的语义难以在同一个batch中提供多样性来供网络学习。</p>
<h5 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h5><p>Increasing mini-batch size requires large learning rate, which may cause discovergence.</p>
<h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>new explanation of linear scaling rule, introduce “warmup” trick to learning rate schedule</li>
<li>Cross GPU Batch Normalization(CGBN)</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="Variance-Equivalence-explanation-for-Linear-Scaling-Rule"><a href="#Variance-Equivalence-explanation-for-Linear-Scaling-Rule" class="headerlink" title="Variance Equivalence explanation for Linear Scaling Rule"></a>Variance Equivalence explanation for Linear Scaling Rule</h3><p>linear scaling rule 来自更改batch size 时，同时放缩learning rate，使得更改后的weight update相比之前小batch size， 多步的weight update类似。而本文用保持loss gradient的方差不变重新解释了linear scaling rule，并指出这一假定仅要求loss gradient是i.i.d，相比保持weight update所假设的不同batch size间loss gradient相似更弱。</p>
<p>参见<a href="https://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/#Motivation">Accurate Large Minibatch SGD: Training ImageNet in One Hour</a>，近似时假设了求和项变化不大，这一条件在Object Detection中可能不成立，不同图片的标注框（大小、个数）差别很大。</p>
<h3 id="WarmUp-Strategy"><a href="#WarmUp-Strategy" class="headerlink" title="WarmUp Strategy"></a>WarmUp Strategy</h3><p>在训练初期，weight抖动明显，引入warmup机制来使用较小的学习率，再逐渐增大到Linear scaling rule要求的学习率。</p>
<h3 id="Cross-GPU-Batch-Normalization"><a href="#Cross-GPU-Batch-Normalization" class="headerlink" title="Cross-GPU Batch Normalization"></a>Cross-GPU Batch Normalization</h3><p>BN是使深度网络得以训练和收敛的关键技术之一，但在检测任务中，fine-tuning阶段常常固定了SOTA分类网络的BN部分参数，不进行更新。</p>
<p>检测中常常需要较大分辨率的图片，而GPU内存限制了单卡上的图片个数，提高batch size意味着BN要在多卡（Cross-GPU）上进行。</p>
<p>BN操作需要对每个batch计算均值和方差来进行标准化，对于多卡，具体做法是，单卡独立计算均值，聚合（类似Map-Reduce中的Reduce）算均值，再将均值下发到每个卡，算差，再聚合起来，计算batch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。</p>
<p>流程如图：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/cgbn.png" alt="cgbn"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在COCO数据集上的架构用预训练ResNet-50作为基础网络，FPN用于提供feature map。</p>
<p>结果显示，不使用BN时，较大的batch size（64,128）不能收敛。使用BN后，增大Batch size能够收敛但仅带来较小的精度提升，而BN的大小也不是越大越好，实验中，32是最好的选择。主要结果如下表：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/results.png" alt="results"></p>
<p>按epoch，精度的变化如下图，小batch（16）在最初的几个epoch表现比大batch（32）要好。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/byepoch.png" alt="accuracy-by-epoch"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇论文读起来总感觉少了些东西。对Linear scale rule的解释固然新颖，但没有引入新的trick（只是确认了检测仍是需要Linear scale rule的）。多卡的BN确实是非常厉害的工程实现（高效性），但实验的结果并没有支持到较大的batch size（128,256）比小batch精度更好的期望，而最后的COCO夺冠模型整合了多种trick，没有更进一步的错误分析，很难支撑说明CGBN带来的关键作用。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。&lt;/p&gt;
&lt;h2 id=&quot;Object-Detection-Progress-Summay&quot;&gt;&lt;a href=&quot;#Ob
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks</title>
    <link href="http://blog.ddlee.cn/2017/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/"/>
    <id>http://blog.ddlee.cn/2017/10/21/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/</id>
    <published>2017-10-21T15:39:34.000Z</published>
    <updated>2018-01-09T16:56:34.007Z</updated>
    
    <content type="html"><![CDATA[<p>Faster R-CNN是2-stage方法的主流方法，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA。</p>
<h2 id="Regional-Proposal-Networks"><a href="#Regional-Proposal-Networks" class="headerlink" title="Regional Proposal Networks"></a>Regional Proposal Networks</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/rpn.png" alt="faster_rcnn_arch"></p>
<p>RPN网络将Proposal这一任务建模为二分类的问题。</p>
<p>第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IOU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本即是anchor box和每个anchor box是否有物体。RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的Loss统一起来，作为RPN网络的目标训练。</p>
<p>RPN网络可调的超参还是很多的，anchor box的大小和长宽比例、IoU的阈值、每张图片上Proposal正负样本的比例等。</p>
<h2 id="Alternate-Training"><a href="#Alternate-Training" class="headerlink" title="Alternate Training"></a>Alternate Training</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/faster_rcnn_netwrok.png" alt="faster_rcnn_arch"></p>
<p>RPN网络是在feature map上进行的，因而可以跟RCNN完全共享feature extractor部分的卷积运算。训练时，RPN和RCNN的训练可以交替进行，即交替地固定RPN和RCNN部分的参数，更新另一部分。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Faster R-CNN的成功之处在于用RPN网络完成了检测任务的“深度化”。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。RPN网络也成为检测2-stage方法的标准部件。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Faster R-CNN是2-stage方法的主流方法，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Fast R-CNN</title>
    <link href="http://blog.ddlee.cn/2017/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Fast-R-CNN/"/>
    <id>http://blog.ddlee.cn/2017/10/15/论文笔记-Fast-R-CNN/</id>
    <published>2017-10-15T15:34:31.000Z</published>
    <updated>2018-01-09T16:56:34.007Z</updated>
    
    <content type="html"><![CDATA[<p>Fast R-CNN 是对R-CNN的改进，作者栏只有RBG一人。文章先指出了R-CNN存在的问题，再介绍了自己的改进思路。文章结构堪称典范，从现存问题，到解决方案、实验细节，再到结果分析、拓展讨论，条分缕析，值得借鉴。而且，RBG开源的代码也影响了后来大部分这一领域的工作。</p>
<h2 id="R-CNN的问题"><a href="#R-CNN的问题" class="headerlink" title="R-CNN的问题"></a>R-CNN的问题</h2><ul>
<li>训练是一个多阶段的过程（Proposal, Classification, Regression）</li>
<li>训练耗时耗力</li>
<li>推断耗时</li>
</ul>
<p>而耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算。</p>
<h2 id="Fast-R-CNN-Architecture"><a href="#Fast-R-CNN-Architecture" class="headerlink" title="Fast R-CNN Architecture"></a>Fast R-CNN Architecture</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/fast-rcnn-arch.png" alt="arch"></p>
<p>上图是Fast R-CNN的架构。图片经过feature extractor产生feature map, 原图上运行Selective Search算法将RoI（Region of Interset）对应到feature map上，再对每个RoI进行RoI Pooling操作便得到等长的feature vector，最后通过FC后并行地进行Classifaction和BBox Regression。</p>
<p>Fast R-CNN的这一结构正是检测任务主流2-stage方法所采用的元结构的雏形。整个系统由Proposal, Feature Extractor, Object Recognition&amp;Localization几个部件组成。Proposal部分被替换成RPN(Faster R-CNN)，Feature Extractor部分使用SOTA的分类CNN网络(ResNet等），而最后的部分常常是并行的多任务结构（Mask R-CNN等）。</p>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><p>这一操作是将不同大小的RoI（feature map上）统一的过程，具体做法是将RoI等分成目标个数的网格，在每个网格上进行max pooling，就得到等长的RoI feature vector。</p>
<h3 id="Mini-batch-Sampling"><a href="#Mini-batch-Sampling" class="headerlink" title="Mini-batch Sampling"></a>Mini-batch Sampling</h3><p>文章指出SPPNet训练较慢的原因在于来自不同图片的RoI不能共享计算，因而Fast R-CNN采用这样的mini-batch采样策略：先采样N张图片，再在每张图片上采样R/N个RoI，构成R大小的mini-batch。</p>
<p>采样时，总是保持25%比例正样本（iou大于0.5），iou在0.1到0.5的作为hard example。</p>
<h3 id="Multi-task-Loss"><a href="#Multi-task-Loss" class="headerlink" title="Multi-task Loss"></a>Multi-task Loss</h3><p>得到RoI feature vector后，后续的操作是一个并行的结构，Fast R-CNN将Classification和Regression的损失统一起来，并且在Regression中用更鲁棒的Smooth L1 Loss代替L2 Loss。</p>
<h3 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h3><p>文章还发现，对于预训练的VGG网络，开放Conv部分的参数更新有助于性能的提升，而不是只更新FC层。<br>将proposal, classification, regression统一在一个框架</p>
<h2 id="Design-Evaluation"><a href="#Design-Evaluation" class="headerlink" title="Design Evaluation"></a>Design Evaluation</h2><p>文章最后还对系统结构进行了讨论：</p>
<ul>
<li>multi-loss traing相比单独训练Classification确有提升</li>
<li>Scale invariance方面，multi-scale相比single-scale精度略有提升，但带来的时间开销更大。一定程度上说明CNN结构可以内在地学习scale invariance</li>
<li>在更多的数据(VOC)上训练后，mAP是有进一步提升的</li>
<li>Softmax分类器比”one vs rest”型的SVM表现略好，引入了类间的竞争</li>
<li>更多的Proposal并不一定带来性能的提升</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Fast R-CNN是对R-CNN的改进，也是对2-stage方法的系统化、架构化。文章将Proposal, Feature Extractor, Object Recognition&amp;Localization统一在一个整体的结构中，并推进共享卷积计算以提高效率的想法演进，是最有贡献的地方。</p>
<p>论文链接：<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">https://arxiv.org/abs/1504.08083</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Fast R-CNN 是对R-CNN的改进，作者栏只有RBG一人。文章先指出了R-CNN存在的问题，再介绍了自己的改进思路。文章结构堪称典范，从现存问题，到解决方案、实验细节，再到结果分析、拓展讨论，条分缕析，值得借鉴。而且，RBG开源的代码也影响了后来大部分这一领域的工作
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
</feed>
