<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>萧爽楼</title>
  <subtitle>李家丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.ddlee.cn/"/>
  <updated>2017-08-03T14:15:45.668Z</updated>
  <id>http://blog.ddlee.cn/</id>
  
  <author>
    <name>ddlee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[源码笔记]keras源码分析之Model</title>
    <link href="http://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/"/>
    <id>http://blog.ddlee.cn/2017/07/30/源码笔记-keras源码分析之Model/</id>
    <published>2017-07-30T15:19:42.000Z</published>
    <updated>2017-08-03T14:15:45.668Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是<a href="https://github.com/fchollet/keras/blob/master/keras/engine/training.py" target="_blank" rel="external">/keras/engine/training.py</a>和<a href="https://github.com/fchollet/keras/blob/master/keras/models.py" target="_blank" rel="external">/keras/model.py</a>，要观察的类是<code>Model</code>和<code>Sequential</code>。</p>
<p>本系列第一篇：<a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">【源码笔记】keras源码分析之Tensor, Node和Layer</a><br>第二篇：<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">【源码笔记】keras源码分析之Container</a></p>
<h3 id="Model：添加了训练信息的Container"><a href="#Model：添加了训练信息的Container" class="headerlink" title="Model：添加了训练信息的Container"></a><code>Model</code>：添加了训练信息的<code>Container</code></h3><p><code>Model.compile()</code>主要完成了配置<code>optimizer</code>, <code>loss</code>, <code>metrics</code>等操作，而要执行的<code>fit</code>, <code>evaluate</code>等则不在<code>compile</code>过程中配置。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compile</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> loss_weights<span class="token operator">=</span>None<span class="token punctuation">,</span>
            sample_weight_mode<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss <span class="token operator">or</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> optimizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>sample_weight_mode <span class="token operator">=</span> sample_weight_mode
    self<span class="token punctuation">.</span>loss <span class="token operator">=</span> loss
    self<span class="token punctuation">.</span>loss_weights <span class="token operator">=</span> loss_weights

    loss_function <span class="token operator">=</span> losses<span class="token punctuation">.</span>get<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    loss_functions <span class="token operator">=</span> <span class="token punctuation">[</span>loss_function <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>loss_functions <span class="token operator">=</span> loss_functions

    <span class="token comment" spellcheck="true"># Prepare targets of model.</span>
    self<span class="token punctuation">.</span>targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>_feed_targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        shape <span class="token operator">=</span> self<span class="token punctuation">.</span>internal_output_shapes<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        name <span class="token operator">=</span> self<span class="token punctuation">.</span>output_names<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        target <span class="token operator">=</span> K<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>ndim<span class="token operator">=</span>len<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span>
                               name<span class="token operator">=</span>name <span class="token operator">+</span> <span class="token string">'_target'</span><span class="token punctuation">,</span>
                               sparse<span class="token operator">=</span>K<span class="token punctuation">.</span>is_sparse<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>K<span class="token punctuation">.</span>dtype<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>targets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_feed_targets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># Prepare metrics.</span>
    self<span class="token punctuation">.</span>metrics <span class="token operator">=</span> metrics
    self<span class="token punctuation">.</span>metrics_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>metrics_tensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># Compute total loss.</span>
    total_loss <span class="token operator">=</span> None
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        y_true <span class="token operator">=</span> self<span class="token punctuation">.</span>targets<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        y_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        loss_weight <span class="token operator">=</span> loss_weights_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">if</span> total_loss <span class="token keyword">is</span> None<span class="token punctuation">:</span>
            total_loss <span class="token operator">=</span> loss_weight <span class="token operator">*</span> output_loss
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            total_loss <span class="token operator">+=</span> loss_weight <span class="token operator">*</span> output_loss

    <span class="token keyword">for</span> loss_tensor <span class="token keyword">in</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">:</span>
        total_loss <span class="token operator">+=</span> loss_tensor

    self<span class="token punctuation">.</span>total_loss <span class="token operator">=</span> total_loss
    self<span class="token punctuation">.</span>sample_weights <span class="token operator">=</span> sample_weights
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Model</code>对象的<code>fit()</code>方法封装了<code>_fit_loop()</code>内部方法，而<code>_fit_loop()</code>方法的关键步骤由<code>_make_train_function()</code>方法完成，返回<code>history</code>对象，用于回调函数的处理。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token operator">=</span>None<span class="token punctuation">,</span> y<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：
      self<span class="token punctuation">.</span>_make_train_function<span class="token punctuation">(</span><span class="token punctuation">)</span>
      f <span class="token operator">=</span> self<span class="token punctuation">.</span>train_function
      <span class="token keyword">return</span> self<span class="token punctuation">.</span>_fit_loop<span class="token punctuation">(</span>f<span class="token punctuation">,</span> ins<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在<code>_fit_loop()</code>方法中，回调函数完成了对训练过程的监控记录等任务，<code>train_function</code>也被应用于传入的数据：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_fit_loop</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> f<span class="token punctuation">,</span> ins<span class="token punctuation">,</span> out_labels<span class="token operator">=</span>None<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
              epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span>
              val_f<span class="token operator">=</span>None<span class="token punctuation">,</span> val_ins<span class="token operator">=</span>None<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              callback_metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> initial_epoch<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>history <span class="token operator">=</span> cbks<span class="token punctuation">.</span>History<span class="token punctuation">(</span><span class="token punctuation">)</span>
    callbacks <span class="token operator">=</span> <span class="token punctuation">[</span>cbks<span class="token punctuation">.</span>BaseLogger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>callbacks <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>history<span class="token punctuation">]</span>
    callbacks <span class="token operator">=</span> cbks<span class="token punctuation">.</span>CallbackList<span class="token punctuation">(</span>callbacks<span class="token punctuation">)</span>
    out_labels <span class="token operator">=</span> out_labels <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    callbacks<span class="token punctuation">.</span>set_model<span class="token punctuation">(</span>callback_model<span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>set_params<span class="token punctuation">(</span><span class="token punctuation">{</span>
        <span class="token string">'batch_size'</span><span class="token punctuation">:</span> batch_size<span class="token punctuation">,</span>
        <span class="token string">'epochs'</span><span class="token punctuation">:</span> epochs<span class="token punctuation">,</span>
        <span class="token string">'samples'</span><span class="token punctuation">:</span> num_train_samples<span class="token punctuation">,</span>
        <span class="token string">'verbose'</span><span class="token punctuation">:</span> verbose<span class="token punctuation">,</span>
        <span class="token string">'do_validation'</span><span class="token punctuation">:</span> do_validation<span class="token punctuation">,</span>
        <span class="token string">'metrics'</span><span class="token punctuation">:</span> callback_metrics <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>on_train_begin<span class="token punctuation">(</span><span class="token punctuation">)</span>
    callback_model<span class="token punctuation">.</span>stop_training <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>initial_epoch<span class="token punctuation">,</span> epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        callbacks<span class="token punctuation">.</span>on_epoch_begin<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
        batches <span class="token operator">=</span> _make_batches<span class="token punctuation">(</span>num_train_samples<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        epoch_logs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        <span class="token keyword">for</span> batch_index<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_start<span class="token punctuation">,</span> batch_end<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
            batch_ids <span class="token operator">=</span> index_array<span class="token punctuation">[</span>batch_start<span class="token punctuation">:</span>batch_end<span class="token punctuation">]</span>
            batch_logs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            batch_logs<span class="token punctuation">[</span><span class="token string">'batch'</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch_index
            batch_logs<span class="token punctuation">[</span><span class="token string">'size'</span><span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>batch_ids<span class="token punctuation">)</span>
            callbacks<span class="token punctuation">.</span>on_batch_begin<span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> batch_logs<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># 应用传入的train_function</span>
            outs <span class="token operator">=</span> f<span class="token punctuation">(</span>ins_batch<span class="token punctuation">)</span>
            callbacks<span class="token punctuation">.</span>on_batch_end<span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> batch_logs<span class="token punctuation">)</span>
        callbacks<span class="token punctuation">.</span>on_epoch_end<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> epoch_logs<span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>history
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>_make_train_function()</code>方法从<code>optimizer</code>获取要更新的参数信息，并传入来自<code>backend</code>的<code>function</code>对象：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_make_train_function</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>train_function <span class="token keyword">is</span> None<span class="token punctuation">:</span>
        inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>_feed_inputs <span class="token operator">+</span> self<span class="token punctuation">.</span>_feed_targets <span class="token operator">+</span> self<span class="token punctuation">.</span>_feed_sample_weights
        training_updates <span class="token operator">=</span> self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>get_updates<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>_collected_trainable_weights<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>constraints<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>total_loss<span class="token punctuation">)</span>
        updates <span class="token operator">=</span> self<span class="token punctuation">.</span>updates <span class="token operator">+</span> training_updates
        <span class="token comment" spellcheck="true"># Gets loss and metrics. Updates weights at each call.</span>
        self<span class="token punctuation">.</span>train_function <span class="token operator">=</span> K<span class="token punctuation">.</span>function<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>
                                         <span class="token punctuation">[</span>self<span class="token punctuation">.</span>total_loss<span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>metrics_tensors<span class="token punctuation">,</span>
                                         updates<span class="token operator">=</span>updates<span class="token punctuation">,</span>
                                         name<span class="token operator">=</span><span class="token string">'train_function'</span><span class="token punctuation">,</span>
                                         <span class="token operator">**</span>self<span class="token punctuation">.</span>_function_kwargs<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Model</code>的其他方法<code>evaluate()</code>等，与<code>fit()</code>的结构类似。</p>
<h3 id="Sequential-构建模型的外层接口"><a href="#Sequential-构建模型的外层接口" class="headerlink" title="Sequential:构建模型的外层接口"></a><code>Sequential</code>:构建模型的外层接口</h3><p><code>Sequential</code>对象是<code>Model</code>对象的进一步封装，也是用户直接面对的接口，其<code>compile()</code>, <code>fit()</code>, <code>predict()</code>等方法与<code>Model</code>几乎一致，所不同的是添加了<code>add()</code>方法，也是我们用于构建网络的最基本操作。</p>
<p><code>Sequential.add()</code>方法的源码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 第一层必须是InputLayer对象</span>
    <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token operator">not</span> layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">:</span>
            x <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span>layer<span class="token punctuation">.</span>batch_input_shape<span class="token punctuation">,</span>
                      dtype<span class="token operator">=</span>layer<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> name<span class="token operator">=</span>layer<span class="token punctuation">.</span>name <span class="token operator">+</span> <span class="token string">'_input'</span><span class="token punctuation">)</span>
            layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>inputs <span class="token operator">=</span> topology<span class="token punctuation">.</span>get_source_inputs<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        topology<span class="token punctuation">.</span>Node<span class="token punctuation">(</span>outbound_layer<span class="token operator">=</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        output_tensor <span class="token operator">=</span> layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> <span class="token punctuation">[</span>output_tensor<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensors <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs

    self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到，<code>add()</code>方法总是确保网络的第一层为<code>InputLayer</code>对象，并将新加入的层应用于<code>outputs</code>，使之更新。因此，从本质上讲，在<code>Model</code>中添加新层还是在更新模型的<code>outputs</code>。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是&lt;a href=&quot;https://github.
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Keras" scheme="http://blog.ddlee.cn/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>[源码笔记]keras源码分析之Container</title>
    <link href="http://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/"/>
    <id>http://blog.ddlee.cn/2017/07/25/源码笔记-keras源码分析之Container/</id>
    <published>2017-07-25T14:08:44.000Z</published>
    <updated>2017-08-03T14:15:15.348Z</updated>
    
    <content type="html"><![CDATA[<p>本篇继续讨论keras的源码结构。</p>
<p><a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">第一篇源码笔记</a>中我们观察了<code>Layer</code>, <code>Tensor</code>和<code>Node</code>是如何耦合在一起的，而本篇的重点是观察多层网络构成的有向无环图（DAG）。主要涉及的文件为<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">keras/engine/topology.py</a>， 要观察的类是<code>Container</code>。</p>
<h3 id="Container对象：DAG的拓扑原型"><a href="#Container对象：DAG的拓扑原型" class="headerlink" title="Container对象：DAG的拓扑原型"></a><code>Container</code>对象：DAG的拓扑原型</h3><p>在第一篇中我们提到，Keras Tensor中增强的<code>\_keras_history</code>属性使得我们仅通过输入和输出的Tensor，就可以构建出整张计算图。而<code>Container</code>对象正是实现了这样的过程。</p>
<h4 id="计算图的构建"><a href="#计算图的构建" class="headerlink" title="计算图的构建"></a>计算图的构建</h4><p>DAG计算图的构建在<code>Container</code>对象实例化时完成，主要包括如下几个操作：</p>
<ol>
<li><p>记录<code>Container</code>的首尾连接信息</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
   layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
   self<span class="token punctuation">.</span>output_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>output_layers_node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>output_layers_tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>

<span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span>
   layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
   self<span class="token punctuation">.</span>input_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>input_layers_node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>input_layers_tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>从<code>output_tensors</code>开始反向递归构建计算图，采用广度优先的准则，本步的关键是构建<code>nodes_in_decreasing_depth</code>这一队列，这些<code>Node</code>包含的连接信息和深度信息将是后续正向传播和反向训练计算执行顺序的依据。</p>
</li>
</ol>
<pre class="line-numbers language-python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">build_map_of_graph</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">)</span><span class="token punctuation">:</span>
      layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> tensor<span class="token punctuation">.</span>_keras_history
      node <span class="token operator">=</span> layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>node_index<span class="token punctuation">]</span>
      nodes_in_progress<span class="token punctuation">.</span>add<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

      <span class="token comment" spellcheck="true"># 广度优先搜索</span>
      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          x <span class="token operator">=</span> node<span class="token punctuation">.</span>input_tensors<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          layer <span class="token operator">=</span> node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          node_index <span class="token operator">=</span> node<span class="token punctuation">.</span>node_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          tensor_index <span class="token operator">=</span> node<span class="token punctuation">.</span>tensor_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          <span class="token comment" spellcheck="true"># 递归调用</span>
          build_map_of_graph<span class="token punctuation">(</span>x<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">,</span>
                             layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index<span class="token punctuation">)</span>

      <span class="token comment" spellcheck="true"># 维护两个队列</span>
      finished_nodes<span class="token punctuation">.</span>add<span class="token punctuation">(</span>node<span class="token punctuation">)</span>
      nodes_in_progress<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>node<span class="token punctuation">)</span>
      nodes_in_decreasing_depth<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 反向构建DAG</span>
  <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
      build_map_of_graph<span class="token punctuation">(</span>x<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li>计算各节点的深度并按深度标定节点在DAG中的位置</li>
</ol>
<pre class="line-numbers language-python"><code class="language-python">  <span class="token comment" spellcheck="true"># 根据队列标定各节点的深度</span>
  <span class="token keyword">for</span> node <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>nodes_in_decreasing_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>
      depth <span class="token operator">=</span> nodes_depths<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span>node<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
      previous_depth <span class="token operator">=</span> layers_depths<span class="token punctuation">.</span>get<span class="token punctuation">(</span>node<span class="token punctuation">.</span>outbound_layer<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
      depth <span class="token operator">=</span> max<span class="token punctuation">(</span>depth<span class="token punctuation">,</span> previous_depth<span class="token punctuation">)</span>
      layers_depths<span class="token punctuation">[</span>node<span class="token punctuation">.</span>outbound_layer<span class="token punctuation">]</span> <span class="token operator">=</span> depth
      nodes_depths<span class="token punctuation">[</span>node<span class="token punctuation">]</span> <span class="token operator">=</span> depth

      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          inbound_layer <span class="token operator">=</span> node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          node_index <span class="token operator">=</span> node<span class="token punctuation">.</span>node_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          inbound_node <span class="token operator">=</span> inbound_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>node_index<span class="token punctuation">]</span>
          previous_depth <span class="token operator">=</span> nodes_depths<span class="token punctuation">.</span>get<span class="token punctuation">(</span>inbound_node<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
          nodes_depths<span class="token punctuation">[</span>inbound_node<span class="token punctuation">]</span> <span class="token operator">=</span> max<span class="token punctuation">(</span>depth <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> previous_depth<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 按深度标定各节点的位置</span>
  nodes_by_depth <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> node<span class="token punctuation">,</span> depth <span class="token keyword">in</span> nodes_depths<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> depth <span class="token operator">not</span> <span class="token keyword">in</span> nodes_by_depth<span class="token punctuation">:</span>
          nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 按深度标定各层的位置</span>
  layers_by_depth <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> layer<span class="token punctuation">,</span> depth <span class="token keyword">in</span> layers_depths<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> depth <span class="token operator">not</span> <span class="token keyword">in</span> layers_by_depth<span class="token punctuation">:</span>
          layers_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      layers_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>

  self<span class="token punctuation">.</span>layers_by_depth <span class="token operator">=</span> layers_by_depth
  self<span class="token punctuation">.</span>nodes_by_depth <span class="token operator">=</span> nodes_by_depth
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li>将整个<code>Container</code>并入<code>Node</code>以保持兼容性<pre class="line-numbers language-python"><code class="language-python">self<span class="token punctuation">.</span>outbound_nodes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
self<span class="token punctuation">.</span>inbound_nodes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
Node<span class="token punctuation">(</span>outbound_layer<span class="token operator">=</span>self<span class="token punctuation">,</span>
    inbound_layers<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    node_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    tensor_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    input_tensors<span class="token operator">=</span>self<span class="token punctuation">.</span>inputs<span class="token punctuation">,</span>
    output_tensors<span class="token operator">=</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ol>
<h3 id="计算图中的计算"><a href="#计算图中的计算" class="headerlink" title="计算图中的计算"></a>计算图中的计算</h3><p>计算在<code>Container</code>对象的<code>call()</code>方法完成，其实现又依靠内部方法<code>run_internal_graph()</code>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_internal_graph</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> masks<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
       depth_keys <span class="token operator">=</span> list<span class="token punctuation">(</span>self<span class="token punctuation">.</span>nodes_by_depth<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
       depth_keys<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
       <span class="token comment" spellcheck="true"># 依据深度</span>
       <span class="token keyword">for</span> depth <span class="token keyword">in</span> depth_keys<span class="token punctuation">:</span>
           nodes <span class="token operator">=</span> self<span class="token punctuation">.</span>nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span>
           <span class="token comment" spellcheck="true"># 对同一深度上的Node进行计算</span>
           <span class="token keyword">for</span> node <span class="token keyword">in</span> nodes<span class="token punctuation">:</span>
               layer <span class="token operator">=</span> node<span class="token punctuation">.</span>outbound_layer <span class="token comment" spellcheck="true"># Node对应的layer</span>
               reference_input_tensors <span class="token operator">=</span> node<span class="token punctuation">.</span>input_tensors
               reference_output_tensors <span class="token operator">=</span> node<span class="token punctuation">.</span>output_tensors
               computed_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
               <span class="token keyword">if</span> len<span class="token punctuation">(</span>computed_data<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>reference_input_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>
                   <span class="token comment" spellcheck="true"># 在Layer中进行计算</span>
                   <span class="token keyword">with</span> K<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
                       <span class="token keyword">if</span> len<span class="token punctuation">(</span>computed_data<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                           computed_tensor<span class="token punctuation">,</span> computed_mask <span class="token operator">=</span> computed_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                           output_tensors <span class="token operator">=</span> _to_list<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>call<span class="token punctuation">(</span>computed_tensor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
                           computed_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>computed_tensor<span class="token punctuation">]</span>
                       <span class="token keyword">else</span><span class="token punctuation">:</span>
                           computed_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> computed_data<span class="token punctuation">]</span>
                           output_tensors <span class="token operator">=</span> _to_list<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>call<span class="token punctuation">(</span>computed_tensors<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
       output_tensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
       output_masks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
       <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
           tensor<span class="token punctuation">,</span> mask <span class="token operator">=</span> tensor_map<span class="token punctuation">[</span>str<span class="token punctuation">(</span>id<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
           output_tensors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
           output_masks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
       <span class="token keyword">return</span> output_tensors<span class="token punctuation">,</span> output_masks
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>从上面的代码可以看到计算是依据深度进行的，并通过更新<code>computed_data</code>和<code>output_tensor</code>等变量完成整张图的遍历计算。</p>
<p>继续阅读系列第三篇：<a href="https://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/">【源码笔记】keras源码分析之Model</a></p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇继续讨论keras的源码结构。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Keras" scheme="http://blog.ddlee.cn/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的权重衰减</title>
    <link href="http://blog.ddlee.cn/2017/07/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/"/>
    <id>http://blog.ddlee.cn/2017/07/22/深度学习中的权重衰减/</id>
    <published>2017-07-22T15:51:13.000Z</published>
    <updated>2017-08-02T15:59:09.816Z</updated>
    
    <content type="html"><![CDATA[<p>权重衰减（weight dacay），即$$L^2$$范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自<a href="https://deeplearningbook.org" target="_blank" rel="external">The Deep Learning Book</a>。</p>
<h3 id="为什么要引入权重衰减"><a href="#为什么要引入权重衰减" class="headerlink" title="为什么要引入权重衰减"></a>为什么要引入权重衰减</h3><p>机器学习的逻辑与我们最初解决问题的思维方式恰恰相反：要解决问题，一种经典的思路是把它拆成小问题，考虑之间的依赖，然后分而治之。而机器学习的哲学是“<em>trail-error-correct</em>”：先假设一堆可能的方案，根据结果去选择/调整这些方案，直到满意。换句话说，机器学习在假设空间中搜索最符合数据的模型：以果推因，即为最大似然的想法。随着数据量的增大，我们越来越需要表达能力更强的模型，而深度学习的优势正符合这一需要：通过分布式表示带来的指数增益，深度学习模型的扩展能力几乎是无限的（详见<a href="https://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/">深度学习和分布式表示</a>）。</p>
<p>有了模型（备选模型集），有了数据，就不得不面对机器学习领域的核心问题：如何保证模型能够描述数据（拟合）和生成数据（泛化）。</p>
<p>粗略来看，有以下三种情况：</p>
<ul>
<li>我们假定的模型族不包含真实数据的生成过程：欠拟合/高偏差</li>
<li>匹配真实的数据生成过程</li>
<li>除了包含真实的生成过程，还包含了其他信息：过拟合/高方差</li>
</ul>
<p>高偏差意味着我们的模型不够准确（模型族不足以描述数据），高方差意味着我们建模了不必要的信息（训练数据的随机性带来的）。前者通过提高模型的表述能力来解决（更深的网络），后者则需要合理的正则化技术来控制。这即是著名的trade-off。</p>
<h3 id="深度学习模型的参数"><a href="#深度学习模型的参数" class="headerlink" title="深度学习模型的参数"></a>深度学习模型的参数</h3><p>对数据建模，其实是从数据中提取我们能够理解的信息。建立的模型，是从数据分布的空间到目标变量所在空间的映射。从这个角度看，我们通过模型带来的变换获得了数据的一种表示，我们认为能够理解和操作的表示。</p>
<p>为了表述这一变换，深度模型的套路是线性层施加变换，非线性层固定信息（不能平移），然后将这样的结构堆叠起来，分层提取数据特征。</p>
<p>这让我想起实变中证明定理的套路：先证明简单函数的情形，再推广到连续函数，再到勒贝格可积的函数。</p>
<p>常规的套路（MLP）在拟合普通的函数任务上能够胜任，但面对更复杂的图像等数据，就需要更灵活的网络结构。</p>
<p>非常出色的CNN, LSTM, Inception块, ResNet, DenseNet等结构，就是加入了人类的先验知识，使之更有效的提取图像/音频数据分布空间的特征。（所以Manning有次在课堂上说，机器学习事实上还是人类在学习：机器只是在求导数、做乘法，最好的模型都是人们学习出来的。）</p>
<p>人们确实设计了很多巧妙的结构来解决不同的问题，但落实到网络的层和单元上，仍是最基本的矩阵乘法、加法运算。决定模型表述能力的，也正是这些普通的乘法运算中涉及的矩阵和向量了。</p>
<h3 id="权重衰减如何起作用"><a href="#权重衰减如何起作用" class="headerlink" title="权重衰减如何起作用"></a>权重衰减如何起作用</h3><p>下面我们通过观察加入权重衰减后目标函数的梯度变化来讨论权重衰减是如何起作用的。可以跳过公式部分直接看最后一段。</p>
<p>——————————————————————————————推导部分————————————————————————————————————————————</p>
<p>简单起见，令偏置为0，模型的目标函数：</p>
<p>$$J_{1}(w; X,y)=\frac{\alpha}{2} w^T w+J(w; X,y)$$</p>
<p>对应的梯度为：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w; X,y) = \alpha w + {\nabla}_{w} J(w; X,y)$$</p>
<p>进行梯度下降，参数的更新规则为：</p>
<p>$$w = w - \epsilon (\alpha w + {\nabla}_{w} J(w; X,y)) $$</p>
<p>也就是：</p>
<p>$$w = (1 - \epsilon \alpha )w - \epsilon {\nabla}_{w} J(w; X,y)$$</p>
<p>从上式可以发现，加入权重衰减后，先对参数进行伸缩，再沿梯度下降。下面令$$x^{(1)}$$为使目标函数达到最优的参数值，在其附近考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>其中$$H$$为近似目标函数在的Hessian矩阵。当近似目标函数最小时，其梯度为$$0$$，即：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>该式也向我们说明了基于梯度的优化算法主要的信息来自Hessian矩阵。添加入权重衰减项之后，上式变为（记此时的最优点为$$w^{(2)}$$）：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w) \approx \alpha w^{(2)} + H(w^{(2)} - w^{(1)}) = 0$$</p>
<p>所以</p>
<p>$$w^{(2)} = (H + \alpha I)^{-1} H w^{(1)} $$</p>
<p>该式表明了了加入正则化对参数最优质点的影响，由Hessian矩阵和正则化系数$$\alpha$$共同决定。</p>
<p>进一步将Hessian矩阵分解，可以得到：</p>
<p>$$w^{(2)} = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^{(1)}$$</p>
<p>其中，$$Q$$为正交矩阵，$$\Lambda$$为对角矩阵。这样可以看到，<em>权重衰减的效果是沿着由$$H$$的特征向量所定义的轴缩放$$w$$</em>， 具体的伸缩因子为$$\frac{ {\lambda}_{i} }{ {\lambda}<em>i + \alpha }$$，其中$${\lambda}</em>{i}$$表示第$$i$$个特征向量对应的特征值。</p>
<p>当特征值$$\lambda$$很大（相比$$\alpha$$）时，缩放因子对权重影响较小，因而更新过程中产生的变化也不大；而当特征值较小时，$$\alpha$$的缩放作用就显现出来，将这个方向的权重衰减到0。</p>
<p>这种效果也可以由下图表示：</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习中的权重衰减/transform.png" alt="transform"></p>
<p>——————————————————————————推导部分结束————————————————————————————————————————————————————————————————————</p>
<p><em>总结来说，目标函数的Hessian矩阵（显式、隐式或者近似的）是现有优化算法进行寻优的主要依据。通过控制权重衰减的$$\alpha$$参数，我们实际上控制的是在Hessian矩阵的特征方向上以多大的幅度缩放权重，相对重要（能够显著减小目标函数）的方向上权重保留比较完好，而无助于目标函数减小的方向上权重在训练过程中逐渐地衰减掉了。而这也就是权重衰减的意义。</em></p>
<p>从宏观上来看，对目标函数来说，特征值较大的方向包含更多有关数据的信息，较小的方向则有随机性的噪声，权重衰减正是通过忽略较少信息方向的变化来对抗过拟合的。</p>
<h3 id="L-1-范数正则化"><a href="#L-1-范数正则化" class="headerlink" title="$$L^1$$范数正则化"></a>$$L^1$$范数正则化</h3><p>通过类似的推导，可以得到加入了$$L^1$$范数惩罚项对参数最优解的影响如下：</p>
<p>$$w^{(2)}<em>{i} = sign(w^{(1)}</em>{i}) max \big{|w^{(1)}<em>{i}| - \frac{\alpha}{H</em>{i,i}}, 0 \big}$$</p>
<p>相比$$L^2$$范数的影响，这是一个离散的结果，因而$$L^1$$范数惩罚会将参数推向更加稀疏的解。这种稀疏性质常被用作特征选择。</p>
<h3 id="权重衰减的贝叶斯解释"><a href="#权重衰减的贝叶斯解释" class="headerlink" title="权重衰减的贝叶斯解释"></a>权重衰减的贝叶斯解释</h3><p>在贝叶斯统计的框架下，常用的推断策略是最大后验点估计(Maximum A Posteriori, MAP)。有如下的推断公式（由贝叶斯定律导出）：</p>
<p>$${\theta}_{MAP} = argmax p(\theta | x) = argmax (log p( x | \theta) + log p(\theta))$$</p>
<p>上式右边第一项是标准的对数似然项，而第二项对应着先验分布。</p>
<p>在这样的视角下，我们只进行最大似然估计是不够的，还要考虑先验$$p(\theta)$$的分布。而当假定参数为正态分布$$N(w; 0, \frac{1}{\lambda}I^2)$$时，带入上式（$$\theta$$为参数），即可发现第二项的结果正比于权重衰减惩罚项$$\lambda w^T w$$，加上一个不依赖于$$w$$也不影响学习过程的项。于是，具有高斯先验权重的MAP贝叶斯推断对应着权重衰减。</p>
<h3 id="权重衰减与提前终止"><a href="#权重衰减与提前终止" class="headerlink" title="权重衰减与提前终止"></a>权重衰减与提前终止</h3><p>提前终止也是一种正则化技术，其想法简单粗暴：每个epoch之后在验证集上评估结果，当验证集误差不再下降的时候，我们认为模型已经尽它所能了，于是终止训练过程。</p>
<p>提前终止以牺牲一部分训练数据来作为验证数据来的代价来对抗过拟合，其逻辑是实证主义的。</p>
<p>然而，在二次近似和简单梯度下降的情形下，可以观察到提前终止可以有相当于权重衰减的效果。</p>
<p>我们仍考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>记最优参数点为$$w^{(1)}$$，其梯度为：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>不加入正则化项，其梯度下降的更新策略（从第$$\tau-1$$步到$$\tau$$步）为：</p>
<p>$$ w^{(\tau)} = w^{\tau - 1)} - \epsilon H (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>累加得到</p>
<p>$$ w^{(\tau)} - w^{(1)} = (I - \epsilon H) (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>将Hessian矩阵分解，得到如下形式</p>
<p>$$ w^{(\tau)} = Q[I - (I - \epsilon \Lambda) ^ {\tau}] Q^T w^{(1)} $$</p>
<p>将加入正则化项的权重影响改写为</p>
<p>$$ w^{(2)} = Q[I - (\Lambda + \alpha I) ^ {-1} \alpha] Q^T w^{(1)} $$</p>
<p>对比可以得到，如果超参数$$\epsilon, \alpha, \tau$$满足</p>
<p>$$ (I - \epsilon \Lambda) ^ {\tau} = (\Lambda + \alpha I) ^ {-1} \alpha $$</p>
<p>则提前终止将与权重衰减有相当的效果。具体的，即第$$\tau$$步结束的训练过程将到达超参数为$$\alpha$$的$$L^2$$正则化得到的最优点。</p>
<p>但提前终止带来的好处是，我们不再需要去找合适的超参数$$\alpha$$，而只需要制定合理的终止策略（如3个epoch均不带来验证集误差的减小即终止训练），在训练成本的节约上，还是很值得的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;权重衰减（weight dacay），即$$L^2$$范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自&lt;a href=&quot;https://deeplearningbook.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Th
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[源码笔记]keras源码分析之Layer、Tensor和Node</title>
    <link href="http://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/"/>
    <id>http://blog.ddlee.cn/2017/07/15/源码笔记-keras源码分析之Layer、Tensor和Node/</id>
    <published>2017-07-15T12:04:54.000Z</published>
    <updated>2017-08-03T14:13:45.334Z</updated>
    
    <content type="html"><![CDATA[<p>Keras架构的主要逻辑实现在<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">/keras/engine/topology.py</a>中，主要有两个基类<code>Node()</code>和<code>Layer()</code>，一个重要函数<code>Input()</code>。具体地，</p>
<ul>
<li><code>Layer()</code>是一个计算层的抽象，完成网络中对Tensor的计算过程；</li>
<li><code>Node()</code>描述两个层之间连接关系的抽象，配合<code>Layer()</code>构建DAG；</li>
<li><code>Input()</code>实例化一个特殊的<code>Layer</code>(<code>InputLayer</code>)，将<code>backend</code>（TensorFlow或Theano）建立的Tensor对象转化为Keras Tensor对象。</li>
</ul>
<h3 id="Keras-Tensor：-增强版Tensor"><a href="#Keras-Tensor：-增强版Tensor" class="headerlink" title="Keras Tensor： 增强版Tensor"></a>Keras Tensor： 增强版Tensor</h3><p>相比原始的TensorFlow或者Theano的张量对象，Keras Tensor加入了如下两个属性，以使Tensor中包含了自己的来源和规模信息：</p>
<ul>
<li>_Keras_history: 保存了最近一个应用于这个Tensor的Layer</li>
<li>_keras_shape: 标准化的Keras shape接口</li>
</ul>
<p>当使用Keras建立深度网络时，传入的数据首先要经过<code>Input()</code>函数。在<code>Input()</code>函数中，实例化一个<code>InputLayer()</code>对象，并将此<code>Layer()</code>对象作为第一个应用于传入张量的Layer，置于<code>_keras_history</code>属性中。此外，<code>InputLayer()</code>和<code>Input()</code>还会对传入的数据进行规模检查和变换等，使之符合后续操作的要求。</p>
<p>代码上实现如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Input</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  input_layer <span class="token operator">=</span> InputLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
  outputs <span class="token operator">=</span> InputLayer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensor
  <span class="token keyword">return</span> outputs

<span class="token keyword">class</span> <span class="token class-name">InputLayer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_tensor<span class="token punctuation">.</span>_keras_history <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    Node<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在下面我们将看到，加入的<code>_keras_history</code>属性在计算图的构建上所起的作用是关键的。仅通过输入和输出的Tensor，我们可以构建出整张计算图。但这样的代价是Tensor对象太重了，包含了Layer的信息。</p>
<h3 id="Node对象：层与层之间链接的抽象"><a href="#Node对象：层与层之间链接的抽象" class="headerlink" title="Node对象：层与层之间链接的抽象"></a><code>Node</code>对象：层与层之间链接的抽象</h3><p>若考虑<code>Layer</code>对象抽象的是完成计算的神经元胞体，则<code>Node</code>对象是对神经元树突结构的抽象。其内聚的主要信息是：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Node</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outbound_layer<span class="token punctuation">,</span>
              inbound_layers<span class="token punctuation">,</span> node_indices<span class="token punctuation">,</span> tensor_indices<span class="token punctuation">,</span>
              input_tensors<span class="token punctuation">,</span> output_tensors<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>其中<code>outbound_layer</code>是施加计算（使<code>input_tensors</code>变为<code>output_tensors</code>）的层，<code>inbound_layers</code>对应了<code>input_tensors</code>来源的层，而<code>node_indices</code>和<code>tensor_indices</code>则记录了<code>Node</code>和<code>Layer</code>之间的标定信息。</p>
<p><code>Node</code>对象总在<code>outbound_layer</code>被执行时创建，并加入<code>outbound_layer</code>的<code>inbound_nodes</code>属性中。在<code>Node</code>对象的表述下，A和B两个层产生连接关系时，<code>Node</code>对象被建立，并被加入<code>A.outbound_nodes</code>和<code>B.inbound_nodes</code>。</p>
<h3 id="Layer对象：计算层的抽象"><a href="#Layer对象：计算层的抽象" class="headerlink" title="Layer对象：计算层的抽象"></a><code>Layer</code>对象：计算层的抽象</h3><p><code>Layer</code>对象是对网络中神经元计算层的抽象，实例化需要如下参数：</p>
<pre class="line-numbers language-python"><code class="language-python">allowed_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'input_shape'</span><span class="token punctuation">,</span>
                  <span class="token string">'batch_input_shape'</span><span class="token punctuation">,</span>
                  <span class="token string">'batch_size'</span><span class="token punctuation">,</span>
                  <span class="token string">'dtype'</span><span class="token punctuation">,</span>
                  <span class="token string">'name'</span><span class="token punctuation">,</span>
                  <span class="token string">'trainable'</span><span class="token punctuation">,</span>
                  <span class="token string">'weights'</span><span class="token punctuation">,</span>
                  <span class="token string">'input_dtype'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># legacy</span>
                  <span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>大部分与传入数据的类型和规模相关，<code>trainable</code>表征该层是否需要更新权重。此外，还有<code>inbound_nodes</code>和<code>outbound_nodes</code>属性来标定与<code>Node</code>对象的链接。</p>
<p><code>Layer</code>对象最重要的方法是<code>__call__()</code>，主要完成如下三件事情：</p>
<ol>
<li><p>验证传入数据的合法性，通过调用内部方法实现：<code>self.assert_input_compatibility(inputs)</code></p>
</li>
<li><p>进行计算<code>outputs = self.call(inputs, ...)</code>，被其子类具体实现，如<code>Linear</code>, <code>Dropout</code>等</p>
</li>
<li><p>更新Tensor中的<code>_keras_history</code>属性，记录该次计算操作，通过内部方法<code>_add_inbound_nodes()</code>实现</p>
</li>
</ol>
<p>方法<code>_add_inbound_nodes()</code>对Tensor的更新是构建<code>Layer</code>之间关系的关键操作，其主要代码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> x <span class="token keyword">in</span> input_tensors<span class="token punctuation">:</span>
    <span class="token keyword">if</span> hasattr<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'_keras_history'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        inbound_layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
        inbound_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>inbound_layer<span class="token punctuation">)</span>
        node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
        tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Node对象的建立过程中将更新self的inbound_nodes属性</span>
Node<span class="token punctuation">(</span>self<span class="token punctuation">,</span>
    inbound_layers<span class="token operator">=</span>inbound_layers<span class="token punctuation">,</span>
    node_indices<span class="token operator">=</span>node_indices<span class="token punctuation">,</span>
    tensor_indices<span class="token operator">=</span>tensor_indices<span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>output_tensors<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     output_tensors<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>_keras_history <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上段代码取出<code>input_tensor</code>的<code>_keras_history</code>属性，建立新的<code>Node</code>，并将当前<code>Layer</code>的信息更新到计算得到的<code>output_tensor</code>中。</p>
<h3 id="实例：Node-Tensor和Layer间连接关系的表征"><a href="#实例：Node-Tensor和Layer间连接关系的表征" class="headerlink" title="实例：Node,Tensor和Layer间连接关系的表征"></a>实例：<code>Node</code>,<code>Tensor</code>和<code>Layer</code>间连接关系的表征</h3><p>下面通过代码来说明三者之间的关系，来自于测试代码：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 建立新的keras Tensor</span>
a <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'input_a'</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'input_b'</span><span class="token punctuation">)</span>

a_layer<span class="token punctuation">,</span> a_node_index<span class="token punctuation">,</span> a_tensor_index <span class="token operator">=</span> a<span class="token punctuation">.</span>_keras_history
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>a_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>
<span class="token keyword">assert</span> a_tensor_index <span class="token keyword">is</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># node和layer之间的关系</span>
node <span class="token operator">=</span> a_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>a_node_index<span class="token punctuation">]</span>
<span class="token keyword">assert</span> node<span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> a_layer

<span class="token comment" spellcheck="true"># 建立连接层，将Tensor传入</span>
dense <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'dense_1'</span><span class="token punctuation">)</span>
a_2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
b_2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>b<span class="token punctuation">)</span>

<span class="token keyword">assert</span> len<span class="token punctuation">(</span>dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>dense<span class="token punctuation">.</span>outbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># 与张量a关联的Node</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>inbound_layers <span class="token operator">==</span> <span class="token punctuation">[</span>a_layer<span class="token punctuation">]</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> dense
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>input_tensors <span class="token operator">==</span> <span class="token punctuation">[</span>a<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 与张量b关联的Node</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>inbound_layers <span class="token operator">==</span> <span class="token punctuation">[</span>b_layer<span class="token punctuation">]</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> dense
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>input_tensors <span class="token operator">==</span> <span class="token punctuation">[</span>b<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>keras利用<code>Node</code>对象描述<code>Layer</code>之间的连接关系，并在<code>Tensor</code>中记录其来源信息。在<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">下篇</a>中，我们将看到keras如何利用这些抽象和增强属性构建DAG，并实现前向传播和反向训练的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Keras架构的主要逻辑实现在&lt;a href=&quot;https://github.com/fchollet/keras/blob/master/keras/engine/topology.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;/keras/eng
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Keras" scheme="http://blog.ddlee.cn/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow最佳实践：试验管理</title>
    <link href="http://blog.ddlee.cn/2017/07/11/Tensorflow%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E8%AF%95%E9%AA%8C%E7%AE%A1%E7%90%86/"/>
    <id>http://blog.ddlee.cn/2017/07/11/Tensorflow最佳实践：试验管理/</id>
    <published>2017-07-11T12:53:27.000Z</published>
    <updated>2017-08-01T13:00:20.447Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要记录使用TensorFlow训练模型中与试验管理相关的最佳实践，主要包括模型训练的大致代码框架、模型的保存与恢复、训练过程的监测、随机性的控制等。主要材料来自<a href="https://web.stanford.edu/class/cs20si/index.html" target="_blank" rel="external">CS 20SI: Tensorflow for Deep Learning Research</a>。</p>
<h3 id="TensorFlow代码框架"><a href="#TensorFlow代码框架" class="headerlink" title="TensorFlow代码框架"></a>TensorFlow代码框架</h3><p>使用TensorFlow构建深度网络模型大致包括数据预处理、图的构建、模型训练、模型推断与评估等部分，大致的代码框架如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment" spellcheck="true"># Data</span>
X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_input<span class="token punctuation">]</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_output<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Parameters</span>
W <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>n_input<span class="token punctuation">,</span> n_output<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>n_output<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Define model</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span> b
y_pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
cost <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax_cross_entropy_with_logits<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y_true<span class="token punctuation">)</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>GradientDescentOptimizer<span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Training</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    tf<span class="token punctuation">.</span>initialize_all_variables<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> x_data<span class="token punctuation">,</span> Y<span class="token punctuation">:</span> y_data<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Prediction</span>
y_test <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型的保存与恢复"><a href="#模型的保存与恢复" class="headerlink" title="模型的保存与恢复"></a>模型的保存与恢复</h3><p>一个很深的网络训练成本是比较高的，因而将模型定期保存（写入硬盘）则有必要。这里的模型，实际上是有组织的一批数据，包括图的结构描述、参数当前值等。因而我们要保存的不仅是模型，还有模型当前的运行状态，实际上每一次保存可以作为一个还原点。</p>
<h4 id="tf-train-Saver类"><a href="#tf-train-Saver类" class="headerlink" title="tf.train.Saver类"></a>tf.train.Saver类</h4><p>使用<code>tf.train.Saver</code>类需传入以下参数：<code>tf.train.Saver.save(sess, save_path, global_step=step)</code>。</p>
<p>首先定义步数变量：<code>self​.​global_step ​=​ tf​.​Variable​(​0​,​ dtype​=​tf​.​int32​,​ trainable​=​False​,name​=​&#39;global_step&#39;)</code></p>
<p>在模型训练的过程中插入还原点的保存：</p>
<pre class="line-numbers language-python"><code class="language-python">self​<span class="token punctuation">.</span>​optimizer ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​train​<span class="token punctuation">.</span>​GradientDescentOptimizer​<span class="token punctuation">(</span>​self​<span class="token punctuation">.</span>​lr​<span class="token punctuation">)</span><span class="token punctuation">.</span>​minimize​<span class="token punctuation">(</span>​self​<span class="token punctuation">.</span>​loss​<span class="token punctuation">,</span>global_step​<span class="token operator">=</span>​self​<span class="token punctuation">.</span>​global_step<span class="token punctuation">)</span>
saver ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​train​<span class="token punctuation">.</span>​Saver​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​
​<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
 sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​tf​<span class="token punctuation">.</span>​global_variables_initializer​<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
 average_loss ​<span class="token operator">=</span>​​<span class="token number">0.0</span>
 <span class="token keyword">for</span>​ index ​<span class="token keyword">in</span>​ range​<span class="token punctuation">(</span>​num_train_steps​<span class="token punctuation">)</span><span class="token punctuation">:</span>            
    batch ​<span class="token operator">=</span>​ batch_gen​<span class="token punctuation">.</span>​next​<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_batch​<span class="token punctuation">,</span>​ _ ​<span class="token operator">=</span>​ sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span><span class="token punctuation">[</span>​model​<span class="token punctuation">.</span>​loss​<span class="token punctuation">,</span>​ model​<span class="token punctuation">.</span>​optimizer​<span class="token punctuation">]</span><span class="token punctuation">,</span> feed_dict​<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
    average_loss ​<span class="token operator">+=</span>​ loss_batch
    <span class="token comment" spellcheck="true"># Save model every 1000 steps</span>
    ​<span class="token keyword">if</span>​​<span class="token punctuation">(</span>​index ​<span class="token operator">+</span>​​<span class="token number">1</span>​<span class="token punctuation">)</span>​​<span class="token operator">%</span>​​<span class="token number">1000</span>​​<span class="token operator">==</span>​​<span class="token number">0</span><span class="token punctuation">:</span>
      saver​<span class="token punctuation">.</span>​save​<span class="token punctuation">(</span>​sess​<span class="token punctuation">,</span>​​<span class="token string">'checkpoints/model'</span>​<span class="token punctuation">,</span>​ global_step​<span class="token operator">=</span>​model​<span class="token punctuation">.</span>​global_step<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在训练过程中，在<code>checkpoints</code>路径下会存储一系列的还原点文件，要恢复session到某个还原点，可使用如下代码：<code>saver.restore(sess, &#39;checkpoints/name_of_the_checkpoint&#39;)</code>。</p>
<h4 id="Keras封装：keras-callbacks-ModelCheckpoint"><a href="#Keras封装：keras-callbacks-ModelCheckpoint" class="headerlink" title="Keras封装：keras.callbacks.ModelCheckpoint()"></a>Keras封装：<code>keras.callbacks.ModelCheckpoint()</code></h4><p>Keras对TensorFlow进行了高层的封装，使用一系列回调函数<code>keras.callbacks.Callback()</code>来进行试验管理。</p>
<p>模型保存<code>ModelCheckpoint()</code>需要传入的参数：<br><code>keras.callbacks.ModelCheckpoint(filepath, monitor=&#39;val_loss&#39;, verbose=0, save_best_only=False, save_weights_only=False, mode=&#39;auto&#39;, period=1)</code></p>
<p>实际的使用中，将上述回调函数类传入<code>model.fit()</code>过程即可：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> ModelCheckpoint

model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'uniform'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Activation<span class="token punctuation">(</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'rmsprop'</span><span class="token punctuation">)</span>
checkpointer <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>filepath<span class="token operator">=</span><span class="token string">'/checkpoints/weights.hdf5'</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> Y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>checkpointer<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型训练过程的监测"><a href="#模型训练过程的监测" class="headerlink" title="模型训练过程的监测"></a>模型训练过程的监测</h3><p>训练过程中，我们常常需要提取阶段性的信息来评估模型是否符合预期效果。</p>
<h4 id="tf-summary"><a href="#tf-summary" class="headerlink" title="tf.summary"></a><code>tf.summary</code></h4><p>首先创建想要观察指标的<code>tf.summary</code>对象：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"summaries"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>scalar<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span>
  tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>scalar<span class="token punctuation">(</span><span class="token string">"accuracy"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>accuracy<span class="token punctuation">)</span>
  tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>histogram<span class="token punctuation">(</span><span class="token string">"histogram loss"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span>
  <span class="token comment" spellcheck="true"># merge them all</span>
  self<span class="token punctuation">.</span>summary_op <span class="token operator">=</span> tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>merge_all<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>tf.summary</code>是一种operation，因而可以随训练过程一同运行：<br><code>loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)</code></p>
<p>最后，将summary加入writer以写入文件：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
  writer ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​summary​<span class="token punctuation">.</span>​FileWriter​<span class="token punctuation">(</span>​<span class="token string">'./summary'</span>​<span class="token punctuation">,</span>​ sess​<span class="token punctuation">.</span>​graph<span class="token punctuation">)</span>
  <span class="token keyword">for</span>​ index ​<span class="token keyword">in</span>​ range​<span class="token punctuation">(</span>​num_train_steps​<span class="token punctuation">)</span><span class="token punctuation">:</span>
    writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary<span class="token punctuation">,</span> global_step<span class="token operator">=</span>step<span class="token punctuation">)</span>
  writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这样，就可以用TensorBoard监测我们关心的指标在训练过程中的变化情况。</p>
<h4 id="Keras封装：keras-callbacks-TensorBoard"><a href="#Keras封装：keras-callbacks-TensorBoard" class="headerlink" title="Keras封装：keras.callbacks.TensorBoard()"></a>Keras封装：<code>keras.callbacks.TensorBoard()</code></h4><p>Keras同样将TensorBoard封装成回调函数的形式，在模型训练时进行调用即可：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> TensorBoard

tensorboard <span class="token operator">=</span> TensorBoard<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token string">"./logs"</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> Y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>tensorboard<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="随机性的控制"><a href="#随机性的控制" class="headerlink" title="随机性的控制"></a>随机性的控制</h3><p>TensorFlow中随机性的控制分为operation和graph两个层面。</p>
<h4 id="Operation层面"><a href="#Operation层面" class="headerlink" title="Operation层面"></a>Operation层面</h4><p>在Operation层面中，建立随机seed之后，新建立的Session每一次调用<code>sess.run()</code>都会遵循同一随机状态：</p>
<pre class="line-numbers language-python"><code class="language-python">c ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span>​<span class="token punctuation">,</span>​ seed​<span class="token operator">=</span>​<span class="token number">2</span><span class="token punctuation">)</span>

<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  <span class="token keyword">print</span>​ sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># >> 3.57493</span>

<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  <span class="token keyword">print</span>​ sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># >> 3.57493</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>而且，不同的operation可以保存自己的seed:</p>
<pre class="line-numbers language-python"><code class="language-python">c ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span>​<span class="token punctuation">,</span>​ seed​<span class="token operator">=</span>​<span class="token number">1</span><span class="token punctuation">)</span>
d ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span>​<span class="token punctuation">,</span>​ seed​<span class="token operator">=</span>​<span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span> ​​<span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​d<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="Graph层面"><a href="#Graph层面" class="headerlink" title="Graph层面"></a>Graph层面</h4><p>在Graph层面，整张图公用一个随机状态，多次运行同一图模型的计算，其随机状态保持一致。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span>​ tensorflow ​<span class="token keyword">as</span>​ tf

tf​<span class="token punctuation">.</span>​set_random_seed​<span class="token punctuation">(</span>​<span class="token number">2</span><span class="token punctuation">)</span>

c ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span><span class="token punctuation">)</span>
d ​<span class="token operator">=</span>​ tf​<span class="token punctuation">.</span>​random_uniform​<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>​​<span class="token operator">-</span>​<span class="token number">10</span>​<span class="token punctuation">,</span>​​<span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">with</span>​ tf​<span class="token punctuation">.</span>​Session​<span class="token punctuation">(</span><span class="token punctuation">)</span>​​ <span class="token keyword">as</span>​ sess<span class="token punctuation">:</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​c<span class="token punctuation">)</span>
  sess​<span class="token punctuation">.</span>​run​<span class="token punctuation">(</span>​d<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要记录使用TensorFlow训练模型中与试验管理相关的最佳实践，主要包括模型训练的大致代码框架、模型的保存与恢复、训练过程的监测、随机性的控制等。主要材料来自&lt;a href=&quot;https://web.stanford.edu/class/cs20si/index.h
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="best practice" scheme="http://blog.ddlee.cn/tags/best-practice/"/>
    
      <category term="Tensorflow" scheme="http://blog.ddlee.cn/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Python最佳实践：遍历列表</title>
    <link href="http://blog.ddlee.cn/2017/06/29/Python%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%9A%E9%81%8D%E5%8E%86%E5%88%97%E8%A1%A8/"/>
    <id>http://blog.ddlee.cn/2017/06/29/Python最佳实践：遍历列表/</id>
    <published>2017-06-29T14:12:50.000Z</published>
    <updated>2017-08-01T13:00:37.746Z</updated>
    
    <content type="html"><![CDATA[<h3 id="enumerate-遍历索引和值"><a href="#enumerate-遍历索引和值" class="headerlink" title="enumerate(): 遍历索引和值"></a>enumerate(): 遍历索引和值</h3><p>对列表进行遍历操作时，常也要用到当前遍历项的索引值：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>flavor_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  flavor <span class="token operator">=</span> flaver<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%d: %s'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> flavor<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>这种写法既要取出列表长度，又要根据索引取列表值。但要改用<code>enumerate()</code>函数，则可同时取出索引值和遍历项值：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> flavor <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>flavor_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%d: %s'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> flavor<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="zip-并行遍历多个列表"><a href="#zip-并行遍历多个列表" class="headerlink" title="zip(): 并行遍历多个列表"></a>zip(): 并行遍历多个列表</h3><p>我们有多个长度相同的列表，需要在同一索引下对遍历项值进行操作：</p>
<pre class="line-numbers language-python"><code class="language-python">names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Cecilia'</span><span class="token punctuation">,</span> <span class="token string">'Lise'</span><span class="token punctuation">,</span> <span class="token string">'Marie'</span><span class="token punctuation">]</span>
letters <span class="token operator">=</span> <span class="token punctuation">[</span>len<span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> names<span class="token punctuation">]</span>

longest_name <span class="token operator">=</span> None
max_letters <span class="token operator">=</span> <span class="token number">0</span>

<span class="token keyword">for</span> i<span class="token punctuation">,</span> name <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>names<span class="token punctuation">)</span><span class="token punctuation">:</span>
  count <span class="token operator">=</span> letters<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
  <span class="token keyword">if</span> count <span class="token operator">></span> max_letters<span class="token punctuation">:</span>
    longest_name <span class="token operator">=</span> name
    max_letters <span class="token operator">=</span> count
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>由上可见，name和letter通过索引值关联起来，而使用<code>zip()</code>函数，可免去根据索引取值的过程：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> count <span class="token keyword">in</span> zip<span class="token punctuation">(</span>names<span class="token punctuation">,</span> letters<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">if</span> count <span class="token operator">></span> max_letters<span class="token punctuation">:</span>
    longest_name <span class="token operator">=</span> name
    max_letters <span class="token operator">=</span> count
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>要注意的是，当多个列表长度不一时，到达最短列表的末尾时，遍历停止。</p>
<p>而且，<code>enumerate()</code>和<code>zip()</code>返回的对象都是lazy generator，相对来说更加高效。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;enumerate-遍历索引和值&quot;&gt;&lt;a href=&quot;#enumerate-遍历索引和值&quot; class=&quot;headerlink&quot; title=&quot;enumerate(): 遍历索引和值&quot;&gt;&lt;/a&gt;enumerate(): 遍历索引和值&lt;/h3&gt;&lt;p&gt;对列表进行遍历操
    
    </summary>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/categories/Programming/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="best practice" scheme="http://blog.ddlee.cn/tags/best-practice/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Accurate, Large Minibatch SGD: Training ImageNet in One Hour</title>
    <link href="http://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/"/>
    <id>http://blog.ddlee.cn/2017/06/14/论文笔记-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/</id>
    <published>2017-06-14T14:43:41.000Z</published>
    <updated>2017-06-14T15:23:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="external">Accurate, Large Minibatch SGD: Training ImageNet in One Hour</a></p>
<p>这篇文章在各处都有很广泛的讨论，作为实验经验并不多的小白，将文中tricks只做些记录。</p>
<h3 id="Linear-Scaling-Rule"><a href="#Linear-Scaling-Rule" class="headerlink" title="Linear Scaling Rule"></a>Linear Scaling Rule</h3><p>进行大批量的Minibatch SGD时会有批量越大，误差越大的问题。本文提出的Linear Scaling Rule正是试图解决这一问题。</p>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>设想两个情景：一是在一次参数更新中使用kn个样本梯度，二是分为k次更新，每次取n个样本梯度。</p>
<p>第一种情景的参数更新公式：<br>
$$w_t+1^{(1)} = w_t^{(1)} - \mu^{(1)} \frac{1}{kn} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t)$$
</p>
<p>第二种情景的参数更新公式：<br>
$$w_t+k^{(2)} = w_t^{(2)} - \mu^{(2)} \frac{1}{n} \sum_{j \leq k} \sum \bigtriangledown l(x, w_t+j)$$
</p>
<p>由上面可以看出，主要的区别是梯度平均时批量的大小不同，前者为kn，后者为每次n，更新k次。</p>
<p>再假设双重求和号内项变化不大时，为使情景二更新k次（即使用同样数量的样本）之后参数与情景一类似，我们自然要将学习速率$\mu$线性提升。</p>
<h3 id="Gradual-Warmup"><a href="#Gradual-Warmup" class="headerlink" title="Gradual Warmup"></a>Gradual Warmup</h3><p>上面提到的Linear Scaling Rule使用的假设是梯度变化不大。但在训练初期，参数随机初始化，梯度变化很大，因而Linear Scaling Rule不再适用。在实践中，可以使学习速率在初始时较小，在经过几个epoch训练后再升至与kn批量相应的大小。</p>
<h3 id="BN-statistics"><a href="#BN-statistics" class="headerlink" title="BN statistics"></a>BN statistics</h3><p>在分布式训练的系统中，对于BN中要估计的均值和方差，文中给出的建议是对所有worker上的样本计算均值和方差，而不是每个worker单独计算。</p>
<h3 id="Weight-Decay"><a href="#Weight-Decay" class="headerlink" title="Weight Decay"></a>Weight Decay</h3><p>由于weight decay的存在，Linear Scaling Rule最好用于学习速率，而非用于Loss Function</p>
<h3 id="Momentum-Correction"><a href="#Momentum-Correction" class="headerlink" title="Momentum Correction"></a>Momentum Correction</h3><p>加入Linear Scaling Rule之后，适用动量加速的SGD需要进行动量更正。</p>
<h3 id="Data-Shuffling"><a href="#Data-Shuffling" class="headerlink" title="Data Shuffling"></a>Data Shuffling</h3><p>在分布式的系统中，先进行Data Shuffling，再分配数据到每个worker上。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1706.02677&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Accurate, Large Minibatch SGD: Training ImageNet in One Hour
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>自用LaTeX中英文简历模板</title>
    <link href="http://blog.ddlee.cn/2017/06/14/%E8%87%AA%E7%94%A8LaTeX%E4%B8%AD%E8%8B%B1%E6%96%87%E5%BB%BA%E7%AB%8B%E6%A8%A1%E6%9D%BF/"/>
    <id>http://blog.ddlee.cn/2017/06/14/自用LaTeX中英文建立模板/</id>
    <published>2017-06-14T10:58:41.000Z</published>
    <updated>2017-06-14T10:58:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在<a href="https://www.sharelatex.com/templates/cv-or-resume/professional-cv" target="_blank" rel="external">ShareLaTeX</a>上分享的模板。</p>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li>Github仓库：<a href="https://github.com/ddlee96/latex_cv_template" target="_blank" rel="external">ddlee96/latex_cv_template</a></li>
<li>编译引擎： XeLaTeX</li>
<li>下载地址： <a href="https://github.com/ddlee96/latex_cv_template/releases/tag/0.1" target="_blank" rel="external">v0.1</a></li>
<li>压缩包内包含.tex文件和所用字体文件，解压后修改.tex文件再编译即可。</li>
<li>在Ubuntu 16.04, Texlive 2016环境下测试通过。</li>
<li>英文字体: Fontin，中文字体：方正兰亭黑</li>
</ul>
<h4 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h4><ul>
<li>.tex代码：Apache 2.0</li>
<li>字体： 仅供个人使用</li>
</ul>
<h4 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h4><h5 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h5><p><img src="http://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv-1.png" alt="en"></p>
<h5 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h5><p><img src="http://static.ddlee.cn/static/img/自用LaTeX中英文建立模板/cv_zh-1.png" alt="zh"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分享一套自用的LaTeX中英文简历模板，改编自Alessandro Plasmati在&lt;a href=&quot;https://www.sharelatex.com/templates/cv-or-resume/professional-cv&quot; target=&quot;_blank&quot; re
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/tags/Individual-Development/"/>
    
      <category term="LaTeX" scheme="http://blog.ddlee.cn/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]On the Effects and Weight Normalization in GAN</title>
    <link href="http://blog.ddlee.cn/2017/06/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-Effects-and-Weight-Normalization-in-GAN/"/>
    <id>http://blog.ddlee.cn/2017/06/10/论文笔记-On-the-Effects-and-Weight-Normalization-in-GAN/</id>
    <published>2017-06-10T14:01:21.000Z</published>
    <updated>2017-06-14T14:57:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1704.03971" target="_blank" rel="external">On the Effects and Weight Normalization in GAN</a></p>
<p>本文探索了参数标准化(Weight Normalization)这一技术在GAN中的应用。BN在mini-batch的层级上计算均值和方差，容易引入噪声，并不适用于GAN这种生成模型，而WN对参数进行重写，引入噪声更少。</p>
<p>我觉得本文的亮点有二：</p>
<h3 id="1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变"><a href="#1-提出T-ReLU并配合Affine-Tranformation使在引入WN后网络的表达能力维持不变" class="headerlink" title="1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变"></a>1. 提出T-ReLU并配合Affine Tranformation使在引入WN后网络的表达能力维持不变</h3><p>朴素的参数标准化层有如下的形式：<br>
$$y=\frac{{w}^{T}x}{\|w\|}$$
<br>文中称这样形式的层为“strict weight-normalized layer”。若将线性层换为这样的层，网络的表达能力会下降，因而需要添加如下的affine transformation:</p>

$$y=\frac{{w}^{T}x}{\|w\|} \gamma + \beta$$

<p>用于恢复网络的表达能力。</p>
<p>将上述变换带入ReLU，简化后可以得到如下T-ReLu:<br>$$TReLU_\alpha (x) = ReLU(x-\alpha) + \alpha$$</p>
<p>文章的一个重要结论是，在网络的最后一层加入affine transformation层之后，堆叠的“线性层+ReLU”与“strict weight-normalized layer + T-ReLU”表达能力相同（在附录中给出证明）。</p>
<p>下面L表示线性层，R表示ReLU，TR表示TReLU，A表示affine transformation，S表示上述的strict weight-normalized layer。</p>
<p>证明的大致思路是，在ReLU与线性层之间加入affine transformation层，由于线性层的存在，affine transformation带来的效果会被吸收（相当于多个线性层叠在一起还是线性层），网络表达能力不变。而”L+R+A”的结构可以等价于”S+TR+A”。如此递归下去，即可得到结论。个人认为相当于把线性层中的bias转嫁成了TReLU中的threshold（即$\alpha$）。</p>
<h3 id="2-提出对生成图形的评估指标"><a href="#2-提出对生成图形的评估指标" class="headerlink" title="2. 提出对生成图形的评估指标"></a>2. 提出对生成图形的评估指标</h3><p>生成式模型的生成效果常常难以评价。DcGAN给出的结果也是生成图片的对比。本文中提出一个评价生成效果的指标，且与人的主观评价一致。</p>
<p>评价的具体指标是生成图片与测试集图片的欧氏距离，评价的对象是生成器是Generator。有如下形式：</p>

$$\frac{1}{m} \sum_{i=1}^{m} min_z {\|G(z)-x^{(i)}\|}^2$$

<p>其中的$min$指使用梯度下降方法等使生成图片的效果最好。但事实上这样做开销很高。</p>
<h3 id="PyTorch实现"><a href="#PyTorch实现" class="headerlink" title="PyTorch实现"></a>PyTorch实现</h3><p>作者将他们的实现代码公布在了<a href="https://github.com/stormraiser/GAN-weight-norm" target="_blank" rel="external">GitHub</a>上。</p>
<p>下面是利用PyTorch对T-ReLU的实现：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TPReLU</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_parameters<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> init<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>num_parameters <span class="token operator">=</span> num_parameters
        super<span class="token punctuation">(</span>TPReLU<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>num_parameters<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>init<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_parameters<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        bias_resize <span class="token operator">=</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_parameters<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>input<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>prelu<span class="token punctuation">(</span>input <span class="token operator">-</span> bias_resize<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> bias_resize
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对 Weigh-normalized layer 的实现：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">WeightNormalizedLinear</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> init_factor<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> init_scale<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>WeightNormalizedLinear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> bias<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'bias'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>
        <span class="token keyword">if</span> scale<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>scale <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>init_scale<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'scale'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span>init_factor<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> factor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        stdv <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">*</span> factor <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>stdv<span class="token punctuation">,</span> stdv<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>stdv<span class="token punctuation">,</span> stdv<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">weight_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">norm_scale_bias</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> input<span class="token punctuation">.</span>div<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>scale <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            output <span class="token operator">=</span> output<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>scale<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            output <span class="token operator">=</span> output<span class="token punctuation">.</span>add<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm_scale_bias<span class="token punctuation">(</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>观察上面的forward函数可以发现，TReLU添加bias这一习得参数，而weight-normalized layer中则对传入的weight进行了标准化。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1704.03971&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;On the Effects and Weight Normalization in GAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="GAN" scheme="http://blog.ddlee.cn/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Large-Scale Evolution of Image Classifiers</title>
    <link href="http://blog.ddlee.cn/2017/06/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Large-Scale-Evolution-of-Image-Classifiers/"/>
    <id>http://blog.ddlee.cn/2017/06/05/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/</id>
    <published>2017-06-05T08:09:52.000Z</published>
    <updated>2017-06-05T08:09:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">Large-Scale Evolution of Image, Classifiers</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>深层网络在图片分类问题上表现优异，但网络结构的设计上并没有统一的指导。进化是构建深度网络架构的一种方式。利用本文的自动化方法得出的深度网络结构，已经能在CIFAR-10上取得可以跟人工设计的网络相媲美的结果</p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Evolution-Algorithm"><a href="#Evolution-Algorithm" class="headerlink" title="Evolution Algorithm"></a>Evolution Algorithm</h3><p>整个算法的核心是如下的tournament selection:</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/tournament.jpg" alt="tournament"></p>
<ul>
<li>population: 供筛选的群体</li>
<li>individual: 个体，带有指标fitness，特别地，指在CV集上的损失</li>
<li>worker: 筛选者，上帝</li>
</ul>
<ol>
<li><em>population</em> 中的 <em>individual</em> 均已在训练集上训练完毕，带有指标 <em>fitness</em></li>
<li><em>worker</em> 随机选择一对 <em>individual</em>，比较 <em>fitness</em>，较差的 <em>individual</em> 被舍弃</li>
<li>表现较好的 <em>individual</em> 成为parent，对其施加 <em>mutation</em> (变异)，得到 <em>child</em></li>
<li>训练 <em>child</em> 并在CV集上得到其 <em>fitness</em>，归还到 <em>population</em> 中</li>
</ol>
<h3 id="Encoding-and-Mutation"><a href="#Encoding-and-Mutation" class="headerlink" title="Encoding and Mutation"></a>Encoding and Mutation</h3><p>个体的网络结构和部分参数被编码为DNA。</p>
<p>能够施加的变异有：</p>
<ul>
<li>改变学习率</li>
<li>恒等（不变）</li>
<li>重设参数</li>
<li>加入卷积层</li>
<li>移除卷积层</li>
<li>更改卷积层的stride参数</li>
<li>更改卷积层的Channel参数</li>
<li>更改卷积核大小</li>
<li>加入skip连接（类似ResNet)</li>
<li>移除skip连接</li>
</ul>
<h3 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h3><p>计算方面采用了并行、异步、无锁的策略。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/comp.jpg" alt="comp"></p>
<p>建立约为 <em>population</em> 数1/4的 <em>worker</em>，分别运行于不同的机器上，之间独立异步。<em>population</em> 共享，若两个 <em>worker</em> 在一个 <em>individual</em> 上产生冲突，则后一个 <em>worker</em> 停止并等待再次尝试。</p>
<h3 id="Weight-Inheritance"><a href="#Weight-Inheritance" class="headerlink" title="Weight Inheritance"></a>Weight Inheritance</h3><p>除了架构之外，子模型还会继承父母模型未经变异影响的隐藏层参数（不仅是DNA中的），这样使子模型的训练时间大幅减小。</p>
<h2 id="Experiments-and-Results"><a href="#Experiments-and-Results" class="headerlink" title="Experiments and Results"></a>Experiments and Results</h2><p>文章的主要结果如下图：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/results.png" alt="results"></p>
<p>最右边的结构是在CIFAR-10上发现的最好（CV集准确度最高）的结构，左边两个是它的祖先。其中白色块相当于简单的线性层，彩色块则带有非线性激活，可以看到，不同于人工设计的网络，某一线性层之后可能包含多个非线性层。</p>
<p>另外，利用本文的模型，也在CIFAR-100上做了实验，可以达到76.3%的准确率，一定程度上说明了算法的扩展性。</p>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/popu.png" alt="popu"></p>
<p>上图说明随着 <em>population</em> 规模和训练步数的增加，模型的整体水平在变好。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Large-Scale-Evolution-of-Image-Classifiers/mutation.png" alt="mutation"></p>
<p>在模型陷入局部最优值时，提高变异率和重设参数会使群体继续进化。这是由于变异中包含恒等映射等不改变模型架构的变异类型，再加上weight Inheritance，一些子模型只是训练次数比其他模型多很多的“活化石”。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Google I/O时就提到了自动筛选最优网络结构，但没有公布论文。但将网络结构自动化，必定是未来的方向。个人认为，ResNet就相当于自动化网络深度（一些层实际上被跳过了），而Inception单元似乎包含了太多的先验，而且也没有逻辑上的证据说明这样的结构更有效。网络结构本身就是先验信息，而要达到通用的人工智能，这些先验也必须由模型自行发觉。</p>
<p>强化学习本身也是一个进化过程，应该也有相关的工作将强化学习的框架应用于网络结构的学习上。</p>
<p>更进一步地，若数据是一阶信息，深度网络的隐藏层学到的表示是二阶信息，深度网络的结构则是三阶信息，从一阶到二阶的框架是不是都可以移植到二阶到三阶上来？关键之处在于我们还没有描述好深度网络的结构空间，但就现在的发展看，深度网络的一些基本结构(conv, BN)等，已经被作为基本单元（离散的）来进行构建和筛选了，也就是说，所有深度网络构成的空间之性质如何，还有大量的工作可以做。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1703.01041&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Large-Scale Evolution of Image, Classifiers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="autoML" scheme="http://blog.ddlee.cn/tags/autoML/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]An Analysis of Deep Neural Network Models for Practical Applications</title>
    <link href="http://blog.ddlee.cn/2017/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/"/>
    <id>http://blog.ddlee.cn/2017/06/03/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/</id>
    <published>2017-06-03T06:27:07.000Z</published>
    <updated>2017-06-03T06:27:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1605.07678" target="_blank" rel="external">An Analysis of Deep Neural Network Models for Practical Applications</a></p>
<p>本文是对现有（论文发表于2016年5月）深度网络的比较，从以下方面入手：</p>
<ul>
<li>accuracy</li>
<li>memory footprint</li>
<li>parameters</li>
<li>operations count</li>
<li>inference time</li>
<li>power consumption</li>
</ul>
<p>以下图片各模型的着色是统一的：蓝色是Inception系，绿色是VGG系，粉色是ResNet系，黄色为AlexNet系。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/top1.png" alt="top1"></p>
<p>上图是Top1准确率与模型参数数、操作数的关系。可以看到Inception系列网络以较少的参数取得相对高的准确率，而VGG系则在这一点上表现很差。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/infer-batch.png" alt="infer-batch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/power-batch.png" alt="power-batch"></p>
<p>上面两图分别是推断耗时和电量消耗与批量大小的关系。可以看到，两者均与批量大小无明显的相关关系。但电量消耗在不同的模型之间也非常类似，而推断时间与模型结构关系很大（VGG再次尴尬）。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/mem-batch.png" alt="mem-batch"></p>
<p>上图展示了模型占用内存大小与批量大小的关系，大部分网络都有相对固定的内存占用，随后随批量大小的上扬而上涨。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-infer.png" alt="infer-ops"></p>
<p>从上图可以发现推断耗时和模型的操作数大体上呈现线性关系。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/ops-power.png" alt="ops-power"></p>
<p>电量消耗与模型的参数数、操作数并没有明显的相关性。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-An-Analysis-of-Deep-Neural-Network-Models-for-Practical-Applications/accuracy-infer.png" alt="accuracy-infer"></p>
<p>注意，上图中点的大小代表模型操作数，横轴代表推断效率，纵轴表示准确率。灰色区域表示模型获得了额外的推断效率或准确率，而白色区域代表非最优。</p>
<p>操作数越多的模型推断效率越低，大部分模型都落在相对平衡的边界上，VGG和小批量情形下的AlexNet落在了非最优区域。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从这篇论文的比较中可以看到，在特定的任务中对网络特定结构的设计（如Inception单元），即加入更强的先验知识，比堆叠网络层数更有效。深度网络还是需要人类的指导才能发挥更大的作用。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1605.07678&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;An Analysis of Deep Neural Network Models for Practical Appl
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>深度学习和分布式表示</title>
    <link href="http://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/"/>
    <id>http://blog.ddlee.cn/2017/06/01/深度学习和分布式表示/</id>
    <published>2017-06-01T14:52:16.000Z</published>
    <updated>2017-08-03T13:49:22.697Z</updated>
    
    <content type="html"><![CDATA[<p>本文的两个主要参考资料：</p>
<ol>
<li>Yoshua Bengio在2016年九月<a href="https://www.bayareadlschool.org/" target="_blank" rel="external">Deep Learning School</a>的演讲Foundations and Challenges of Deep Learning。<a href="https://www.youtube.com/watch?v=11rsu_WwZTc" target="_blank" rel="external">YouTube</a></li>
<li><a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning</a>, Goodfellow et al, Section 15.4</li>
</ol>
<h3 id="从机器学习到人工智能"><a href="#从机器学习到人工智能" class="headerlink" title="从机器学习到人工智能"></a>从机器学习到人工智能</h3><p>在演讲中，Bengio提到从机器学习到人工智能有五个关键的飞跃：</p>
<ol>
<li>Lots of data</li>
<li>Very flexible models</li>
<li>Enough computing power</li>
<li>Powerful priors that can defeat the curse of dimensionality</li>
<li>Computationally efficient inference</li>
</ol>
<p>第一点已经发生，到处都提大数据，到处都在招数据分析师。<br>我在读高中时，就曾预感数据将是新时代的石油和煤炭，因为数据正是人类社会经验的总结，数据带来的知识和见解将在驱动社会进步中发挥越来越重要的作用，而自己要立志成为新时代的矿工。</p>
<p>第二点在我看来有两个例子，一是核技巧，通过核函数对分布空间的转换，赋予了模型更强大的表述能力；二是深度神经网络，多层的框架和非线性的引入使得模型理论上可以拟合任意函数。</p>
<p>第三点，借云计算的浪潮，计算力不再是一项资产而是一项可供消费的服务，我们学生也可以廉价地接触到根本负担不起的计算力资源。而GPU等芯片技术的进步也为AI的浩浩征程添砖加瓦。</p>
<p>第五点，近期发布的Tensorflow Lite和Caffe2等工具也有助于越来越多地将计算任务分配在终端上进行，而非作为一个发送与接收器。</p>
<p>最后第四点，也是这篇文章的中心话题：借助分布式表示的强大能力，深度学习正尝试解决维度带来的灾难。</p>
<h3 id="没有免费的午餐"><a href="#没有免费的午餐" class="headerlink" title="没有免费的午餐"></a>没有免费的午餐</h3><p>简单说，没有免费的午餐定理指出找不到一个在任何问题上都表现最优的模型/算法。不同的模型都有其擅长的问题，这由该模型建立时引入的先验知识决定。</p>
<p>那么，深度学习加入的先验知识是什么？</p>
<p>Bengio用的词是Compositionality，即复合性，<em>某一概念之意义由其组成部分的意义以及组合规则决定</em>。复合性的原则可以用于高效地描述我们的世界，而深度学习模型中隐藏的层正是去学习其组成部分，网络的结构则代表了组合规则。这正是深度学习模型潜在的信念。</p>
<h3 id="分布式表示带来的指数增益"><a href="#分布式表示带来的指数增益" class="headerlink" title="分布式表示带来的指数增益"></a>分布式表示带来的指数增益</h3><p>分布式表示(Distributed Representation)是连接主义的核心概念，与复合性的原理相合。整体由组成它的个体及其组合来表示。请看下面的例子：</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/distributed.webp" alt="Distributed"></p>
<p>描述一个形状，我们将其分解为不同的特征来表述。分布式表示是一种解耦，它试图复杂的概念分离成独立的部分。而这也引出了分布式表示带来的缺点：隐藏层学到的分解特征难以得到显式的解释。</p>
<p>传统的机器学习算法，如K-Means聚类、决策树等，大多使用的是非分布式表示，即用特定的参数去描述特定的区域。如K-Means聚类，我们要划分多少区域，就需要有多少个中心点。因而，这类算法的特点是，随着参数个数的提升，其能描述的概念线性增长。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/non-dist.png" alt="non-dist"><br>使用分布式表示的深度网络，则可以享受到指数级的增益，即，随着参数个数的提升，其表述能力是指数级的增长。具有$k$个值的$n$个特征，可以描述${k}^{n}$个不同的概念。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/dist.png" alt="dist"></p>
<h3 id="分布式表示在泛化上的优势"><a href="#分布式表示在泛化上的优势" class="headerlink" title="分布式表示在泛化上的优势"></a>分布式表示在泛化上的优势</h3><p>分布式的想法还可以得到额外的泛化优势。通过重新组合在原有数据中抽离出来的特征，可以表示得到原有数据中不存在的实例。在Radford et al.的工作中，生成模型区习得了性别，并能从“戴眼镜的男人”-“男人”+“女人”=“戴眼镜的女人”这样的抽象概念表达式中生成实例。</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/generative.png" alt="generative"></p>
<h3 id="分布式表示与巻积神经网络"><a href="#分布式表示与巻积神经网络" class="headerlink" title="分布式表示与巻积神经网络"></a>分布式表示与巻积神经网络</h3><p>巻积神经网络不同的滤波器习得的特征可以为分布式表示的概念分解这一特性提供一些例子。下图是VGG16不同滤波器得到结果的可视化表示，<br>出自Francois Chollet的博文<a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" target="_blank" rel="external">How convolutional neural networks see the world</a></p>
<p><img src="http://static.ddlee.cn/static/img/深度学习和分布式表示/filters.jpg" alt="filters"></p>
<p>可以看到，浅层的滤波器学到的是简单的颜色、线条走向等特征，较深的滤波器学到复杂的纹理。</p>
<h3 id="量子计算机与分布式表示"><a href="#量子计算机与分布式表示" class="headerlink" title="量子计算机与分布式表示"></a>量子计算机与分布式表示</h3><p>在我看来，量子计算机的激动人心之处也在于其表示能力。一个量子态可以表示原先两个静态表示的信息，原先需要8个单位静态存储表示的信息只需要3个量子态单位即可表示，这也是指数级的增益。在这一点上，计算模型和概念模型已然殊途同归。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从经验中总结原则，用原则生成套路，正是我们自己处理和解决新问题的途径。通过解耦得到的信息来消除未知和不确定性，是我们智能的一部分。我们眼中的世界，只是适合我们的一种表示而已。也许，真正的人工智能到来那一刻，会是我们创造的机器“理解”了自己的表示系统之时——我们所关注的可解释性，也就无关紧要了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文的两个主要参考资料：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Yoshua Bengio在2016年九月&lt;a href=&quot;https://www.bayareadlschool.org/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deep Learning Sc
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]On-the-fly Operation Batching in Dynamic Computation Graphs</title>
    <link href="http://blog.ddlee.cn/2017/05/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/"/>
    <id>http://blog.ddlee.cn/2017/05/30/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/</id>
    <published>2017-05-30T07:24:34.000Z</published>
    <updated>2017-05-30T07:24:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="http://arxiv.org/abs/1705.07860" target="_blank" rel="external">On-the-fly Operation Batching in Dynamic Computaion Graphs</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>基于动态图的深度学习框架如<code>Pytorch</code>,<code>DyNet</code>提供了更为灵活的结构和数据维度的选择，但要求开发者自行将数据批量化，才能最大限度地发挥框架的并行计算优势。</p>
<h2 id="当前的状况：灵活的结构与高效计算"><a href="#当前的状况：灵活的结构与高效计算" class="headerlink" title="当前的状况：灵活的结构与高效计算"></a>当前的状况：灵活的结构与高效计算</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/comparison.png" alt="左图为循环结构，右图将序列补齐，批量化"><br>左图为循环结构，右图将序列补齐，批量化</p>
<ol>
<li>灵活的结构和数据输入维度，采用朴素的循环结构实现，但不高效，因为尽管维度不同，在循环内数据接受的是同样的操作。</li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/loop.png" alt="loop"></p>
<ol>
<li>对数据做“Padding”，即用傀儡数据将输入维度对齐，进而实现向量化，但这种操作对开发者并不友好，会使开发者浪费掉很多本该投入到结构设计等方面的精力。</li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/padding.png" alt="padding"></p>
<h2 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h2><h3 id="三个部分"><a href="#三个部分" class="headerlink" title="三个部分"></a>三个部分</h3><ol>
<li>Graph Definition</li>
<li>Operation Batching</li>
<li>Computation</li>
</ol>
<p>第一步和第三步在当前已被大部分深度学习框架较好地实现。主要特点是，构建计算图与计算的分离，即”Lazy Evaluation”。比如在<code>Tensorflow</code>中，一个抽象层负责解析计算图各节点之间的依赖，决定执行计算的顺序，而另一个抽象层则负责分配计算资源。</p>
<h3 id="Operation-Batching"><a href="#Operation-Batching" class="headerlink" title="Operation Batching"></a>Operation Batching</h3><h4 id="Computing-compatibility-groups"><a href="#Computing-compatibility-groups" class="headerlink" title="Computing compatibility groups"></a>Computing compatibility groups</h4><p>这一步是建立可以批量化计算的节点组。具体做法是，给每一个计算节点建立 <em>signature</em>，用于描述节点计算的特性，文中举出了如下几个例子:</p>
<ol>
<li>Component-wise operations: 直接施加在每个张量元素上的计算，跟张量的维度无关，如$tanh$,$log$</li>
<li>Dimension-sensitive operations: 基于维度的计算，如线性传递$Wh+b$，要求$W$和$h$维度相符，<em>signature</em> 中要包含维度信息</li>
<li>Operations with shared elements: 包含共享元素的计算，如共享的权值$W$</li>
<li>Unbatchable operations: 其他</li>
</ol>
<h4 id="Determining-execution-order"><a href="#Determining-execution-order" class="headerlink" title="Determining execution order"></a>Determining execution order</h4><p>执行顺序要满足两个目标：</p>
<ol>
<li>每一节点的计算要在其依赖之后</li>
<li>带有同样 <em>signature</em> 且没有依赖关系的节点放在同一批量执行</li>
</ol>
<p>但在一般情况下找到最大化批量规模的执行顺序是个NP问题。有如下两种策略：</p>
<ol>
<li>Depth-based Batching: 库<code>Tensorflow Fold</code>中使用的方法。某一节点的深度定义为其子节点到其本身的最大长度，同一深度的节点进行批量计算。但由于输入序列长度不一，可能会错失一些批量化的机会。</li>
<li>Agenda-based Batching: 本文的方法，核心的想法是维护一个 <em>agenda</em> 序列，所有依赖已经被解析的节点入列，每次迭代时从 <em>agenda</em> 序列中按 <em>signature</em> 相同的原则取出节点进行批量计算。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章选取了四个模型：BiLSTM, BiLSTM w/char, Tree-structured LSTMs, Transition-based Dependency Parsing。</p>
<p>实验结果：（单位为Sentences/second）<br><img src="http://static.ddlee.cn/static/img/论文笔记-On-the-fly-Operation-Batching-in-Dynamic-Computation-Graphs/result.png" alt="result"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本来读到题目还是蛮惊喜的，期待的是从模型构建的角度解决序列长度不一带来的计算上的不便。但通读下来发现是在计算图的计算这一层面进行的优化，有些失望但也感激，作者使用<code>DyNet</code>框架实现了他们的方法，希望自己也可以为<code>Pytorch</code>等框架该算法的实现出一份力。</p>
<p>感谢这些开源的框架，正一步步拉近人类构建模型和机器高效计算之间的距离。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;http://arxiv.org/abs/1705.07860&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;On-the-fly Operation Batching in Dynamic Computaion Graphs&lt;/a
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Paper" scheme="http://blog.ddlee.cn/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>LSTM:Pytorch实现</title>
    <link href="http://blog.ddlee.cn/2017/05/29/LSTM-Pytorch%E5%AE%9E%E7%8E%B0/"/>
    <id>http://blog.ddlee.cn/2017/05/29/LSTM-Pytorch实现/</id>
    <published>2017-05-28T17:06:44.000Z</published>
    <updated>2017-05-28T17:16:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM是一种循环神经网络，适用于对序列化的输入建模。Chris Olah的这篇<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">文章</a>细致地解释了一个LSTM单元的运作方式，建议阅读。</p>
<h3 id="两个想法"><a href="#两个想法" class="headerlink" title="两个想法"></a>两个想法</h3><h4 id="Gate：信息流动的闸门"><a href="#Gate：信息流动的闸门" class="headerlink" title="Gate：信息流动的闸门"></a>Gate：信息流动的闸门</h4><p>$$i<em>t = sigmoid(W</em>{xi} x<em>t  +  W</em>{hi}h_{t-1} + b_i)$$<br>$$f<em>t = sigmoid(W</em>{xf} x<em>t  +  W</em>{hf}h_{t-1} + b_f)$$<br>$$o<em>t = sigmoid(W</em>{xo} x<em>t  +  W</em>{ho}h_{t-1} + b_o)$$<br>$x$ 表示输入，$h$表示隐藏状态，用$sigmoid$函数将输入二者的传递结果映射到$（0,1)$上，分别赋予输入门、遗忘门、输出门的含义，来控制不同神经单元（同一神经元不同时间点的状态）之间信息流动。</p>
<h4 id="Cell：记忆池"><a href="#Cell：记忆池" class="headerlink" title="Cell：记忆池"></a>Cell：记忆池</h4><p>$$c_t = f<em>t \odot c</em>{t - 1} + i<em>t \odot tanh(W</em>{xc} x<em>t  +  W</em>{hc}h_{t-1} + b_c)\<br>h_t = o_t \odot tanh(c_t)$$<br>$h$表示隐藏状态，$C$表示记忆池，通过Gate，上一单元（状态）的信息有控制地遗忘，当前的输入有控制地流入，记忆池中的信息有控制地流入隐藏状态。</p>
<h3 id="与普通RNN的对比"><a href="#与普通RNN的对比" class="headerlink" title="与普通RNN的对比"></a>与普通RNN的对比</h3><p><img src="http://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM3-SimpleRNN.png" alt="RNN"><br>普通RNN只有一个自更新的隐藏状态单元。</p>
<p><img src="http://static.ddlee.cn/static/img/./LSTM-Pytorch实现/LSTM.jpg" alt="LSTM"><br>LSTM增加了记忆池Cell，并通过几个Gate将信息有控制地更新在记忆池中，并通过记忆池中的信息来决定隐藏状态。</p>
<h2 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h2><p>下面是手动实现LSTM的代码，继承了基类<code>nn.Module</code>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable

<span class="token keyword">class</span> <span class="token class-name">LSTM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> cell_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>LSTM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>cell_size <span class="token operator">=</span> cell_size
        self<span class="token punctuation">.</span>gate <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size <span class="token operator">+</span> hidden_size<span class="token punctuation">,</span> cell_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh <span class="token operator">=</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span><span class="token punctuation">:</span>
        combined <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        f_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        i_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        o_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        f_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>f_gate<span class="token punctuation">)</span>
        i_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>i_gate<span class="token punctuation">)</span>
        o_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>o_gate<span class="token punctuation">)</span>
        cell_helper <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>combined<span class="token punctuation">)</span>
        cell_helper <span class="token operator">=</span> self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cell_helper<span class="token punctuation">)</span>
        cell <span class="token operator">=</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> f_gate<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>cell_helper<span class="token punctuation">,</span> i_gate<span class="token punctuation">)</span><span class="token punctuation">)</span>
        hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">,</span> o_gate<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> cell

    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">initCell</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>cell_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>几个关键点：</p>
<ol>
<li>Tensor的大小</li>
<li>信息的传递顺序</li>
</ol>
<h2 id="Pytorch-Module"><a href="#Pytorch-Module" class="headerlink" title="Pytorch Module"></a>Pytorch Module</h2><p>Pytorch库本身对LSTM的实现封装了更多功能，类和函数的组织也非常有借鉴意义。我对其实现的理解基于以下两点展开：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p>下面开始按图索骥，源码见<a href="https://github.com/pytorch/pytorch/tree/master/torch" target="_blank" rel="external">GitHub</a>。</p>
<h4 id="LSTM类"><a href="#LSTM类" class="headerlink" title="LSTM类"></a>LSTM类</h4><p>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" target="_blank" rel="external">nn/modules/rnn.py</a></p>
<pre class="line-numbers language-Python"><code class="language-Python"># nn/modules/rnn.py
class RNNBase(Module):
  def __init__(self, mode, input_size, output_size):
      pass
  def forward(self, input, hx=None):
      if hx is None:
          hx = torch.autograd.Variable()
      if self.mode == 'LSTM':
          hx = (hx, hx)
      func = self._backend.RNN() #!!!
      output, hidden = func(input, self.all_weights, hx) #!!!
      return output, hidden

class LSTM(RNNBase):
    def __init__(self, *args, **kwargs):
        super(LSTM, self).__init__('LSTM', *args, **kwargs)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>LSTM</code>类只是<code>RNNBase</code>类的一个装饰器。</li>
<li>在基类<code>nn.Module</code>中，把<code>__call__()</code>定义为调用<code>forward()</code>方法，因而真正的功能实现在<code>_backend.RNN()</code>中</li>
</ol>
<h4 id="AutogradRNN函数"><a href="#AutogradRNN函数" class="headerlink" title="AutogradRNN函数"></a>AutogradRNN函数</h4><p>下面寻找<code>_backend.RNN</code>。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/backends/thnn.py" target="_blank" rel="external">nn/backends/thnn.py</a></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn/backends/thnn.py</span>
<span class="token keyword">def</span> <span class="token function">_initialize_backend</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>_functions<span class="token punctuation">.</span>rnn <span class="token keyword">import</span> RNN<span class="token punctuation">,</span> LSTMCell
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>原来，<code>_backend</code>也是索引。</p>
<p>终于找到<code>RNN()</code>函数。<br>文件：<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py" target="_blank" rel="external">nn/_functions/rnn.py</a></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn/_functions/rnn.py</span>
<span class="token keyword">def</span> <span class="token function">RNN</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> <span class="token operator">*</span>fargs<span class="token punctuation">,</span> <span class="token operator">**</span>fkwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        func <span class="token operator">=</span> AutogradRNN<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> func<span class="token punctuation">(</span>input<span class="token punctuation">,</span> <span class="token operator">*</span>fargs<span class="token punctuation">,</span> <span class="token operator">**</span>fkwargs<span class="token punctuation">)</span>
    <span class="token keyword">return</span> forward

<span class="token keyword">def</span> <span class="token function">AutogradRNN</span><span class="token punctuation">(</span>mode<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cell <span class="token operator">=</span> LSTMCell
    rec_factory <span class="token operator">=</span> Recurrent
    layer <span class="token operator">=</span> <span class="token punctuation">(</span>rec_factory<span class="token punctuation">(</span>cell<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    func <span class="token operator">=</span> StackedRNN<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> num_layers<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nexth<span class="token punctuation">,</span> output <span class="token operator">=</span> func<span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> weight<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> nexth
    <span class="token keyword">return</span> forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>RNN()</code>是一个装饰器，根据是否有<code>cudnn</code>库决定调用<code>AutogradRNN()</code>还是<code>CudnnRNN()</code>，这里仅观察<code>AutogradRNN()</code></li>
<li><code>AutogradRNN()</code>选用了<code>LSTMCell</code>，用<code>Recurrent()</code>函数处理了<code>Cell</code>构成<code>Layer</code>，再将<code>Layer</code>传入<code>StackedRNN()</code>函数</li>
<li><code>RNN()</code>和<code>AutogradRNN()</code>返回的都是其<code>forward()</code>函数句柄</li>
</ol>
<p>下面是<code>Recurrent()</code>函数：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Recurrent</span><span class="token punctuation">(</span>inner<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> weight<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        steps <span class="token operator">=</span> range<span class="token punctuation">(</span>input<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> steps<span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> inner<span class="token punctuation">(</span>input<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> <span class="token operator">*</span>weight<span class="token punctuation">)</span>
            output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> hidden<span class="token punctuation">,</span> output
    <span class="token keyword">return</span> forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>Recurrent()</code>函数实现了“递归”的结构，根据输入的大小组合<code>Cell</code>，完成了隐藏状态和参数的迭代。</li>
<li><code>Recurrent()</code>函数将<code>Cell(inner)</code>组合为<code>Layer</code>。</li>
</ol>
<h4 id="StackedRNN-函数"><a href="#StackedRNN-函数" class="headerlink" title="StackedRNN()函数"></a>StackedRNN()函数</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">StackedRNN</span><span class="token punctuation">(</span>inners<span class="token punctuation">,</span> num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_directions <span class="token operator">=</span> len<span class="token punctuation">(</span>inners<span class="token punctuation">)</span>
    total_layers <span class="token operator">=</span> num_layers <span class="token operator">*</span> num_directions
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> weight<span class="token punctuation">)</span><span class="token punctuation">:</span>
        next_hidden <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        hidden <span class="token operator">=</span> list<span class="token punctuation">(</span>zip<span class="token punctuation">(</span><span class="token operator">*</span>hidden<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
          all_output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
          <span class="token keyword">for</span> j<span class="token punctuation">,</span> inner <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>inners<span class="token punctuation">)</span><span class="token punctuation">:</span>
              hy<span class="token punctuation">,</span> output <span class="token operator">=</span> inner<span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> weight<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">)</span>
              next_hidden<span class="token punctuation">.</span>append<span class="token punctuation">(</span>hy<span class="token punctuation">)</span>
              all_output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
          input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>all_output<span class="token punctuation">,</span> input<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
        next_h<span class="token punctuation">,</span> next_c <span class="token operator">=</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>next_hidden<span class="token punctuation">)</span>
        next_hidden <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>next_h<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>total_layers<span class="token punctuation">,</span> <span class="token operator">*</span>next_h<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>next_c<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>total_layers<span class="token punctuation">,</span> <span class="token operator">*</span>next_c<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> next_hidden<span class="token punctuation">,</span> input
    <span class="token keyword">return</span> forward
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ol>
<li><code>StackedRNN()</code>函数将<code>Layer(inner)</code>组合为栈</li>
</ol>
<p>最后的最后，一个基本的LSTM单元内的计算由<code>LSTMCell()</code>函数实现。</p>
<h4 id="LSTMCell-函数"><a href="#LSTMCell-函数" class="headerlink" title="LSTMCell()函数"></a>LSTMCell()函数</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">LSTMCell</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> w_ih<span class="token punctuation">,</span> w_hh<span class="token punctuation">,</span> b_ih<span class="token operator">=</span>None<span class="token punctuation">,</span> b_hh<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> input<span class="token punctuation">.</span>is_cuda<span class="token punctuation">:</span>
        igates <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> w_ih<span class="token punctuation">)</span>
        hgates <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w_hh<span class="token punctuation">)</span>
        state <span class="token operator">=</span> fusedBackend<span class="token punctuation">.</span>LSTMFused<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> state<span class="token punctuation">(</span>igates<span class="token punctuation">,</span> hgates<span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span> b_ih <span class="token keyword">is</span> None <span class="token keyword">else</span> state<span class="token punctuation">(</span>igates<span class="token punctuation">,</span> hgates<span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> b_ih<span class="token punctuation">,</span> b_hh<span class="token punctuation">)</span>

    hx<span class="token punctuation">,</span> cx <span class="token operator">=</span> hidden
    gates <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> w_ih<span class="token punctuation">,</span> b_ih<span class="token punctuation">)</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hx<span class="token punctuation">,</span> w_hh<span class="token punctuation">,</span> b_hh<span class="token punctuation">)</span>

    ingate<span class="token punctuation">,</span> forgetgate<span class="token punctuation">,</span> cellgate<span class="token punctuation">,</span> outgate <span class="token operator">=</span> gates<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    ingate <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>ingate<span class="token punctuation">)</span>
    forgetgate <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>forgetgate<span class="token punctuation">)</span>
    cellgate <span class="token operator">=</span> F<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cellgate<span class="token punctuation">)</span>
    outgate <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>outgate<span class="token punctuation">)</span>

    cy <span class="token operator">=</span> <span class="token punctuation">(</span>forgetgate <span class="token operator">*</span> cx<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>ingate <span class="token operator">*</span> cellgate<span class="token punctuation">)</span>
    hy <span class="token operator">=</span> outgate <span class="token operator">*</span> F<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>cy<span class="token punctuation">)</span>

    <span class="token keyword">return</span> hy<span class="token punctuation">,</span> cy
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>观察上面的代码，即是LSTM的基本信息传递公式。至此，我们的旅程完成。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><blockquote>
<p>没有什么是增加一层抽象不能解决的，如果不能，那就再加一层。</p>
</blockquote>
<p>重复一下我对上述代码的理解：</p>
<ol>
<li>胞(cell)、层(layer)、栈(stacked layer)的层次化解耦，每一层抽象处理一部分参数（结构）</li>
<li>函数句柄的传递：处理好参数后返回函数句柄<code>forward</code></li>
</ol>
<p><img src="http://static.ddlee.cn/static/img/LSTM-Pytorch实现/str.jpg" alt="str"></p>
<p>如洋葱一般，我们剥到最后，发现处理的信息正是输入、隐藏状态和LSTM单元几个控制门的参数。在一层一层的抽象之中，Pytorch在不同的层面处理了不同的参数，保证了扩展性和抽象层之间的解耦。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文讨论LSTM网络的Pytorch实现，兼论Pytorch库的代码组织方式和架构设计。&lt;/p&gt;
&lt;h2 id=&quot;LSTM&quot;&gt;&lt;a href=&quot;#LSTM&quot; class=&quot;headerlink&quot; title=&quot;LSTM&quot;&gt;&lt;/a&gt;LSTM&lt;/h2&gt;&lt;p&gt;LSTM是一种循环神
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Pytorch" scheme="http://blog.ddlee.cn/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pandas速度优化</title>
    <link href="http://blog.ddlee.cn/2017/05/28/Pandas%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96/"/>
    <id>http://blog.ddlee.cn/2017/05/28/Pandas速度优化/</id>
    <published>2017-05-28T12:06:14.000Z</published>
    <updated>2017-05-28T12:17:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要内容取自Sofia Heisler在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=HN5d490_KKk" target="_blank" rel="external">No More Sad Pandas Optimizing Pandas Code for Speed and Efficiency</a>，讲稿代码和幻灯片见<a href="https://github.com/sversh/pycon2017-optimizing-pandas" target="_blank" rel="external">GitHub</a>。</p>
<h2 id="Set-Up"><a href="#Set-Up" class="headerlink" title="Set Up"></a>Set Up</h2><h4 id="示例数据"><a href="#示例数据" class="headerlink" title="示例数据"></a>示例数据</h4><table>
<thead>
<tr>
<th></th>
<th>ean_hotel_id</th>
<th>name</th>
<th>address1</th>
<th>city</th>
<th>state_province</th>
<th>postal_code</th>
<th>latitude</th>
<th>longitude</th>
<th>star_rating</th>
<th>high_rate</th>
<th>low_rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>269955</td>
<td>Hilton Garden Inn Albany/SUNY Area</td>
<td>1389 Washington Ave</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68751</td>
<td>-73.81643</td>
<td>3.0</td>
<td>154.0272</td>
<td>124.0216</td>
</tr>
<tr>
<td>1</td>
<td>113431</td>
<td>Courtyard by Marriott Albany Thruway</td>
<td>1455 Washington Avenue</td>
<td>Albany</td>
<td>NY</td>
<td>12206</td>
<td>42.68971</td>
<td>-73.82021</td>
<td>3.0</td>
<td>179.0100</td>
<td>134.0000</td>
</tr>
<tr>
<td>2</td>
<td>108151</td>
<td>Radisson Hotel Albany</td>
<td>205 Wolf Rd</td>
<td>Albany</td>
<td>NY</td>
<td>12205</td>
<td>42.72410</td>
<td>-73.79822</td>
<td>3.0</td>
<td>134.1700</td>
<td>84.1600</td>
</tr>
</tbody>
</table>
<h4 id="示例函数：Haversine-Distance"><a href="#示例函数：Haversine-Distance" class="headerlink" title="示例函数：Haversine Distance"></a>示例函数：Haversine Distance</h4><pre class="line-numbers language-Python"><code class="language-Python">def haversine(lat1, lon1, lat2, lon2):
    miles_constant = 3959
    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    mi = miles_constant * c
    return mi
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="优化它之前，先测量它"><a href="#优化它之前，先测量它" class="headerlink" title="优化它之前，先测量它"></a>优化它之前，先测量它</h2><h4 id="IPython-Notebook的Magic-Command-timeit"><a href="#IPython-Notebook的Magic-Command-timeit" class="headerlink" title="IPython Notebook的Magic Command: %timeit"></a>IPython Notebook的Magic Command: <code>%timeit</code></h4><p>既可以测量某一行代码的执行时间，又可以测量整个单元格里代码快的执行时间。</p>
<h4 id="Package-line-profiler"><a href="#Package-line-profiler" class="headerlink" title="Package: line_profiler"></a>Package: line_profiler</h4><p>记录每行代码的执行次数和执行时间。</p>
<p>在IPython Notebook中使用时，先运行<code>%load_ext line_profiler</code>， 之后可以用<code>%lprun -f [function name]</code>命令记录指定函数的执行情况。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h4 id="对行做循环-Baseline"><a href="#对行做循环-Baseline" class="headerlink" title="对行做循环(Baseline)"></a>对行做循环(Baseline)</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span><span class="token operator">%</span>timeit
haversine_series <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> index<span class="token punctuation">,</span> row <span class="token keyword">in</span> df<span class="token punctuation">.</span>iterrows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    haversine_series<span class="token punctuation">.</span>append<span class="token punctuation">(</span>haversine<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                                      row<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
df<span class="token punctuation">[</span><span class="token string">'distance'</span><span class="token punctuation">]</span> <span class="token operator">=</span> haversine_series
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>197 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><h4 id="pd-DataFrame-apply-方法"><a href="#pd-DataFrame-apply-方法" class="headerlink" title="pd.DataFrame.apply()方法"></a>pd.DataFrame.apply()方法</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>lprun <span class="token operator">-</span>f haversine \
df<span class="token punctuation">.</span>apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> row<span class="token punctuation">:</span> haversine<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                               row<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>90.6 ms ± 7.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.049982 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2      1631         1535      0.9      3.1      miles_constant = 3959
     3      1631        16602     10.2     33.2      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4      1631         2019      1.2      4.0      dlat = lat2 - lat1
     5      1631         1143      0.7      2.3      dlon = lon2 - lon1
     6      1631        18128     11.1     36.3      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7      1631         7857      4.8     15.7      c = 2 * np.arcsin(np.sqrt(a))
     8      1631         1708      1.0      3.4      mi = miles_constant * c
     9      1631          990      0.6      2.0      return mi
</code></pre><p>观察Hits这一列可以看到，<code>apply()</code>方法还是将函数一行行地应用于每行。</p>
<h4 id="向量化：将pd-Series传入函数"><a href="#向量化：将pd-Series传入函数" class="headerlink" title="向量化：将pd.Series传入函数"></a>向量化：将pd.Series传入函数</h4><pre><code>%lprun -f haversine haversine(40.671, -73.985,\
                              df[&#39;latitude&#39;], df[&#39;longitude&#39;])
</code></pre><p>Output:</p>
<pre><code>2.21 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.008601 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.0      miles_constant = 3959
     3         1          838    838.0      9.7      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1          597    597.0      6.9      dlat = lat2 - lat1
     5         1          572    572.0      6.7      dlon = lon2 - lon1
     6         1         5033   5033.0     58.5      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1         1060   1060.0     12.3      c = 2 * np.arcsin(np.sqrt(a))
     8         1          496    496.0      5.8      mi = miles_constant * c
     9         1            2      2.0      0.0      return mi
</code></pre><p>向量化之后，函数内的每行操作只被访问一次，达到了行结构上的并行。</p>
<h3 id="向量化：将np-array传入函数"><a href="#向量化：将np-array传入函数" class="headerlink" title="向量化：将np.array传入函数"></a>向量化：将np.array传入函数</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>lprun <span class="token operator">-</span>f haversine df<span class="token punctuation">[</span><span class="token string">'distance'</span><span class="token punctuation">]</span> <span class="token operator">=</span> haversine<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                        df<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>Output：</p>
<pre><code>370 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre><pre><code>Timer unit: 1e-06 s

Total time: 0.001382 s
File: &lt;ipython-input-3-19c704a927b7&gt;
Function: haversine at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def haversine(lat1, lon1, lat2, lon2):
     2         1            3      3.0      0.2      miles_constant = 3959
     3         1          292    292.0     21.1      lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])
     4         1           40     40.0      2.9      dlat = lat2 - lat1
     5         1           29     29.0      2.1      dlon = lon2 - lon1
     6         1          815    815.0     59.0      a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
     7         1          183    183.0     13.2      c = 2 * np.arcsin(np.sqrt(a))
     8         1           18     18.0      1.3      mi = miles_constant * c
     9         1            2      2.0      0.1      return mi
</code></pre><p>相比<code>pd.Series</code>，<code>np.array</code>不含索引等额外信息，因而更加高效。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><table>
<thead>
<tr>
<th>Methodology</th>
<th>Avg.    single    run    time</th>
<th>Marginal    performance    improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Looping    with    iterrows</td>
<td>184.00</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Looping    with    apply</td>
<td>78.10</td>
<td>2.4x</td>
</tr>
<tr>
<td>Vectorization    with    Pandas    series</td>
<td>1.79</td>
<td>43.6x</td>
</tr>
<tr>
<td>Vectorization    with    NumPy    arrays</td>
<td>0.37</td>
<td>4.8x</td>
</tr>
</tbody>
</table>
<p>通过上面的对比，我们比最初的baseline快了近500倍。最大的提升来自于向量化。因而，实现的函数能够很方便地向量化是高效处理的关键。</p>
<h2 id="用Cython优化"><a href="#用Cython优化" class="headerlink" title="用Cython优化"></a>用<code>Cython</code>优化</h2><p><code>Cython</code>可以将<code>python</code>代码转化为<code>C</code>代码来执行，可以进行如下优化（静态化变量类型，调用C函数库）</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>load_ext cython

<span class="token operator">%</span><span class="token operator">%</span>cython <span class="token operator">-</span>a
<span class="token comment" spellcheck="true"># Haversine cythonized</span>
<span class="token keyword">from</span> libc<span class="token punctuation">.</span>math cimport sin<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> acos<span class="token punctuation">,</span> asin<span class="token punctuation">,</span> sqrt

cdef deg2rad_cy<span class="token punctuation">(</span>float deg<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cdef float rad
    rad <span class="token operator">=</span> <span class="token number">0.01745329252</span><span class="token operator">*</span>deg
    <span class="token keyword">return</span> rad

cpdef haversine_cy_dtyped<span class="token punctuation">(</span>float lat1<span class="token punctuation">,</span> float lon1<span class="token punctuation">,</span> float lat2<span class="token punctuation">,</span> float lon2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cdef<span class="token punctuation">:</span>
        float dlon
        float dlat
        float a
        float c
        float mi

    lat1<span class="token punctuation">,</span> lon1<span class="token punctuation">,</span> lat2<span class="token punctuation">,</span> lon2 <span class="token operator">=</span> map<span class="token punctuation">(</span>deg2rad_cy<span class="token punctuation">,</span> <span class="token punctuation">[</span>lat1<span class="token punctuation">,</span> lon1<span class="token punctuation">,</span> lat2<span class="token punctuation">,</span> lon2<span class="token punctuation">]</span><span class="token punctuation">)</span>
    dlat <span class="token operator">=</span> lat2 <span class="token operator">-</span> lat1
    dlon <span class="token operator">=</span> lon2 <span class="token operator">-</span> lon1
    a <span class="token operator">=</span> sin<span class="token punctuation">(</span>dlat<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> cos<span class="token punctuation">(</span>lat1<span class="token punctuation">)</span> <span class="token operator">*</span> cos<span class="token punctuation">(</span>lat2<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">(</span>dlon<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>
    c <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> asin<span class="token punctuation">(</span>sqrt<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
    mi <span class="token operator">=</span> <span class="token number">3959</span> <span class="token operator">*</span> c
    <span class="token keyword">return</span> mi
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>嵌套于循坏中：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>timeit df<span class="token punctuation">[</span><span class="token string">'distance'</span><span class="token punctuation">]</span> <span class="token operator">=</span>\
df<span class="token punctuation">.</span>apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> row<span class="token punctuation">:</span> haversine_cy_dtyped<span class="token punctuation">(</span><span class="token number">40.671</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">73.985</span><span class="token punctuation">,</span>\
                              row<span class="token punctuation">[</span><span class="token string">'latitude'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">'longitude'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>Output:</p>
<pre><code>10 loops, best of 3: 68.4 ms per loop
</code></pre><p>可以看到，<code>Cython</code>确实带来速度上的提升，但效果不及向量化（并行化）。</p>
]]></content>
    
    <summary type="html">
    
      一些优化Pandas库处理大批量数据速度的技巧
    
    </summary>
    
      <category term="Data Science" scheme="http://blog.ddlee.cn/categories/Data-Science/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Data Science" scheme="http://blog.ddlee.cn/tags/Data-Science/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
  </entry>
  
  <entry>
    <title>Python可视化工具指引</title>
    <link href="http://blog.ddlee.cn/2017/05/28/Python%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E6%8C%87%E5%BC%95/"/>
    <id>http://blog.ddlee.cn/2017/05/28/Python可视化工具指引/</id>
    <published>2017-05-27T16:12:28.000Z</published>
    <updated>2017-06-03T04:27:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲<a href="https://www.youtube.com/watch?v=FytuB8nFHPQ" target="_blank" rel="external">Python’s Visualization Landscape</a></p>
<p>Python真是越来越火了。活跃的开源社区为Python这门语言贡献着长青的活力。</p>
<p>子曾经曰过：轮子多了，车就稳了。</p>
<p>本文帮助你选好轮子，也祝愿可视化的车开得越来越稳。</p>
<h2 id="The-Landscape"><a href="#The-Landscape" class="headerlink" title="The Landscape"></a>The Landscape</h2><p><img src="http://static.ddlee.cn/static/img/Python可视化工具指引/landscape.png" alt="landscape"></p>
<p>如图。</p>
<p>VanderPlas在展示完这张全景图后给大家贴了这张图：</p>
<p><img src="http://static.ddlee.cn/static/img/Python可视化工具指引/chan.png" alt="chan"></p>
<p>我差点笑喷。我们的表情包可能要在人民币之前走向国际化了。</p>
<p>回到正题，可视化工具有两个主要阵营，一是基于matplotlib，二是基于JavaScript。还有的接入了JS下著名的D3.js库。</p>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>numpy, pandas, matplotlib可以说是python数据科学的三驾马车。凡以python为教学语言的数据科学相关课程必提这三个库。而matplotlib又有什么特点呢？</p>
<p>先说优点：</p>
<ol>
<li>像MATLAB的语法，对MATLAB用户好上手</li>
<li>稳定，久经考验</li>
<li>渲染后端丰富，跨平台（GTK, Qt5, svg, pdf等）</li>
</ol>
<p>缺点也有很多：</p>
<ol>
<li>API过于繁琐</li>
<li>默认配色太丑</li>
<li>对web支持差，交互性差</li>
<li>对大数据集处理较慢</li>
</ol>
<p>于是就有了很多基于matplotlib的扩展，提供了更丰富、更人性化的API。</p>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/matplotlib.png" alt="matplotlib"></p>
<p>下面是几个比较受欢迎的包：</p>
<h3 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h3><p>pandas的DataFrame对象是有plot()方法的，如：<br><code>iris.plot.scatter(&#39;petalLength&#39;, &#39;petalWidth&#39;)</code>生成二维散点图，只需指明两个轴取自哪一列数据即可。</p>
<h3 id="seaborn"><a href="#seaborn" class="headerlink" title="seaborn"></a>seaborn</h3><p>seaborn(<a href="http://seaborn.pydata.org/examples/" target="_blank" rel="external">gallery</a>)专注于统计数据可视化，默认配色也还可以。语法示例：</p>
<pre class="line-numbers language-Python"><code class="language-Python">import seaborn as sns
sns.lmplot('petalLength', 'sepalWidth', iris, hue='species', fit_reg=False)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="类ggplot"><a href="#类ggplot" class="headerlink" title="类ggplot"></a>类ggplot</h3><p>对于R用户，最熟悉的可视化包可能是ggplot2，在python中可以考虑ggpy(<a href="https://github.com/yhat/ggpy)和近期上了Github" target="_blank" rel="external">https://github.com/yhat/ggpy)和近期上了Github</a> Trends的plotnie(<a href="https://github.com/has2k1/plotnine)。" target="_blank" rel="external">https://github.com/has2k1/plotnine)。</a></p>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p>基于JS的包常常具有非常好的交互性，其共同点是将图形格式化为json文件，再由JS完成渲染。</p>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/js.png" alt="js"></p>
<h3 id="Bokeh"><a href="#Bokeh" class="headerlink" title="Bokeh"></a>Bokeh</h3><p>Bokeh(<a href="http://bokeh.pydata.org/en/latest/docs/gallery.html" target="_blank" rel="external">Gallery</a>)定位于绘制用于浏览器展示的交互式图形。其优点是交互性、能够处理大量数据和流数据。语法示例：</p>
<pre class="line-numbers language-python"><code class="language-python">p <span class="token operator">=</span> figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
p<span class="token punctuation">.</span>circle<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>petalLength<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>sepalWidth<span class="token punctuation">)</span>
show<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="Plotly"><a href="#Plotly" class="headerlink" title="Plotly"></a>Plotly</h3><p>Plotly(<a href="https://plot.ly/python/" target="_blank" rel="external">Gallery</a>)跟Bokeh类似。但其提供了多种语言接口(JS, R, Python, MATLAB)，并且支持3D和动画效果，缺点是有些功能需要付费。<br>语法示例：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> plotly<span class="token punctuation">.</span>graph_objs <span class="token keyword">import</span> Scatter
<span class="token keyword">from</span> plotly<span class="token punctuation">.</span>offline <span class="token keyword">import</span> iplot
p <span class="token operator">=</span> Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>iris<span class="token punctuation">.</span>petalLength<span class="token punctuation">,</span>
            y<span class="token operator">=</span>iris<span class="token punctuation">.</span>sepalWidth<span class="token punctuation">,</span>
            mode<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">)</span>
iplot<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="处理大型数据集"><a href="#处理大型数据集" class="headerlink" title="处理大型数据集"></a>处理大型数据集</h2><p>对于大型数据集，可以考虑的包包括datashader, Vaex, 基于OpenGL的Vispy和Glumpy，GlueViz等。这里介绍datashader。</p>
<h3 id="datashader"><a href="#datashader" class="headerlink" title="datashader"></a>datashader</h3><p><a href="https://github.com/bokeh/datashader" target="_blank" rel="external">datashader</a>是Bokeh的子项目，为处理大型数据集而生。</p>
<p>示例语法：</p>
<pre class="line-numbers language-Python"><code class="language-Python">from colorcet import fire
export(tf.shade(agg, cmap=cm(fire, 0.2), how='eq_hist'), 'census_ds_fier_eq_hist')
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><img src="http://static.ddlee.cn/static/img/./Python可视化工具指引/fire.jpg" alt="fire"></p>
<h2 id="最终的建议"><a href="#最终的建议" class="headerlink" title="最终的建议"></a>最终的建议</h2><p>上车忠告：</p>
<ol>
<li>matplotlib必会</li>
<li>R用户：ggpy/plotnine</li>
<li>交互式：plotly(与R接口统一)/bokeh(免费)</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要材料来自Jake VanderPlas在PyCon 2017上的演讲&lt;a href=&quot;https://www.youtube.com/watch?v=FytuB8nFHPQ&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Python’s Visua
    
    </summary>
    
      <category term="Data Science" scheme="http://blog.ddlee.cn/categories/Data-Science/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Visualization" scheme="http://blog.ddlee.cn/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Deep Learning</title>
    <link href="http://blog.ddlee.cn/2017/05/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Learning/"/>
    <id>http://blog.ddlee.cn/2017/05/23/论文笔记-Deep-Learning/</id>
    <published>2017-05-23T11:09:09.000Z</published>
    <updated>2017-06-02T11:24:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" target="_blank" rel="external">Deep Learning</a></p>
<p>这篇文章是三位大牛15年发表在Nature上有关深度学习的综述，尽管这两年深度学习又有更多的模型和成果出现，文章显得有些过时，但来自三位领军人物对深度学习的深度阐述还是值得反复回味。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>摘要的第一句话可以说给深度学习下了定义。有一些观点认为深度学习就是堆叠了很多层的神经网络，因计算力的提升而迎来第二春。但请看三位是怎么说的：</p>
<blockquote>
<p>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.</p>
</blockquote>
<p>也就是说，深度学习是允许由 <em>多个处理层构成的计算模型</em> 用多个层次的 <em>抽象</em> 来习得 <em>数据表示</em> 的技术。我的解读如下：</p>
<ol>
<li>深度学习不限于神经网络模型，其关键之处在于多层的表示</li>
<li>深度学习属于表示学习，目的是习得数据的某种表示，而这种表示由多个层次的抽象完成</li>
</ol>
<p>在第一段的导言中，文章总结了深度学习技术取得突破性成果的各个领域，也再次指出了深度学习与传统学习算法的不同之处：</p>
<ul>
<li>传统学习模型需要特征工程和领域知识来从数据构建较好的特征</li>
<li>深度学习中，多层的特征由通用的学习过程得到，而不需要人类工程师的参与</li>
</ul>
<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h2><p>这一段概述了监督学习的一般框架、优化策略，并指出浅层学习需要Feature Extractor来提取对最适合目标问题的特征。</p>
<h2 id="Backpropagation-to-train-multilayer-architectures"><a href="#Backpropagation-to-train-multilayer-architectures" class="headerlink" title="Backpropagation to train multilayer architectures"></a>Backpropagation to train multilayer architectures</h2><p>这一段指出BP算法的关键在于目标函数关于某一子模块输入的导数可以反向通过目标函数关于该子模块输出的导数得出，而这一过程是可迭代的。BP算法曾因容易陷于局部最优解而被冷落，但对于大型网络，在实践中，理论和经验都表明尽管落于局部最优解，但这个解的效果却和全局最优解相差无几，而且几乎所有的局部最优解都可以取得类似的效果。</p>
<h2 id="Convolutional-neural-networks"><a href="#Convolutional-neural-networks" class="headerlink" title="Convolutional neural networks"></a>Convolutional neural networks</h2><p>巻积网络背后有四个关键想法：</p>
<ul>
<li>local connections</li>
<li>shared weights</li>
<li>pooling</li>
<li>the use of many layers</li>
</ul>
<p>巻积网络常由巻积层、池化层和激活层构成，巻积层用于提取局部特征，池化层用于整合相似的特征，激活层用于加入非线性。这样的结构有两点理由：</p>
<ol>
<li>张量性数据的局部数值常常高度相关，局部特征容易发现</li>
<li>局部特征跟位置无关（平移不变性）</li>
</ol>
<p>文章也提到了这种巻积结构的仿生学证据。</p>
<h2 id="Image-understanding-with-deep-convolutional-networks"><a href="#Image-understanding-with-deep-convolutional-networks" class="headerlink" title="Image understanding with deep convolutional networks"></a>Image understanding with deep convolutional networks</h2><p>这一段总结了巻积网路在图像方面取得的成就。</p>
<h2 id="Distributed-representations-and-language-processing"><a href="#Distributed-representations-and-language-processing" class="headerlink" title="Distributed representations and language processing"></a>Distributed representations and language processing</h2><p>分布式表示在两点上可以取得指数级增益：</p>
<ol>
<li>习得特征的不同组合可以泛化出训练数据中不存在的类型</li>
<li>特征组合的个数的增加关于层数是指数级的</li>
</ol>
<p>文章还比较了分布式表示相比传统的词频统计在表述人类语言方面的优势。</p>
<h2 id="Recurrent-neural-networks"><a href="#Recurrent-neural-networks" class="headerlink" title="Recurrent neural networks"></a>Recurrent neural networks</h2><p>这一段概述了循环神经网络的动态特性和LSTM等结构上的改进。</p>
<h2 id="The-future-of-deep-learning"><a href="#The-future-of-deep-learning" class="headerlink" title="The future of deep learning"></a>The future of deep learning</h2><p>作者认为在长期看来，无监督学习会更为重要，人工智能领域的重大飞跃将由组合了表示学习和复杂推理的系统取得。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章是三位
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>Dropout-Pytorch实现</title>
    <link href="http://blog.ddlee.cn/2017/05/17/Dropout-Pytorch%E5%AE%9E%E7%8E%B0/"/>
    <id>http://blog.ddlee.cn/2017/05/17/Dropout-Pytorch实现/</id>
    <published>2017-05-17T11:30:44.000Z</published>
    <updated>2017-06-03T08:32:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。</p>
<h3 id="Dropout的洞察"><a href="#Dropout的洞察" class="headerlink" title="Dropout的洞察"></a>Dropout的洞察</h3><p>关于Dropout，一个流行的解释是，通过随机行为训练网络，并平均多个随机决定的结果，实现了参数共享的Bagging。如下图，通过随机地冻结/抛弃某些隐藏单元，我们得到了新的子网络，而参数共享是说，与Bagging中子模型相互独立的参数不同，深度网络中Dropout生成的子网络是串行的，后一个子模型继承了前一个子模型的某些参数。</p>
<p><img src="http://static.ddlee.cn/static/img/Dropout-Pytorch实现/dropout.jpg" alt="dropout"></p>
<p>Dropout是模型自我破坏的一种形式，这种破坏使得存活下来的部分更加鲁棒。例如，某一隐藏单元学得了脸部鼻子的特征，而在Dropout中遭到破坏，则在之后的迭代中，要么该隐藏单元重新学习到鼻子的特征，要么学到别的特征，后者则说明，鼻子特征对该任务来说是冗余的，因而，通过Dropout，保留下来的特征更加稳定和富有信息。</p>
<p>Hinton曾用生物学的观点解释这一点。神经网络的训练过程可以看做是生物种群逐渐适应环境的过程，在迭代中传递的模型参数可以看做种群的基因，Dropout以随机信号的方式给环境随机的干扰，使得传递的基因不得不适应更多的情况才能存活。</p>
<p>另一个需要指出的地方是，Dropout给隐藏单元加入的噪声是乘性的，不像Bias那样加在隐藏单元上，这样在进行反向传播时，Dropout引入的噪声仍能够起作用。</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面看在实践中，Dropout层是如何实现的。简单来说，就是生成一系列随机数作为mask，然后再用mask点乘原有的输入，达到引入噪声的效果。</p>
<h4 id="From-Scratch"><a href="#From-Scratch" class="headerlink" title="From Scratch"></a>From Scratch</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># forward pass</span>
<span class="token keyword">def</span> <span class="token function">dropout_forward</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> dropout_param<span class="token punctuation">)</span><span class="token punctuation">:</span>
  p<span class="token punctuation">,</span> mode <span class="token operator">=</span> dropout_param<span class="token punctuation">[</span><span class="token string">'p'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dropout_param<span class="token punctuation">[</span><span class="token string">'mode'</span><span class="token punctuation">]</span>
  <span class="token comment" spellcheck="true"># p: dropout rate; mode: train or test</span>
  <span class="token keyword">if</span> <span class="token string">'seed'</span> <span class="token keyword">in</span> dropout_param<span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random_seed<span class="token punctuation">(</span>dropout_param<span class="token punctuation">[</span><span class="token string">'seed'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token comment" spellcheck="true"># seed: random seed</span>
  mask <span class="token operator">=</span> None
  out <span class="token operator">=</span> None
  <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>
    mask <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token operator">*</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">>=</span> p<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>p<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 1-p as normalization multiplier: to keep the size of input</span>
    out <span class="token operator">=</span> x <span class="token operator">*</span> mask
  <span class="token keyword">elif</span> mode <span class="token operator">==</span> <span class="token string">'test'</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># do nothing when perform inference</span>
    out <span class="token operator">=</span> x
  cache <span class="token operator">=</span> <span class="token punctuation">(</span>dropout_param<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
  out <span class="token operator">=</span> out<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> copy<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> out<span class="token punctuation">,</span> cache

<span class="token comment" spellcheck="true"># backward pass</span>
<span class="token keyword">def</span> <span class="token function">dropout_backward</span><span class="token punctuation">(</span>dout<span class="token punctuation">,</span> cache<span class="token punctuation">)</span><span class="token punctuation">:</span>
  dropout_param<span class="token punctuation">,</span> mask <span class="token operator">=</span> cache
  mode <span class="token operator">=</span> dropout_param<span class="token punctuation">[</span><span class="token string">'mode'</span><span class="token punctuation">]</span>

  dx <span class="token operator">=</span> None
  <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'train'</span><span class="token punctuation">:</span>
    dx <span class="token operator">=</span> dout <span class="token operator">*</span> mask
  <span class="token keyword">elif</span> mode <span class="token operator">==</span> <span class="token string">'test'</span><span class="token punctuation">:</span>
    dx <span class="token operator">=</span> dout
  <span class="token keyword">return</span> dx
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h3><p>file: <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/dropout.py" target="_blank" rel="external">/torch/nn/_functions/dropout.py</a></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Dropout</span><span class="token punctuation">(</span>InplaceFunction<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Dropout<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> p <span class="token operator">&lt;</span> <span class="token number">0</span> <span class="token operator">or</span> p <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"dropout probability has to be between 0 and 1, "</span>
                             <span class="token string">"but got {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> p
        self<span class="token punctuation">.</span>train <span class="token operator">=</span> train
        self<span class="token punctuation">.</span>inplace <span class="token operator">=</span> inplace

    <span class="token keyword">def</span> <span class="token function">_make_noise</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># generate random signal</span>
        <span class="token keyword">return</span> input<span class="token punctuation">.</span>new<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>resize_as_<span class="token punctuation">(</span>input<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>inplace<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>mark_dirty<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
            output <span class="token operator">=</span> input
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            output <span class="token operator">=</span> input<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>p <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>train<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>noise <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_noise<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># multiply mask to input</span>
            self<span class="token punctuation">.</span>noise<span class="token punctuation">.</span>bernoulli_<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>p<span class="token punctuation">)</span><span class="token punctuation">.</span>div_<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>p<span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>p <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>noise<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>noise <span class="token operator">=</span> self<span class="token punctuation">.</span>noise<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>input<span class="token punctuation">)</span>
            output<span class="token punctuation">.</span>mul_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>noise<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>p <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> self<span class="token punctuation">.</span>train<span class="token punctuation">:</span>
            <span class="token keyword">return</span> grad_output<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>noise<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> grad_output
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Dropout技术是Srivastava等人在2012年提出的技术，现在已然成为各种深度模型的标配。其中心思想是随机地冻结一部分模型参数，用于提高模型的泛化性能。&lt;/p&gt;
&lt;h3 id=&quot;Dropout的洞察&quot;&gt;&lt;a href=&quot;#Dropout的洞察&quot; class=&quot;he
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Python" scheme="http://blog.ddlee.cn/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Pytorch" scheme="http://blog.ddlee.cn/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Visualizing and Understanding Recurrent Networks</title>
    <link href="http://blog.ddlee.cn/2017/05/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Visualizing-and-Understanding-Recurrent-Networks/"/>
    <id>http://blog.ddlee.cn/2017/05/13/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/</id>
    <published>2017-05-13T06:06:51.000Z</published>
    <updated>2017-06-03T04:35:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文： <a href="http://arxiv.org/abs/1506.02078" target="_blank" rel="external">Visualizing and Understanding Recurrent Networks</a></p>
<h2 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h2><p>字母级的循环神经网络，用Torch实现，代码见<a href="http://github.com/karpathy/char-rnn" target="_blank" rel="external">GitHub</a>。字母嵌入成One-hot向量。优化方面，采用了RMSProp算法，加入了学习速率的decay和early stopping。</p>
<p>数据集采用了托尔斯泰的《战争与和平》和Linux核心的代码。</p>
<h2 id="可解释性激活的例子"><a href="#可解释性激活的例子" class="headerlink" title="可解释性激活的例子"></a>可解释性激活的例子</h2><p>$tanh$函数激活的例子，$-1$为红色，$+1$为蓝色。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane1.png" alt="pane1"></p>
<p>上图分别是记录了行位置、引文和if语句特征的例子和失败的例子。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/pane2.png" alt="pane2"></p>
<p>上图分别是记录代码中注释、代码嵌套深度和行末标记特征的例子。</p>
<h2 id="Gates数值的统计"><a href="#Gates数值的统计" class="headerlink" title="Gates数值的统计"></a>Gates数值的统计</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/gates.png" alt="gates"></p>
<p>此图信息量很大。</p>
<ol>
<li>left-saturated和right-saturated表示各个Gates激活函数（$sigmoid$）小于0.1和大于0.9，即总是阻止信息流过和总是允许信息流过。</li>
<li>横轴和纵轴表示该Gate处于这两种状态的时间比例，即有多少时间是阻塞状态，有多少时间是畅通状态。</li>
<li>三种颜色表示不同的层。</li>
</ol>
<p>有以下几个观察：</p>
<ol>
<li>第一层的门总是比较中庸，既不阻塞，也不畅通</li>
<li>第二三层的门在这两种状态间比较分散，经常处于畅通状态的门可能记录了长期的依赖信息，而经常处于阻塞状态的门则负责了短期信息的控制。</li>
</ol>
<h2 id="错误来源分析"><a href="#错误来源分析" class="headerlink" title="错误来源分析"></a>错误来源分析</h2><p>在这一节，作者用了“剥洋葱”的方法，建立了不同的模型将错误进行分解。此处错误指LSTM预测下一个字母产生的错误，数据集为托尔斯泰的《战争与和平》。</p>
<ol>
<li>n-gram</li>
<li>Dynamic n-long memory，即对已经出现过得单词的复现。如句子”Jon yelled at<br>Mary but Mary couldn’t hear him.”中的Mary。</li>
<li>Rare words，不常见单词</li>
<li>Word model，单词首字母、新行、空格之后出现的错误</li>
<li>Punctuation，标点之后</li>
<li>Boost，其他错误</li>
</ol>
<p>根据作者的实验，错误的来源有如下分解：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Visualizing-and-Understanding-Recurrent-Networks/error.png" alt="error"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇文章是打开LSTM黑箱的尝试，提供了序列维度上共享权值的合理性证据，对Gates状态的可视化也非常值得关注，最后对误差的分解可能对新的网络结构有所启发（比如，如何将单词级别和字母级别的LSTM嵌套起来，解决首字母预测的问题？）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文： &lt;a href=&quot;http://arxiv.org/abs/1506.02078&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Visualizing and Understanding Recurrent Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Deep Residual Learning for Image Recognition</title>
    <link href="http://blog.ddlee.cn/2017/04/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Deep-Residual-Learning-for-Image-Recognition/"/>
    <id>http://blog.ddlee.cn/2017/04/30/论文笔记-Deep-Residual-Learning-for-Image-Recognition/</id>
    <published>2017-04-30T15:12:11.000Z</published>
    <updated>2017-05-30T13:37:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>网络在堆叠到越来越深之后，由于BP算法所依赖的链式法则的连乘形式，会出现梯度消失和梯度下降的问题。初始标准化和中间标准化参数在一定程度上缓解了这一问题，但仍然存在更深的网络比浅层网络具有更大的训练误差的问题。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/error.png" alt="error"></p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><h3 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h3><p>多层的网络结构能够任意接近地拟合目标映射$H(x)$，那么也能任意接近地拟合其关于恒等映射的残差函数$H(x)-x$。记$F(x)=H(x)-x$，则原来的目标映射表为$F(x)+x$。由此，可以设计如下结构。</p>
<h3 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/block.jpg" alt="Residual Learning: a building block"></p>
<p>残差单元包含一条恒等映射的捷径，不会给原有的网络结构增添新的参数。</p>
<h2 id="动机-启发"><a href="#动机-启发" class="headerlink" title="动机/启发"></a>动机/启发</h2><p>层数的加深会导致更大的训练误差，但只增加恒等映射层则一定不会使训练误差增加，而若多层网络块要拟合的映射与恒等映射十分类似时，加入的捷径便可方便的发挥作用。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章中列举了大量在ImagNet和CIFAR-10上的分类表现，效果很好，在此不表。</p>
<h2 id="拾遗"><a href="#拾遗" class="headerlink" title="拾遗"></a>拾遗</h2><h4 id="Deeper-Bottleneck-Architectures"><a href="#Deeper-Bottleneck-Architectures" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/Bottleneck.png" alt="Bottleneck"></p>
<p>两头的1 * 1巻积核先降维再升维，中间的3 * 3巻积核成为“瓶颈”，用于提取重要的特征。这样的结构跟恒等映射捷径配合，在ImageNet上有很好的分类效果。</p>
<h4 id="Standard-deviations-of-layer-responses"><a href="#Standard-deviations-of-layer-responses" class="headerlink" title="Standard deviations of layer responses"></a>Standard deviations of layer responses</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-Deep-Residual-Learning-for-Image-Recognition/std.png" alt="std"><br>上图是在CIFAR-10数据集上训练的网络各层的相应方差（Batch-Normalization之后，激活之前）。可以看到，残差网络相对普通网络有更小的方差。这一结果支持了残差函数比非残差函数更接近于0的想法（即更接近恒等映射）。此外，还显示出网络越深，越倾向于保留流过的信息。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>深度残差网络在当年的比赛中几乎是满贯。<br>下面是我的一些（未经实验证实的）理解：</p>
<p>首先，其”跳级”的网络结构对深度网络的设计是一种启发，通过“跳级”，可以把之前网络的信息相对完整的跟后层网络结合起来，即低层次解耦得到的特征和高层次解耦得到的特征再组合。<br>再者，这种分叉的结构可以看作网络结构层面的”Dropout”: 如果被跳过的网络块不能习得更有用的信息，就被恒等映射跳过了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;http://arxiv.org/abs/1512.03385&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
  </entry>
  
</feed>
