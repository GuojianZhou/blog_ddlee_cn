<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>萧爽楼</title>
  <subtitle>李家丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.ddlee.cn/"/>
  <updated>2018-01-06T13:23:16.040Z</updated>
  <id>http://blog.ddlee.cn/</id>
  
  <author>
    <name>ddlee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[论文笔记]Densely Connected Convolutional Networks</title>
    <link href="http://blog.ddlee.cn/2018/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Densely-Connected-Convolutional-Networks/"/>
    <id>http://blog.ddlee.cn/2018/01/06/论文笔记-Densely-Connected-Convolutional-Networks/</id>
    <published>2018-01-06T13:23:16.000Z</published>
    <updated>2018-01-06T13:23:16.040Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">https://arxiv.org/abs/1608.06993</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>DenseNet将shortcut-connection的思路发挥到极致。在一个DenseBlock内部，每一层的输出均跟后面的层建立shortcut，特别需要注意的是，不同于ResNet中的相加，DenseNet连接shortcut的方式是Concat，这样越深的层则输入channel数越大。</p>
<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/arch.png" alt="arch"></p>
<p>整个网络被分为Dense Block和Transition Layer，前者内部进行密集连接，保持同样大小的feature map，后者为DenseBlock之间的连接层，完成下采样操作。</p>
<p>在每个DenseBlock内部，接受的数据维度会随层数加深而变大（因为不断拼接了之前层的输出），增长的速率即为初始的channel数，文章称这一channel数为growth rate，作为模型的一个超参数。初始的growth rate为32时，在DenseNet121架构下，最后一层的channel数将增长到1024。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/56cb18697f42eb0374d933446f45b151" target="_blank" rel="external">Netscope Vis</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>作者在CIFAR和ImageNet上都做了实验，DenseNet取得了跟ResNet相当的表现，加入Bottleneck和一部分压缩技巧后，用较少的参数就能达到跟ResNet相当的效果：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Densely-Connected-Convolutional-Networks/result.png" alt="arch"></p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1608.06993&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Aggregated Residual Transformations for Deep Neural Networks</title>
    <link href="http://blog.ddlee.cn/2018/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/"/>
    <id>http://blog.ddlee.cn/2018/01/06/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/</id>
    <published>2018-01-06T13:19:34.000Z</published>
    <updated>2018-01-07T08:54:40.638Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">https://arxiv.org/abs/1611.05431</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为”cardinality”。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也添加了层与层间的shortcut connection。但由于其多路径的设计特征，我将其归为Inception系网络。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>深度网络结构上的设计已经有三种经典的范式：</p>
<ul>
<li>Repeat. 由AlexNet和VGG等开拓，几乎被之后所有的网络采用。即堆叠相同的拓扑结构，整个网络成为模块化的结构。</li>
<li>Multi-path. 由Inception系列发扬，将前一层的输入分割到不同的路径上进行变换，最后拼接结果。</li>
<li>Skip-connection. 最初出现于Highway Network，由ResNet发扬并成为标配。即建立浅层信息与深层信息的传递通道，改变原有的单一线性结构。</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>文章将残差函数表示为：</p>
<p>其中，C为本层进行的变换数目，即”cardinality”。</p>
<p>相比Inception-ResNet，ResNeXt相当于将其Inception Module的每条路径规范化了，并将规范后的路径数目作为新的超参数。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/multi-path.png" alt="multi-path"></p>
<p>上图中，路径被扩展为多条，而每条路径的宽度（channel数）也变窄了（64-&gt;4）。</p>
<p><a href="http://ethereon.github.io/netscope/#/gist/c2ba521fcb60520abb0b0da0e9c0f2ef" target="_blank" rel="external">NetScope Vis</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<h3 id="Experiements"><a href="#Experiements" class="headerlink" title="Experiements"></a>Experiements</h3><p>ResNeXt试图在保持参数数目的情况下提高网络性能，提升cardinality的同时使每条路径的宽度变窄。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/setting.png" alt="setting"></p>
<p>对比其他网络的结果：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Aggregated-Residual-Transformations-for-Deep-Neural-Networks/result.png" alt="result"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>ResNeXt较为突出的是把Inception单元规范化了，摆脱了需要精心设计Inception单元中卷积结构的问题，更好地组织了参数。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      本文提出了深度网络的新维度，除了深度、宽度（Channel数）外，作者将在某一层并行transform的路径数提取为第三维度，称为&quot;cardinality&quot;。跟Inception单元不同的是，这些并行路径均共享同一拓扑结构，而非精心设计的卷积核并联。除了并行相同的路径外，也添加了层与层间的shortcut connection。但由于其多路径的设计特征，我将其归为Inception系网络。
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <link href="http://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/"/>
    <id>http://blog.ddlee.cn/2018/01/04/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/</id>
    <published>2018-01-04T13:10:09.000Z</published>
    <updated>2018-01-06T13:19:20.261Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>MobileNets系列可以看做是继Xception之后对Depthwise Separable Convolution的又一推动。利用深度可分离的特征，MobileNets系列引入两个模型精度和大小的超参，在保持相当精度的同时享有非常小的计算消耗，适用于移动端情形，因而被命名为”MobileNets”。</p>
<h3 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a>Depthwise Separable Convolution</h3><p>深度可分离卷积是近期深度网络设计的重要趋势。最早见于L. Sifre的PhD论文Rigid-motion scattering for image classification，其1×1卷积在Inception, ResNet, SqueezeNet等网络中作为降维bottleneck使用。Xception指出，Inception单元本质上假设了跨通道和跨空间相关性的解耦关系，并将这一解耦关系推向极端，用Depthwise Separable Convolution改造了Inception结构。</p>
<p>深度可分离卷积受欢迎的另一重要原因是其参数高效性。将原有卷积换成深度可分离卷积后，可以享受到模型压缩的增益。</p>
<p>标准的卷积操作，可以认为是大小为DK的窗口在DF大小的特征图上滑动计算，计算复杂性为：</p>
<p>DK×DK × M×N × DF×DF</p>
<p>其中，M和N分别代表输入channel数和输出channel数。</p>
<p>替换为深度可分离卷积后，先进行Depthwise Convolution，再进行1×1 Pointwise Convolution，计算复杂性为：</p>
<p>DK×DK × M × DF×DF + M×N × DF×DF</p>
<p>相比下，深度可分离卷积的计算复杂性约为原来的(1/N+1/DK^2)。</p>
<p>进一步地，MobileNet添加两个超参来控制这一压缩程度，alpha为channel数压缩系数，rho为分辨率压缩系数：</p>
<p>DK×DK × alpha×M × rho×DF× rho×DF + alpha×M × alpha×N × rho×DF × rho×DF</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/depthwise-seperable.png" alt="depthwise-seperable"></p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>MobileNet的基本结构如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/arch.png" alt="arch"></p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1704.04861&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;&gt;&lt;a h
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Xception: Deep Learning with Depthwise Seperable Convolutions</title>
    <link href="http://blog.ddlee.cn/2018/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/"/>
    <id>http://blog.ddlee.cn/2018/01/02/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/</id>
    <published>2018-01-02T13:04:30.000Z</published>
    <updated>2018-01-06T13:26:12.593Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="external">https://arxiv.org/abs/1610.02357</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本篇是keras库作者的文章，对Inception结构进行了改进：用Depth-wise seperable convolution替换了Inception单元中的1×1卷积和3×3卷积。</p>
<p>文章对Inception结构的评论非常有见地。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>文章指出，Inception单元背后的假设是跨Channel和跨空间的相关性可以充分解耦，类似的还有长度和高度方向上的卷积结构（在Inception-v3里的3×3卷积被1×3和3×1卷积替代）。</p>
<p>进一步的，Xception基于更强的假设：跨channel和跨空间的相关性完全解耦。这也是Depthwise Separable Convolution所建模的理念。</p>
<p>一个简化的Inception单元：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception.png" alt="inception"></p>
<p>等价于：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception2.png" alt="inception"></p>
<p>将channel推向极端，即每个channel都由独立的3×3卷积处理：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/inception3.png" alt="inception"></p>
<p>这样就得到了Depthwise Separable Convolution。</p>
<h3 id="Architectrue"><a href="#Architectrue" class="headerlink" title="Architectrue"></a>Architectrue</h3><p>简单讲，Xception是线性堆叠的Depthwise Separable卷积，附加了Skip-connection。</p>
<p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/931d7c91b22109f83bbbb7ff1a215f5f" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Xception-Deep-Learning-with-Depthwise-Seperable-Convolutions/arch.png" alt="inception"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文的实验部分并没有像其他论文那样集成一个在ImageNet上SOTA的结果，而是以Inception-v3为基线，对比了参数数量和性能，认为提升正来自于更合理的参数利用。文章还对比了Residual的作用，在Xception网络中，Skip-connection不仅能提高训练速度，还能增强模型的性能。</p>
<h3 id="Concolusion"><a href="#Concolusion" class="headerlink" title="Concolusion"></a>Concolusion</h3><p>本文贡献主要对Inception单元的解读和引入Depthwise Seperable Convolution。更多对于Depthwise Seperable Convolution的描述，请参考<a href="https://blog.ddlee.cn/2018/01/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications/">MobileNets</a>的笔记。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1610.02357&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
    <link href="http://blog.ddlee.cn/2017/12/26/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/"/>
    <id>http://blog.ddlee.cn/2017/12/26/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/</id>
    <published>2017-12-26T12:59:02.000Z</published>
    <updated>2018-01-06T13:18:43.158Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">https://arxiv.org/abs/1602.07261</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在15年，ResNet成为那年最耀眼的卷积网络结构，skip-connection的结构也成为避不开的考虑选项。Inception系列也参考ResNet更新了自己的结构。同时推出了第四代和跟ResNet的结合版：Inception-v4和Inception-ResNet。</p>
<p>然而，这是一篇几乎都是图的论文。</p>
<p>所以，上图。</p>
<h3 id="Inception-v4-Architecture"><a href="#Inception-v4-Architecture" class="headerlink" title="Inception-v4 Architecture"></a>Inception-v4 Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/e0ac64013b167844053184d97b380978" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch1.jpg" alt="arch1"></p>
<h3 id="Inception-ResNet-v2-Architecture"><a href="#Inception-ResNet-v2-Architecture" class="headerlink" title="Inception-ResNet(v2) Architecture"></a>Inception-ResNet(v2) Architecture</h3><p>NetScope Vis请参见<a href="http://ethereon.github.io/netscope/#gist/aadd97383baccabb8b827ba507c24162" target="_blank" rel="external">这里</a>，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning/arch2.jpg" alt="arch1"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章在实验部分提到，不借助Skip-connection的结构也可以将Inception网络提升到SOTA的水准，但加入Skip-connection可以有效增加训练速度。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>卷积网络结构的演进遇到了瓶颈，在ImageNet上的提升边界似乎碰到天花板，且更多来自训练技巧和集成。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07261&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1602.07261&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Speed/accuracy trade-offs for modern convolutional object detectors</title>
    <link href="http://blog.ddlee.cn/2017/12/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/"/>
    <id>http://blog.ddlee.cn/2017/12/24/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/</id>
    <published>2017-12-24T13:55:22.000Z</published>
    <updated>2017-12-27T14:02:35.092Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="external">https://arxiv.org/abs/1611.10012</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。</p>
<p>文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了Proposal个数、输入图片尺寸，生成Feature map的大小等作为超参，并行实验，探索精度和速度方面的trade-off。</p>
<p>文章也将源码公开，作为Tensorflow的Object Detection API。</p>
<p>下图是三种元结构的图示：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/meta-arch.png" alt="meta-arch"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig2.png" alt="meta-arch"></p>
<p>信息量非常大的一张图。</p>
<ul>
<li>横纵两个维度分别代表速度和准确度，横轴越靠左说明用时越少，纵轴越靠上说明mAP表现越好，因而，sweet spot应分布在左上角</li>
<li>两个超维是元结构和特征提取网络，元结构由形状代表，特征提取网络由颜色代表</li>
<li>虚线代表理想中的trade-off边界</li>
</ul>
<p>分析：</p>
<ul>
<li>准确度最高的由Faster-RCNN元结构、Inception-ResNet提取网络，高分图片，使用较大的feature map达到，如图右上角</li>
<li>较快的网络中准确度表现最好的由使用Inception和Mobilenet的SSD达到</li>
<li>sweet spot区特征提取网络由ResNet统治，较少Proposal的Faster-RCNN可以跟R-FCN相当</li>
<li>特征提取网络方面，Inception V2和MobileNet在高速度区，Incep-ResNet和ResNet在sweet spot和高精度区，Inception V3和VGG则远离理想边界（虚线）</li>
</ul>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig3.png" alt="meta-arch"></p>
<p>上图是特征提取网络对三种元结构的影响，横轴是特征提取网络的分类准确率，纵轴是检测任务上的mAP表现，可以看到，SSD在纵轴方向上方差最小，而Faster-RCNN和R-FCN对特征提取网络更为敏感。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig4.png" alt="meta-arch"></p>
<p>上图的横轴是不同的特征提取网络，组内是三种元结构的对比，纵轴是不同尺寸物体的mAP。</p>
<p>可以看到，在大物体的检测上，使用较小的网络时，SSD的效果跟两阶段方法相当，更深的特征提取网络则对两阶段方法的中型和小型物体的检测提升较大（ResNet101和Incep-ResNet都显现了两阶段方法在小物体上的提升）</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig5.png" alt="meta-arch"></p>
<p>上图显示了输入图片尺寸对mAP的影响。高分的图片对小物体检测帮助明显，因而拥有更高的精度，但相对运行速度会变慢。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig6.png" alt="meta-arch"></p>
<p>上图探究了两阶段方法中Proposal个数的影响，左边是Faster-RCNN，右边是R-FCN，实线是mAP，虚线是推断时间。<br>分析：</p>
<ul>
<li>相比R-FCN，Faster-RCNN推断时间对Proposal个数相当敏感（因为有per ROI的计算）</li>
<li>减少Proposal的个数，并不会给精度带来致命的下降</li>
</ul>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig7.png" alt="meta-arch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig8.png" alt="meta-arch"></p>
<p>上面两图是对FLOPS的记录，相对GPU时间更为中立，在图8中，GPU部分显现了ResNet跟Inception的分野（关于45度线，此时FLOPS跟GPU时间相当），文章认为分解操作(Factorization)减少了FLOPs，但增加了内存的IO时间，或者是GPU指令集更适合密集的卷积计算。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig9.png" alt="meta-arch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig10.png" alt="meta-arch"></p>
<p>上两图是对内存占用的分析，总体来说，特征提取网络越精简、feature map尺寸越小，占用内存越少，运行时间也越短。</p>
<p>最后，文章描述了他们ensemble的思路，在一系列不同stride、loss和配置的Faster-RCNN中（ResNet和Incep-ResNet为特征提取网络），贪心地选择验证集上AP较高的，并且去除类AP相似的模型。选择的5个用于ensemble的模型如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/table5.png" alt="meta-arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇文章是不错的实验结果报告，测试了足够多的模型，也得出了合理的和有启发的结论。几点想法：</p>
<ul>
<li>RFCN并没有很好的解决定位跟分类的矛盾，per ROI的子网络最好还是要有，但要限制Proposal的个数（实际大部分都是负样本）来减少冗余</li>
<li>小物体的检测仍然是最大的难点，增大分辨率和更深的网络确有帮助，但不是实质的。</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.10012&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1611.10012&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Light-Head R-CNN: In Defense of Two-Stage Object Detector</title>
    <link href="http://blog.ddlee.cn/2017/12/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/"/>
    <id>http://blog.ddlee.cn/2017/12/22/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/</id>
    <published>2017-12-22T13:55:36.000Z</published>
    <updated>2017-12-27T14:00:34.351Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="external">https://arxiv.org/abs/1711.07264</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。</p>
<p>针对这两种元结构(Faster-RCNN和RFCN)，文章提出了“头”轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。</p>
<h2 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/arch.png" alt="arch"></p>
<p>如上图，虚线框出的部分是三种结构的RCNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为sperable convolution，产生的Score map也减少至10×p×p（相比原先的#class×p×p）。</p>
<p>一个可能的解释是，“瘦”（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较“厚”的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。</p>
<p>进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。</p>
<p>light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。</p>
<p>粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分，文章验证了较“瘦”的Score map不会对精度产生太大损害，也展现了ROI Align, Multiscale train等技巧对基线的提升过程。</p>
<p>文章的主要结果如下面两图（第一个为高精度，第二个为高速度）：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result1.png" alt="result1"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result2.png" alt="result2"></p>
<p>只能说这样的对比比较诡异。</p>
<p>第一张图中三个light-head结果并不能跟上面的其他结构构成多少有效的对照组，要么scale不同，要么FPN, multi-scale, ROI Align不同。唯一的有效对照是跟Mask-RCNN。</p>
<p>在高精度方面，基础网络不同，采用的scale也不同，没有有效的对照组。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我并不觉得这是对两阶段方法的Defense。文章对两阶段方法在精度和速度方面的分析比较有见地，但实验的结果并不能可靠地支撑light-head的有效性。相比之下Google的那篇trade-off可能更有参考价值。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.07264&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1711.07264&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]You Only Look Once: Unified, Real Time Object Detection</title>
    <link href="http://blog.ddlee.cn/2017/12/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-You-Only-Look-Once-Unified-Real-Time-Object-Detection/"/>
    <id>http://blog.ddlee.cn/2017/12/20/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/</id>
    <published>2017-12-20T13:38:31.000Z</published>
    <updated>2017-12-27T14:03:40.021Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">https://arxiv.org/abs/1506.02640</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。</p>
<p>YOLO的主要优点：</p>
<ul>
<li>快。</li>
<li>全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。</li>
<li>泛化性能好，在艺术作品上做检测时，YOLO表现好。</li>
</ul>
<h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><p>YOLO的大致工作流程如下：<br>1.准备数据：将图片缩放，划分为等分的网格，每个网格按跟ground truth的IOU分配到所要预测的样本。<br>2.卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IOU表示）。测试时，分数如下计算：</p>
<p>等式左边第一项由网格预测，后两项由每个box预测，综合起来变得到每个box含有不同类别物体的分数。<br>因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，S为网格数，B为每个网格生成box个数，C为类别数。<br>3.后处理：使用NMS过滤得到的box</p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/loss.jpg" alt="loss-function"></p>
<p>图片来自<a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/24916786</a></p>
<p>损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，loss中添加了权重并将长宽取根号。</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/error.png" alt="error"></p>
<p>相比Fast-RCNN，YOLO的背景误检在错误中占比重小，而位置错误占比大（未采用log编码）。</p>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>YOLO划分网格的思路还是比较粗糙的，每个网格生成的box个数也限制了其对小物体和相近物体的检测。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1506.02640&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Rethinking the Inception Architecture for Computer Vision</title>
    <link href="http://blog.ddlee.cn/2017/12/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/"/>
    <id>http://blog.ddlee.cn/2017/12/16/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/</id>
    <published>2017-12-16T12:53:29.000Z</published>
    <updated>2018-01-06T12:54:12.429Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">https://arxiv.org/abs/1512.00567</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一网络结构，v3版本相比v2版本加入了RMSProp，Label Smoothing等技巧。</p>
<p>文章表述了Inception系列的几个设计原则，并根据这些原则改进了GoogLeNet的结构。</p>
<h3 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h3><ul>
<li>Avoid representational bottlenecks, especially early in the network. 建议不要在过浅的阶段进行特征压缩，而维度只是一个表达复杂性的参考，并不能作为特征复杂性的绝对衡量标准。</li>
<li>Higher dimensional representations are easier to process locally with a network. 高阶的表示更有局部描述力，增加非线性有助于固化这些描述力。</li>
<li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. 基于空间的聚合信息可以在低维空间里处理，而不必担心有太多信息损失。这一点也佐证了1×1卷积的降维作用。</li>
<li>Balance the width and depth of the network.  宽度和深度的增加都有助于网络的表达能力，最好的做法是同时在这两个方向上推进，而非只顾及一个。</li>
</ul>
<h3 id="Factorizing-Convolution"><a href="#Factorizing-Convolution" class="headerlink" title="Factorizing Convolution"></a>Factorizing Convolution</h3><p>分解一直是计算数学里经典的思路。从牛顿法到BFGS，就是把Hessian矩阵（或其逆）用一系列的向量操作来表示和近似，避免矩阵的计算。</p>
<p>本文提出了两种卷积结构方面的分解，一个是在卷积核的层面，另一个是在空间方面。</p>
<p>第一种分解是将大核卷积分解成串联的小核卷积。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor3.png" alt="factor5"></p>
<p>用两个3×3的卷积代替5×5的卷积，带来的参数减少为(9+9)/(5×5).</p>
<p>第二种分解是在卷积核本身上，引入非对称卷积：用3×1和1×3的卷积串联代替3×3卷积。如下图所示。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor1.png" alt="factor3"></p>
<p>这种分解也可以推广到n维情况，且n越大，带来的收益越明显。</p>
<p>空间上的卷积分解建模了这样的情形：两个方向上的卷积参数互相正交，便被空间分解卷积解耦。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/spatial-seperable.png" alt="factor5"></p>
<h3 id="Utility-of-Auxiliary-Classifiers"><a href="#Utility-of-Auxiliary-Classifiers" class="headerlink" title="Utility of Auxiliary Classifiers"></a>Utility of Auxiliary Classifiers</h3><p>在GoogLeNet中，作者用loss监督了低维的特征图的学习，但进一步的实验发现，加入BN层后，这些增益被抵消了，于是Auxiliary Classifier可被看做是某种正则化技术，在加入BN的网络中便不再应用。</p>
<h3 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h3><p>这一节讨论网络中的特征降维，即下采样的过程，通常由卷积层或Pooling层的stride参数控制。文章为避免原则一中提到的Representation Bottleneck，在进行Pooling之前将网络加宽（通过Channel数的增加），这也对应了平衡宽度和深度的原则。</p>
<p>最终结合了Inception结构和下采样需求的单元如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/downsample.png" alt="factor5"></p>
<p>不同于Inception单元，上面的1×1卷积扩展了Channel，并且3×3卷积采用了stride=2。</p>
<h3 id="Inception-v2-amp-Inception-v3-Architecture"><a href="#Inception-v2-amp-Inception-v3-Architecture" class="headerlink" title="Inception-v2 &amp; Inception-v3 Architecture"></a>Inception-v2 &amp; Inception-v3 Architecture</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/arch.png" alt="factor5"></p>
<p>可以看到随深度增加，Channel数也在扩展，而Inception单元也遵从了堆叠的范式。</p>
<p>其中三种Inception单元分别为：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptiona.png" alt="factor5"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionb.png" alt="factor5"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionc.png" alt="factor5"></p>
<p>另外，也可以查看<a href="http://ethereon.github.io/netscope/#gist/a2394c1c4a9738469078f096a8979346" target="_blank" rel="external">NetScope Vis</a>来熟悉Inception-v3的结构，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下面是Inception结构演化带来的增益分解：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/experiment.png" alt="factor5"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本篇是对Inception系网络的推进，其分解的思想成为又一网络设计的指导原则。</p>
<p>对卷积的进一步理解，可以参考这个<a href="https://graphics.stanford.edu/courses/cs178-10/applets/convolution.html" target="_blank" rel="external">页面</a>，这一工具可视化了不同卷积核对输入的处理，给出的例子都是在早期人们手工设计的滤波器，而深度网络隐式地学习到了这些滤波器的卷积表达。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1512.00567&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;&gt;&lt;a h
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]SSD: Single Shot MultiBox Detector</title>
    <link href="http://blog.ddlee.cn/2017/12/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-SSD-Single-Shot-MultiBox-Detector/"/>
    <id>http://blog.ddlee.cn/2017/12/12/论文笔记-SSD-Single-Shot-MultiBox-Detector/</id>
    <published>2017-12-12T13:37:56.000Z</published>
    <updated>2018-01-06T14:58:03.601Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/151.023325" target="_blank" rel="external">https://arxiv.org/abs/151.023325</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保持较快的运行速度。</p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-SSD-Single-Shot-MultiBox-Detector/arch.jpg" alt="arch"></p>
<ul>
<li><p>多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。</p>
</li>
<li><p>更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SSD有点像多分类的RPN，生成anchor box，再对box预测分数和位置调整值。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/151.023325&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/151.023325&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Feature Pyramid Networks for Object Detection</title>
    <link href="http://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Feature-Pyramid-Networks-for-Object-Detection/"/>
    <id>http://blog.ddlee.cn/2017/12/07/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/</id>
    <published>2017-12-07T13:55:59.000Z</published>
    <updated>2017-12-27T14:00:23.112Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">https://arxiv.org/abs/1612.03144</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。</p>
<p>回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。</p>
<p>一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的内存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。</p>
<p>本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。</p>
<h3 id="Feature-Pyramid-Networks"><a href="#Feature-Pyramid-Networks" class="headerlink" title="Feature Pyramid Networks"></a>Feature Pyramid Networks</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/arch.png" alt="arch"></p>
<p>从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行元素位置上的叠加（lateral connection），构成这一深度上的特征。</p>
<p>这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章的主要实验结果如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/result.png" alt="Experiments results"></p>
<p>对比不同head部分，输入feature的变化对检测精度确实有提升，而且，lateral和top-down两个操作也是缺一不可。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。</p>
<p>个人方面，FPN跟multi-scale的区别在哪，还值得进一步探索。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.03144&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1612.03144&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]R-FCN: Object Detection via Region-based Fully Convolutinal Networks</title>
    <link href="http://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/"/>
    <id>http://blog.ddlee.cn/2017/12/07/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/</id>
    <published>2017-12-07T13:55:48.000Z</published>
    <updated>2017-12-27T14:04:15.263Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="external">https://arxiv.org/abs/1605.06409</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等），这一点是由分类任务的平移不变性和检测任务的平移敏感性之间的矛盾导致的。换句话说，检测模型采用了分类模型的特征提取器，丢失了位置信息。这篇文章提出采用“位置敏感分数图”的方法解决这一问题。</p>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI Pooling"></a>Position-sensitive score maps &amp; Position-sensitive RoI Pooling</h3><p>位置敏感分数图的生成有两个重要操作，一是生成更“厚”的feature map，二是在RoI Pooling时选择性地输入feature map。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/rfcn.png" alt="arch"></p>
<p>Faster R-CNN中，经过RPN得到RoI，转化成分类任务，还加入了一定量的卷积操作（ResNet中的conv5部分），而这一部分卷积操作是不能共享的。R-FCN则着眼于全卷积结构，利用卷积操作在Channel这一维度上的自由性，赋予其位置敏感的意义。下面是具体的操作：</p>
<ul>
<li>在全卷积网络的最后一层，生成k^2(C+1)个Channel的Feature map，其中C为类别数，k^2代表k×k网格，用于分别检测目标物体的k×k个部分。即是用不同channel的feature map代表物体的不同局部（如左上部分，右下部分）。</li>
<li>将RPN网络得到的Proposal映射到上一步得到的feature map（厚度为k×k×(C+1)，）后，相应的，将RoI等分为k×k个bin，对第(i,j)个bin，仅考虑对应(i,j)位置的(C+1)个feature map，进行如下计算：其中(x0,y0)是这个RoI的锚点，得到的即是(i,j)号bin对C类别的相应分数。</li>
<li>经过上一步，每个RoI得到的结果是k^2(C+1)大小的分数张量，k×k编码着物体的局部分数信息，进行vote（平均）后得到(C+1)维的分数向量，再接入softmax得到每一类的概率。</li>
</ul>
<p>上面第二步操作中“仅选取第(i, j)号feature map”是位置信息产生意义的关键。</p>
<p>这样设计的网络结构，所有可学习的参数都分布在可共享的卷积层，因而在训练和测试性能上均有提升。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>R-FCN是对Faster R-CNN结构上的改进，部分地解决了位置不变性和位置敏感性的矛盾。通过最大化地共享卷积参数，使得在精度相当的情况下训练和测试效率都有了很大的提升。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.06409&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1605.06409&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Going deeper with convolutions</title>
    <link href="http://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/"/>
    <id>http://blog.ddlee.cn/2017/11/30/论文笔记-Going-deeper-with-convolutions/</id>
    <published>2017-11-30T12:39:58.000Z</published>
    <updated>2018-01-06T12:46:06.285Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的GoogLeNet拿下了ILSVRC14分类任务的头名。文章也探讨了网络在不断加深的情况下如何更好地利用计算资源，这一理念也是Inception系列网络的核心。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Inception单元的启发主要来自Network in Network结构和Arora等人在神经科学方面的工作。</p>
<p>提高深度模型的一个简单想法是增加深度，但这样带来过拟合的风险和巨大的计算资源消耗，对数据量和计算力的要求可能会超过网络加深带来的收益。</p>
<p>解决这些问题的基本思路是使用稀疏连接的网络，而这也跟Arora等人工作中的Hebbian principle吻合：共同激活的神经元常常集聚在一起。换句话说，某一层激活的神经元只向下一层中特定的几个神经元传递激活信号，而向其他神经元几乎不传递信息，即仅有少部分连接是真正有效的，这也是稀疏的含义。</p>
<p>然而另一方面，现代计算架构对稀疏的计算非常低效，更适合的是密集的计算，这样便产生了矛盾。而Inception单元的提出就是为了用密集的结构来近似稀疏结构，在建模稀疏连接的同时又能利用密集计算的优势。</p>
<p>很多文章认为inception结构的意义在于将不同大小核的卷积并行连接，然后让网络自行决定采用哪种卷积来提取特征，有些无监督的意味，然后将1×1的卷积解释为降维操作。这种想法有待验证，是否在5×5卷积有较强激活的时候，3×3卷积大部分没有激活，还是两者能够同时有较强的激活？不同的处理阶段这两种卷积核的选择有没有规律？</p>
<p>在此提出一个个人的理解，欢迎讨论。</p>
<p>首先是channel的意义。我们知道，卷积之所以有效，是因为它建模了张量数据在空间上的局部相关性，加之Pooling操作，将这些相关性赋予平移不变性（即泛化能力）。而channel则是第三维，它实际上是卷积结构中的隐藏单元，是中间神经元的个数。卷积层在事实上是全连接的：每个Input channel都会和output channel互动，互动的信息只不过从全连接层的weight和bias变成了卷积核的weight。</p>
<p>这种全连接是冗余的，本质上应是一个稀疏的结构。Inception单元便在channel这个维度上做文章，采用的是类似矩阵分块的思想。</p>
<p>根据Hebbian principle，跨channel的这些神经元，应是高度相关的，于是有信息压缩的空间，因而使用跨channel的1×1的卷积将它们嵌入到低维的空间里（比如，Inception4a单元的输入channel是512，不同分支的1×1卷积输出channel则是192,96,16和64，见下面GoogLeNet结构表），在这个低维空间里，用密集的全连接建模（即3×5和5×5卷积），它们的输出channel相加也再恢复到原来的输入channel维度（Inception4a分别是192+208+48+64），最后的连接由Concat操作完成（分块矩阵的合并），这样就完成了分块密集矩阵对稀疏矩阵的近似。</p>
<p>这样来看，3×3和5×5大小的选择并不是本质的，本质的是分块低维嵌入和concat的分治思路。而在ResNeXt的工作中，这里的分块被认为是新的维度（称为cardinality），采用相同的拓扑结构。</p>
<h3 id="Stacked-Inception-Module"><a href="#Stacked-Inception-Module" class="headerlink" title="Stacked Inception Module"></a>Stacked Inception Module</h3><p>在GoogLeNet中，借鉴了AlexNet和VGG的stack(repeat)策略，将Inception单元重复串联起来，构成基本的特征提取结构。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/arch.png" alt="arch"></p>
<h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><p>朴素版本的Inception单元会带来Channel维数的不断增长，加入的1×1卷积则起到低维嵌入的作用，使Inception单元前后channel数保持稳定。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/inception.png" alt="arch"></p>
<h3 id="Auxililary-Classifier"><a href="#Auxililary-Classifier" class="headerlink" title="Auxililary Classifier"></a>Auxililary Classifier</h3><p>这里是本文的另一个贡献，将监督信息传入中间的feature map，构成一个整合loss，作者认为这样有助于浅层特征的学习。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/auxililary.png" alt="arch"></p>
<h3 id="Architecture-of-GoogLeNet"><a href="#Architecture-of-GoogLeNet" class="headerlink" title="Architecture of GoogLeNet"></a>Architecture of GoogLeNet</h3><p>下面的表显示了GoogLeNet的整体架构，可以留意到Inception单元的堆叠和Channel数在子路径中的变化。NetScope可视化可参见<a href="http://ethereon.github.io/netscope/#/gist/db8754ee4b239920b3df5ab93220a84b" target="_blank" rel="external">GoogLeNet Vis</a>。源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/table.png" alt="arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>文章是对NiN思想的继承和推进，不同于AlexNet和VGG，网络的模块化更加凸显，多路径的结构也成为新的网络设计范本，启发了众多后续网络结构的设计。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的G
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]MegDet: A Large Mini-Batch Object Detector</title>
    <link href="http://blog.ddlee.cn/2017/11/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MegDet-A-Large-Mini-Batch-Object-Detector/"/>
    <id>http://blog.ddlee.cn/2017/11/21/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/</id>
    <published>2017-11-21T15:30:56.000Z</published>
    <updated>2017-11-21T15:44:58.572Z</updated>
    
    <content type="html"><![CDATA[<p>本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。</p>
<h2 id="Object-Detection-Progress-Summay"><a href="#Object-Detection-Progress-Summay" class="headerlink" title="Object Detection Progress Summay"></a>Object Detection Progress Summay</h2><p>检测方法回顾：R-CNN, Fast/Faster R-CNN, Mask RCNN, RetinaNet(Focal Loss), ResNet(backbone network),</p>
<p>文章先指出前述方法大多是框架、loss等的更新，而均采用非常小的batch（2张图片）训练，有如下不足：</p>
<ul>
<li>training slow</li>
<li>fails to provide accurate statistics for BN</li>
</ul>
<p>这里涉及一个问题，检测任务的源数据，到底应该是图片还是标注框。在Fast R-CNN中，RBG提到SPPNet等每个batch采样的标注框来自不同的图片，之间不能共享卷积运算（卷积运算是以图片为单位的）。为了共享这部分计算，Fast R-CNN采用了“先选图片，再选标注框”的策略来确定每个batch，文章提到这种操作会引入相关性，但在实际中却影响不大。之后的Faster R-CNN，每张图片经过RPN产生约300个Proposal，传入RCNN做法也成了通用做法。</p>
<p>个人认为检测任务的数据，应该是以图片为单位的。物体在图片的背景中才会产生语义，而尽管每张图片有多个Proposal（近似分类任务中的batch大小），但它们共享的是同一个语义（场景），而单一的语义难以在同一个batch中提供多样性来供网络学习。</p>
<h5 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h5><p>Increasing mini-batch size requires large learning rate, which may cause discovergence.</p>
<h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>new explanation of linear scaling rule, introduce “warmup” trick to learning rate schedule</li>
<li>Cross GPU Batch Normalization(CGBN)</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="Variance-Equivalence-explanation-for-Linear-Scaling-Rule"><a href="#Variance-Equivalence-explanation-for-Linear-Scaling-Rule" class="headerlink" title="Variance Equivalence explanation for Linear Scaling Rule"></a>Variance Equivalence explanation for Linear Scaling Rule</h3><p>linear scaling rule 来自更改batch size 时，同时放缩learning rate，使得更改后的weight update相比之前小batch size， 多步的weight update类似。而本文用保持loss gradient的方差不变重新解释了linear scaling rule，并指出这一假定仅要求loss gradient是i.i.d，相比保持weight update所假设的不同batch size间loss gradient相似更弱。</p>
<p>参见<a href="https://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/#Motivation">Accurate Large Minibatch SGD: Training ImageNet in One Hour</a>，近似时假设了求和项变化不大，这一条件在Object Detection中可能不成立，不同图片的标注框（大小、个数）差别很大。</p>
<h3 id="WarmUp-Strategy"><a href="#WarmUp-Strategy" class="headerlink" title="WarmUp Strategy"></a>WarmUp Strategy</h3><p>在训练初期，weight抖动明显，引入warmup机制来使用较小的学习率，再逐渐增大到Linear scaling rule要求的学习率。</p>
<h3 id="Cross-GPU-Batch-Normalization"><a href="#Cross-GPU-Batch-Normalization" class="headerlink" title="Cross-GPU Batch Normalization"></a>Cross-GPU Batch Normalization</h3><p>BN是使深度网络得以训练和收敛的关键技术之一，但在检测任务中，fine-tuning阶段常常固定了SOTA分类网络的BN部分参数，不进行更新。</p>
<p>检测中常常需要较大分辨率的图片，而GPU内存限制了单卡上的图片个数，提高batch size意味着BN要在多卡（Cross-GPU）上进行。</p>
<p>BN操作需要对每个batch计算均值和方差来进行标准化，对于多卡，具体做法是，单卡独立计算均值，聚合（类似Map-Reduce中的Reduce）算均值，再将均值下发到每个卡，算差，再聚合起来，计算batch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。</p>
<p>流程如图：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/cgbn.png" alt="cgbn"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在COCO数据集上的架构用预训练ResNet-50作为基础网络，FPN用于提供feature map。</p>
<p>结果显示，不使用BN时，较大的batch size（64,128）不能收敛。使用BN后，增大Batch size能够收敛但仅带来较小的精度提升，而BN的大小也不是越大越好，实验中，32是最好的选择。主要结果如下表：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/results.png" alt="results"></p>
<p>按epoch，精度的变化如下图，小batch（16）在最初的几个epoch表现比大batch（32）要好。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/byepoch.png" alt="accuracy-by-epoch"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇论文读起来总感觉少了些东西。对Linear scale rule的解释固然新颖，但没有引入新的trick（只是确认了检测仍是需要Linear scale rule的）。多卡的BN确实是非常厉害的工程实现（高效性），但实验的结果并没有支持到较大的batch size（128,256）比小batch精度更好的期望，而最后的COCO夺冠模型整合了多种trick，没有更进一步的错误分析，很难支撑说明CGBN带来的关键作用。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。&lt;/p&gt;
&lt;h2 id=&quot;Object-Detection-Progress-Summay&quot;&gt;&lt;a href=&quot;#Ob
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks</title>
    <link href="http://blog.ddlee.cn/2017/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/"/>
    <id>http://blog.ddlee.cn/2017/10/21/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/</id>
    <published>2017-10-21T15:39:34.000Z</published>
    <updated>2017-12-27T14:08:20.395Z</updated>
    
    <content type="html"><![CDATA[<p>Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks</p>
<p><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Faster R-CNN是2-stage方法的主流方法，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA。</p>
<h2 id="Regional-Proposal-Networks"><a href="#Regional-Proposal-Networks" class="headerlink" title="Regional Proposal Networks"></a>Regional Proposal Networks</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/rpn.png" alt="faster_rcnn_arch"></p>
<p>RPN网络将Proposal这一任务建模为二分类的问题。</p>
<p>第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IOU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本即是anchor box和每个anchor box是否有物体。RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的Loss统一起来，作为RPN网络的目标训练。</p>
<p>RPN网络可调的超参还是很多的，anchor box的大小和长宽比例、IoU的阈值、每张图片上Proposal正负样本的比例等。</p>
<h2 id="Alternate-Training"><a href="#Alternate-Training" class="headerlink" title="Alternate Training"></a>Alternate Training</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/faster_rcnn_netwrok.png" alt="faster_rcnn_arch"></p>
<p>RPN网络是在feature map上进行的，因而可以跟RCNN完全共享feature extractor部分的卷积运算。训练时，RPN和RCNN的训练可以交替进行，即交替地固定RPN和RCNN部分的参数，更新另一部分。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Faster R-CNN的成功之处在于用RPN网络完成了检测任务的“深度化”。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。RPN网络也成为检测2-stage方法的标准部件。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot; target=&quot;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Fast R-CNN</title>
    <link href="http://blog.ddlee.cn/2017/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Fast-R-CNN/"/>
    <id>http://blog.ddlee.cn/2017/10/15/论文笔记-Fast-R-CNN/</id>
    <published>2017-10-15T15:34:31.000Z</published>
    <updated>2017-11-21T15:47:15.805Z</updated>
    
    <content type="html"><![CDATA[<p>Fast R-CNN <a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">https://arxiv.org/abs/1504.08083</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Fast R-CNN 是对R-CNN的改进，作者栏只有RBG一人。文章先指出了R-CNN存在的问题，再介绍了自己的改进思路。文章结构堪称典范，从现存问题，到解决方案、实验细节，再到结果分析、拓展讨论，条分缕析，值得借鉴。而且，RBG开源的代码也影响了后来大部分这一领域的工作。</p>
<h2 id="R-CNN的问题"><a href="#R-CNN的问题" class="headerlink" title="R-CNN的问题"></a>R-CNN的问题</h2><ul>
<li>训练是一个多阶段的过程（Proposal, Classification, Regression）</li>
<li>训练耗时耗力</li>
<li>推断耗时</li>
</ul>
<p>而耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算。</p>
<h2 id="Fast-R-CNN-Architecture"><a href="#Fast-R-CNN-Architecture" class="headerlink" title="Fast R-CNN Architecture"></a>Fast R-CNN Architecture</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/fast-rcnn-arch.png" alt="arch"></p>
<p>上图是Fast R-CNN的架构。图片经过feature extractor产生feature map, 原图上运行Selective Search算法将RoI（Region of Interset）对应到feature map上，再对每个RoI进行RoI Pooling操作便得到等长的feature vector，最后通过FC后并行地进行Classifaction和BBox Regression。</p>
<p>Fast R-CNN的这一结构正是检测任务主流2-stage方法所采用的元结构的雏形。整个系统由Proposal, Feature Extractor, Object Recognition&amp;Localization几个部件组成。Proposal部分被替换成RPN(Faster R-CNN)，Feature Extractor部分使用SOTA的分类CNN网络(ResNet等），而最后的部分常常是并行的多任务结构（Mask R-CNN等）。</p>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><p>这一操作是将不同大小的RoI（feature map上）统一的过程，具体做法是将RoI等分成目标个数的网格，在每个网格上进行max pooling，就得到等长的RoI feature vector。</p>
<h3 id="Mini-batch-Sampling"><a href="#Mini-batch-Sampling" class="headerlink" title="Mini-batch Sampling"></a>Mini-batch Sampling</h3><p>文章指出SPPNet训练较慢的原因在于来自不同图片的RoI不能共享计算，因而Fast R-CNN采用这样的mini-batch采样策略：先采样N张图片，再在每张图片上采样R/N个RoI，构成R大小的mini-batch。</p>
<p>采样时，总是保持25%比例正样本（iou大于0.5），iou在0.1到0.5的作为hard example。</p>
<h3 id="Multi-task-Loss"><a href="#Multi-task-Loss" class="headerlink" title="Multi-task Loss"></a>Multi-task Loss</h3><p>得到RoI feature vector后，后续的操作是一个并行的结构，Fast R-CNN将Classification和Regression的损失统一起来，并且在Regression中用更鲁棒的Smooth L1 Loss代替L2 Loss。</p>
<h3 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h3><p>文章还发现，对于预训练的VGG网络，开放Conv部分的参数更新有助于性能的提升，而不是只更新FC层。<br>将proposal, classification, regression统一在一个框架</p>
<h2 id="Design-Evaluation"><a href="#Design-Evaluation" class="headerlink" title="Design Evaluation"></a>Design Evaluation</h2><p>文章最后还对系统结构进行了讨论：</p>
<ul>
<li>multi-loss traing相比单独训练Classification确有提升</li>
<li>Scale invariance方面，multi-scale相比single-scale精度略有提升，但带来的时间开销更大。一定程度上说明CNN结构可以内在地学习scale invariance</li>
<li>在更多的数据(VOC)上训练后，mAP是有进一步提升的</li>
<li>Softmax分类器比”one vs rest”型的SVM表现略好，引入了类间的竞争</li>
<li>更多的Proposal并不一定带来性能的提升</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Fast R-CNN是对R-CNN的改进，也是对2-stage方法的系统化、架构化。文章将Proposal, Feature Extractor, Object Recognition&amp;Localization统一在一个整体的结构中，并推进共享卷积计算以提高效率的想法演进，是最有贡献的地方。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Fast R-CNN &lt;a href=&quot;https://arxiv.org/abs/1504.08083&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1504.08083&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Ove
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link href="http://blog.ddlee.cn/2017/10/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/"/>
    <id>http://blog.ddlee.cn/2017/10/13/论文笔记-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</id>
    <published>2017-10-12T16:57:35.000Z</published>
    <updated>2017-12-27T14:10:01.998Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>R-CNN系列的开山之作，2-stage的想法至今仍是精确度优先方法的主流。而且，本文中的众多做法也成为检测任务pipeline的标准配置。</p>
<p>摘要中提到的两大贡献：1）CNN可用于基于区域的定位和分割物体；2）监督训练样本数紧缺时，在额外的数据上预训练的模型经过fine-tuning可以取得很好的效果。</p>
<p>第一个贡献影响了之后几乎所有2-stage方法，而第二个贡献中用分类任务（Imagenet）中训练好的模型作为基网络，在检测问题上fine-tuning的做法也在之后的工作中一直沿用。</p>
<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Features Matter. Traditional hand-design feature(SIFT, HOG) -&gt; Learned feature(CNN). 从图像识别的经验来看，CNN网络自动习得的特征已经超出了手工设计的特征。</p>
<p>解决检测任务中的定位问题：”recognition using regions”，即基于区域的识别（分类）。</p>
<p>检测任务中样本不足的问题（对大型网络）：在大数据集上预训练分类模型，在小数据集上fine-tuning检测任务。</p>
<h3 id="Object-Detection-with-R-CNN"><a href="#Object-Detection-with-R-CNN" class="headerlink" title="Object Detection with R-CNN"></a>Object Detection with R-CNN</h3><p>Region Proposal: Selective Search</p>
<p>Feature Extraction: AlexNet(NIPS 2012), 4096-dim feature vector from every region proposal</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/rcnn.png" alt="arch"></p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>现在ILSVRC2012上预训练达到STOA，再在Pascal VOC上fine-tuning。根据IOU来给region proposal打标签，在每个batch中保持一定的正样本比例（背景类非常多）。这些都已成为标准做法，后续很多工作也是对这些细节进行改进（OHEM等）。</p>
<p>文章中特别提到，IOU的选择（即正负样例的标签准备）对结果影响显著，这里要谈两个threshold，一个用来识别正样本（IOU跟ground truth较高），另一个用来标记负样本（即背景类），而介于两者之间的则为hard negatives，若标为正类，则包含了过多的背景信息，反之又包含了要检测物体的特征，因而这些proposal便被忽略掉。</p>
<p>另一个重要的问题是bounding-box regression，这一过程是proposal向ground truth调整，实现时加入了log/exp变换来使loss保持在合理的量级上。</p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>R-CNN的想法直接明了，即是将CNN在分类上取得的成就运用在检测上，是深度学习方法在检测任务上的试水。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。尽管如此，这篇论文的很多做法仍然广泛地影响着检测任务上的深度模型革命，后续的很多工作也都是针对改进文章中的pipeline而展开，此篇可以称得上”the first paper”。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1311.2524&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Overview&quot;&gt;&lt;a hre
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Linux Reborn: 个人存档</title>
    <link href="http://blog.ddlee.cn/2017/09/19/Linux%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>http://blog.ddlee.cn/2017/09/19/Linux软件推荐/</id>
    <published>2017-09-19T13:35:09.000Z</published>
    <updated>2017-12-23T15:58:30.705Z</updated>
    
    <content type="html"><![CDATA[<h3 id="个人的需求与应用场景"><a href="#个人的需求与应用场景" class="headerlink" title="个人的需求与应用场景"></a>个人的需求与应用场景</h3><p>我是（几乎）完全使用Linux的，它也满足了我的大部分需求，主要以下几块：</p>
<ol>
<li>编程，主流IDE, Editor都会有Linux版本，而软件库大部分也是先支持Linux的（服务器）</li>
<li>上网，一个chrome几乎足够</li>
<li>娱乐，steam有Linux客户端，很多游戏也有相应版本（当然也有很多没有）</li>
</ol>
<p>下面是我整理的一个列表，算是给自己的存档。</p>
<h3 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h3><h4 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h4><h5 id="Text-Editor"><a href="#Text-Editor" class="headerlink" title="Text Editor"></a>Text Editor</h5><p>选择有很多，我用Sublime Text写代码，用Atom写博客（中文支持好），用VS Code读代码。总之精分，各取所好。</p>
<p>另外讲两个小技巧：</p>
<ol>
<li>利用<code>sshfs</code>把服务器的文件挂在到本地用编辑器编辑，然后远程终端运行</li>
<li><code>ssh -L localhost:8888:localhost:8888 user@host</code>命令可以将远程端口映射到本地，这样可以在服务器端开启<code>jupyter notebook</code>，再在本地用浏览器访问</li>
</ol>
<h5 id="tmux-amp-zsh"><a href="#tmux-amp-zsh" class="headerlink" title="tmux&amp;zsh"></a>tmux&amp;zsh</h5><p>tmux是一个终端多窗口管理器，可以打开多个终端窗口、挂起和挂载终端回话等，利器。</p>
<pre><code>sudo apt install tmux
</code></pre><h5 id="GitKraken"><a href="#GitKraken" class="headerlink" title="GitKraken"></a>GitKraken</h5><p>Git图形客户端</p>
<h4 id="效率类"><a href="#效率类" class="headerlink" title="效率类"></a>效率类</h4><h5 id="Whatever-Evernote-alternative"><a href="#Whatever-Evernote-alternative" class="headerlink" title="Whatever- Evernote alternative"></a><a href="https://cellard0-0r.github.io/whatever/" target="_blank" rel="external">Whatever- Evernote alternative</a></h5><p>Evernote的第三方客户端，调用网页API，不占用免费版的客户端限制个数</p>
<h5 id="Stacer-System-Cleaner"><a href="#Stacer-System-Cleaner" class="headerlink" title="Stacer- System Cleaner"></a><a href="https://github.com/oguzhaninan/Stacer/releases" target="_blank" rel="external">Stacer- System Cleaner</a></h5><p>提供系统监视器和清理功能，也可以卸载包</p>
<h5 id="synapse-App-Launcher"><a href="#synapse-App-Launcher" class="headerlink" title="synapse- App Launcher"></a>synapse- App Launcher</h5><p>一个类似lauchy的应用启动器，可以直接用apt安装</p>
<pre><code>sudo apt install synapse
</code></pre><h5 id="Gdebi-Package-Installer"><a href="#Gdebi-Package-Installer" class="headerlink" title="Gdebi- Package Installer"></a>Gdebi- Package Installer</h5><p>包安装程序，比自带的安装好用一些（安装deb包等）</p>
<pre><code>sudo apt install gdebi
</code></pre><h5 id="Mailspring-Mail-Client"><a href="#Mailspring-Mail-Client" class="headerlink" title="Mailspring- Mail Client"></a><a href="https://getmailspring.com/" target="_blank" rel="external">Mailspring- Mail Client</a></h5><p>邮件客户端，比thunderbird, evolution等界面美观一些</p>
<h5 id="Gparted-Disk-Management"><a href="#Gparted-Disk-Management" class="headerlink" title="Gparted- Disk Management"></a>Gparted- Disk Management</h5><p>磁盘管理程序，用于分区、格式化等等</p>
<pre><code>sudo apt install gparted
</code></pre><h4 id="Okular-PDF-Reader"><a href="#Okular-PDF-Reader" class="headerlink" title="Okular- PDF Reader"></a>Okular- PDF Reader</h4><p>功能强大的PDF阅读器</p>
<pre><code>sudo apt install okular
</code></pre><h4 id="WPS-Office"><a href="#WPS-Office" class="headerlink" title="WPS Office"></a><a href="https://www.wps.com/linux" target="_blank" rel="external">WPS Office</a></h4><p>WPS的Linux版本，比Libre要好用很多</p>
<h4 id="Shutter"><a href="#Shutter" class="headerlink" title="Shutter"></a>Shutter</h4><p>截屏软件，可以通过ubunut软件中心安装</p>
<h3 id="通讯与娱乐"><a href="#通讯与娱乐" class="headerlink" title="通讯与娱乐"></a>通讯与娱乐</h3><h5 id="Wewechat-Wechat-client"><a href="#Wewechat-Wechat-client" class="headerlink" title="Wewechat- Wechat client"></a><a href="https://github.com/trazyn/weweChat/releases" target="_blank" rel="external">Wewechat- Wechat client</a></h5><p>微信的第三方客户端，还有<a href="https://github.com/geeeeeeeeek/electronic-wechat" target="_blank" rel="external">electron-wechat</a>，wewechat界面更好，而后者可以看公众号的文章。</p>
<h5 id="IeaseMusic-Netease-Music-Client"><a href="#IeaseMusic-Netease-Music-Client" class="headerlink" title="IeaseMusic- Netease Music Client"></a><a href="https://github.com/trazyn/ieaseMusic/releases" target="_blank" rel="external">IeaseMusic- Netease Music Client</a></h5><p>网易云音乐的第三方客户端，界面漂亮，我一般用于听FM。功能上更全的自然是<a href="http://music.163.com/#/download" target="_blank" rel="external">官方版本</a>。</p>
<h5 id="1Listen"><a href="#1Listen" class="headerlink" title="1Listen"></a><a href="https://listen1.github.io/listen1/" target="_blank" rel="external">1Listen</a></h5><p>综合了网易，QQ，虾米三家的曲库，用于找想听的歌，建议下载chrome插件版。</p>
<h5 id="Steam"><a href="#Steam" class="headerlink" title="Steam"></a><a href="http://store.steampowered.com/linux" target="_blank" rel="external">Steam</a></h5><p>Dota2是可以通过steam的Linux版本玩的，我买过的大部分解密游戏也可以。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;个人的需求与应用场景&quot;&gt;&lt;a href=&quot;#个人的需求与应用场景&quot; class=&quot;headerlink&quot; title=&quot;个人的需求与应用场景&quot;&gt;&lt;/a&gt;个人的需求与应用场景&lt;/h3&gt;&lt;p&gt;我是（几乎）完全使用Linux的，它也满足了我的大部分需求，主要以下几块：&lt;
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
      <category term="Linux" scheme="http://blog.ddlee.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>IOS Reborn: 个人APP存档</title>
    <link href="http://blog.ddlee.cn/2017/09/02/IOS-Reborn-%E4%B8%AA%E4%BA%BAAPP%E5%AD%98%E6%A1%A3/"/>
    <id>http://blog.ddlee.cn/2017/09/02/IOS-Reborn-个人APP存档/</id>
    <published>2017-09-01T16:07:47.000Z</published>
    <updated>2017-11-21T16:08:28.096Z</updated>
    
    <content type="html"><![CDATA[<p>IOS系统个人应用列表备份，iPad主要为阅读、娱乐功能，iPod用于听歌、播客。</p>
<h3 id="iPad-mini-4"><a href="#iPad-mini-4" class="headerlink" title="iPad mini 4"></a>iPad mini 4</h3><h4 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h4><ul>
<li>Documents，文件中转中心，连接云服务、私有云。PDF文档中心</li>
<li>Google Keep，记录琐事、备忘</li>
<li>Pushbullet，多客户端跨平台文字、链接转发</li>
<li>PDF Expert，为Documents提供PDF标注编辑等功能</li>
<li>Git2Go，GitHub客户端，读代码</li>
<li>百度网盘，转存文件、电子书</li>
</ul>
<h4 id="Reading"><a href="#Reading" class="headerlink" title="Reading"></a>Reading</h4><ul>
<li>Reeder 3，Feedly客户端，咨讯中心</li>
<li>Pocket，稍后再读，配合Reeder 3</li>
<li>知乎，内容索引</li>
<li>多看阅读，电子书中心</li>
<li>Quora，内容索引</li>
<li>kindle，电子书，Amazon内容</li>
<li>Medium，高质量的写作社区</li>
<li>iBooks，少部分电子书</li>
</ul>
<h4 id="LifeStyle"><a href="#LifeStyle" class="headerlink" title="LifeStyle"></a>LifeStyle</h4><ul>
<li>导航犬离线地图，地图备查</li>
<li>Bilibili HD</li>
<li>AVPlayer HD，本地视频，私有云视频</li>
<li>网易云音乐</li>
</ul>
<h3 id="iPod"><a href="#iPod" class="headerlink" title="iPod"></a>iPod</h3><h4 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h4><ul>
<li>网易云音乐</li>
<li>QQ音乐</li>
<li>KUSC，南加州古典音乐电台</li>
<li>overcast，Podcast客户端</li>
</ul>
<h4 id="LifeStyle-1"><a href="#LifeStyle-1" class="headerlink" title="LifeStyle"></a>LifeStyle</h4><ul>
<li>Bilibili</li>
<li>地铁通</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IOS系统个人应用列表备份，iPad主要为阅读、娱乐功能，iPod用于听歌、播客。&lt;/p&gt;
&lt;h3 id=&quot;iPad-mini-4&quot;&gt;&lt;a href=&quot;#iPad-mini-4&quot; class=&quot;headerlink&quot; title=&quot;iPad mini 4&quot;&gt;&lt;/a&gt;iPad
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
      <category term="IOS" scheme="http://blog.ddlee.cn/tags/IOS/"/>
    
  </entry>
  
  <entry>
    <title>Android Reborn: 个人APP存档</title>
    <link href="http://blog.ddlee.cn/2017/08/31/Android-Reborn-%E4%B8%AA%E4%BA%BAAPP%E5%AD%98%E6%A1%A3/"/>
    <id>http://blog.ddlee.cn/2017/08/31/Android-Reborn-个人APP存档/</id>
    <published>2017-08-31T15:57:23.000Z</published>
    <updated>2017-11-21T16:07:31.431Z</updated>
    
    <content type="html"><![CDATA[<p>个人使用Android的应用列表备份。</p>
<h3 id="GApps"><a href="#GApps" class="headerlink" title="GApps"></a>GApps</h3><ul>
<li>Google Photos</li>
<li>Google Chrome</li>
<li>YouTube</li>
<li>Google Keep</li>
<li>Inbox by Gmail</li>
<li>Google Pinyin Keyboard</li>
<li>Google Now</li>
</ul>
<h3 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h3><ul>
<li>Solid File Explorer， 文件管理，云服务等中转</li>
<li>Steam，Steam二次验证工具</li>
<li>Evernote，笔记同步</li>
<li>WPS Office，文档阅读与编辑</li>
<li>Pushbullet，多端文字链接通信</li>
<li>Pulse Secure，学校指定VPN工具</li>
<li>Google Authenticator(Nutstore Rvoked)，二次验证工具</li>
<li>CamScanner，文档扫描</li>
<li>FeedMe，RSS阅读，配合feedly</li>
</ul>
<h3 id="System-Optimization"><a href="#System-Optimization" class="headerlink" title="System Optimization"></a>System Optimization</h3><ul>
<li>Nova LancherI(config)，桌面（已备份）</li>
<li>SD Maid，系统清理，APP管理</li>
<li>Tasker，自动化任务编排</li>
<li>Ice Box，流氓应用管理（已备份）</li>
<li>SMS Backup，通话记录和短信备份</li>
<li>SuperSU，ROOT权限管理</li>
<li>MyAndroidTools，系统级的应用活动、服务管理（已备份）</li>
<li>Titanium Backup，系统备份</li>
<li>Brevent，系统进程管理</li>
<li>Greenify，系统进程管理</li>
</ul>
<h3 id="LifeStyle"><a href="#LifeStyle" class="headerlink" title="LifeStyle"></a>LifeStyle</h3><ul>
<li>Wechat，微信</li>
<li>Mobike，共享单车</li>
<li>Resplash，高质量图片，壁纸图库</li>
<li>Prisma，智能风格转换，滤镜</li>
<li>Photo Scan，旧实体照片数字化</li>
<li>Snapseed，图片处理</li>
<li>AliPay，支付宝</li>
<li>Max+，DOTA2资讯</li>
<li>C5Game，DOTA2饰品</li>
<li>TIM，工作版QQ</li>
<li>Retrorika，图标包</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;个人使用Android的应用列表备份。&lt;/p&gt;
&lt;h3 id=&quot;GApps&quot;&gt;&lt;a href=&quot;#GApps&quot; class=&quot;headerlink&quot; title=&quot;GApps&quot;&gt;&lt;/a&gt;GApps&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Google Photos&lt;/li&gt;
&lt;li&gt;Go
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Android" scheme="http://blog.ddlee.cn/tags/Android/"/>
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
  </entry>
  
</feed>
