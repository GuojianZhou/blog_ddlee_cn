<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>萧爽楼</title>
  <subtitle>李家丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.ddlee.cn/"/>
  <updated>2017-12-27T14:02:35.092Z</updated>
  <id>http://blog.ddlee.cn/</id>
  
  <author>
    <name>ddlee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[论文笔记]Speed/accuracy trade-offs for modern convolutional object detectors</title>
    <link href="http://blog.ddlee.cn/2017/12/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/"/>
    <id>http://blog.ddlee.cn/2017/12/24/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/</id>
    <published>2017-12-24T13:55:22.000Z</published>
    <updated>2017-12-27T14:02:35.092Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="external">https://arxiv.org/abs/1611.10012</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>这篇文章偏综述和实验报告的性质，前几个部分对检测模型有不错的概括，重头在实验结果部分，实验细节也描述的比较清楚，可以用来参考。</p>
<p>文章将检测模型分为三种元结构：Faster-RCNN、R-FCN和SSD，将特征提取网络网络独立出来作为元结构的一个部件，并松动了Proposal个数、输入图片尺寸，生成Feature map的大小等作为超参，并行实验，探索精度和速度方面的trade-off。</p>
<p>文章也将源码公开，作为Tensorflow的Object Detection API。</p>
<p>下图是三种元结构的图示：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/meta-arch.png" alt="meta-arch"></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig2.png" alt="meta-arch"></p>
<p>信息量非常大的一张图。</p>
<ul>
<li>横纵两个维度分别代表速度和准确度，横轴越靠左说明用时越少，纵轴越靠上说明mAP表现越好，因而，sweet spot应分布在左上角</li>
<li>两个超维是元结构和特征提取网络，元结构由形状代表，特征提取网络由颜色代表</li>
<li>虚线代表理想中的trade-off边界</li>
</ul>
<p>分析：</p>
<ul>
<li>准确度最高的由Faster-RCNN元结构、Inception-ResNet提取网络，高分图片，使用较大的feature map达到，如图右上角</li>
<li>较快的网络中准确度表现最好的由使用Inception和Mobilenet的SSD达到</li>
<li>sweet spot区特征提取网络由ResNet统治，较少Proposal的Faster-RCNN可以跟R-FCN相当</li>
<li>特征提取网络方面，Inception V2和MobileNet在高速度区，Incep-ResNet和ResNet在sweet spot和高精度区，Inception V3和VGG则远离理想边界（虚线）</li>
</ul>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig3.png" alt="meta-arch"></p>
<p>上图是特征提取网络对三种元结构的影响，横轴是特征提取网络的分类准确率，纵轴是检测任务上的mAP表现，可以看到，SSD在纵轴方向上方差最小，而Faster-RCNN和R-FCN对特征提取网络更为敏感。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig4.png" alt="meta-arch"></p>
<p>上图的横轴是不同的特征提取网络，组内是三种元结构的对比，纵轴是不同尺寸物体的mAP。</p>
<p>可以看到，在大物体的检测上，使用较小的网络时，SSD的效果跟两阶段方法相当，更深的特征提取网络则对两阶段方法的中型和小型物体的检测提升较大（ResNet101和Incep-ResNet都显现了两阶段方法在小物体上的提升）</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig5.png" alt="meta-arch"></p>
<p>上图显示了输入图片尺寸对mAP的影响。高分的图片对小物体检测帮助明显，因而拥有更高的精度，但相对运行速度会变慢。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig6.png" alt="meta-arch"></p>
<p>上图探究了两阶段方法中Proposal个数的影响，左边是Faster-RCNN，右边是R-FCN，实线是mAP，虚线是推断时间。<br>分析：</p>
<ul>
<li>相比R-FCN，Faster-RCNN推断时间对Proposal个数相当敏感（因为有per ROI的计算）</li>
<li>减少Proposal的个数，并不会给精度带来致命的下降</li>
</ul>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig7.png" alt="meta-arch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig8.png" alt="meta-arch"></p>
<p>上面两图是对FLOPS的记录，相对GPU时间更为中立，在图8中，GPU部分显现了ResNet跟Inception的分野（关于45度线，此时FLOPS跟GPU时间相当），文章认为分解操作(Factorization)减少了FLOPs，但增加了内存的IO时间，或者是GPU指令集更适合密集的卷积计算。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig9.png" alt="meta-arch"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/fig10.png" alt="meta-arch"></p>
<p>上两图是对内存占用的分析，总体来说，特征提取网络越精简、feature map尺寸越小，占用内存越少，运行时间也越短。</p>
<p>最后，文章描述了他们ensemble的思路，在一系列不同stride、loss和配置的Faster-RCNN中（ResNet和Incep-ResNet为特征提取网络），贪心地选择验证集上AP较高的，并且去除类AP相似的模型。选择的5个用于ensemble的模型如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Speed-accuracy-trade-offs-for-modern-convolutional-object-detectors/table5.png" alt="meta-arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇文章是不错的实验结果报告，测试了足够多的模型，也得出了合理的和有启发的结论。几点想法：</p>
<ul>
<li>RFCN并没有很好的解决定位跟分类的矛盾，per ROI的子网络最好还是要有，但要限制Proposal的个数（实际大部分都是负样本）来减少冗余</li>
<li>小物体的检测仍然是最大的难点，增大分辨率和更深的网络确有帮助，但不是实质的。</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.10012&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1611.10012&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Light-Head R-CNN: In Defense of Two-Stage Object Detector</title>
    <link href="http://blog.ddlee.cn/2017/12/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/"/>
    <id>http://blog.ddlee.cn/2017/12/22/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/</id>
    <published>2017-12-22T13:55:36.000Z</published>
    <updated>2017-12-27T14:00:34.351Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="external">https://arxiv.org/abs/1711.07264</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文章指出两阶段检测器通常在生成Proposal后进行分类的“头”(head)部分进行密集的计算，如ResNet为基础网络的Faster-RCNN将整个stage5（或两个FC）放在RCNN部分， RFCN要生成一个具有随类别数线性增长的channel数的Score map，这些密集计算正是两阶段方法在精度上领先而在推断速度上难以满足实时要求的原因。</p>
<p>针对这两种元结构(Faster-RCNN和RFCN)，文章提出了“头”轻量化方法，试图在保持精度的同时又能减少冗余的计算量，从而实现精度和速度的Trade-off。</p>
<h2 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/arch.png" alt="arch"></p>
<p>如上图，虚线框出的部分是三种结构的RCNN子网络（在每个RoI上进行的计算），light-head R-CNN中，在生成Score map前，ResNet的stage5中卷积被替换为sperable convolution，产生的Score map也减少至10×p×p（相比原先的#class×p×p）。</p>
<p>一个可能的解释是，“瘦”（channel数较少）的score map使用于分类的特征信息更加紧凑，原先较“厚”的score map在经过PSROIPooling的操作时，大部分信息并没有提取（只提取了特定类和特定位置的信息，与这一信息处在同一score map上的其他数据都被忽略了）。</p>
<p>进一步地，位置敏感的思路将位置性在channel上表达出来，同时隐含地使用了更类别数相同长度的向量表达了分类性（这一长度相同带来的好处即是RCNN子网络可以免去参数）。</p>
<p>light-head在这里的改进则是把这一个隐藏的嵌入空间压缩到较小的值，而在RCNN子网络中加入FC层再使这个空间扩展到类别数的规模，相当于是把计算量分担到了RCNN子网络中。</p>
<p>粗看来，light-head将原来RFCN的score map的职责两步化了：thin score map主攻位置信息，RCNN子网络中的FC主攻分类信息。另外，global average pool的操作被去掉，用于保持精度。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>实验部分，文章验证了较“瘦”的Score map不会对精度产生太大损害，也展现了ROI Align, Multiscale train等技巧对基线的提升过程。</p>
<p>文章的主要结果如下面两图（第一个为高精度，第二个为高速度）：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result1.png" alt="result1"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Light-Head-R-CNN-In-Defense-of-Two-Stage-Object-Detector/result2.png" alt="result2"></p>
<p>只能说这样的对比比较诡异。</p>
<p>第一张图中三个light-head结果并不能跟上面的其他结构构成多少有效的对照组，要么scale不同，要么FPN, multi-scale, ROI Align不同。唯一的有效对照是跟Mask-RCNN。</p>
<p>在高精度方面，基础网络不同，采用的scale也不同，没有有效的对照组。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我并不觉得这是对两阶段方法的Defense。文章对两阶段方法在精度和速度方面的分析比较有见地，但实验的结果并不能可靠地支撑light-head的有效性。相比之下Google的那篇trade-off可能更有参考价值。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.07264&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1711.07264&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]You Only Look Once: Unified, Real Time Object Detection</title>
    <link href="http://blog.ddlee.cn/2017/12/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-You-Only-Look-Once-Unified-Real-Time-Object-Detection/"/>
    <id>http://blog.ddlee.cn/2017/12/20/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/</id>
    <published>2017-12-20T13:38:31.000Z</published>
    <updated>2017-12-27T14:03:40.021Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">https://arxiv.org/abs/1506.02640</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>YOLO是单阶段方法的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名。</p>
<p>YOLO的主要优点：</p>
<ul>
<li>快。</li>
<li>全局处理使得背景错误相对少，相比基于局部（区域）的方法， 如Fast RCNN。</li>
<li>泛化性能好，在艺术作品上做检测时，YOLO表现好。</li>
</ul>
<h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><p>YOLO的大致工作流程如下：<br>1.准备数据：将图片缩放，划分为等分的网格，每个网格按跟ground truth的IOU分配到所要预测的样本。<br>2.卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IOU表示）。测试时，分数如下计算：</p>
<p>等式左边第一项由网格预测，后两项由每个box预测，综合起来变得到每个box含有不同类别物体的分数。<br>因而，卷积网络共输出的预测值个数为S×S×(B×5+C)，S为网格数，B为每个网格生成box个数，C为类别数。<br>3.后处理：使用NMS过滤得到的box</p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><img src="http://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/loss.jpg" alt="loss-function"></p>
<p>图片来自<a href="https://zhuanlan.zhihu.com/p/24916786" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/24916786</a></p>
<p>损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，loss中添加了权重并将长宽取根号。</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-You-Only-Look-Once-Unified-Real-Time-Object-Detection/error.png" alt="error"></p>
<p>相比Fast-RCNN，YOLO的背景误检在错误中占比重小，而位置错误占比大（未采用log编码）。</p>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>YOLO划分网格的思路还是比较粗糙的，每个网格生成的box个数也限制了其对小物体和相近物体的检测。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1506.02640&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Rethinking the Inception Architecture for Computer Vision</title>
    <link href="http://blog.ddlee.cn/2017/12/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rethinking-the-Inception-Architecture-for-Computer-Vision/"/>
    <id>http://blog.ddlee.cn/2017/12/16/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/</id>
    <published>2017-12-16T12:53:29.000Z</published>
    <updated>2018-01-06T12:54:12.429Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">https://arxiv.org/abs/1512.00567</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>本文是作者推进inception结构的第2.5步。在更早的文章里，同一作者提出Batch Normalization并且用来改进了Inception结构，称为Inception-BN。而在这篇文章里，作者提出了Inception-v2和Inception-v3，两者共享同一网络结构，v3版本相比v2版本加入了RMSProp，Label Smoothing等技巧。</p>
<p>文章表述了Inception系列的几个设计原则，并根据这些原则改进了GoogLeNet的结构。</p>
<h3 id="General-Design-Principles"><a href="#General-Design-Principles" class="headerlink" title="General Design Principles"></a>General Design Principles</h3><ul>
<li>Avoid representational bottlenecks, especially early in the network. 建议不要在过浅的阶段进行特征压缩，而维度只是一个表达复杂性的参考，并不能作为特征复杂性的绝对衡量标准。</li>
<li>Higher dimensional representations are easier to process locally with a network. 高阶的表示更有局部描述力，增加非线性有助于固化这些描述力。</li>
<li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. 基于空间的聚合信息可以在低维空间里处理，而不必担心有太多信息损失。这一点也佐证了1×1卷积的降维作用。</li>
<li>Balance the width and depth of the network.  宽度和深度的增加都有助于网络的表达能力，最好的做法是同时在这两个方向上推进，而非只顾及一个。</li>
</ul>
<h3 id="Factorizing-Convolution"><a href="#Factorizing-Convolution" class="headerlink" title="Factorizing Convolution"></a>Factorizing Convolution</h3><p>分解一直是计算数学里经典的思路。从牛顿法到BFGS，就是把Hessian矩阵（或其逆）用一系列的向量操作来表示和近似，避免矩阵的计算。</p>
<p>本文提出了两种卷积结构方面的分解，一个是在卷积核的层面，另一个是在空间方面。</p>
<p>第一种分解是将大核卷积分解成串联的小核卷积。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor3.png" alt="factor5"></p>
<p>用两个3×3的卷积代替5×5的卷积，带来的参数减少为(9+9)/(5×5).</p>
<p>第二种分解是在卷积核本身上，引入非对称卷积：用3×1和1×3的卷积串联代替3×3卷积。如下图所示。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/factor1.png" alt="factor3"></p>
<p>这种分解也可以推广到n维情况，且n越大，带来的收益越明显。</p>
<p>空间上的卷积分解建模了这样的情形：两个方向上的卷积参数互相正交，便被空间分解卷积解耦。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/spatial-seperable.png" alt="factor5"></p>
<h3 id="Utility-of-Auxiliary-Classifiers"><a href="#Utility-of-Auxiliary-Classifiers" class="headerlink" title="Utility of Auxiliary Classifiers"></a>Utility of Auxiliary Classifiers</h3><p>在GoogLeNet中，作者用loss监督了低维的特征图的学习，但进一步的实验发现，加入BN层后，这些增益被抵消了，于是Auxiliary Classifier可被看做是某种正则化技术，在加入BN的网络中便不再应用。</p>
<h3 id="Efficient-Grid-Size-Reduction"><a href="#Efficient-Grid-Size-Reduction" class="headerlink" title="Efficient Grid Size Reduction"></a>Efficient Grid Size Reduction</h3><p>这一节讨论网络中的特征降维，即下采样的过程，通常由卷积层或Pooling层的stride参数控制。文章为避免原则一中提到的Representation Bottleneck，在进行Pooling之前将网络加宽（通过Channel数的增加），这也对应了平衡宽度和深度的原则。</p>
<p>最终结合了Inception结构和下采样需求的单元如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/downsample.png" alt="factor5"></p>
<p>不同于Inception单元，上面的1×1卷积扩展了Channel，并且3×3卷积采用了stride=2。</p>
<h3 id="Inception-v2-amp-Inception-v3-Architecture"><a href="#Inception-v2-amp-Inception-v3-Architecture" class="headerlink" title="Inception-v2 &amp; Inception-v3 Architecture"></a>Inception-v2 &amp; Inception-v3 Architecture</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/arch.png" alt="factor5"></p>
<p>可以看到随深度增加，Channel数也在扩展，而Inception单元也遵从了堆叠的范式。</p>
<p>其中三种Inception单元分别为：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptiona.png" alt="factor5"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionb.png" alt="factor5"></p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/inceptionc.png" alt="factor5"></p>
<p>另外，也可以查看<a href="http://ethereon.github.io/netscope/#gist/a2394c1c4a9738469078f096a8979346" target="_blank" rel="external">NetScope Vis</a>来熟悉Inception-v3的结构，源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下面是Inception结构演化带来的增益分解：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Rethinking-the-Inception-Architecture-for-Computer-Vision/experiment.png" alt="factor5"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本篇是对Inception系网络的推进，其分解的思想成为又一网络设计的指导原则。</p>
<p>对卷积的进一步理解，可以参考这个<a href="https://graphics.stanford.edu/courses/cs178-10/applets/convolution.html" target="_blank" rel="external">页面</a>，这一工具可视化了不同卷积核对输入的处理，给出的例子都是在早期人们手工设计的滤波器，而深度网络隐式地学习到了这些滤波器的卷积表达。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1512.00567&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;&gt;&lt;a h
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]SSD: Single Shot MultiBox Detector</title>
    <link href="http://blog.ddlee.cn/2017/12/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-SSD-Single-Shot-MultiBox-Detector/"/>
    <id>http://blog.ddlee.cn/2017/12/12/论文笔记-SSD-Single-Shot-MultiBox-Detector/</id>
    <published>2017-12-12T13:37:56.000Z</published>
    <updated>2017-12-27T13:40:01.860Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/151.023325" target="_blank" rel="external">https://arxiv.org/abs/151.023325</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>SSD是对YOLO的改进，其达到跟两阶段方法相当的精度，又保持较快的运行速度。</p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-SSD-Single-Shot-MultiBox-Detector/arch.jpn" alt="arch"></p>
<ul>
<li><p>多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。</p>
</li>
<li><p>更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SSD有点像多分类的RPN，生成anchor box，再对box预测分数和位置调整值。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/151.023325&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/151.023325&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Feature Pyramid Networks for Object Detection</title>
    <link href="http://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Feature-Pyramid-Networks-for-Object-Detection/"/>
    <id>http://blog.ddlee.cn/2017/12/07/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/</id>
    <published>2017-12-07T13:55:59.000Z</published>
    <updated>2017-12-27T14:00:23.112Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">https://arxiv.org/abs/1612.03144</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>对图片信息的理解常常关系到对位置和规模上不变性的建模。在较为成功的图片分类模型中，Max-Pooling这一操作建模了位置上的不变性：从局部中挑选最大的响应，这一响应在局部的位置信息就被忽略掉了。而在规模不变性的方向上，添加不同大小感受野的卷积核（VGG），用小卷积核堆叠感受较大的范围（GoogLeNet），自动选择感受野的大小（Inception）等结构也展现了其合理的一面。</p>
<p>回到检测任务，与分类任务不同的是，检测所面临的物体规模问题是跨类别的、处于同一语义场景中的。</p>
<p>一个直观的思路是用不同大小的图片去生成相应大小的feature map，但这样带来巨大的参数，使本来就只能跑个位数图片的内存更加不够用。另一个思路是直接使用不同深度的卷积层生成的feature map，但较浅层的feature map上包含的低等级特征又会干扰分类的精度。</p>
<p>本文提出的方法是在高等级feature map上将特征向下回传，反向构建特征金字塔。</p>
<h3 id="Feature-Pyramid-Networks"><a href="#Feature-Pyramid-Networks" class="headerlink" title="Feature Pyramid Networks"></a>Feature Pyramid Networks</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/arch.png" alt="arch"></p>
<p>从图片开始，照常进行级联式的特征提取，再添加一条回传路径：从最高级的feature map开始，向下进行最近邻上采样得到与低等级的feature map相同大小的回传feature map，再进行元素位置上的叠加（lateral connection），构成这一深度上的特征。</p>
<p>这种操作的信念是，低等级的feature map包含更多的位置信息，高等级的feature map则包含更好的分类信息，将这两者结合，力图达到检测任务的位置分类双要求。</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章的主要实验结果如下：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Feature-Pyramid-Networks-for-Object-Detection/result.png" alt="Experiments results"></p>
<p>对比不同head部分，输入feature的变化对检测精度确实有提升，而且，lateral和top-down两个操作也是缺一不可。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>特征金字塔本是很自然的想法，但如何构建金字塔同时平衡检测任务的定位和分类双目标，又能保证显存的有效利用，是本文做的比较好的地方。如今，FPN也几乎成为特征提取网络的标配，更说明了这种组合方式的有效性。</p>
<p>个人方面，FPN跟multi-scale的区别在哪，还值得进一步探索。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.03144&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1612.03144&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]R-FCN: Object Detection via Region-based Fully Convolutinal Networks</title>
    <link href="http://blog.ddlee.cn/2017/12/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/"/>
    <id>http://blog.ddlee.cn/2017/12/07/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/</id>
    <published>2017-12-07T13:55:48.000Z</published>
    <updated>2017-12-27T14:04:15.263Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="external">https://arxiv.org/abs/1605.06409</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>文章指出了检测任务之前的框架存在不自然的设计，即全卷积的特征提取部分+全连接的分类器，而表现最好的图像分类器都是全卷积的结构（ResNet等），这一点是由分类任务的平移不变性和检测任务的平移敏感性之间的矛盾导致的。换句话说，检测模型采用了分类模型的特征提取器，丢失了位置信息。这篇文章提出采用“位置敏感分数图”的方法解决这一问题。</p>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-Pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI Pooling"></a>Position-sensitive score maps &amp; Position-sensitive RoI Pooling</h3><p>位置敏感分数图的生成有两个重要操作，一是生成更“厚”的feature map，二是在RoI Pooling时选择性地输入feature map。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-R-FCN-Object-Detection-via-Region-based-Fully-Convolutinal-Networks/rfcn.png" alt="arch"></p>
<p>Faster R-CNN中，经过RPN得到RoI，转化成分类任务，还加入了一定量的卷积操作（ResNet中的conv5部分），而这一部分卷积操作是不能共享的。R-FCN则着眼于全卷积结构，利用卷积操作在Channel这一维度上的自由性，赋予其位置敏感的意义。下面是具体的操作：</p>
<ul>
<li>在全卷积网络的最后一层，生成k^2(C+1)个Channel的Feature map，其中C为类别数，k^2代表k×k网格，用于分别检测目标物体的k×k个部分。即是用不同channel的feature map代表物体的不同局部（如左上部分，右下部分）。</li>
<li>将RPN网络得到的Proposal映射到上一步得到的feature map（厚度为k×k×(C+1)，）后，相应的，将RoI等分为k×k个bin，对第(i,j)个bin，仅考虑对应(i,j)位置的(C+1)个feature map，进行如下计算：其中(x0,y0)是这个RoI的锚点，得到的即是(i,j)号bin对C类别的相应分数。</li>
<li>经过上一步，每个RoI得到的结果是k^2(C+1)大小的分数张量，k×k编码着物体的局部分数信息，进行vote（平均）后得到(C+1)维的分数向量，再接入softmax得到每一类的概率。</li>
</ul>
<p>上面第二步操作中“仅选取第(i, j)号feature map”是位置信息产生意义的关键。</p>
<p>这样设计的网络结构，所有可学习的参数都分布在可共享的卷积层，因而在训练和测试性能上均有提升。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>R-FCN是对Faster R-CNN结构上的改进，部分地解决了位置不变性和位置敏感性的矛盾。通过最大化地共享卷积参数，使得在精度相当的情况下训练和测试效率都有了很大的提升。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.06409&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1605.06409&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Going deeper with convolutions</title>
    <link href="http://blog.ddlee.cn/2017/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Going-deeper-with-convolutions/"/>
    <id>http://blog.ddlee.cn/2017/11/30/论文笔记-Going-deeper-with-convolutions/</id>
    <published>2017-11-30T12:39:58.000Z</published>
    <updated>2018-01-06T12:46:06.285Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的GoogLeNet拿下了ILSVRC14分类任务的头名。文章也探讨了网络在不断加深的情况下如何更好地利用计算资源，这一理念也是Inception系列网络的核心。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Inception单元的启发主要来自Network in Network结构和Arora等人在神经科学方面的工作。</p>
<p>提高深度模型的一个简单想法是增加深度，但这样带来过拟合的风险和巨大的计算资源消耗，对数据量和计算力的要求可能会超过网络加深带来的收益。</p>
<p>解决这些问题的基本思路是使用稀疏连接的网络，而这也跟Arora等人工作中的Hebbian principle吻合：共同激活的神经元常常集聚在一起。换句话说，某一层激活的神经元只向下一层中特定的几个神经元传递激活信号，而向其他神经元几乎不传递信息，即仅有少部分连接是真正有效的，这也是稀疏的含义。</p>
<p>然而另一方面，现代计算架构对稀疏的计算非常低效，更适合的是密集的计算，这样便产生了矛盾。而Inception单元的提出就是为了用密集的结构来近似稀疏结构，在建模稀疏连接的同时又能利用密集计算的优势。</p>
<p>很多文章认为inception结构的意义在于将不同大小核的卷积并行连接，然后让网络自行决定采用哪种卷积来提取特征，有些无监督的意味，然后将1×1的卷积解释为降维操作。这种想法有待验证，是否在5×5卷积有较强激活的时候，3×3卷积大部分没有激活，还是两者能够同时有较强的激活？不同的处理阶段这两种卷积核的选择有没有规律？</p>
<p>在此提出一个个人的理解，欢迎讨论。</p>
<p>首先是channel的意义。我们知道，卷积之所以有效，是因为它建模了张量数据在空间上的局部相关性，加之Pooling操作，将这些相关性赋予平移不变性（即泛化能力）。而channel则是第三维，它实际上是卷积结构中的隐藏单元，是中间神经元的个数。卷积层在事实上是全连接的：每个Input channel都会和output channel互动，互动的信息只不过从全连接层的weight和bias变成了卷积核的weight。</p>
<p>这种全连接是冗余的，本质上应是一个稀疏的结构。Inception单元便在channel这个维度上做文章，采用的是类似矩阵分块的思想。</p>
<p>根据Hebbian principle，跨channel的这些神经元，应是高度相关的，于是有信息压缩的空间，因而使用跨channel的1×1的卷积将它们嵌入到低维的空间里（比如，Inception4a单元的输入channel是512，不同分支的1×1卷积输出channel则是192,96,16和64，见下面GoogLeNet结构表），在这个低维空间里，用密集的全连接建模（即3×5和5×5卷积），它们的输出channel相加也再恢复到原来的输入channel维度（Inception4a分别是192+208+48+64），最后的连接由Concat操作完成（分块矩阵的合并），这样就完成了分块密集矩阵对稀疏矩阵的近似。</p>
<p>这样来看，3×3和5×5大小的选择并不是本质的，本质的是分块低维嵌入和concat的分治思路。而在ResNeXt的工作中，这里的分块被认为是新的维度（称为cardinality），采用相同的拓扑结构。</p>
<h3 id="Stacked-Inception-Module"><a href="#Stacked-Inception-Module" class="headerlink" title="Stacked Inception Module"></a>Stacked Inception Module</h3><p>在GoogLeNet中，借鉴了AlexNet和VGG的stack(repeat)策略，将Inception单元重复串联起来，构成基本的特征提取结构。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/arch.png" alt="arch"></p>
<h4 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h4><p>朴素版本的Inception单元会带来Channel维数的不断增长，加入的1×1卷积则起到低维嵌入的作用，使Inception单元前后channel数保持稳定。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/inception.png" alt="arch"></p>
<h3 id="Auxililary-Classifier"><a href="#Auxililary-Classifier" class="headerlink" title="Auxililary Classifier"></a>Auxililary Classifier</h3><p>这里是本文的另一个贡献，将监督信息传入中间的feature map，构成一个整合loss，作者认为这样有助于浅层特征的学习。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/auxililary.png" alt="arch"></p>
<h3 id="Architecture-of-GoogLeNet"><a href="#Architecture-of-GoogLeNet" class="headerlink" title="Architecture of GoogLeNet"></a>Architecture of GoogLeNet</h3><p>下面的表显示了GoogLeNet的整体架构，可以留意到Inception单元的堆叠和Channel数在子路径中的变化。NetScope可视化可参见<a href="http://ethereon.github.io/netscope/#/gist/db8754ee4b239920b3df5ab93220a84b" target="_blank" rel="external">GoogLeNet Vis</a>。源文件位于<a href="https://github.com/ddlee96/NN_structures/tree/master/caffe_vis" target="_blank" rel="external">NN_Structures/caffe_vis/</a>。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Going-deeper-with-convolutions/table.png" alt="arch"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>文章是对NiN思想的继承和推进，不同于AlexNet和VGG，网络的模块化更加凸显，多路径的结构也成为新的网络设计范本，启发了众多后续网络结构的设计。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;本作是Inception系列网络的第一篇，提出了Inception单元结构，基于这一结构的G
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Neural Network" scheme="http://blog.ddlee.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]MegDet: A Large Mini-Batch Object Detector</title>
    <link href="http://blog.ddlee.cn/2017/11/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-MegDet-A-Large-Mini-Batch-Object-Detector/"/>
    <id>http://blog.ddlee.cn/2017/11/21/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/</id>
    <published>2017-11-21T15:30:56.000Z</published>
    <updated>2017-11-21T15:44:58.572Z</updated>
    
    <content type="html"><![CDATA[<p>本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。</p>
<h2 id="Object-Detection-Progress-Summay"><a href="#Object-Detection-Progress-Summay" class="headerlink" title="Object Detection Progress Summay"></a>Object Detection Progress Summay</h2><p>检测方法回顾：R-CNN, Fast/Faster R-CNN, Mask RCNN, RetinaNet(Focal Loss), ResNet(backbone network),</p>
<p>文章先指出前述方法大多是框架、loss等的更新，而均采用非常小的batch（2张图片）训练，有如下不足：</p>
<ul>
<li>training slow</li>
<li>fails to provide accurate statistics for BN</li>
</ul>
<p>这里涉及一个问题，检测任务的源数据，到底应该是图片还是标注框。在Fast R-CNN中，RBG提到SPPNet等每个batch采样的标注框来自不同的图片，之间不能共享卷积运算（卷积运算是以图片为单位的）。为了共享这部分计算，Fast R-CNN采用了“先选图片，再选标注框”的策略来确定每个batch，文章提到这种操作会引入相关性，但在实际中却影响不大。之后的Faster R-CNN，每张图片经过RPN产生约300个Proposal，传入RCNN做法也成了通用做法。</p>
<p>个人认为检测任务的数据，应该是以图片为单位的。物体在图片的背景中才会产生语义，而尽管每张图片有多个Proposal（近似分类任务中的batch大小），但它们共享的是同一个语义（场景），而单一的语义难以在同一个batch中提供多样性来供网络学习。</p>
<h5 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h5><p>Increasing mini-batch size requires large learning rate, which may cause discovergence.</p>
<h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>new explanation of linear scaling rule, introduce “warmup” trick to learning rate schedule</li>
<li>Cross GPU Batch Normalization(CGBN)</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="Variance-Equivalence-explanation-for-Linear-Scaling-Rule"><a href="#Variance-Equivalence-explanation-for-Linear-Scaling-Rule" class="headerlink" title="Variance Equivalence explanation for Linear Scaling Rule"></a>Variance Equivalence explanation for Linear Scaling Rule</h3><p>linear scaling rule 来自更改batch size 时，同时放缩learning rate，使得更改后的weight update相比之前小batch size， 多步的weight update类似。而本文用保持loss gradient的方差不变重新解释了linear scaling rule，并指出这一假定仅要求loss gradient是i.i.d，相比保持weight update所假设的不同batch size间loss gradient相似更弱。</p>
<p>参见<a href="https://blog.ddlee.cn/2017/06/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Accurate-Large-Minibatch-SGD-Training-ImageNet-in-One-Hour/#Motivation">Accurate Large Minibatch SGD: Training ImageNet in One Hour</a>，近似时假设了求和项变化不大，这一条件在Object Detection中可能不成立，不同图片的标注框（大小、个数）差别很大。</p>
<h3 id="WarmUp-Strategy"><a href="#WarmUp-Strategy" class="headerlink" title="WarmUp Strategy"></a>WarmUp Strategy</h3><p>在训练初期，weight抖动明显，引入warmup机制来使用较小的学习率，再逐渐增大到Linear scaling rule要求的学习率。</p>
<h3 id="Cross-GPU-Batch-Normalization"><a href="#Cross-GPU-Batch-Normalization" class="headerlink" title="Cross-GPU Batch Normalization"></a>Cross-GPU Batch Normalization</h3><p>BN是使深度网络得以训练和收敛的关键技术之一，但在检测任务中，fine-tuning阶段常常固定了SOTA分类网络的BN部分参数，不进行更新。</p>
<p>检测中常常需要较大分辨率的图片，而GPU内存限制了单卡上的图片个数，提高batch size意味着BN要在多卡（Cross-GPU）上进行。</p>
<p>BN操作需要对每个batch计算均值和方差来进行标准化，对于多卡，具体做法是，单卡独立计算均值，聚合（类似Map-Reduce中的Reduce）算均值，再将均值下发到每个卡，算差，再聚合起来，计算batch的方差，最后将方差下发到每个卡，结合之前下发的均值进行标准化。</p>
<p>流程如图：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/cgbn.png" alt="cgbn"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在COCO数据集上的架构用预训练ResNet-50作为基础网络，FPN用于提供feature map。</p>
<p>结果显示，不使用BN时，较大的batch size（64,128）不能收敛。使用BN后，增大Batch size能够收敛但仅带来较小的精度提升，而BN的大小也不是越大越好，实验中，32是最好的选择。主要结果如下表：</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/results.png" alt="results"></p>
<p>按epoch，精度的变化如下图，小batch（16）在最初的几个epoch表现比大batch（32）要好。</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-MegDet-A-Large-Mini-Batch-Object-Detector/byepoch.png" alt="accuracy-by-epoch"></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇论文读起来总感觉少了些东西。对Linear scale rule的解释固然新颖，但没有引入新的trick（只是确认了检测仍是需要Linear scale rule的）。多卡的BN确实是非常厉害的工程实现（高效性），但实验的结果并没有支持到较大的batch size（128,256）比小batch精度更好的期望，而最后的COCO夺冠模型整合了多种trick，没有更进一步的错误分析，很难支撑说明CGBN带来的关键作用。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇论文介绍了旷视取得2017 MS COCO Detection chanllenge第一名的模型。提出大批量训练检测网络，并用多卡BN保证网络的收敛性。&lt;/p&gt;
&lt;h2 id=&quot;Object-Detection-Progress-Summay&quot;&gt;&lt;a href=&quot;#Ob
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks</title>
    <link href="http://blog.ddlee.cn/2017/10/21/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/"/>
    <id>http://blog.ddlee.cn/2017/10/21/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/</id>
    <published>2017-10-21T15:39:34.000Z</published>
    <updated>2017-12-27T14:08:20.395Z</updated>
    
    <content type="html"><![CDATA[<p>Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks</p>
<p><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Faster R-CNN是2-stage方法的主流方法，提出的RPN网络取代Selective Search算法使得检测任务可以由神经网络端到端地完成。粗略的讲，Faster R-CNN = RPN + Fast R-CNN，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA。</p>
<h2 id="Regional-Proposal-Networks"><a href="#Regional-Proposal-Networks" class="headerlink" title="Regional Proposal Networks"></a>Regional Proposal Networks</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/rpn.png" alt="faster_rcnn_arch"></p>
<p>RPN网络将Proposal这一任务建模为二分类的问题。</p>
<p>第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IOU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本即是anchor box和每个anchor box是否有物体。RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的Loss统一起来，作为RPN网络的目标训练。</p>
<p>RPN网络可调的超参还是很多的，anchor box的大小和长宽比例、IoU的阈值、每张图片上Proposal正负样本的比例等。</p>
<h2 id="Alternate-Training"><a href="#Alternate-Training" class="headerlink" title="Alternate Training"></a>Alternate Training</h2><p><img src="http://static.ddlee.cn/static/img/论文笔记-Faster-R-CNN-Towards-Real-Iime-Object-Detection-with-Region-Proposal-Networks/faster_rcnn_netwrok.png" alt="faster_rcnn_arch"></p>
<p>RPN网络是在feature map上进行的，因而可以跟RCNN完全共享feature extractor部分的卷积运算。训练时，RPN和RCNN的训练可以交替进行，即交替地固定RPN和RCNN部分的参数，更新另一部分。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Faster R-CNN的成功之处在于用RPN网络完成了检测任务的“深度化”。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。RPN网络也成为检测2-stage方法的标准部件。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Faster R-CNN: Towards Real Time Object Detection with Region Proposal Networks&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot; target=&quot;
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Fast R-CNN</title>
    <link href="http://blog.ddlee.cn/2017/10/15/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Fast-R-CNN/"/>
    <id>http://blog.ddlee.cn/2017/10/15/论文笔记-Fast-R-CNN/</id>
    <published>2017-10-15T15:34:31.000Z</published>
    <updated>2017-11-21T15:47:15.805Z</updated>
    
    <content type="html"><![CDATA[<p>Fast R-CNN <a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">https://arxiv.org/abs/1504.08083</a></p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Fast R-CNN 是对R-CNN的改进，作者栏只有RBG一人。文章先指出了R-CNN存在的问题，再介绍了自己的改进思路。文章结构堪称典范，从现存问题，到解决方案、实验细节，再到结果分析、拓展讨论，条分缕析，值得借鉴。而且，RBG开源的代码也影响了后来大部分这一领域的工作。</p>
<h2 id="R-CNN的问题"><a href="#R-CNN的问题" class="headerlink" title="R-CNN的问题"></a>R-CNN的问题</h2><ul>
<li>训练是一个多阶段的过程（Proposal, Classification, Regression）</li>
<li>训练耗时耗力</li>
<li>推断耗时</li>
</ul>
<p>而耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算。</p>
<h2 id="Fast-R-CNN-Architecture"><a href="#Fast-R-CNN-Architecture" class="headerlink" title="Fast R-CNN Architecture"></a>Fast R-CNN Architecture</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="http://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/fast-rcnn-arch.png" alt="arch"></p>
<p>上图是Fast R-CNN的架构。图片经过feature extractor产生feature map, 原图上运行Selective Search算法将RoI（Region of Interset）对应到feature map上，再对每个RoI进行RoI Pooling操作便得到等长的feature vector，最后通过FC后并行地进行Classifaction和BBox Regression。</p>
<p>Fast R-CNN的这一结构正是检测任务主流2-stage方法所采用的元结构的雏形。整个系统由Proposal, Feature Extractor, Object Recognition&amp;Localization几个部件组成。Proposal部分被替换成RPN(Faster R-CNN)，Feature Extractor部分使用SOTA的分类CNN网络(ResNet等），而最后的部分常常是并行的多任务结构（Mask R-CNN等）。</p>
<h3 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h3><p>这一操作是将不同大小的RoI（feature map上）统一的过程，具体做法是将RoI等分成目标个数的网格，在每个网格上进行max pooling，就得到等长的RoI feature vector。</p>
<h3 id="Mini-batch-Sampling"><a href="#Mini-batch-Sampling" class="headerlink" title="Mini-batch Sampling"></a>Mini-batch Sampling</h3><p>文章指出SPPNet训练较慢的原因在于来自不同图片的RoI不能共享计算，因而Fast R-CNN采用这样的mini-batch采样策略：先采样N张图片，再在每张图片上采样R/N个RoI，构成R大小的mini-batch。</p>
<p>采样时，总是保持25%比例正样本（iou大于0.5），iou在0.1到0.5的作为hard example。</p>
<h3 id="Multi-task-Loss"><a href="#Multi-task-Loss" class="headerlink" title="Multi-task Loss"></a>Multi-task Loss</h3><p>得到RoI feature vector后，后续的操作是一个并行的结构，Fast R-CNN将Classification和Regression的损失统一起来，并且在Regression中用更鲁棒的Smooth L1 Loss代替L2 Loss。</p>
<h3 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h3><p>文章还发现，对于预训练的VGG网络，开放Conv部分的参数更新有助于性能的提升，而不是只更新FC层。<br>将proposal, classification, regression统一在一个框架</p>
<h2 id="Design-Evaluation"><a href="#Design-Evaluation" class="headerlink" title="Design Evaluation"></a>Design Evaluation</h2><p>文章最后还对系统结构进行了讨论：</p>
<ul>
<li>multi-loss traing相比单独训练Classification确有提升</li>
<li>Scale invariance方面，multi-scale相比single-scale精度略有提升，但带来的时间开销更大。一定程度上说明CNN结构可以内在地学习scale invariance</li>
<li>在更多的数据(VOC)上训练后，mAP是有进一步提升的</li>
<li>Softmax分类器比”one vs rest”型的SVM表现略好，引入了类间的竞争</li>
<li>更多的Proposal并不一定带来性能的提升</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Fast R-CNN是对R-CNN的改进，也是对2-stage方法的系统化、架构化。文章将Proposal, Feature Extractor, Object Recognition&amp;Localization统一在一个整体的结构中，并推进共享卷积计算以提高效率的想法演进，是最有贡献的地方。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Fast R-CNN &lt;a href=&quot;https://arxiv.org/abs/1504.08083&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1504.08083&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Ove
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://blog.ddlee.cn/tags/Computer-Vision/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[论文笔记]Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link href="http://blog.ddlee.cn/2017/10/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/"/>
    <id>http://blog.ddlee.cn/2017/10/13/论文笔记-Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</id>
    <published>2017-10-12T16:57:35.000Z</published>
    <updated>2017-12-27T14:10:01.998Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>R-CNN系列的开山之作，2-stage的想法至今仍是精确度优先方法的主流。而且，本文中的众多做法也成为检测任务pipeline的标准配置。</p>
<p>摘要中提到的两大贡献：1）CNN可用于基于区域的定位和分割物体；2）监督训练样本数紧缺时，在额外的数据上预训练的模型经过fine-tuning可以取得很好的效果。</p>
<p>第一个贡献影响了之后几乎所有2-stage方法，而第二个贡献中用分类任务（Imagenet）中训练好的模型作为基网络，在检测问题上fine-tuning的做法也在之后的工作中一直沿用。</p>
<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>Features Matter. Traditional hand-design feature(SIFT, HOG) -&gt; Learned feature(CNN). 从图像识别的经验来看，CNN网络自动习得的特征已经超出了手工设计的特征。</p>
<p>解决检测任务中的定位问题：”recognition using regions”，即基于区域的识别（分类）。</p>
<p>检测任务中样本不足的问题（对大型网络）：在大数据集上预训练分类模型，在小数据集上fine-tuning检测任务。</p>
<h3 id="Object-Detection-with-R-CNN"><a href="#Object-Detection-with-R-CNN" class="headerlink" title="Object Detection with R-CNN"></a>Object Detection with R-CNN</h3><p>Region Proposal: Selective Search</p>
<p>Feature Extraction: AlexNet(NIPS 2012), 4096-dim feature vector from every region proposal</p>
<p><img src="http://static.ddlee.cn/static/img/论文笔记-Fast-R-CNN/rcnn.png" alt="arch"></p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>现在ILSVRC2012上预训练达到STOA，再在Pascal VOC上fine-tuning。根据IOU来给region proposal打标签，在每个batch中保持一定的正样本比例（背景类非常多）。这些都已成为标准做法，后续很多工作也是对这些细节进行改进（OHEM等）。</p>
<p>文章中特别提到，IOU的选择（即正负样例的标签准备）对结果影响显著，这里要谈两个threshold，一个用来识别正样本（IOU跟ground truth较高），另一个用来标记负样本（即背景类），而介于两者之间的则为hard negatives，若标为正类，则包含了过多的背景信息，反之又包含了要检测物体的特征，因而这些proposal便被忽略掉。</p>
<p>另一个重要的问题是bounding-box regression，这一过程是proposal向ground truth调整，实现时加入了log/exp变换来使loss保持在合理的量级上。</p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>R-CNN的想法直接明了，即是将CNN在分类上取得的成就运用在检测上，是深度学习方法在检测任务上的试水。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。尽管如此，这篇论文的很多做法仍然广泛地影响着检测任务上的深度模型革命，后续的很多工作也都是针对改进文章中的pipeline而展开，此篇可以称得上”the first paper”。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1311.2524&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Overview&quot;&gt;&lt;a hre
    
    </summary>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/categories/Papers/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="Papers" scheme="http://blog.ddlee.cn/tags/Papers/"/>
    
      <category term="Object Detection" scheme="http://blog.ddlee.cn/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Linux Reborn: 个人存档</title>
    <link href="http://blog.ddlee.cn/2017/09/19/Linux%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>http://blog.ddlee.cn/2017/09/19/Linux软件推荐/</id>
    <published>2017-09-19T13:35:09.000Z</published>
    <updated>2017-12-23T15:58:30.705Z</updated>
    
    <content type="html"><![CDATA[<h3 id="个人的需求与应用场景"><a href="#个人的需求与应用场景" class="headerlink" title="个人的需求与应用场景"></a>个人的需求与应用场景</h3><p>我是（几乎）完全使用Linux的，它也满足了我的大部分需求，主要以下几块：</p>
<ol>
<li>编程，主流IDE, Editor都会有Linux版本，而软件库大部分也是先支持Linux的（服务器）</li>
<li>上网，一个chrome几乎足够</li>
<li>娱乐，steam有Linux客户端，很多游戏也有相应版本（当然也有很多没有）</li>
</ol>
<p>下面是我整理的一个列表，算是给自己的存档。</p>
<h3 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h3><h4 id="编程相关"><a href="#编程相关" class="headerlink" title="编程相关"></a>编程相关</h4><h5 id="Text-Editor"><a href="#Text-Editor" class="headerlink" title="Text Editor"></a>Text Editor</h5><p>选择有很多，我用Sublime Text写代码，用Atom写博客（中文支持好），用VS Code读代码。总之精分，各取所好。</p>
<p>另外讲两个小技巧：</p>
<ol>
<li>利用<code>sshfs</code>把服务器的文件挂在到本地用编辑器编辑，然后远程终端运行</li>
<li><code>ssh -L localhost:8888:localhost:8888 user@host</code>命令可以将远程端口映射到本地，这样可以在服务器端开启<code>jupyter notebook</code>，再在本地用浏览器访问</li>
</ol>
<h5 id="tmux-amp-zsh"><a href="#tmux-amp-zsh" class="headerlink" title="tmux&amp;zsh"></a>tmux&amp;zsh</h5><p>tmux是一个终端多窗口管理器，可以打开多个终端窗口、挂起和挂载终端回话等，利器。</p>
<pre><code>sudo apt install tmux
</code></pre><h5 id="GitKraken"><a href="#GitKraken" class="headerlink" title="GitKraken"></a>GitKraken</h5><p>Git图形客户端</p>
<h4 id="效率类"><a href="#效率类" class="headerlink" title="效率类"></a>效率类</h4><h5 id="Whatever-Evernote-alternative"><a href="#Whatever-Evernote-alternative" class="headerlink" title="Whatever- Evernote alternative"></a><a href="https://cellard0-0r.github.io/whatever/" target="_blank" rel="external">Whatever- Evernote alternative</a></h5><p>Evernote的第三方客户端，调用网页API，不占用免费版的客户端限制个数</p>
<h5 id="Stacer-System-Cleaner"><a href="#Stacer-System-Cleaner" class="headerlink" title="Stacer- System Cleaner"></a><a href="https://github.com/oguzhaninan/Stacer/releases" target="_blank" rel="external">Stacer- System Cleaner</a></h5><p>提供系统监视器和清理功能，也可以卸载包</p>
<h5 id="synapse-App-Launcher"><a href="#synapse-App-Launcher" class="headerlink" title="synapse- App Launcher"></a>synapse- App Launcher</h5><p>一个类似lauchy的应用启动器，可以直接用apt安装</p>
<pre><code>sudo apt install synapse
</code></pre><h5 id="Gdebi-Package-Installer"><a href="#Gdebi-Package-Installer" class="headerlink" title="Gdebi- Package Installer"></a>Gdebi- Package Installer</h5><p>包安装程序，比自带的安装好用一些（安装deb包等）</p>
<pre><code>sudo apt install gdebi
</code></pre><h5 id="Mailspring-Mail-Client"><a href="#Mailspring-Mail-Client" class="headerlink" title="Mailspring- Mail Client"></a><a href="https://getmailspring.com/" target="_blank" rel="external">Mailspring- Mail Client</a></h5><p>邮件客户端，比thunderbird, evolution等界面美观一些</p>
<h5 id="Gparted-Disk-Management"><a href="#Gparted-Disk-Management" class="headerlink" title="Gparted- Disk Management"></a>Gparted- Disk Management</h5><p>磁盘管理程序，用于分区、格式化等等</p>
<pre><code>sudo apt install gparted
</code></pre><h4 id="Okular-PDF-Reader"><a href="#Okular-PDF-Reader" class="headerlink" title="Okular- PDF Reader"></a>Okular- PDF Reader</h4><p>功能强大的PDF阅读器</p>
<pre><code>sudo apt install okular
</code></pre><h4 id="WPS-Office"><a href="#WPS-Office" class="headerlink" title="WPS Office"></a><a href="https://www.wps.com/linux" target="_blank" rel="external">WPS Office</a></h4><p>WPS的Linux版本，比Libre要好用很多</p>
<h4 id="Shutter"><a href="#Shutter" class="headerlink" title="Shutter"></a>Shutter</h4><p>截屏软件，可以通过ubunut软件中心安装</p>
<h3 id="通讯与娱乐"><a href="#通讯与娱乐" class="headerlink" title="通讯与娱乐"></a>通讯与娱乐</h3><h5 id="Wewechat-Wechat-client"><a href="#Wewechat-Wechat-client" class="headerlink" title="Wewechat- Wechat client"></a><a href="https://github.com/trazyn/weweChat/releases" target="_blank" rel="external">Wewechat- Wechat client</a></h5><p>微信的第三方客户端，还有<a href="https://github.com/geeeeeeeeek/electronic-wechat" target="_blank" rel="external">electron-wechat</a>，wewechat界面更好，而后者可以看公众号的文章。</p>
<h5 id="IeaseMusic-Netease-Music-Client"><a href="#IeaseMusic-Netease-Music-Client" class="headerlink" title="IeaseMusic- Netease Music Client"></a><a href="https://github.com/trazyn/ieaseMusic/releases" target="_blank" rel="external">IeaseMusic- Netease Music Client</a></h5><p>网易云音乐的第三方客户端，界面漂亮，我一般用于听FM。功能上更全的自然是<a href="http://music.163.com/#/download" target="_blank" rel="external">官方版本</a>。</p>
<h5 id="1Listen"><a href="#1Listen" class="headerlink" title="1Listen"></a><a href="https://listen1.github.io/listen1/" target="_blank" rel="external">1Listen</a></h5><p>综合了网易，QQ，虾米三家的曲库，用于找想听的歌，建议下载chrome插件版。</p>
<h5 id="Steam"><a href="#Steam" class="headerlink" title="Steam"></a><a href="http://store.steampowered.com/linux" target="_blank" rel="external">Steam</a></h5><p>Dota2是可以通过steam的Linux版本玩的，我买过的大部分解密游戏也可以。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;个人的需求与应用场景&quot;&gt;&lt;a href=&quot;#个人的需求与应用场景&quot; class=&quot;headerlink&quot; title=&quot;个人的需求与应用场景&quot;&gt;&lt;/a&gt;个人的需求与应用场景&lt;/h3&gt;&lt;p&gt;我是（几乎）完全使用Linux的，它也满足了我的大部分需求，主要以下几块：&lt;
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
      <category term="Linux" scheme="http://blog.ddlee.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>IOS Reborn: 个人APP存档</title>
    <link href="http://blog.ddlee.cn/2017/09/02/IOS-Reborn-%E4%B8%AA%E4%BA%BAAPP%E5%AD%98%E6%A1%A3/"/>
    <id>http://blog.ddlee.cn/2017/09/02/IOS-Reborn-个人APP存档/</id>
    <published>2017-09-01T16:07:47.000Z</published>
    <updated>2017-11-21T16:08:28.096Z</updated>
    
    <content type="html"><![CDATA[<p>IOS系统个人应用列表备份，iPad主要为阅读、娱乐功能，iPod用于听歌、播客。</p>
<h3 id="iPad-mini-4"><a href="#iPad-mini-4" class="headerlink" title="iPad mini 4"></a>iPad mini 4</h3><h4 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h4><ul>
<li>Documents，文件中转中心，连接云服务、私有云。PDF文档中心</li>
<li>Google Keep，记录琐事、备忘</li>
<li>Pushbullet，多客户端跨平台文字、链接转发</li>
<li>PDF Expert，为Documents提供PDF标注编辑等功能</li>
<li>Git2Go，GitHub客户端，读代码</li>
<li>百度网盘，转存文件、电子书</li>
</ul>
<h4 id="Reading"><a href="#Reading" class="headerlink" title="Reading"></a>Reading</h4><ul>
<li>Reeder 3，Feedly客户端，咨讯中心</li>
<li>Pocket，稍后再读，配合Reeder 3</li>
<li>知乎，内容索引</li>
<li>多看阅读，电子书中心</li>
<li>Quora，内容索引</li>
<li>kindle，电子书，Amazon内容</li>
<li>Medium，高质量的写作社区</li>
<li>iBooks，少部分电子书</li>
</ul>
<h4 id="LifeStyle"><a href="#LifeStyle" class="headerlink" title="LifeStyle"></a>LifeStyle</h4><ul>
<li>导航犬离线地图，地图备查</li>
<li>Bilibili HD</li>
<li>AVPlayer HD，本地视频，私有云视频</li>
<li>网易云音乐</li>
</ul>
<h3 id="iPod"><a href="#iPod" class="headerlink" title="iPod"></a>iPod</h3><h4 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h4><ul>
<li>网易云音乐</li>
<li>QQ音乐</li>
<li>KUSC，南加州古典音乐电台</li>
<li>overcast，Podcast客户端</li>
</ul>
<h4 id="LifeStyle-1"><a href="#LifeStyle-1" class="headerlink" title="LifeStyle"></a>LifeStyle</h4><ul>
<li>Bilibili</li>
<li>地铁通</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;IOS系统个人应用列表备份，iPad主要为阅读、娱乐功能，iPod用于听歌、播客。&lt;/p&gt;
&lt;h3 id=&quot;iPad-mini-4&quot;&gt;&lt;a href=&quot;#iPad-mini-4&quot; class=&quot;headerlink&quot; title=&quot;iPad mini 4&quot;&gt;&lt;/a&gt;iPad
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
      <category term="IOS" scheme="http://blog.ddlee.cn/tags/IOS/"/>
    
  </entry>
  
  <entry>
    <title>Android Reborn: 个人APP存档</title>
    <link href="http://blog.ddlee.cn/2017/08/31/Android-Reborn-%E4%B8%AA%E4%BA%BAAPP%E5%AD%98%E6%A1%A3/"/>
    <id>http://blog.ddlee.cn/2017/08/31/Android-Reborn-个人APP存档/</id>
    <published>2017-08-31T15:57:23.000Z</published>
    <updated>2017-11-21T16:07:31.431Z</updated>
    
    <content type="html"><![CDATA[<p>个人使用Android的应用列表备份。</p>
<h3 id="GApps"><a href="#GApps" class="headerlink" title="GApps"></a>GApps</h3><ul>
<li>Google Photos</li>
<li>Google Chrome</li>
<li>YouTube</li>
<li>Google Keep</li>
<li>Inbox by Gmail</li>
<li>Google Pinyin Keyboard</li>
<li>Google Now</li>
</ul>
<h3 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h3><ul>
<li>Solid File Explorer， 文件管理，云服务等中转</li>
<li>Steam，Steam二次验证工具</li>
<li>Evernote，笔记同步</li>
<li>WPS Office，文档阅读与编辑</li>
<li>Pushbullet，多端文字链接通信</li>
<li>Pulse Secure，学校指定VPN工具</li>
<li>Google Authenticator(Nutstore Rvoked)，二次验证工具</li>
<li>CamScanner，文档扫描</li>
<li>FeedMe，RSS阅读，配合feedly</li>
</ul>
<h3 id="System-Optimization"><a href="#System-Optimization" class="headerlink" title="System Optimization"></a>System Optimization</h3><ul>
<li>Nova LancherI(config)，桌面（已备份）</li>
<li>SD Maid，系统清理，APP管理</li>
<li>Tasker，自动化任务编排</li>
<li>Ice Box，流氓应用管理（已备份）</li>
<li>SMS Backup，通话记录和短信备份</li>
<li>SuperSU，ROOT权限管理</li>
<li>MyAndroidTools，系统级的应用活动、服务管理（已备份）</li>
<li>Titanium Backup，系统备份</li>
<li>Brevent，系统进程管理</li>
<li>Greenify，系统进程管理</li>
</ul>
<h3 id="LifeStyle"><a href="#LifeStyle" class="headerlink" title="LifeStyle"></a>LifeStyle</h3><ul>
<li>Wechat，微信</li>
<li>Mobike，共享单车</li>
<li>Resplash，高质量图片，壁纸图库</li>
<li>Prisma，智能风格转换，滤镜</li>
<li>Photo Scan，旧实体照片数字化</li>
<li>Snapseed，图片处理</li>
<li>AliPay，支付宝</li>
<li>Max+，DOTA2资讯</li>
<li>C5Game，DOTA2饰品</li>
<li>TIM，工作版QQ</li>
<li>Retrorika，图标包</li>
</ul>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;个人使用Android的应用列表备份。&lt;/p&gt;
&lt;h3 id=&quot;GApps&quot;&gt;&lt;a href=&quot;#GApps&quot; class=&quot;headerlink&quot; title=&quot;GApps&quot;&gt;&lt;/a&gt;GApps&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Google Photos&lt;/li&gt;
&lt;li&gt;Go
    
    </summary>
    
      <category term="Individual Development" scheme="http://blog.ddlee.cn/categories/Individual-Development/"/>
    
    
      <category term="Android" scheme="http://blog.ddlee.cn/tags/Android/"/>
    
      <category term="Software" scheme="http://blog.ddlee.cn/tags/Software/"/>
    
  </entry>
  
  <entry>
    <title>墓畔哀歌</title>
    <link href="http://blog.ddlee.cn/2017/08/06/%E5%A2%93%E7%95%94%E5%93%80%E6%AD%8C/"/>
    <id>http://blog.ddlee.cn/2017/08/06/墓畔哀歌/</id>
    <published>2017-08-06T10:17:48.000Z</published>
    <updated>2017-08-06T10:17:48.963Z</updated>
    
    <content type="html"><![CDATA[<p>按：石评梅(1902-1928)为纪念恋人高君宇所作。初读是在高中一年级时的语文阅读材料里，读毕怅然若失，之后日日早读必大声读一遍，忘乎所以。如今重逢，只可默默抄写，竟不能放声读矣。</p>
<p>一</p>
<p>我由冬的残梦里惊醒，春正吻着我的睡靥低吟！晨曦照上了窗纱，望见往日令我醺醉的朝霞，我想让丹彩的云流，再认认我当年的颜色。</p>
<p>披上那件绣著蛱蝶的衣裳，姗姗地走到尘网封锁的妆台旁。呵！明镜里照见我憔悴的枯颜，像一朵颤动在风雨中苍白雕零的梨花。</p>
<p>我爱，我原想追回那美丽的皎容，祭献在你碧草如茵的墓旁，谁知道青春的残蕾已和你一同殉葬。</p>
<p>二</p>
<p>假如我的眼泪真凝成一粒一粒珍珠，到如今我已替你缀织成绕你玉颈的围巾。</p>
<p>假如我的相思真化作一颗一颗的红豆，到如今我已替你堆集永久勿忘的爱心。</p>
<p>哀愁深埋在我心头。</p>
<p>我愿燃烧我的肉身化成灰烬，我愿放浪我的热情怒涛汹涌，天呵！这蛇似的蜿蜒，蚕似的缠绵，就这样悄悄地偷去了我生命的青焰。</p>
<p>我爱，我吻遍了你墓头青草在日落黄昏；我祷告，就是空幻的梦吧，也让我再见见你的英魂。</p>
<p>三</p>
<p>明知道人生的尽头便是死的故乡，我将来也是一座孤冢，衰草斜阳。有一天呵！我离开繁华的人寰，悄悄入葬，这悲艳的爱情一样是烟消云散，昙花一现，梦醒后飞落在心头的都是些残泪点点。</p>
<p>然而我不能把记忆毁灭，把埋我心墟上的残骸抛却，只求我能永久徘徊在这垒垒荒冢之间，为了看守你的墓茔，祭献那茉莉花环。</p>
<p>我爱，你知否我无言的忧衷，怀想着往日轻盈之梦。梦中我低低唤着你小名，醒来只是深夜长空有孤雁哀鸣！</p>
<p>四</p>
<p>黯淡的天幕下，没有明月也无星光这宇宙像数千年的古墓；皑皑白骨上，飞动闪映着惨绿的磷花。我匍匐哀泣于此残銹的铁栏之旁，愿烘我愤怒的心火，烧毁这黑暗丑恶的地狱之网。</p>
<p>命运的魔鬼有意捉弄我弱小的灵魂，罚我在冰雪寒天中，寻觅那雕零了的碎梦。求上帝饶恕我，不要再惨害我这仅有的生命，剩得此残躯在，容我杀死那狞恶的敌人！</p>
<p>我爱，纵然宇宙变成烬余的战场，野烟都腥：在你给我的甜梦里，我心长系驻于虹桥之中，赞美永生！</p>
<p>五</p>
<p>我镇天踟蹰于垒垒荒冢，看遍了春花秋月不同的风景，抛弃了一切名利虚荣，来到此无人烟的旷野，哀吟缓行。我登了高岭，向云天苍茫的西方招魂，在绚烂的彩霞里，望见了我沈落的希望之陨星。</p>
<p>远处是烟雾冲天的古城，火星似金箭向四方飞游！隐约的听见刀枪搏击之声，那狂热的欢呼令人震惊！在碧草萋萋的墓头，我举起了胜利的金觥，饮吧我爱，我奠祭你静寂无言的孤冢！</p>
<p>星月满天时，我把你遗我的宝剑纤手轻擎，宣誓向长空：</p>
<p>愿此生永埋了英雄儿女的热情。</p>
<p>六</p>
<p>假如人生只是虚幻的梦影，那我这些可爱的映影，便是你赠与我的全生命。我常觉你在我身后的树林里，骑着马轻轻地走过去。常觉你停息在我的窗前，徘徊著等我的影消灯熄。常觉你随着我唤你的声音悄悄走近了我，又含泪退到了墙角。常觉你站在我低垂的雪帐外，哀哀地对月光而叹息！</p>
<p>在人海尘途中，偶然逢见个像你的人，我停步凝视后，这颗心呵！便如秋风横扫落叶般冷森凄零！我默思我已经得到爱的之心，如今只是荒草夕阳下，一座静寂无语的孤冢。</p>
<p>我的心是深夜梦里，寒光闪灼的残月，我的情是青碧冷静，永不再流的湖水。残月照着你的墓碑，湖水环绕着你的坟，我爱，这是我的梦，也是你的梦，安息吧，敬爱的灵魂！</p>
<p>七</p>
<p>我自从混迹到尘世间，便忘却了我自己；在你的灵魂我才知是谁？</p>
<p>记得也是这样夜里。我们在河堤的柳丝中走过来，走过去。我们无语，心海的波浪也只有月儿能领会。你倚在树上望明月沈思，我枕在你胸前听你的呼吸。抬头看见黑翼飞来掩遮住月儿的清光，你抖颤著问我：假如这苍黑的翼是我们的命运时，应该怎样？</p>
<p>我认识了欢乐，也随来了悲哀，接受了你的热情，同时也随来了冷酷的秋风。往日，我怕恶魔的眼睛凶，白牙如利刃；我总是藏伏在你的腋下趑趄不敢进，你一手执宝剑，一手扶着我践踏着荆棘的途径，投奔那如花的前程！</p>
<p>如今，这道上还留着你斑斑血痕，恶魔的眼睛和牙齿再是那样凶狠。但是我爱，你不要怕我孤零，我愿用这一纤细的弱玉腕，建设那如意的梦境。</p>
<p>八</p>
<p>春来了，催开桃蕾又飘到柳梢，这般温柔慵懒的天气真使人恼！她似乎躲在我眼底有意缭绕，一阵阵风翼，吹起了我灵海深处的波涛。</p>
<p>这世界已换上了装束，如少女般那样娇娆，她披拖着浅绿的轻纱，蹁跹在她那（姹）紫嫣红中舞蹈。伫立于白杨下，我心如捣，强睁开模糊的泪眼，细认你墓头，萋萋芳草。</p>
<p>满腔辛酸与谁道？愿此恨吐向青空将天地包。它纠结围绕着我的心，像一堆枯黄的蔓草，我爱，我待你用宝剑来挥扫，我待你用火花来焚烧。</p>
<p>九</p>
<p>垒垒荒冢上，火光熊熊，纸灰缭绕，清明到了。这是碧草绿水的春郊。墓畔有白发老翁，有红颜年少，向这一杯黄土致不尽的怀忆和哀悼，云天苍茫处我将魂招；白杨萧条，暮鸦声声，怕孤魂归路迢迢。</p>
<p>逝去了，欢乐的好梦，不能随墓草而复生，明朝此日，谁知天涯何处寄此身？叹漂泊我已如落花浮萍，且高歌，且痛饮，拼一醉烧熄此心头余情。</p>
<p>我爱，这一杯苦酒细细斟，邀残月与孤星和泪共饮，不管黄昏，不论夜深，醉卧在你墓碑傍，任霜露侵凌吧！我再不醒。</p>
<p>十六年清明陶然亭畔</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;按：石评梅(1902-1928)为纪念恋人高君宇所作。初读是在高中一年级时的语文阅读材料里，读毕怅然若失，之后日日早读必大声读一遍，忘乎所以。如今重逢，只可默默抄写，竟不能放声读矣。&lt;/p&gt;
&lt;p&gt;一&lt;/p&gt;
&lt;p&gt;我由冬的残梦里惊醒，春正吻着我的睡靥低吟！晨曦照上了窗纱，
    
    </summary>
    
      <category term="Reading" scheme="http://blog.ddlee.cn/categories/Reading/"/>
    
    
      <category term="Reading" scheme="http://blog.ddlee.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>[源码笔记]keras源码分析之Model</title>
    <link href="http://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/"/>
    <id>http://blog.ddlee.cn/2017/07/30/源码笔记-keras源码分析之Model/</id>
    <published>2017-07-30T15:19:42.000Z</published>
    <updated>2017-08-03T14:15:45.668Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是<a href="https://github.com/fchollet/keras/blob/master/keras/engine/training.py" target="_blank" rel="external">/keras/engine/training.py</a>和<a href="https://github.com/fchollet/keras/blob/master/keras/models.py" target="_blank" rel="external">/keras/model.py</a>，要观察的类是<code>Model</code>和<code>Sequential</code>。</p>
<p>本系列第一篇：<a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">【源码笔记】keras源码分析之Tensor, Node和Layer</a><br>第二篇：<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">【源码笔记】keras源码分析之Container</a></p>
<h3 id="Model：添加了训练信息的Container"><a href="#Model：添加了训练信息的Container" class="headerlink" title="Model：添加了训练信息的Container"></a><code>Model</code>：添加了训练信息的<code>Container</code></h3><p><code>Model.compile()</code>主要完成了配置<code>optimizer</code>, <code>loss</code>, <code>metrics</code>等操作，而要执行的<code>fit</code>, <code>evaluate</code>等则不在<code>compile</code>过程中配置。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compile</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> loss_weights<span class="token operator">=</span>None<span class="token punctuation">,</span>
            sample_weight_mode<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss <span class="token operator">or</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> optimizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>sample_weight_mode <span class="token operator">=</span> sample_weight_mode
    self<span class="token punctuation">.</span>loss <span class="token operator">=</span> loss
    self<span class="token punctuation">.</span>loss_weights <span class="token operator">=</span> loss_weights

    loss_function <span class="token operator">=</span> losses<span class="token punctuation">.</span>get<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    loss_functions <span class="token operator">=</span> <span class="token punctuation">[</span>loss_function <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>loss_functions <span class="token operator">=</span> loss_functions

    <span class="token comment" spellcheck="true"># Prepare targets of model.</span>
    self<span class="token punctuation">.</span>targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>_feed_targets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        shape <span class="token operator">=</span> self<span class="token punctuation">.</span>internal_output_shapes<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        name <span class="token operator">=</span> self<span class="token punctuation">.</span>output_names<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        target <span class="token operator">=</span> K<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>ndim<span class="token operator">=</span>len<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span>
                               name<span class="token operator">=</span>name <span class="token operator">+</span> <span class="token string">'_target'</span><span class="token punctuation">,</span>
                               sparse<span class="token operator">=</span>K<span class="token punctuation">.</span>is_sparse<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>K<span class="token punctuation">.</span>dtype<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>targets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_feed_targets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># Prepare metrics.</span>
    self<span class="token punctuation">.</span>metrics <span class="token operator">=</span> metrics
    self<span class="token punctuation">.</span>metrics_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>metrics_tensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># Compute total loss.</span>
    total_loss <span class="token operator">=</span> None
    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        y_true <span class="token operator">=</span> self<span class="token punctuation">.</span>targets<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        y_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        loss_weight <span class="token operator">=</span> loss_weights_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">if</span> total_loss <span class="token keyword">is</span> None<span class="token punctuation">:</span>
            total_loss <span class="token operator">=</span> loss_weight <span class="token operator">*</span> output_loss
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            total_loss <span class="token operator">+=</span> loss_weight <span class="token operator">*</span> output_loss

    <span class="token keyword">for</span> loss_tensor <span class="token keyword">in</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">:</span>
        total_loss <span class="token operator">+=</span> loss_tensor

    self<span class="token punctuation">.</span>total_loss <span class="token operator">=</span> total_loss
    self<span class="token punctuation">.</span>sample_weights <span class="token operator">=</span> sample_weights
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Model</code>对象的<code>fit()</code>方法封装了<code>_fit_loop()</code>内部方法，而<code>_fit_loop()</code>方法的关键步骤由<code>_make_train_function()</code>方法完成，返回<code>history</code>对象，用于回调函数的处理。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token operator">=</span>None<span class="token punctuation">,</span> y<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：
      self<span class="token punctuation">.</span>_make_train_function<span class="token punctuation">(</span><span class="token punctuation">)</span>
      f <span class="token operator">=</span> self<span class="token punctuation">.</span>train_function
      <span class="token keyword">return</span> self<span class="token punctuation">.</span>_fit_loop<span class="token punctuation">(</span>f<span class="token punctuation">,</span> ins<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>在<code>_fit_loop()</code>方法中，回调函数完成了对训练过程的监控记录等任务，<code>train_function</code>也被应用于传入的数据：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_fit_loop</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> f<span class="token punctuation">,</span> ins<span class="token punctuation">,</span> out_labels<span class="token operator">=</span>None<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
              epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span>
              val_f<span class="token operator">=</span>None<span class="token punctuation">,</span> val_ins<span class="token operator">=</span>None<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              callback_metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> initial_epoch<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>history <span class="token operator">=</span> cbks<span class="token punctuation">.</span>History<span class="token punctuation">(</span><span class="token punctuation">)</span>
    callbacks <span class="token operator">=</span> <span class="token punctuation">[</span>cbks<span class="token punctuation">.</span>BaseLogger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>callbacks <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>history<span class="token punctuation">]</span>
    callbacks <span class="token operator">=</span> cbks<span class="token punctuation">.</span>CallbackList<span class="token punctuation">(</span>callbacks<span class="token punctuation">)</span>
    out_labels <span class="token operator">=</span> out_labels <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    callbacks<span class="token punctuation">.</span>set_model<span class="token punctuation">(</span>callback_model<span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>set_params<span class="token punctuation">(</span><span class="token punctuation">{</span>
        <span class="token string">'batch_size'</span><span class="token punctuation">:</span> batch_size<span class="token punctuation">,</span>
        <span class="token string">'epochs'</span><span class="token punctuation">:</span> epochs<span class="token punctuation">,</span>
        <span class="token string">'samples'</span><span class="token punctuation">:</span> num_train_samples<span class="token punctuation">,</span>
        <span class="token string">'verbose'</span><span class="token punctuation">:</span> verbose<span class="token punctuation">,</span>
        <span class="token string">'do_validation'</span><span class="token punctuation">:</span> do_validation<span class="token punctuation">,</span>
        <span class="token string">'metrics'</span><span class="token punctuation">:</span> callback_metrics <span class="token operator">or</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>on_train_begin<span class="token punctuation">(</span><span class="token punctuation">)</span>
    callback_model<span class="token punctuation">.</span>stop_training <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>initial_epoch<span class="token punctuation">,</span> epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        callbacks<span class="token punctuation">.</span>on_epoch_begin<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
        batches <span class="token operator">=</span> _make_batches<span class="token punctuation">(</span>num_train_samples<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        epoch_logs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
        <span class="token keyword">for</span> batch_index<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_start<span class="token punctuation">,</span> batch_end<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
            batch_ids <span class="token operator">=</span> index_array<span class="token punctuation">[</span>batch_start<span class="token punctuation">:</span>batch_end<span class="token punctuation">]</span>
            batch_logs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            batch_logs<span class="token punctuation">[</span><span class="token string">'batch'</span><span class="token punctuation">]</span> <span class="token operator">=</span> batch_index
            batch_logs<span class="token punctuation">[</span><span class="token string">'size'</span><span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>batch_ids<span class="token punctuation">)</span>
            callbacks<span class="token punctuation">.</span>on_batch_begin<span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> batch_logs<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># 应用传入的train_function</span>
            outs <span class="token operator">=</span> f<span class="token punctuation">(</span>ins_batch<span class="token punctuation">)</span>
            callbacks<span class="token punctuation">.</span>on_batch_end<span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> batch_logs<span class="token punctuation">)</span>
        callbacks<span class="token punctuation">.</span>on_epoch_end<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> epoch_logs<span class="token punctuation">)</span>
    callbacks<span class="token punctuation">.</span>on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>history
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>_make_train_function()</code>方法从<code>optimizer</code>获取要更新的参数信息，并传入来自<code>backend</code>的<code>function</code>对象：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_make_train_function</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>train_function <span class="token keyword">is</span> None<span class="token punctuation">:</span>
        inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>_feed_inputs <span class="token operator">+</span> self<span class="token punctuation">.</span>_feed_targets <span class="token operator">+</span> self<span class="token punctuation">.</span>_feed_sample_weights
        training_updates <span class="token operator">=</span> self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>get_updates<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>_collected_trainable_weights<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>constraints<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>total_loss<span class="token punctuation">)</span>
        updates <span class="token operator">=</span> self<span class="token punctuation">.</span>updates <span class="token operator">+</span> training_updates
        <span class="token comment" spellcheck="true"># Gets loss and metrics. Updates weights at each call.</span>
        self<span class="token punctuation">.</span>train_function <span class="token operator">=</span> K<span class="token punctuation">.</span>function<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>
                                         <span class="token punctuation">[</span>self<span class="token punctuation">.</span>total_loss<span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>metrics_tensors<span class="token punctuation">,</span>
                                         updates<span class="token operator">=</span>updates<span class="token punctuation">,</span>
                                         name<span class="token operator">=</span><span class="token string">'train_function'</span><span class="token punctuation">,</span>
                                         <span class="token operator">**</span>self<span class="token punctuation">.</span>_function_kwargs<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Model</code>的其他方法<code>evaluate()</code>等，与<code>fit()</code>的结构类似。</p>
<h3 id="Sequential-构建模型的外层接口"><a href="#Sequential-构建模型的外层接口" class="headerlink" title="Sequential:构建模型的外层接口"></a><code>Sequential</code>:构建模型的外层接口</h3><p><code>Sequential</code>对象是<code>Model</code>对象的进一步封装，也是用户直接面对的接口，其<code>compile()</code>, <code>fit()</code>, <code>predict()</code>等方法与<code>Model</code>几乎一致，所不同的是添加了<code>add()</code>方法，也是我们用于构建网络的最基本操作。</p>
<p><code>Sequential.add()</code>方法的源码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 第一层必须是InputLayer对象</span>
    <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token operator">not</span> layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">:</span>
            x <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span>layer<span class="token punctuation">.</span>batch_input_shape<span class="token punctuation">,</span>
                      dtype<span class="token operator">=</span>layer<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> name<span class="token operator">=</span>layer<span class="token punctuation">.</span>name <span class="token operator">+</span> <span class="token string">'_input'</span><span class="token punctuation">)</span>
            layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> <span class="token punctuation">[</span>layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>inputs <span class="token operator">=</span> topology<span class="token punctuation">.</span>get_source_inputs<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        topology<span class="token punctuation">.</span>Node<span class="token punctuation">(</span>outbound_layer<span class="token operator">=</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        output_tensor <span class="token operator">=</span> layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> <span class="token punctuation">[</span>output_tensor<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensors <span class="token operator">=</span> self<span class="token punctuation">.</span>outputs

    self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到，<code>add()</code>方法总是确保网络的第一层为<code>InputLayer</code>对象，并将新加入的层应用于<code>outputs</code>，使之更新。因此，从本质上讲，在<code>Model</code>中添加新层还是在更新模型的<code>outputs</code>。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是keras源码笔记系列的第三篇。在前两篇中，我们分析了keras对Tensor和Layer等概念的处理，并说明了它们是如何作用别弄个构成有向无环图的。本篇着眼于多层网络模型层面的抽象，即与用户距离最近的接口，源代码文件是&lt;a href=&quot;https://github.
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Keras" scheme="http://blog.ddlee.cn/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>[源码笔记]keras源码分析之Container</title>
    <link href="http://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/"/>
    <id>http://blog.ddlee.cn/2017/07/25/源码笔记-keras源码分析之Container/</id>
    <published>2017-07-25T14:08:44.000Z</published>
    <updated>2017-08-03T14:17:13.948Z</updated>
    
    <content type="html"><![CDATA[<p>本篇继续讨论keras的源码结构。</p>
<p><a href="https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/">第一篇源码笔记</a>中我们观察了<code>Layer</code>, <code>Tensor</code>和<code>Node</code>是如何耦合在一起的，而本篇的重点是观察多层网络构成的有向无环图（DAG）。主要涉及的文件为<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">keras/engine/topology.py</a>， 要观察的类是<code>Container</code>。</p>
<h3 id="Container对象：DAG的拓扑原型"><a href="#Container对象：DAG的拓扑原型" class="headerlink" title="Container对象：DAG的拓扑原型"></a><code>Container</code>对象：DAG的拓扑原型</h3><p>在第一篇中我们提到，Keras Tensor中增强的<code>\_keras_history</code>属性使得我们仅通过输入和输出的Tensor，就可以构建出整张计算图。而<code>Container</code>对象正是实现了这样的过程。</p>
<h4 id="计算图的构建"><a href="#计算图的构建" class="headerlink" title="计算图的构建"></a>计算图的构建</h4><p>DAG计算图的构建在<code>Container</code>对象实例化时完成，主要包括如下几个操作：</p>
<p>1） 记录<code>Container</code>的首尾连接信息</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
      layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
      self<span class="token punctuation">.</span>output_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>output_layers_node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>output_layers_tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>

  <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span>
      layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
      self<span class="token punctuation">.</span>input_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>input_layers_node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>input_layers_tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>2） 从<code>output_tensors</code>开始反向递归构建计算图，采用广度优先的准则，本步的关键是构建<code>nodes_in_decreasing_depth</code>这一队列，这些<code>Node</code>包含的连接信息和深度信息将是后续正向传播和反向训练计算执行顺序的依据。</p>
<pre class="line-numbers language-python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">build_map_of_graph</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">)</span><span class="token punctuation">:</span>
      layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> tensor<span class="token punctuation">.</span>_keras_history
      node <span class="token operator">=</span> layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>node_index<span class="token punctuation">]</span>
      nodes_in_progress<span class="token punctuation">.</span>add<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

      <span class="token comment" spellcheck="true"># 广度优先搜索</span>
      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          x <span class="token operator">=</span> node<span class="token punctuation">.</span>input_tensors<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          layer <span class="token operator">=</span> node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          node_index <span class="token operator">=</span> node<span class="token punctuation">.</span>node_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          tensor_index <span class="token operator">=</span> node<span class="token punctuation">.</span>tensor_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          <span class="token comment" spellcheck="true"># 递归调用</span>
          build_map_of_graph<span class="token punctuation">(</span>x<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">,</span>
                             layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index<span class="token punctuation">)</span>

      <span class="token comment" spellcheck="true"># 维护两个队列</span>
      finished_nodes<span class="token punctuation">.</span>add<span class="token punctuation">(</span>node<span class="token punctuation">)</span>
      nodes_in_progress<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>node<span class="token punctuation">)</span>
      nodes_in_decreasing_depth<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 反向构建DAG</span>
  <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
      build_map_of_graph<span class="token punctuation">(</span>x<span class="token punctuation">,</span> finished_nodes<span class="token punctuation">,</span> nodes_in_progress<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>3） 计算各节点的深度并按深度标定节点在DAG中的位置</p>
<pre class="line-numbers language-python"><code class="language-python">  <span class="token comment" spellcheck="true"># 根据队列标定各节点的深度</span>
  <span class="token keyword">for</span> node <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>nodes_in_decreasing_depth<span class="token punctuation">)</span><span class="token punctuation">:</span>
      depth <span class="token operator">=</span> nodes_depths<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span>node<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
      previous_depth <span class="token operator">=</span> layers_depths<span class="token punctuation">.</span>get<span class="token punctuation">(</span>node<span class="token punctuation">.</span>outbound_layer<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
      depth <span class="token operator">=</span> max<span class="token punctuation">(</span>depth<span class="token punctuation">,</span> previous_depth<span class="token punctuation">)</span>
      layers_depths<span class="token punctuation">[</span>node<span class="token punctuation">.</span>outbound_layer<span class="token punctuation">]</span> <span class="token operator">=</span> depth
      nodes_depths<span class="token punctuation">[</span>node<span class="token punctuation">]</span> <span class="token operator">=</span> depth

      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          inbound_layer <span class="token operator">=</span> node<span class="token punctuation">.</span>inbound_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          node_index <span class="token operator">=</span> node<span class="token punctuation">.</span>node_indices<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
          inbound_node <span class="token operator">=</span> inbound_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>node_index<span class="token punctuation">]</span>
          previous_depth <span class="token operator">=</span> nodes_depths<span class="token punctuation">.</span>get<span class="token punctuation">(</span>inbound_node<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
          nodes_depths<span class="token punctuation">[</span>inbound_node<span class="token punctuation">]</span> <span class="token operator">=</span> max<span class="token punctuation">(</span>depth <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> previous_depth<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 按深度标定各节点的位置</span>
  nodes_by_depth <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> node<span class="token punctuation">,</span> depth <span class="token keyword">in</span> nodes_depths<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> depth <span class="token operator">not</span> <span class="token keyword">in</span> nodes_by_depth<span class="token punctuation">:</span>
          nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>node<span class="token punctuation">)</span>

  <span class="token comment" spellcheck="true"># 按深度标定各层的位置</span>
  layers_by_depth <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">for</span> layer<span class="token punctuation">,</span> depth <span class="token keyword">in</span> layers_depths<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> depth <span class="token operator">not</span> <span class="token keyword">in</span> layers_by_depth<span class="token punctuation">:</span>
          layers_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      layers_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>

  self<span class="token punctuation">.</span>layers_by_depth <span class="token operator">=</span> layers_by_depth
  self<span class="token punctuation">.</span>nodes_by_depth <span class="token operator">=</span> nodes_by_depth
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>4）将整个<code>Container</code>并入<code>Node</code>以保持兼容性</p>
<pre class="line-numbers language-python"><code class="language-python">  self<span class="token punctuation">.</span>outbound_nodes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  self<span class="token punctuation">.</span>inbound_nodes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
  Node<span class="token punctuation">(</span>outbound_layer<span class="token operator">=</span>self<span class="token punctuation">,</span>
       inbound_layers<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       node_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       tensor_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       input_tensors<span class="token operator">=</span>self<span class="token punctuation">.</span>inputs<span class="token punctuation">,</span>
       output_tensors<span class="token operator">=</span>self<span class="token punctuation">.</span>outputs<span class="token punctuation">,</span>
       <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="计算图中的计算"><a href="#计算图中的计算" class="headerlink" title="计算图中的计算"></a>计算图中的计算</h3><p>计算在<code>Container</code>对象的<code>call()</code>方法完成，其实现又依靠内部方法<code>run_internal_graph()</code>。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_internal_graph</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> masks<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
       depth_keys <span class="token operator">=</span> list<span class="token punctuation">(</span>self<span class="token punctuation">.</span>nodes_by_depth<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
       depth_keys<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
       <span class="token comment" spellcheck="true"># 依据深度</span>
       <span class="token keyword">for</span> depth <span class="token keyword">in</span> depth_keys<span class="token punctuation">:</span>
           nodes <span class="token operator">=</span> self<span class="token punctuation">.</span>nodes_by_depth<span class="token punctuation">[</span>depth<span class="token punctuation">]</span>
           <span class="token comment" spellcheck="true"># 对同一深度上的Node进行计算</span>
           <span class="token keyword">for</span> node <span class="token keyword">in</span> nodes<span class="token punctuation">:</span>
               layer <span class="token operator">=</span> node<span class="token punctuation">.</span>outbound_layer <span class="token comment" spellcheck="true"># Node对应的layer</span>
               reference_input_tensors <span class="token operator">=</span> node<span class="token punctuation">.</span>input_tensors
               reference_output_tensors <span class="token operator">=</span> node<span class="token punctuation">.</span>output_tensors
               computed_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  
               <span class="token keyword">if</span> len<span class="token punctuation">(</span>computed_data<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>reference_input_tensors<span class="token punctuation">)</span><span class="token punctuation">:</span>
                   <span class="token comment" spellcheck="true"># 在Layer中进行计算</span>
                   <span class="token keyword">with</span> K<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
                       <span class="token keyword">if</span> len<span class="token punctuation">(</span>computed_data<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                           computed_tensor<span class="token punctuation">,</span> computed_mask <span class="token operator">=</span> computed_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                           output_tensors <span class="token operator">=</span> _to_list<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>call<span class="token punctuation">(</span>computed_tensor<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
                           computed_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>computed_tensor<span class="token punctuation">]</span>
                       <span class="token keyword">else</span><span class="token punctuation">:</span>
                           computed_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> computed_data<span class="token punctuation">]</span>
                           output_tensors <span class="token operator">=</span> _to_list<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>call<span class="token punctuation">(</span>computed_tensors<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
       output_tensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
       output_masks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
       <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
           tensor<span class="token punctuation">,</span> mask <span class="token operator">=</span> tensor_map<span class="token punctuation">[</span>str<span class="token punctuation">(</span>id<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
           output_tensors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
           output_masks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
       <span class="token keyword">return</span> output_tensors<span class="token punctuation">,</span> output_masks
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>从上面的代码可以看到计算是依据深度进行的，并通过更新<code>computed_data</code>和<code>output_tensor</code>等变量完成整张图的遍历计算。</p>
<p>继续阅读系列第三篇：<a href="https://blog.ddlee.cn/2017/07/30/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BModel/">【源码笔记】keras源码分析之Model</a></p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇继续讨论keras的源码结构。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Keras" scheme="http://blog.ddlee.cn/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的权重衰减</title>
    <link href="http://blog.ddlee.cn/2017/07/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/"/>
    <id>http://blog.ddlee.cn/2017/07/22/深度学习中的权重衰减/</id>
    <published>2017-07-22T15:51:13.000Z</published>
    <updated>2017-08-03T14:18:06.028Z</updated>
    
    <content type="html"><![CDATA[<p>权重衰减（weight dacay），即L^2范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自<a href="https://deeplearningbook.org" target="_blank" rel="external">The Deep Learning Book</a>。</p>
<h3 id="为什么要引入权重衰减"><a href="#为什么要引入权重衰减" class="headerlink" title="为什么要引入权重衰减"></a>为什么要引入权重衰减</h3><p>机器学习的逻辑与我们最初解决问题的思维方式恰恰相反：要解决问题，一种经典的思路是把它拆成小问题，考虑之间的依赖，然后分而治之。而机器学习的哲学是“<em>trail-error-correct</em>”：先假设一堆可能的方案，根据结果去选择/调整这些方案，直到满意。换句话说，机器学习在假设空间中搜索最符合数据的模型：以果推因，即为最大似然的想法。随着数据量的增大，我们越来越需要表达能力更强的模型，而深度学习的优势正符合这一需要：通过分布式表示带来的指数增益，深度学习模型的扩展能力几乎是无限的（详见<a href="https://blog.ddlee.cn/2017/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E7%A4%BA/">深度学习和分布式表示</a>）。</p>
<p>有了模型（备选模型集），有了数据，就不得不面对机器学习领域的核心问题：如何保证模型能够描述数据（拟合）和生成数据（泛化）。</p>
<p>粗略来看，有以下三种情况：</p>
<ul>
<li>我们假定的模型族不包含真实数据的生成过程：欠拟合/高偏差</li>
<li>匹配真实的数据生成过程</li>
<li>除了包含真实的生成过程，还包含了其他信息：过拟合/高方差</li>
</ul>
<p>高偏差意味着我们的模型不够准确（模型族不足以描述数据），高方差意味着我们建模了不必要的信息（训练数据的随机性带来的）。前者通过提高模型的表述能力来解决（更深的网络），后者则需要合理的正则化技术来控制。这即是著名的trade-off。</p>
<h3 id="深度学习模型的参数"><a href="#深度学习模型的参数" class="headerlink" title="深度学习模型的参数"></a>深度学习模型的参数</h3><p>对数据建模，其实是从数据中提取我们能够理解的信息。建立的模型，是从数据分布的空间到目标变量所在空间的映射。从这个角度看，我们通过模型带来的变换获得了数据的一种表示，我们认为能够理解和操作的表示。</p>
<p>为了表述这一变换，深度模型的套路是线性层施加变换，非线性层固定信息（不能平移），然后将这样的结构堆叠起来，分层提取数据特征。</p>
<p>这让我想起实变中证明定理的套路：先证明简单函数的情形，再推广到连续函数，再到勒贝格可积的函数。</p>
<p>常规的套路（MLP）在拟合普通的函数任务上能够胜任，但面对更复杂的图像等数据，就需要更灵活的网络结构。</p>
<p>非常出色的CNN, LSTM, Inception块, ResNet, DenseNet等结构，就是加入了人类的先验知识，使之更有效的提取图像/音频数据分布空间的特征。（所以Manning有次在课堂上说，机器学习事实上还是人类在学习：机器只是在求导数、做乘法，最好的模型都是人们学习出来的。）</p>
<p>人们确实设计了很多巧妙的结构来解决不同的问题，但落实到网络的层和单元上，仍是最基本的矩阵乘法、加法运算。决定模型表述能力的，也正是这些普通的乘法运算中涉及的矩阵和向量了。</p>
<h3 id="权重衰减如何起作用"><a href="#权重衰减如何起作用" class="headerlink" title="权重衰减如何起作用"></a>权重衰减如何起作用</h3><p>下面我们通过观察加入权重衰减后目标函数的梯度变化来讨论权重衰减是如何起作用的。可以跳过公式部分直接看最后一段。</p>
<p>——————————————————————————————推导部分————————————————————————————————————————————</p>
<p>简单起见，令偏置为0，模型的目标函数：</p>
<p>$$J_{1}(w; X,y)=\frac{\alpha}{2} w^T w+J(w; X,y)$$</p>
<p>对应的梯度为：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w; X,y) = \alpha w + {\nabla}_{w} J(w; X,y)$$</p>
<p>进行梯度下降，参数的更新规则为：</p>
<p>$$w = w - \epsilon (\alpha w + {\nabla}_{w} J(w; X,y)) $$</p>
<p>也就是：</p>
<p>$$w = (1 - \epsilon \alpha )w - \epsilon {\nabla}_{w} J(w; X,y)$$</p>
<p>从上式可以发现，加入权重衰减后，先对参数进行伸缩，再沿梯度下降。下面令$$x^{(1)}$$为使目标函数达到最优的参数值，在其附近考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>其中$$H$$为近似目标函数在的Hessian矩阵。当近似目标函数最小时，其梯度为$$0$$，即：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>该式也向我们说明了基于梯度的优化算法主要的信息来自Hessian矩阵。添加入权重衰减项之后，上式变为（记此时的最优点为$$w^{(2)}$$）：</p>
<p>$${\nabla}<em>{w} J</em>{1}(w) \approx \alpha w^{(2)} + H(w^{(2)} - w^{(1)}) = 0$$</p>
<p>所以</p>
<p>$$w^{(2)} = (H + \alpha I)^{-1} H w^{(1)} $$</p>
<p>该式表明了了加入正则化对参数最优质点的影响，由Hessian矩阵和正则化系数$$\alpha$$共同决定。</p>
<p>进一步将Hessian矩阵分解，可以得到：</p>
<p>$$w^{(2)} = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^{(1)}$$</p>
<p>其中，$$Q$$为正交矩阵，$$\Lambda$$为对角矩阵。这样可以看到，<em>权重衰减的效果是沿着由$$H$$的特征向量所定义的轴缩放$$w$$</em>， 具体的伸缩因子为$$\frac{ {\lambda}_{i} }{ {\lambda}<em>i + \alpha }$$，其中$${\lambda}</em>{i}$$表示第$$i$$个特征向量对应的特征值。</p>
<p>当特征值$$\lambda$$很大（相比$$\alpha$$）时，缩放因子对权重影响较小，因而更新过程中产生的变化也不大；而当特征值较小时，$$\alpha$$的缩放作用就显现出来，将这个方向的权重衰减到0。</p>
<p>这种效果也可以由下图表示：</p>
<p><img src="http://static.ddlee.cn/static/img/深度学习中的权重衰减/transform.png" alt="transform"></p>
<p>——————————————————————————推导部分结束————————————————————————————————————————————————————————————————————</p>
<p><em>总结来说，目标函数的Hessian矩阵（显式、隐式或者近似的）是现有优化算法进行寻优的主要依据。通过控制权重衰减的$$\alpha$$参数，我们实际上控制的是在Hessian矩阵的特征方向上以多大的幅度缩放权重，相对重要（能够显著减小目标函数）的方向上权重保留比较完好，而无助于目标函数减小的方向上权重在训练过程中逐渐地衰减掉了。而这也就是权重衰减的意义。</em></p>
<p>从宏观上来看，对目标函数来说，特征值较大的方向包含更多有关数据的信息，较小的方向则有随机性的噪声，权重衰减正是通过忽略较少信息方向的变化来对抗过拟合的。</p>
<h3 id="L-1-范数正则化"><a href="#L-1-范数正则化" class="headerlink" title="$$L^1$$范数正则化"></a>$$L^1$$范数正则化</h3><p>通过类似的推导，可以得到加入了$$L^1$$范数惩罚项对参数最优解的影响如下：</p>
<p>$$w^{(2)}<em>{i} = sign(w^{(1)}</em>{i}) max \big{|w^{(1)}<em>{i}| - \frac{\alpha}{H</em>{i,i}}, 0 \big}$$</p>
<p>相比$$L^2$$范数的影响，这是一个离散的结果，因而$$L^1$$范数惩罚会将参数推向更加稀疏的解。这种稀疏性质常被用作特征选择。</p>
<h3 id="权重衰减的贝叶斯解释"><a href="#权重衰减的贝叶斯解释" class="headerlink" title="权重衰减的贝叶斯解释"></a>权重衰减的贝叶斯解释</h3><p>在贝叶斯统计的框架下，常用的推断策略是最大后验点估计(Maximum A Posteriori, MAP)。有如下的推断公式（由贝叶斯定律导出）：</p>
<p>$${\theta}_{MAP} = argmax p(\theta | x) = argmax (log p( x | \theta) + log p(\theta))$$</p>
<p>上式右边第一项是标准的对数似然项，而第二项对应着先验分布。</p>
<p>在这样的视角下，我们只进行最大似然估计是不够的，还要考虑先验$$p(\theta)$$的分布。而当假定参数为正态分布$$N(w; 0, \frac{1}{\lambda}I^2)$$时，带入上式（$$\theta$$为参数），即可发现第二项的结果正比于权重衰减惩罚项$$\lambda w^T w$$，加上一个不依赖于$$w$$也不影响学习过程的项。于是，具有高斯先验权重的MAP贝叶斯推断对应着权重衰减。</p>
<h3 id="权重衰减与提前终止"><a href="#权重衰减与提前终止" class="headerlink" title="权重衰减与提前终止"></a>权重衰减与提前终止</h3><p>提前终止也是一种正则化技术，其想法简单粗暴：每个epoch之后在验证集上评估结果，当验证集误差不再下降的时候，我们认为模型已经尽它所能了，于是终止训练过程。</p>
<p>提前终止以牺牲一部分训练数据来作为验证数据来的代价来对抗过拟合，其逻辑是实证主义的。</p>
<p>然而，在二次近似和简单梯度下降的情形下，可以观察到提前终止可以有相当于权重衰减的效果。</p>
<p>我们仍考虑目标函数的二次近似：</p>
<p>$$J(w) \approx J(w^{(1)}) + \frac{1}{2} (w - w^{(1)})^T H (w - w^{(1)})$$</p>
<p>记最优参数点为$$w^{(1)}$$，其梯度为：</p>
<p>$${\nabla}_{w} J(w) \approx H(w - w^{(1)})$$</p>
<p>不加入正则化项，其梯度下降的更新策略（从第$$\tau-1$$步到$$\tau$$步）为：</p>
<p>$$ w^{(\tau)} = w^{\tau - 1)} - \epsilon H (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>累加得到</p>
<p>$$ w^{(\tau)} - w^{(1)} = (I - \epsilon H) (w^{(\tau - 1)} - w^{(1)})$$</p>
<p>将Hessian矩阵分解，得到如下形式</p>
<p>$$ w^{(\tau)} = Q[I - (I - \epsilon \Lambda) ^ {\tau}] Q^T w^{(1)} $$</p>
<p>将加入正则化项的权重影响改写为</p>
<p>$$ w^{(2)} = Q[I - (\Lambda + \alpha I) ^ {-1} \alpha] Q^T w^{(1)} $$</p>
<p>对比可以得到，如果超参数$$\epsilon, \alpha, \tau$$满足</p>
<p>$$ (I - \epsilon \Lambda) ^ {\tau} = (\Lambda + \alpha I) ^ {-1} \alpha $$</p>
<p>则提前终止将与权重衰减有相当的效果。具体的，即第$$\tau$$步结束的训练过程将到达超参数为$$\alpha$$的$$L^2$$正则化得到的最优点。</p>
<p>但提前终止带来的好处是，我们不再需要去找合适的超参数$$\alpha$$，而只需要制定合理的终止策略（如3个epoch均不带来验证集误差的减小即终止训练），在训练成本的节约上，还是很值得的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;权重衰减（weight dacay），即L^2范数惩罚，是最常见的正则化技术之一。本文将介绍它是如何起作用的。主要材料来自&lt;a href=&quot;https://deeplearningbook.org&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;The De
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Machine Learning" scheme="http://blog.ddlee.cn/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[源码笔记]keras源码分析之Layer、Tensor和Node</title>
    <link href="http://blog.ddlee.cn/2017/07/15/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BLayer%E3%80%81Tensor%E5%92%8CNode/"/>
    <id>http://blog.ddlee.cn/2017/07/15/源码笔记-keras源码分析之Layer、Tensor和Node/</id>
    <published>2017-07-15T12:04:54.000Z</published>
    <updated>2017-08-03T14:13:45.334Z</updated>
    
    <content type="html"><![CDATA[<p>Keras架构的主要逻辑实现在<a href="https://github.com/fchollet/keras/blob/master/keras/engine/topology.py" target="_blank" rel="external">/keras/engine/topology.py</a>中，主要有两个基类<code>Node()</code>和<code>Layer()</code>，一个重要函数<code>Input()</code>。具体地，</p>
<ul>
<li><code>Layer()</code>是一个计算层的抽象，完成网络中对Tensor的计算过程；</li>
<li><code>Node()</code>描述两个层之间连接关系的抽象，配合<code>Layer()</code>构建DAG；</li>
<li><code>Input()</code>实例化一个特殊的<code>Layer</code>(<code>InputLayer</code>)，将<code>backend</code>（TensorFlow或Theano）建立的Tensor对象转化为Keras Tensor对象。</li>
</ul>
<h3 id="Keras-Tensor：-增强版Tensor"><a href="#Keras-Tensor：-增强版Tensor" class="headerlink" title="Keras Tensor： 增强版Tensor"></a>Keras Tensor： 增强版Tensor</h3><p>相比原始的TensorFlow或者Theano的张量对象，Keras Tensor加入了如下两个属性，以使Tensor中包含了自己的来源和规模信息：</p>
<ul>
<li>_Keras_history: 保存了最近一个应用于这个Tensor的Layer</li>
<li>_keras_shape: 标准化的Keras shape接口</li>
</ul>
<p>当使用Keras建立深度网络时，传入的数据首先要经过<code>Input()</code>函数。在<code>Input()</code>函数中，实例化一个<code>InputLayer()</code>对象，并将此<code>Layer()</code>对象作为第一个应用于传入张量的Layer，置于<code>_keras_history</code>属性中。此外，<code>InputLayer()</code>和<code>Input()</code>还会对传入的数据进行规模检查和变换等，使之符合后续操作的要求。</p>
<p>代码上实现如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">Input</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  input_layer <span class="token operator">=</span> InputLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
  outputs <span class="token operator">=</span> InputLayer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>output_tensor
  <span class="token keyword">return</span> outputs

<span class="token keyword">class</span> <span class="token class-name">InputLayer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_tensor<span class="token punctuation">.</span>_keras_history <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    Node<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在下面我们将看到，加入的<code>_keras_history</code>属性在计算图的构建上所起的作用是关键的。仅通过输入和输出的Tensor，我们可以构建出整张计算图。但这样的代价是Tensor对象太重了，包含了Layer的信息。</p>
<h3 id="Node对象：层与层之间链接的抽象"><a href="#Node对象：层与层之间链接的抽象" class="headerlink" title="Node对象：层与层之间链接的抽象"></a><code>Node</code>对象：层与层之间链接的抽象</h3><p>若考虑<code>Layer</code>对象抽象的是完成计算的神经元胞体，则<code>Node</code>对象是对神经元树突结构的抽象。其内聚的主要信息是：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Node</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outbound_layer<span class="token punctuation">,</span>
              inbound_layers<span class="token punctuation">,</span> node_indices<span class="token punctuation">,</span> tensor_indices<span class="token punctuation">,</span>
              input_tensors<span class="token punctuation">,</span> output_tensors<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>其中<code>outbound_layer</code>是施加计算（使<code>input_tensors</code>变为<code>output_tensors</code>）的层，<code>inbound_layers</code>对应了<code>input_tensors</code>来源的层，而<code>node_indices</code>和<code>tensor_indices</code>则记录了<code>Node</code>和<code>Layer</code>之间的标定信息。</p>
<p><code>Node</code>对象总在<code>outbound_layer</code>被执行时创建，并加入<code>outbound_layer</code>的<code>inbound_nodes</code>属性中。在<code>Node</code>对象的表述下，A和B两个层产生连接关系时，<code>Node</code>对象被建立，并被加入<code>A.outbound_nodes</code>和<code>B.inbound_nodes</code>。</p>
<h3 id="Layer对象：计算层的抽象"><a href="#Layer对象：计算层的抽象" class="headerlink" title="Layer对象：计算层的抽象"></a><code>Layer</code>对象：计算层的抽象</h3><p><code>Layer</code>对象是对网络中神经元计算层的抽象，实例化需要如下参数：</p>
<pre class="line-numbers language-python"><code class="language-python">allowed_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'input_shape'</span><span class="token punctuation">,</span>
                  <span class="token string">'batch_input_shape'</span><span class="token punctuation">,</span>
                  <span class="token string">'batch_size'</span><span class="token punctuation">,</span>
                  <span class="token string">'dtype'</span><span class="token punctuation">,</span>
                  <span class="token string">'name'</span><span class="token punctuation">,</span>
                  <span class="token string">'trainable'</span><span class="token punctuation">,</span>
                  <span class="token string">'weights'</span><span class="token punctuation">,</span>
                  <span class="token string">'input_dtype'</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># legacy</span>
                  <span class="token punctuation">}</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>大部分与传入数据的类型和规模相关，<code>trainable</code>表征该层是否需要更新权重。此外，还有<code>inbound_nodes</code>和<code>outbound_nodes</code>属性来标定与<code>Node</code>对象的链接。</p>
<p><code>Layer</code>对象最重要的方法是<code>__call__()</code>，主要完成如下三件事情：</p>
<ol>
<li><p>验证传入数据的合法性，通过调用内部方法实现：<code>self.assert_input_compatibility(inputs)</code></p>
</li>
<li><p>进行计算<code>outputs = self.call(inputs, ...)</code>，被其子类具体实现，如<code>Linear</code>, <code>Dropout</code>等</p>
</li>
<li><p>更新Tensor中的<code>_keras_history</code>属性，记录该次计算操作，通过内部方法<code>_add_inbound_nodes()</code>实现</p>
</li>
</ol>
<p>方法<code>_add_inbound_nodes()</code>对Tensor的更新是构建<code>Layer</code>之间关系的关键操作，其主要代码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> x <span class="token keyword">in</span> input_tensors<span class="token punctuation">:</span>
    <span class="token keyword">if</span> hasattr<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'_keras_history'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        inbound_layer<span class="token punctuation">,</span> node_index<span class="token punctuation">,</span> tensor_index <span class="token operator">=</span> x<span class="token punctuation">.</span>_keras_history
        inbound_layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>inbound_layer<span class="token punctuation">)</span>
        node_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>node_index<span class="token punctuation">)</span>
        tensor_indices<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tensor_index<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># Node对象的建立过程中将更新self的inbound_nodes属性</span>
Node<span class="token punctuation">(</span>self<span class="token punctuation">,</span>
    inbound_layers<span class="token operator">=</span>inbound_layers<span class="token punctuation">,</span>
    node_indices<span class="token operator">=</span>node_indices<span class="token punctuation">,</span>
    tensor_indices<span class="token operator">=</span>tensor_indices<span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>output_tensors<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     output_tensors<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>_keras_history <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上段代码取出<code>input_tensor</code>的<code>_keras_history</code>属性，建立新的<code>Node</code>，并将当前<code>Layer</code>的信息更新到计算得到的<code>output_tensor</code>中。</p>
<h3 id="实例：Node-Tensor和Layer间连接关系的表征"><a href="#实例：Node-Tensor和Layer间连接关系的表征" class="headerlink" title="实例：Node,Tensor和Layer间连接关系的表征"></a>实例：<code>Node</code>,<code>Tensor</code>和<code>Layer</code>间连接关系的表征</h3><p>下面通过代码来说明三者之间的关系，来自于测试代码：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 建立新的keras Tensor</span>
a <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'input_a'</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'input_b'</span><span class="token punctuation">)</span>

a_layer<span class="token punctuation">,</span> a_node_index<span class="token punctuation">,</span> a_tensor_index <span class="token operator">=</span> a<span class="token punctuation">.</span>_keras_history
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>a_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>
<span class="token keyword">assert</span> a_tensor_index <span class="token keyword">is</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># node和layer之间的关系</span>
node <span class="token operator">=</span> a_layer<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span>a_node_index<span class="token punctuation">]</span>
<span class="token keyword">assert</span> node<span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> a_layer

<span class="token comment" spellcheck="true"># 建立连接层，将Tensor传入</span>
dense <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'dense_1'</span><span class="token punctuation">)</span>
a_2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
b_2 <span class="token operator">=</span> dense<span class="token punctuation">(</span>b<span class="token punctuation">)</span>

<span class="token keyword">assert</span> len<span class="token punctuation">(</span>dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>
<span class="token keyword">assert</span> len<span class="token punctuation">(</span>dense<span class="token punctuation">.</span>outbound_nodes<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span>

<span class="token comment" spellcheck="true"># 与张量a关联的Node</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>inbound_layers <span class="token operator">==</span> <span class="token punctuation">[</span>a_layer<span class="token punctuation">]</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> dense
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>input_tensors <span class="token operator">==</span> <span class="token punctuation">[</span>a<span class="token punctuation">]</span>

<span class="token comment" spellcheck="true"># 与张量b关联的Node</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>inbound_layers <span class="token operator">==</span> <span class="token punctuation">[</span>b_layer<span class="token punctuation">]</span>
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outbound_layer <span class="token operator">==</span> dense
<span class="token keyword">assert</span> dense<span class="token punctuation">.</span>inbound_nodes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>input_tensors <span class="token operator">==</span> <span class="token punctuation">[</span>b<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>keras利用<code>Node</code>对象描述<code>Layer</code>之间的连接关系，并在<code>Tensor</code>中记录其来源信息。在<a href="https://blog.ddlee.cn/2017/07/25/%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0-keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BContainer/">下篇</a>中，我们将看到keras如何利用这些抽象和增强属性构建DAG，并实现前向传播和反向训练的。</p>
<p>@<a href="https://ddlee.cn" target="_blank" rel="external">ddlee</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Keras架构的主要逻辑实现在&lt;a href=&quot;https://github.com/fchollet/keras/blob/master/keras/engine/topology.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;/keras/eng
    
    </summary>
    
      <category term="AI" scheme="http://blog.ddlee.cn/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="http://blog.ddlee.cn/tags/Deep-Learning/"/>
    
      <category term="AI" scheme="http://blog.ddlee.cn/tags/AI/"/>
    
      <category term="Programming" scheme="http://blog.ddlee.cn/tags/Programming/"/>
    
      <category term="Keras" scheme="http://blog.ddlee.cn/tags/Keras/"/>
    
  </entry>
  
</feed>
